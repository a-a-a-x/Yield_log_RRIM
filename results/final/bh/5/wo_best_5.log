cuda available with GPU: Tesla V100-PCIE-16GB
==========Load Seed==========
set_random_seed
0
==========Training Start==========
Training Graphs:  2491
Valid Graphs:  277
Test Graphs:  1187
============Not pretrained weights used============
============Creating new layers============
============Creating Model============
Training Graphs Batches:  78
Valid Graphs Batches:  9
Test Graphs Batches:  37
[0/299] timecost: 61.96, lr: 0.000100, Train: (LOSS: 0.2103, MAE: 0.2103, RMSE: 0.2594, R2: 0.0487), Valid: (LOSS: 0.1752, MAE: 0.1752, RMSE: 0.2354, R2: 0.2147), PNorm: 187.0349, GNorm: 0.5000
[1/299] timecost: 60.00, lr: 0.000100, Train: (LOSS: 0.1777, MAE: 0.1777, RMSE: 0.2262, R2: 0.2815), Valid: (LOSS: 0.1581, MAE: 0.1581, RMSE: 0.2136, R2: 0.3494), PNorm: 187.0582, GNorm: 0.5000
[2/299] timecost: 60.05, lr: 0.000100, Train: (LOSS: 0.1696, MAE: 0.1696, RMSE: 0.2188, R2: 0.3181), Valid: (LOSS: 0.1643, MAE: 0.1643, RMSE: 0.2188, R2: 0.3160), PNorm: 187.0829, GNorm: 0.5000
[3/299] timecost: 59.76, lr: 0.000100, Train: (LOSS: 0.1596, MAE: 0.1596, RMSE: 0.2073, R2: 0.4022), Valid: (LOSS: 0.1327, MAE: 0.1327, RMSE: 0.1838, R2: 0.5123), PNorm: 187.1146, GNorm: 0.5000
[4/299] timecost: 59.57, lr: 0.000100, Train: (LOSS: 0.1393, MAE: 0.1393, RMSE: 0.1907, R2: 0.4912), Valid: (LOSS: 0.1342, MAE: 0.1342, RMSE: 0.1834, R2: 0.4852), PNorm: 187.1361, GNorm: 0.5000
[5/299] timecost: 59.84, lr: 0.000100, Train: (LOSS: 0.1368, MAE: 0.1368, RMSE: 0.1866, R2: 0.5003), Valid: (LOSS: 0.1339, MAE: 0.1339, RMSE: 0.1788, R2: 0.5345), PNorm: 187.1660, GNorm: 0.4481
[6/299] timecost: 59.62, lr: 0.000100, Train: (LOSS: 0.1322, MAE: 0.1322, RMSE: 0.1806, R2: 0.5299), Valid: (LOSS: 0.1655, MAE: 0.1655, RMSE: 0.2265, R2: 0.2467), PNorm: 187.1958, GNorm: 0.5000
[7/299] timecost: 59.54, lr: 0.000100, Train: (LOSS: 0.1314, MAE: 0.1314, RMSE: 0.1816, R2: 0.5234), Valid: (LOSS: 0.1277, MAE: 0.1277, RMSE: 0.1781, R2: 0.5238), PNorm: 187.2170, GNorm: 0.5000
[8/299] timecost: 59.21, lr: 0.000100, Train: (LOSS: 0.1210, MAE: 0.1210, RMSE: 0.1691, R2: 0.5939), Valid: (LOSS: 0.1093, MAE: 0.1093, RMSE: 0.1500, R2: 0.6468), PNorm: 187.2304, GNorm: 0.5000
[9/299] timecost: 59.85, lr: 0.000100, Train: (LOSS: 0.1212, MAE: 0.1212, RMSE: 0.1700, R2: 0.5817), Valid: (LOSS: 0.1096, MAE: 0.1096, RMSE: 0.1539, R2: 0.6416), PNorm: 187.2560, GNorm: 0.5000
[10/299] timecost: 60.25, lr: 0.000100, Train: (LOSS: 0.1214, MAE: 0.1214, RMSE: 0.1687, R2: 0.5754), Valid: (LOSS: 0.1046, MAE: 0.1046, RMSE: 0.1473, R2: 0.6622), PNorm: 187.2818, GNorm: 0.4466
[11/299] timecost: 60.12, lr: 0.000100, Train: (LOSS: 0.1153, MAE: 0.1153, RMSE: 0.1622, R2: 0.6160), Valid: (LOSS: 0.1134, MAE: 0.1134, RMSE: 0.1587, R2: 0.5920), PNorm: 187.3128, GNorm: 0.5000
[12/299] timecost: 59.88, lr: 0.000100, Train: (LOSS: 0.1003, MAE: 0.1003, RMSE: 0.1461, R2: 0.6979), Valid: (LOSS: 0.0829, MAE: 0.0829, RMSE: 0.1196, R2: 0.7800), PNorm: 187.3528, GNorm: 0.2857
[13/299] timecost: 59.17, lr: 0.000100, Train: (LOSS: 0.0965, MAE: 0.0965, RMSE: 0.1421, R2: 0.7034), Valid: (LOSS: 0.0940, MAE: 0.0940, RMSE: 0.1381, R2: 0.7095), PNorm: 187.3843, GNorm: 0.5000
[14/299] timecost: 60.10, lr: 0.000100, Train: (LOSS: 0.0908, MAE: 0.0908, RMSE: 0.1366, R2: 0.7230), Valid: (LOSS: 0.0896, MAE: 0.0896, RMSE: 0.1301, R2: 0.7453), PNorm: 187.4040, GNorm: 0.5000
[15/299] timecost: 59.67, lr: 0.000100, Train: (LOSS: 0.0849, MAE: 0.0849, RMSE: 0.1266, R2: 0.7646), Valid: (LOSS: 0.0964, MAE: 0.0964, RMSE: 0.1353, R2: 0.7184), PNorm: 187.4257, GNorm: 0.3879
[16/299] timecost: 59.74, lr: 0.000100, Train: (LOSS: 0.0794, MAE: 0.0794, RMSE: 0.1191, R2: 0.7873), Valid: (LOSS: 0.0732, MAE: 0.0732, RMSE: 0.1130, R2: 0.8096), PNorm: 187.4438, GNorm: 0.5000
[17/299] timecost: 60.80, lr: 0.000100, Train: (LOSS: 0.0780, MAE: 0.0780, RMSE: 0.1195, R2: 0.7860), Valid: (LOSS: 0.0745, MAE: 0.0745, RMSE: 0.1141, R2: 0.8086), PNorm: 187.4613, GNorm: 0.5000
[18/299] timecost: 64.63, lr: 0.000100, Train: (LOSS: 0.0719, MAE: 0.0719, RMSE: 0.1108, R2: 0.8202), Valid: (LOSS: 0.0735, MAE: 0.0735, RMSE: 0.1108, R2: 0.8197), PNorm: 187.4806, GNorm: 0.5000
[19/299] timecost: 64.49, lr: 0.000100, Train: (LOSS: 0.0700, MAE: 0.0700, RMSE: 0.1080, R2: 0.8272), Valid: (LOSS: 0.0712, MAE: 0.0712, RMSE: 0.1105, R2: 0.8170), PNorm: 187.4989, GNorm: 0.5000
[20/299] timecost: 64.74, lr: 0.000100, Train: (LOSS: 0.0681, MAE: 0.0681, RMSE: 0.1047, R2: 0.8332), Valid: (LOSS: 0.0681, MAE: 0.0681, RMSE: 0.1040, R2: 0.8348), PNorm: 187.5164, GNorm: 0.5000
[21/299] timecost: 64.44, lr: 0.000100, Train: (LOSS: 0.0663, MAE: 0.0663, RMSE: 0.1021, R2: 0.8445), Valid: (LOSS: 0.0618, MAE: 0.0618, RMSE: 0.0964, R2: 0.8540), PNorm: 187.5385, GNorm: 0.5000
[22/299] timecost: 64.72, lr: 0.000100, Train: (LOSS: 0.0612, MAE: 0.0612, RMSE: 0.0958, R2: 0.8609), Valid: (LOSS: 0.0633, MAE: 0.0633, RMSE: 0.1012, R2: 0.8501), PNorm: 187.5565, GNorm: 0.5000
[23/299] timecost: 61.30, lr: 0.000100, Train: (LOSS: 0.0619, MAE: 0.0619, RMSE: 0.0950, R2: 0.8635), Valid: (LOSS: 0.0670, MAE: 0.0670, RMSE: 0.1039, R2: 0.8313), PNorm: 187.5763, GNorm: 0.5000
[24/299] timecost: 60.85, lr: 0.000100, Train: (LOSS: 0.0564, MAE: 0.0564, RMSE: 0.0887, R2: 0.8813), Valid: (LOSS: 0.0596, MAE: 0.0596, RMSE: 0.0951, R2: 0.8595), PNorm: 187.5958, GNorm: 0.5000
[25/299] timecost: 61.03, lr: 0.000100, Train: (LOSS: 0.0541, MAE: 0.0541, RMSE: 0.0847, R2: 0.8927), Valid: (LOSS: 0.0596, MAE: 0.0596, RMSE: 0.0962, R2: 0.8510), PNorm: 187.6144, GNorm: 0.4010
[26/299] timecost: 61.17, lr: 0.000100, Train: (LOSS: 0.0530, MAE: 0.0530, RMSE: 0.0829, R2: 0.8959), Valid: (LOSS: 0.0573, MAE: 0.0573, RMSE: 0.0867, R2: 0.8830), PNorm: 187.6337, GNorm: 0.4806
[27/299] timecost: 60.78, lr: 0.000100, Train: (LOSS: 0.0502, MAE: 0.0502, RMSE: 0.0784, R2: 0.9071), Valid: (LOSS: 0.0526, MAE: 0.0526, RMSE: 0.0838, R2: 0.8886), PNorm: 187.6487, GNorm: 0.3951
[28/299] timecost: 60.99, lr: 0.000100, Train: (LOSS: 0.0513, MAE: 0.0513, RMSE: 0.0798, R2: 0.9018), Valid: (LOSS: 0.0529, MAE: 0.0529, RMSE: 0.0844, R2: 0.8764), PNorm: 187.6676, GNorm: 0.5000
[29/299] timecost: 61.45, lr: 0.000100, Train: (LOSS: 0.0501, MAE: 0.0501, RMSE: 0.0784, R2: 0.9034), Valid: (LOSS: 0.0494, MAE: 0.0494, RMSE: 0.0792, R2: 0.8987), PNorm: 187.6863, GNorm: 0.5000
[30/299] timecost: 61.16, lr: 0.000100, Train: (LOSS: 0.0473, MAE: 0.0473, RMSE: 0.0746, R2: 0.9132), Valid: (LOSS: 0.0521, MAE: 0.0521, RMSE: 0.0834, R2: 0.8904), PNorm: 187.7023, GNorm: 0.5000
[31/299] timecost: 61.31, lr: 0.000100, Train: (LOSS: 0.0448, MAE: 0.0448, RMSE: 0.0698, R2: 0.9245), Valid: (LOSS: 0.0583, MAE: 0.0583, RMSE: 0.0944, R2: 0.8537), PNorm: 187.7154, GNorm: 0.5000
[32/299] timecost: 61.42, lr: 0.000100, Train: (LOSS: 0.0462, MAE: 0.0462, RMSE: 0.0717, R2: 0.9243), Valid: (LOSS: 0.0487, MAE: 0.0487, RMSE: 0.0768, R2: 0.9078), PNorm: 187.7319, GNorm: 0.5000
[33/299] timecost: 62.34, lr: 0.000100, Train: (LOSS: 0.0443, MAE: 0.0443, RMSE: 0.0682, R2: 0.9250), Valid: (LOSS: 0.0498, MAE: 0.0498, RMSE: 0.0755, R2: 0.9089), PNorm: 187.7485, GNorm: 0.4533
[34/299] timecost: 62.27, lr: 0.000100, Train: (LOSS: 0.0431, MAE: 0.0431, RMSE: 0.0675, R2: 0.9276), Valid: (LOSS: 0.0530, MAE: 0.0530, RMSE: 0.0849, R2: 0.8834), PNorm: 187.7647, GNorm: 0.5000
[35/299] timecost: 62.82, lr: 0.000100, Train: (LOSS: 0.0422, MAE: 0.0422, RMSE: 0.0663, R2: 0.9326), Valid: (LOSS: 0.0486, MAE: 0.0486, RMSE: 0.0784, R2: 0.8996), PNorm: 187.7773, GNorm: 0.5000
[36/299] timecost: 63.27, lr: 0.000100, Train: (LOSS: 0.0413, MAE: 0.0413, RMSE: 0.0650, R2: 0.9358), Valid: (LOSS: 0.0470, MAE: 0.0470, RMSE: 0.0756, R2: 0.9112), PNorm: 187.7904, GNorm: 0.5000
[37/299] timecost: 62.90, lr: 0.000100, Train: (LOSS: 0.0406, MAE: 0.0406, RMSE: 0.0630, R2: 0.9388), Valid: (LOSS: 0.0471, MAE: 0.0471, RMSE: 0.0761, R2: 0.9038), PNorm: 187.8055, GNorm: 0.5000
[38/299] timecost: 61.43, lr: 0.000100, Train: (LOSS: 0.0388, MAE: 0.0388, RMSE: 0.0609, R2: 0.9424), Valid: (LOSS: 0.0480, MAE: 0.0480, RMSE: 0.0816, R2: 0.8917), PNorm: 187.8199, GNorm: 0.4375
[39/299] timecost: 61.40, lr: 0.000100, Train: (LOSS: 0.0401, MAE: 0.0401, RMSE: 0.0620, R2: 0.9411), Valid: (LOSS: 0.0476, MAE: 0.0476, RMSE: 0.0803, R2: 0.8996), PNorm: 187.8340, GNorm: 0.4568
[40/299] timecost: 60.86, lr: 0.000100, Train: (LOSS: 0.0369, MAE: 0.0369, RMSE: 0.0563, R2: 0.9485), Valid: (LOSS: 0.0494, MAE: 0.0494, RMSE: 0.0826, R2: 0.8915), PNorm: 187.8480, GNorm: 0.5000
[41/299] timecost: 61.15, lr: 0.000100, Train: (LOSS: 0.0372, MAE: 0.0372, RMSE: 0.0604, R2: 0.9367), Valid: (LOSS: 0.0495, MAE: 0.0495, RMSE: 0.0811, R2: 0.8957), PNorm: 187.8641, GNorm: 0.5000
[42/299] timecost: 61.24, lr: 0.000100, Train: (LOSS: 0.0370, MAE: 0.0370, RMSE: 0.0593, R2: 0.9452), Valid: (LOSS: 0.0479, MAE: 0.0479, RMSE: 0.0795, R2: 0.8977), PNorm: 187.8793, GNorm: 0.5000
[43/299] timecost: 62.42, lr: 0.000100, Train: (LOSS: 0.0365, MAE: 0.0365, RMSE: 0.0577, R2: 0.9471), Valid: (LOSS: 0.0464, MAE: 0.0464, RMSE: 0.0796, R2: 0.8973), PNorm: 187.8915, GNorm: 0.5000
[44/299] timecost: 62.82, lr: 0.000100, Train: (LOSS: 0.0357, MAE: 0.0357, RMSE: 0.0573, R2: 0.9455), Valid: (LOSS: 0.0451, MAE: 0.0451, RMSE: 0.0791, R2: 0.8971), PNorm: 187.9053, GNorm: 0.4116
[45/299] timecost: 63.07, lr: 0.000100, Train: (LOSS: 0.0351, MAE: 0.0351, RMSE: 0.0556, R2: 0.9513), Valid: (LOSS: 0.0459, MAE: 0.0459, RMSE: 0.0789, R2: 0.8975), PNorm: 187.9188, GNorm: 0.5000
[46/299] timecost: 62.41, lr: 0.000100, Train: (LOSS: 0.0341, MAE: 0.0341, RMSE: 0.0533, R2: 0.9549), Valid: (LOSS: 0.0480, MAE: 0.0480, RMSE: 0.0826, R2: 0.8909), PNorm: 187.9303, GNorm: 0.4972
[47/299] timecost: 62.89, lr: 0.000100, Train: (LOSS: 0.0346, MAE: 0.0346, RMSE: 0.0543, R2: 0.9547), Valid: (LOSS: 0.0443, MAE: 0.0443, RMSE: 0.0754, R2: 0.9091), PNorm: 187.9437, GNorm: 0.3988
[48/299] timecost: 62.69, lr: 0.000100, Train: (LOSS: 0.0323, MAE: 0.0323, RMSE: 0.0512, R2: 0.9574), Valid: (LOSS: 0.0459, MAE: 0.0459, RMSE: 0.0766, R2: 0.9104), PNorm: 187.9564, GNorm: 0.3435
[49/299] timecost: 63.18, lr: 0.000100, Train: (LOSS: 0.0322, MAE: 0.0322, RMSE: 0.0514, R2: 0.9571), Valid: (LOSS: 0.0453, MAE: 0.0453, RMSE: 0.0778, R2: 0.9048), PNorm: 187.9679, GNorm: 0.4054
[50/299] timecost: 62.82, lr: 0.000100, Train: (LOSS: 0.0328, MAE: 0.0328, RMSE: 0.0516, R2: 0.9571), Valid: (LOSS: 0.0452, MAE: 0.0452, RMSE: 0.0791, R2: 0.8971), PNorm: 187.9797, GNorm: 0.4229
[51/299] timecost: 62.63, lr: 0.000100, Train: (LOSS: 0.0321, MAE: 0.0321, RMSE: 0.0520, R2: 0.9560), Valid: (LOSS: 0.0461, MAE: 0.0461, RMSE: 0.0788, R2: 0.9003), PNorm: 187.9918, GNorm: 0.5000
[52/299] timecost: 62.32, lr: 0.000100, Train: (LOSS: 0.0308, MAE: 0.0308, RMSE: 0.0493, R2: 0.9603), Valid: (LOSS: 0.0463, MAE: 0.0463, RMSE: 0.0813, R2: 0.8945), PNorm: 188.0021, GNorm: 0.4965
[53/299] timecost: 61.87, lr: 0.000100, Train: (LOSS: 0.0304, MAE: 0.0304, RMSE: 0.0486, R2: 0.9615), Valid: (LOSS: 0.0417, MAE: 0.0417, RMSE: 0.0711, R2: 0.9183), PNorm: 188.0138, GNorm: 0.4169
[54/299] timecost: 62.25, lr: 0.000100, Train: (LOSS: 0.0313, MAE: 0.0313, RMSE: 0.0501, R2: 0.9586), Valid: (LOSS: 0.0455, MAE: 0.0455, RMSE: 0.0773, R2: 0.9002), PNorm: 188.0284, GNorm: 0.5000
[55/299] timecost: 62.50, lr: 0.000100, Train: (LOSS: 0.0290, MAE: 0.0290, RMSE: 0.0469, R2: 0.9623), Valid: (LOSS: 0.0426, MAE: 0.0426, RMSE: 0.0774, R2: 0.9005), PNorm: 188.0398, GNorm: 0.4669
[56/299] timecost: 61.67, lr: 0.000100, Train: (LOSS: 0.0286, MAE: 0.0286, RMSE: 0.0458, R2: 0.9646), Valid: (LOSS: 0.0408, MAE: 0.0408, RMSE: 0.0747, R2: 0.9100), PNorm: 188.0508, GNorm: 0.4699
[57/299] timecost: 61.45, lr: 0.000100, Train: (LOSS: 0.0286, MAE: 0.0286, RMSE: 0.0456, R2: 0.9656), Valid: (LOSS: 0.0451, MAE: 0.0451, RMSE: 0.0801, R2: 0.8917), PNorm: 188.0602, GNorm: 0.5000
[58/299] timecost: 61.58, lr: 0.000100, Train: (LOSS: 0.0284, MAE: 0.0284, RMSE: 0.0467, R2: 0.9646), Valid: (LOSS: 0.0447, MAE: 0.0447, RMSE: 0.0760, R2: 0.9064), PNorm: 188.0727, GNorm: 0.3865
[59/299] timecost: 61.29, lr: 0.000100, Train: (LOSS: 0.0282, MAE: 0.0282, RMSE: 0.0459, R2: 0.9634), Valid: (LOSS: 0.0454, MAE: 0.0454, RMSE: 0.0840, R2: 0.8853), PNorm: 188.0826, GNorm: 0.3756
[60/299] timecost: 61.19, lr: 0.000100, Train: (LOSS: 0.0294, MAE: 0.0294, RMSE: 0.0482, R2: 0.9618), Valid: (LOSS: 0.0446, MAE: 0.0446, RMSE: 0.0726, R2: 0.9156), PNorm: 188.0962, GNorm: 0.5000
[61/299] timecost: 61.40, lr: 0.000100, Train: (LOSS: 0.0294, MAE: 0.0294, RMSE: 0.0471, R2: 0.9625), Valid: (LOSS: 0.0438, MAE: 0.0438, RMSE: 0.0721, R2: 0.9109), PNorm: 188.1113, GNorm: 0.4477
[62/299] timecost: 61.03, lr: 0.000100, Train: (LOSS: 0.0281, MAE: 0.0281, RMSE: 0.0463, R2: 0.9638), Valid: (LOSS: 0.0434, MAE: 0.0434, RMSE: 0.0760, R2: 0.9065), PNorm: 188.1260, GNorm: 0.5000
[63/299] timecost: 61.14, lr: 0.000100, Train: (LOSS: 0.0277, MAE: 0.0277, RMSE: 0.0444, R2: 0.9665), Valid: (LOSS: 0.0425, MAE: 0.0425, RMSE: 0.0755, R2: 0.9041), PNorm: 188.1378, GNorm: 0.4740
[64/299] timecost: 61.25, lr: 0.000100, Train: (LOSS: 0.0261, MAE: 0.0261, RMSE: 0.0425, R2: 0.9687), Valid: (LOSS: 0.0409, MAE: 0.0409, RMSE: 0.0724, R2: 0.9136), PNorm: 188.1486, GNorm: 0.4565
[65/299] timecost: 61.44, lr: 0.000100, Train: (LOSS: 0.0268, MAE: 0.0268, RMSE: 0.0445, R2: 0.9668), Valid: (LOSS: 0.0433, MAE: 0.0433, RMSE: 0.0765, R2: 0.9042), PNorm: 188.1592, GNorm: 0.4898
[66/299] timecost: 61.26, lr: 0.000100, Train: (LOSS: 0.0257, MAE: 0.0257, RMSE: 0.0422, R2: 0.9704), Valid: (LOSS: 0.0448, MAE: 0.0448, RMSE: 0.0792, R2: 0.8973), PNorm: 188.1704, GNorm: 0.5000
[67/299] timecost: 61.15, lr: 0.000100, Train: (LOSS: 0.0263, MAE: 0.0263, RMSE: 0.0423, R2: 0.9690), Valid: (LOSS: 0.0427, MAE: 0.0427, RMSE: 0.0744, R2: 0.9114), PNorm: 188.1814, GNorm: 0.5000
[68/299] timecost: 61.47, lr: 0.000100, Train: (LOSS: 0.0256, MAE: 0.0256, RMSE: 0.0424, R2: 0.9684), Valid: (LOSS: 0.0422, MAE: 0.0422, RMSE: 0.0748, R2: 0.9072), PNorm: 188.1930, GNorm: 0.5000
[69/299] timecost: 59.62, lr: 0.000100, Train: (LOSS: 0.0249, MAE: 0.0249, RMSE: 0.0409, R2: 0.9713), Valid: (LOSS: 0.0443, MAE: 0.0443, RMSE: 0.0768, R2: 0.9021), PNorm: 188.2027, GNorm: 0.4365
[70/299] timecost: 59.00, lr: 0.000100, Train: (LOSS: 0.0245, MAE: 0.0245, RMSE: 0.0401, R2: 0.9712), Valid: (LOSS: 0.0425, MAE: 0.0425, RMSE: 0.0727, R2: 0.9158), PNorm: 188.2144, GNorm: 0.4622
[71/299] timecost: 58.81, lr: 0.000100, Train: (LOSS: 0.0249, MAE: 0.0249, RMSE: 0.0410, R2: 0.9714), Valid: (LOSS: 0.0436, MAE: 0.0436, RMSE: 0.0740, R2: 0.9116), PNorm: 188.2302, GNorm: 0.3577
Epoch 00073: reducing learning rate of group 0 to 8.5000e-05.
[72/299] timecost: 59.55, lr: 0.000085, Train: (LOSS: 0.0249, MAE: 0.0249, RMSE: 0.0414, R2: 0.9716), Valid: (LOSS: 0.0422, MAE: 0.0422, RMSE: 0.0719, R2: 0.9173), PNorm: 188.2409, GNorm: 0.3907
[73/299] timecost: 60.85, lr: 0.000085, Train: (LOSS: 0.0233, MAE: 0.0233, RMSE: 0.0390, R2: 0.9736), Valid: (LOSS: 0.0428, MAE: 0.0428, RMSE: 0.0721, R2: 0.9160), PNorm: 188.2505, GNorm: 0.4558
[74/299] timecost: 61.00, lr: 0.000085, Train: (LOSS: 0.0230, MAE: 0.0230, RMSE: 0.0387, R2: 0.9740), Valid: (LOSS: 0.0399, MAE: 0.0399, RMSE: 0.0693, R2: 0.9228), PNorm: 188.2573, GNorm: 0.3644
[75/299] timecost: 60.99, lr: 0.000085, Train: (LOSS: 0.0225, MAE: 0.0225, RMSE: 0.0378, R2: 0.9724), Valid: (LOSS: 0.0410, MAE: 0.0410, RMSE: 0.0746, R2: 0.9073), PNorm: 188.2655, GNorm: 0.4148
[76/299] timecost: 61.14, lr: 0.000085, Train: (LOSS: 0.0228, MAE: 0.0228, RMSE: 0.0385, R2: 0.9731), Valid: (LOSS: 0.0444, MAE: 0.0444, RMSE: 0.0769, R2: 0.9028), PNorm: 188.2758, GNorm: 0.4287
[77/299] timecost: 61.30, lr: 0.000085, Train: (LOSS: 0.0233, MAE: 0.0233, RMSE: 0.0381, R2: 0.9747), Valid: (LOSS: 0.0411, MAE: 0.0411, RMSE: 0.0689, R2: 0.9242), PNorm: 188.2853, GNorm: 0.5000
[78/299] timecost: 62.43, lr: 0.000085, Train: (LOSS: 0.0224, MAE: 0.0224, RMSE: 0.0375, R2: 0.9744), Valid: (LOSS: 0.0418, MAE: 0.0418, RMSE: 0.0746, R2: 0.9108), PNorm: 188.2936, GNorm: 0.5000
[79/299] timecost: 61.84, lr: 0.000085, Train: (LOSS: 0.0224, MAE: 0.0224, RMSE: 0.0379, R2: 0.9712), Valid: (LOSS: 0.0408, MAE: 0.0408, RMSE: 0.0694, R2: 0.9225), PNorm: 188.3040, GNorm: 0.5000
[80/299] timecost: 61.55, lr: 0.000085, Train: (LOSS: 0.0220, MAE: 0.0220, RMSE: 0.0374, R2: 0.9741), Valid: (LOSS: 0.0406, MAE: 0.0406, RMSE: 0.0732, R2: 0.9102), PNorm: 188.3122, GNorm: 0.4012
[81/299] timecost: 61.37, lr: 0.000085, Train: (LOSS: 0.0214, MAE: 0.0214, RMSE: 0.0358, R2: 0.9769), Valid: (LOSS: 0.0434, MAE: 0.0434, RMSE: 0.0725, R2: 0.9144), PNorm: 188.3209, GNorm: 0.5000
[82/299] timecost: 62.78, lr: 0.000085, Train: (LOSS: 0.0214, MAE: 0.0214, RMSE: 0.0365, R2: 0.9770), Valid: (LOSS: 0.0411, MAE: 0.0411, RMSE: 0.0720, R2: 0.9127), PNorm: 188.3313, GNorm: 0.5000
[83/299] timecost: 61.97, lr: 0.000085, Train: (LOSS: 0.0208, MAE: 0.0208, RMSE: 0.0354, R2: 0.9764), Valid: (LOSS: 0.0431, MAE: 0.0431, RMSE: 0.0763, R2: 0.9045), PNorm: 188.3402, GNorm: 0.3539
[84/299] timecost: 62.34, lr: 0.000085, Train: (LOSS: 0.0216, MAE: 0.0216, RMSE: 0.0368, R2: 0.9764), Valid: (LOSS: 0.0430, MAE: 0.0430, RMSE: 0.0719, R2: 0.9160), PNorm: 188.3492, GNorm: 0.4893
[85/299] timecost: 61.31, lr: 0.000085, Train: (LOSS: 0.0211, MAE: 0.0211, RMSE: 0.0362, R2: 0.9772), Valid: (LOSS: 0.0400, MAE: 0.0400, RMSE: 0.0720, R2: 0.9131), PNorm: 188.3592, GNorm: 0.3965
[86/299] timecost: 59.31, lr: 0.000085, Train: (LOSS: 0.0208, MAE: 0.0208, RMSE: 0.0358, R2: 0.9778), Valid: (LOSS: 0.0418, MAE: 0.0418, RMSE: 0.0678, R2: 0.9250), PNorm: 188.3661, GNorm: 0.3854
[87/299] timecost: 59.58, lr: 0.000085, Train: (LOSS: 0.0205, MAE: 0.0205, RMSE: 0.0348, R2: 0.9784), Valid: (LOSS: 0.0433, MAE: 0.0433, RMSE: 0.0774, R2: 0.9007), PNorm: 188.3762, GNorm: 0.4613
[88/299] timecost: 60.16, lr: 0.000085, Train: (LOSS: 0.0209, MAE: 0.0209, RMSE: 0.0353, R2: 0.9776), Valid: (LOSS: 0.0433, MAE: 0.0433, RMSE: 0.0758, R2: 0.9078), PNorm: 188.3885, GNorm: 0.4774
[89/299] timecost: 59.05, lr: 0.000085, Train: (LOSS: 0.0203, MAE: 0.0203, RMSE: 0.0348, R2: 0.9782), Valid: (LOSS: 0.0445, MAE: 0.0445, RMSE: 0.0783, R2: 0.8982), PNorm: 188.3976, GNorm: 0.5000
Epoch 00091: reducing learning rate of group 0 to 7.2250e-05.
[90/299] timecost: 59.25, lr: 0.000072, Train: (LOSS: 0.0203, MAE: 0.0203, RMSE: 0.0352, R2: 0.9772), Valid: (LOSS: 0.0411, MAE: 0.0411, RMSE: 0.0713, R2: 0.9151), PNorm: 188.4060, GNorm: 0.4305
[91/299] timecost: 59.22, lr: 0.000072, Train: (LOSS: 0.0196, MAE: 0.0196, RMSE: 0.0339, R2: 0.9797), Valid: (LOSS: 0.0402, MAE: 0.0402, RMSE: 0.0706, R2: 0.9169), PNorm: 188.4138, GNorm: 0.4425
[92/299] timecost: 59.10, lr: 0.000072, Train: (LOSS: 0.0198, MAE: 0.0198, RMSE: 0.0338, R2: 0.9791), Valid: (LOSS: 0.0415, MAE: 0.0415, RMSE: 0.0723, R2: 0.9137), PNorm: 188.4218, GNorm: 0.4002
[93/299] timecost: 59.20, lr: 0.000072, Train: (LOSS: 0.0182, MAE: 0.0182, RMSE: 0.0324, R2: 0.9810), Valid: (LOSS: 0.0403, MAE: 0.0403, RMSE: 0.0711, R2: 0.9164), PNorm: 188.4261, GNorm: 0.4030
[94/299] timecost: 58.95, lr: 0.000072, Train: (LOSS: 0.0179, MAE: 0.0179, RMSE: 0.0315, R2: 0.9805), Valid: (LOSS: 0.0411, MAE: 0.0411, RMSE: 0.0710, R2: 0.9173), PNorm: 188.4325, GNorm: 0.4180
[95/299] timecost: 59.12, lr: 0.000072, Train: (LOSS: 0.0184, MAE: 0.0184, RMSE: 0.0322, R2: 0.9807), Valid: (LOSS: 0.0416, MAE: 0.0416, RMSE: 0.0727, R2: 0.9134), PNorm: 188.4395, GNorm: 0.4206
[96/299] timecost: 58.97, lr: 0.000072, Train: (LOSS: 0.0181, MAE: 0.0181, RMSE: 0.0322, R2: 0.9812), Valid: (LOSS: 0.0399, MAE: 0.0399, RMSE: 0.0673, R2: 0.9237), PNorm: 188.4451, GNorm: 0.4242
[97/299] timecost: 58.69, lr: 0.000072, Train: (LOSS: 0.0177, MAE: 0.0177, RMSE: 0.0322, R2: 0.9808), Valid: (LOSS: 0.0406, MAE: 0.0406, RMSE: 0.0695, R2: 0.9212), PNorm: 188.4519, GNorm: 0.5000
[98/299] timecost: 59.45, lr: 0.000072, Train: (LOSS: 0.0169, MAE: 0.0169, RMSE: 0.0303, R2: 0.9821), Valid: (LOSS: 0.0409, MAE: 0.0409, RMSE: 0.0687, R2: 0.9234), PNorm: 188.4591, GNorm: 0.5000
[99/299] timecost: 59.45, lr: 0.000072, Train: (LOSS: 0.0183, MAE: 0.0183, RMSE: 0.0315, R2: 0.9824), Valid: (LOSS: 0.0401, MAE: 0.0401, RMSE: 0.0674, R2: 0.9246), PNorm: 188.4668, GNorm: 0.5000
[100/299] timecost: 59.27, lr: 0.000072, Train: (LOSS: 0.0178, MAE: 0.0178, RMSE: 0.0313, R2: 0.9817), Valid: (LOSS: 0.0393, MAE: 0.0393, RMSE: 0.0678, R2: 0.9240), PNorm: 188.4732, GNorm: 0.4385
[101/299] timecost: 58.87, lr: 0.000072, Train: (LOSS: 0.0176, MAE: 0.0176, RMSE: 0.0312, R2: 0.9829), Valid: (LOSS: 0.0389, MAE: 0.0389, RMSE: 0.0669, R2: 0.9293), PNorm: 188.4796, GNorm: 0.4365
[102/299] timecost: 59.39, lr: 0.000072, Train: (LOSS: 0.0174, MAE: 0.0174, RMSE: 0.0303, R2: 0.9815), Valid: (LOSS: 0.0413, MAE: 0.0413, RMSE: 0.0696, R2: 0.9210), PNorm: 188.4847, GNorm: 0.4347
[103/299] timecost: 58.70, lr: 0.000072, Train: (LOSS: 0.0172, MAE: 0.0172, RMSE: 0.0305, R2: 0.9827), Valid: (LOSS: 0.0427, MAE: 0.0427, RMSE: 0.0740, R2: 0.9097), PNorm: 188.4940, GNorm: 0.5000
[104/299] timecost: 58.84, lr: 0.000072, Train: (LOSS: 0.0166, MAE: 0.0166, RMSE: 0.0299, R2: 0.9833), Valid: (LOSS: 0.0413, MAE: 0.0413, RMSE: 0.0709, R2: 0.9174), PNorm: 188.5019, GNorm: 0.5000
[105/299] timecost: 59.22, lr: 0.000072, Train: (LOSS: 0.0173, MAE: 0.0173, RMSE: 0.0304, R2: 0.9830), Valid: (LOSS: 0.0425, MAE: 0.0425, RMSE: 0.0730, R2: 0.9128), PNorm: 188.5090, GNorm: 0.5000
[106/299] timecost: 59.46, lr: 0.000072, Train: (LOSS: 0.0170, MAE: 0.0170, RMSE: 0.0300, R2: 0.9819), Valid: (LOSS: 0.0421, MAE: 0.0421, RMSE: 0.0750, R2: 0.9088), PNorm: 188.5157, GNorm: 0.4527
[107/299] timecost: 59.17, lr: 0.000072, Train: (LOSS: 0.0172, MAE: 0.0172, RMSE: 0.0297, R2: 0.9845), Valid: (LOSS: 0.0425, MAE: 0.0425, RMSE: 0.0755, R2: 0.9038), PNorm: 188.5239, GNorm: 0.3234
[108/299] timecost: 59.37, lr: 0.000072, Train: (LOSS: 0.0167, MAE: 0.0167, RMSE: 0.0295, R2: 0.9833), Valid: (LOSS: 0.0413, MAE: 0.0413, RMSE: 0.0711, R2: 0.9160), PNorm: 188.5288, GNorm: 0.3786
[109/299] timecost: 59.41, lr: 0.000072, Train: (LOSS: 0.0163, MAE: 0.0163, RMSE: 0.0292, R2: 0.9828), Valid: (LOSS: 0.0396, MAE: 0.0396, RMSE: 0.0676, R2: 0.9239), PNorm: 188.5358, GNorm: 0.5000
[110/299] timecost: 60.21, lr: 0.000072, Train: (LOSS: 0.0167, MAE: 0.0167, RMSE: 0.0291, R2: 0.9833), Valid: (LOSS: 0.0425, MAE: 0.0425, RMSE: 0.0718, R2: 0.9157), PNorm: 188.5454, GNorm: 0.5000
[111/299] timecost: 58.68, lr: 0.000072, Train: (LOSS: 0.0168, MAE: 0.0168, RMSE: 0.0297, R2: 0.9835), Valid: (LOSS: 0.0419, MAE: 0.0419, RMSE: 0.0694, R2: 0.9202), PNorm: 188.5513, GNorm: 0.3918
[112/299] timecost: 60.08, lr: 0.000072, Train: (LOSS: 0.0166, MAE: 0.0166, RMSE: 0.0296, R2: 0.9843), Valid: (LOSS: 0.0421, MAE: 0.0421, RMSE: 0.0700, R2: 0.9194), PNorm: 188.5599, GNorm: 0.4601
[113/299] timecost: 60.57, lr: 0.000072, Train: (LOSS: 0.0162, MAE: 0.0162, RMSE: 0.0285, R2: 0.9854), Valid: (LOSS: 0.0420, MAE: 0.0420, RMSE: 0.0705, R2: 0.9167), PNorm: 188.5678, GNorm: 0.5000
[114/299] timecost: 60.74, lr: 0.000072, Train: (LOSS: 0.0157, MAE: 0.0157, RMSE: 0.0278, R2: 0.9849), Valid: (LOSS: 0.0389, MAE: 0.0389, RMSE: 0.0656, R2: 0.9293), PNorm: 188.5757, GNorm: 0.3810
[115/299] timecost: 60.70, lr: 0.000072, Train: (LOSS: 0.0159, MAE: 0.0159, RMSE: 0.0278, R2: 0.9852), Valid: (LOSS: 0.0403, MAE: 0.0403, RMSE: 0.0663, R2: 0.9291), PNorm: 188.5833, GNorm: 0.4218
[116/299] timecost: 60.83, lr: 0.000072, Train: (LOSS: 0.0161, MAE: 0.0161, RMSE: 0.0274, R2: 0.9864), Valid: (LOSS: 0.0421, MAE: 0.0421, RMSE: 0.0700, R2: 0.9223), PNorm: 188.5914, GNorm: 0.3657
Epoch 00118: reducing learning rate of group 0 to 6.1413e-05.
[117/299] timecost: 59.66, lr: 0.000061, Train: (LOSS: 0.0166, MAE: 0.0166, RMSE: 0.0292, R2: 0.9851), Valid: (LOSS: 0.0391, MAE: 0.0391, RMSE: 0.0671, R2: 0.9235), PNorm: 188.5996, GNorm: 0.5000
[118/299] timecost: 58.40, lr: 0.000061, Train: (LOSS: 0.0149, MAE: 0.0149, RMSE: 0.0266, R2: 0.9869), Valid: (LOSS: 0.0402, MAE: 0.0402, RMSE: 0.0684, R2: 0.9209), PNorm: 188.6063, GNorm: 0.4228
[119/299] timecost: 58.63, lr: 0.000061, Train: (LOSS: 0.0148, MAE: 0.0148, RMSE: 0.0270, R2: 0.9855), Valid: (LOSS: 0.0399, MAE: 0.0399, RMSE: 0.0656, R2: 0.9307), PNorm: 188.6127, GNorm: 0.5000
[120/299] timecost: 58.75, lr: 0.000061, Train: (LOSS: 0.0145, MAE: 0.0145, RMSE: 0.0257, R2: 0.9877), Valid: (LOSS: 0.0372, MAE: 0.0372, RMSE: 0.0611, R2: 0.9403), PNorm: 188.6189, GNorm: 0.4074
[121/299] timecost: 58.62, lr: 0.000061, Train: (LOSS: 0.0145, MAE: 0.0145, RMSE: 0.0262, R2: 0.9876), Valid: (LOSS: 0.0404, MAE: 0.0404, RMSE: 0.0685, R2: 0.9237), PNorm: 188.6233, GNorm: 0.4576
[122/299] timecost: 58.63, lr: 0.000061, Train: (LOSS: 0.0141, MAE: 0.0141, RMSE: 0.0249, R2: 0.9878), Valid: (LOSS: 0.0399, MAE: 0.0399, RMSE: 0.0676, R2: 0.9220), PNorm: 188.6296, GNorm: 0.5000
[123/299] timecost: 59.11, lr: 0.000061, Train: (LOSS: 0.0146, MAE: 0.0146, RMSE: 0.0247, R2: 0.9877), Valid: (LOSS: 0.0404, MAE: 0.0404, RMSE: 0.0676, R2: 0.9239), PNorm: 188.6362, GNorm: 0.5000
[124/299] timecost: 59.89, lr: 0.000061, Train: (LOSS: 0.0135, MAE: 0.0135, RMSE: 0.0238, R2: 0.9892), Valid: (LOSS: 0.0400, MAE: 0.0400, RMSE: 0.0675, R2: 0.9238), PNorm: 188.6421, GNorm: 0.4543
[125/299] timecost: 60.57, lr: 0.000061, Train: (LOSS: 0.0143, MAE: 0.0143, RMSE: 0.0246, R2: 0.9887), Valid: (LOSS: 0.0407, MAE: 0.0407, RMSE: 0.0662, R2: 0.9273), PNorm: 188.6476, GNorm: 0.5000
[126/299] timecost: 60.92, lr: 0.000061, Train: (LOSS: 0.0135, MAE: 0.0135, RMSE: 0.0240, R2: 0.9890), Valid: (LOSS: 0.0405, MAE: 0.0405, RMSE: 0.0668, R2: 0.9253), PNorm: 188.6537, GNorm: 0.5000
[127/299] timecost: 60.62, lr: 0.000061, Train: (LOSS: 0.0136, MAE: 0.0136, RMSE: 0.0234, R2: 0.9895), Valid: (LOSS: 0.0384, MAE: 0.0384, RMSE: 0.0635, R2: 0.9324), PNorm: 188.6584, GNorm: 0.5000
[128/299] timecost: 61.10, lr: 0.000061, Train: (LOSS: 0.0137, MAE: 0.0137, RMSE: 0.0236, R2: 0.9896), Valid: (LOSS: 0.0372, MAE: 0.0372, RMSE: 0.0601, R2: 0.9436), PNorm: 188.6678, GNorm: 0.4422
[129/299] timecost: 61.80, lr: 0.000061, Train: (LOSS: 0.0136, MAE: 0.0136, RMSE: 0.0234, R2: 0.9895), Valid: (LOSS: 0.0412, MAE: 0.0412, RMSE: 0.0682, R2: 0.9230), PNorm: 188.6728, GNorm: 0.4875
[130/299] timecost: 63.16, lr: 0.000061, Train: (LOSS: 0.0136, MAE: 0.0136, RMSE: 0.0237, R2: 0.9894), Valid: (LOSS: 0.0365, MAE: 0.0365, RMSE: 0.0588, R2: 0.9449), PNorm: 188.6797, GNorm: 0.3887
[131/299] timecost: 59.33, lr: 0.000061, Train: (LOSS: 0.0138, MAE: 0.0138, RMSE: 0.0233, R2: 0.9900), Valid: (LOSS: 0.0393, MAE: 0.0393, RMSE: 0.0649, R2: 0.9287), PNorm: 188.6887, GNorm: 0.4011
[132/299] timecost: 59.89, lr: 0.000061, Train: (LOSS: 0.0130, MAE: 0.0130, RMSE: 0.0225, R2: 0.9903), Valid: (LOSS: 0.0390, MAE: 0.0390, RMSE: 0.0654, R2: 0.9288), PNorm: 188.6946, GNorm: 0.5000
[133/299] timecost: 59.71, lr: 0.000061, Train: (LOSS: 0.0134, MAE: 0.0134, RMSE: 0.0230, R2: 0.9895), Valid: (LOSS: 0.0388, MAE: 0.0388, RMSE: 0.0660, R2: 0.9251), PNorm: 188.6998, GNorm: 0.5000
[134/299] timecost: 59.57, lr: 0.000061, Train: (LOSS: 0.0132, MAE: 0.0132, RMSE: 0.0226, R2: 0.9902), Valid: (LOSS: 0.0375, MAE: 0.0375, RMSE: 0.0616, R2: 0.9364), PNorm: 188.7065, GNorm: 0.3354
[135/299] timecost: 59.23, lr: 0.000061, Train: (LOSS: 0.0131, MAE: 0.0131, RMSE: 0.0223, R2: 0.9905), Valid: (LOSS: 0.0391, MAE: 0.0391, RMSE: 0.0652, R2: 0.9291), PNorm: 188.7131, GNorm: 0.4410
[136/299] timecost: 59.67, lr: 0.000061, Train: (LOSS: 0.0132, MAE: 0.0132, RMSE: 0.0222, R2: 0.9914), Valid: (LOSS: 0.0389, MAE: 0.0389, RMSE: 0.0641, R2: 0.9315), PNorm: 188.7178, GNorm: 0.4522
[137/299] timecost: 60.49, lr: 0.000061, Train: (LOSS: 0.0128, MAE: 0.0128, RMSE: 0.0219, R2: 0.9914), Valid: (LOSS: 0.0372, MAE: 0.0372, RMSE: 0.0599, R2: 0.9423), PNorm: 188.7255, GNorm: 0.4049
[138/299] timecost: 60.30, lr: 0.000061, Train: (LOSS: 0.0124, MAE: 0.0124, RMSE: 0.0213, R2: 0.9913), Valid: (LOSS: 0.0411, MAE: 0.0411, RMSE: 0.0663, R2: 0.9271), PNorm: 188.7327, GNorm: 0.4642
[139/299] timecost: 60.50, lr: 0.000061, Train: (LOSS: 0.0126, MAE: 0.0126, RMSE: 0.0211, R2: 0.9917), Valid: (LOSS: 0.0396, MAE: 0.0396, RMSE: 0.0635, R2: 0.9318), PNorm: 188.7379, GNorm: 0.4277
[140/299] timecost: 61.15, lr: 0.000061, Train: (LOSS: 0.0129, MAE: 0.0129, RMSE: 0.0217, R2: 0.9909), Valid: (LOSS: 0.0395, MAE: 0.0395, RMSE: 0.0637, R2: 0.9342), PNorm: 188.7455, GNorm: 0.3852
[141/299] timecost: 62.04, lr: 0.000061, Train: (LOSS: 0.0130, MAE: 0.0130, RMSE: 0.0217, R2: 0.9917), Valid: (LOSS: 0.0372, MAE: 0.0372, RMSE: 0.0619, R2: 0.9366), PNorm: 188.7515, GNorm: 0.5000
[142/299] timecost: 62.04, lr: 0.000061, Train: (LOSS: 0.0125, MAE: 0.0125, RMSE: 0.0209, R2: 0.9919), Valid: (LOSS: 0.0375, MAE: 0.0375, RMSE: 0.0619, R2: 0.9347), PNorm: 188.7594, GNorm: 0.4470
[143/299] timecost: 62.32, lr: 0.000061, Train: (LOSS: 0.0123, MAE: 0.0123, RMSE: 0.0208, R2: 0.9922), Valid: (LOSS: 0.0355, MAE: 0.0355, RMSE: 0.0565, R2: 0.9491), PNorm: 188.7641, GNorm: 0.4184
[144/299] timecost: 62.28, lr: 0.000061, Train: (LOSS: 0.0118, MAE: 0.0118, RMSE: 0.0199, R2: 0.9929), Valid: (LOSS: 0.0364, MAE: 0.0364, RMSE: 0.0573, R2: 0.9475), PNorm: 188.7719, GNorm: 0.4174
[145/299] timecost: 62.07, lr: 0.000061, Train: (LOSS: 0.0118, MAE: 0.0118, RMSE: 0.0192, R2: 0.9934), Valid: (LOSS: 0.0360, MAE: 0.0360, RMSE: 0.0569, R2: 0.9484), PNorm: 188.7802, GNorm: 0.5000
[146/299] timecost: 62.42, lr: 0.000061, Train: (LOSS: 0.0127, MAE: 0.0127, RMSE: 0.0209, R2: 0.9917), Valid: (LOSS: 0.0378, MAE: 0.0378, RMSE: 0.0593, R2: 0.9436), PNorm: 188.7858, GNorm: 0.4226
[147/299] timecost: 62.09, lr: 0.000061, Train: (LOSS: 0.0119, MAE: 0.0119, RMSE: 0.0191, R2: 0.9924), Valid: (LOSS: 0.0373, MAE: 0.0373, RMSE: 0.0593, R2: 0.9397), PNorm: 188.7937, GNorm: 0.4075
[148/299] timecost: 62.32, lr: 0.000061, Train: (LOSS: 0.0115, MAE: 0.0115, RMSE: 0.0193, R2: 0.9922), Valid: (LOSS: 0.0362, MAE: 0.0362, RMSE: 0.0561, R2: 0.9501), PNorm: 188.7987, GNorm: 0.4242
[149/299] timecost: 62.66, lr: 0.000061, Train: (LOSS: 0.0115, MAE: 0.0115, RMSE: 0.0188, R2: 0.9927), Valid: (LOSS: 0.0369, MAE: 0.0369, RMSE: 0.0574, R2: 0.9471), PNorm: 188.8037, GNorm: 0.4674
[150/299] timecost: 62.14, lr: 0.000061, Train: (LOSS: 0.0115, MAE: 0.0115, RMSE: 0.0188, R2: 0.9929), Valid: (LOSS: 0.0360, MAE: 0.0360, RMSE: 0.0565, R2: 0.9486), PNorm: 188.8095, GNorm: 0.4737
[151/299] timecost: 62.38, lr: 0.000061, Train: (LOSS: 0.0112, MAE: 0.0112, RMSE: 0.0185, R2: 0.9936), Valid: (LOSS: 0.0355, MAE: 0.0355, RMSE: 0.0565, R2: 0.9496), PNorm: 188.8160, GNorm: 0.3526
[152/299] timecost: 62.06, lr: 0.000061, Train: (LOSS: 0.0109, MAE: 0.0109, RMSE: 0.0178, R2: 0.9934), Valid: (LOSS: 0.0360, MAE: 0.0360, RMSE: 0.0571, R2: 0.9487), PNorm: 188.8199, GNorm: 0.4446
[153/299] timecost: 61.88, lr: 0.000061, Train: (LOSS: 0.0114, MAE: 0.0114, RMSE: 0.0185, R2: 0.9934), Valid: (LOSS: 0.0363, MAE: 0.0363, RMSE: 0.0584, R2: 0.9457), PNorm: 188.8259, GNorm: 0.4243
[154/299] timecost: 62.30, lr: 0.000061, Train: (LOSS: 0.0107, MAE: 0.0107, RMSE: 0.0178, R2: 0.9931), Valid: (LOSS: 0.0362, MAE: 0.0362, RMSE: 0.0575, R2: 0.9471), PNorm: 188.8329, GNorm: 0.5000
[155/299] timecost: 62.19, lr: 0.000061, Train: (LOSS: 0.0108, MAE: 0.0108, RMSE: 0.0173, R2: 0.9943), Valid: (LOSS: 0.0350, MAE: 0.0350, RMSE: 0.0539, R2: 0.9546), PNorm: 188.8379, GNorm: 0.5000
[156/299] timecost: 61.82, lr: 0.000061, Train: (LOSS: 0.0116, MAE: 0.0116, RMSE: 0.0183, R2: 0.9934), Valid: (LOSS: 0.0364, MAE: 0.0364, RMSE: 0.0562, R2: 0.9499), PNorm: 188.8432, GNorm: 0.4064
[157/299] timecost: 61.59, lr: 0.000061, Train: (LOSS: 0.0107, MAE: 0.0107, RMSE: 0.0177, R2: 0.9933), Valid: (LOSS: 0.0350, MAE: 0.0350, RMSE: 0.0546, R2: 0.9524), PNorm: 188.8485, GNorm: 0.3995
[158/299] timecost: 62.34, lr: 0.000061, Train: (LOSS: 0.0107, MAE: 0.0107, RMSE: 0.0171, R2: 0.9933), Valid: (LOSS: 0.0351, MAE: 0.0351, RMSE: 0.0545, R2: 0.9530), PNorm: 188.8529, GNorm: 0.3362
[159/299] timecost: 62.03, lr: 0.000061, Train: (LOSS: 0.0102, MAE: 0.0102, RMSE: 0.0166, R2: 0.9942), Valid: (LOSS: 0.0361, MAE: 0.0361, RMSE: 0.0558, R2: 0.9492), PNorm: 188.8594, GNorm: 0.3631
[160/299] timecost: 60.86, lr: 0.000061, Train: (LOSS: 0.0101, MAE: 0.0101, RMSE: 0.0166, R2: 0.9939), Valid: (LOSS: 0.0352, MAE: 0.0352, RMSE: 0.0551, R2: 0.9523), PNorm: 188.8652, GNorm: 0.5000
[161/299] timecost: 58.83, lr: 0.000061, Train: (LOSS: 0.0108, MAE: 0.0108, RMSE: 0.0173, R2: 0.9941), Valid: (LOSS: 0.0349, MAE: 0.0349, RMSE: 0.0551, R2: 0.9518), PNorm: 188.8708, GNorm: 0.5000
[162/299] timecost: 59.05, lr: 0.000061, Train: (LOSS: 0.0104, MAE: 0.0104, RMSE: 0.0169, R2: 0.9945), Valid: (LOSS: 0.0368, MAE: 0.0368, RMSE: 0.0567, R2: 0.9496), PNorm: 188.8750, GNorm: 0.3465
[163/299] timecost: 58.97, lr: 0.000061, Train: (LOSS: 0.0109, MAE: 0.0109, RMSE: 0.0173, R2: 0.9936), Valid: (LOSS: 0.0340, MAE: 0.0340, RMSE: 0.0519, R2: 0.9574), PNorm: 188.8820, GNorm: 0.4058
[164/299] timecost: 58.76, lr: 0.000061, Train: (LOSS: 0.0108, MAE: 0.0108, RMSE: 0.0172, R2: 0.9936), Valid: (LOSS: 0.0344, MAE: 0.0344, RMSE: 0.0545, R2: 0.9532), PNorm: 188.8889, GNorm: 0.4495
[165/299] timecost: 59.22, lr: 0.000061, Train: (LOSS: 0.0105, MAE: 0.0105, RMSE: 0.0170, R2: 0.9941), Valid: (LOSS: 0.0359, MAE: 0.0359, RMSE: 0.0559, R2: 0.9515), PNorm: 188.8937, GNorm: 0.3690
[166/299] timecost: 58.88, lr: 0.000061, Train: (LOSS: 0.0105, MAE: 0.0105, RMSE: 0.0167, R2: 0.9944), Valid: (LOSS: 0.0369, MAE: 0.0369, RMSE: 0.0572, R2: 0.9490), PNorm: 188.9012, GNorm: 0.4313
[167/299] timecost: 60.62, lr: 0.000061, Train: (LOSS: 0.0108, MAE: 0.0108, RMSE: 0.0171, R2: 0.9946), Valid: (LOSS: 0.0353, MAE: 0.0353, RMSE: 0.0542, R2: 0.9539), PNorm: 188.9061, GNorm: 0.4283
[168/299] timecost: 64.02, lr: 0.000061, Train: (LOSS: 0.0105, MAE: 0.0105, RMSE: 0.0169, R2: 0.9944), Valid: (LOSS: 0.0375, MAE: 0.0375, RMSE: 0.0592, R2: 0.9435), PNorm: 188.9122, GNorm: 0.5000
[169/299] timecost: 63.70, lr: 0.000061, Train: (LOSS: 0.0104, MAE: 0.0104, RMSE: 0.0172, R2: 0.9939), Valid: (LOSS: 0.0364, MAE: 0.0364, RMSE: 0.0562, R2: 0.9511), PNorm: 188.9189, GNorm: 0.3602
[170/299] timecost: 64.22, lr: 0.000061, Train: (LOSS: 0.0103, MAE: 0.0103, RMSE: 0.0165, R2: 0.9945), Valid: (LOSS: 0.0353, MAE: 0.0353, RMSE: 0.0554, R2: 0.9523), PNorm: 188.9251, GNorm: 0.3814
[171/299] timecost: 64.56, lr: 0.000061, Train: (LOSS: 0.0100, MAE: 0.0100, RMSE: 0.0161, R2: 0.9946), Valid: (LOSS: 0.0349, MAE: 0.0349, RMSE: 0.0541, R2: 0.9549), PNorm: 188.9310, GNorm: 0.4502
[172/299] timecost: 63.82, lr: 0.000061, Train: (LOSS: 0.0100, MAE: 0.0100, RMSE: 0.0163, R2: 0.9943), Valid: (LOSS: 0.0345, MAE: 0.0345, RMSE: 0.0526, R2: 0.9572), PNorm: 188.9357, GNorm: 0.4609
[173/299] timecost: 63.50, lr: 0.000061, Train: (LOSS: 0.0100, MAE: 0.0100, RMSE: 0.0159, R2: 0.9950), Valid: (LOSS: 0.0359, MAE: 0.0359, RMSE: 0.0562, R2: 0.9503), PNorm: 188.9407, GNorm: 0.4033
[174/299] timecost: 63.57, lr: 0.000061, Train: (LOSS: 0.0103, MAE: 0.0103, RMSE: 0.0168, R2: 0.9942), Valid: (LOSS: 0.0354, MAE: 0.0354, RMSE: 0.0564, R2: 0.9502), PNorm: 188.9451, GNorm: 0.5000
[175/299] timecost: 63.84, lr: 0.000061, Train: (LOSS: 0.0101, MAE: 0.0101, RMSE: 0.0161, R2: 0.9946), Valid: (LOSS: 0.0349, MAE: 0.0349, RMSE: 0.0539, R2: 0.9539), PNorm: 188.9525, GNorm: 0.4277
[176/299] timecost: 62.45, lr: 0.000061, Train: (LOSS: 0.0098, MAE: 0.0098, RMSE: 0.0159, R2: 0.9949), Valid: (LOSS: 0.0362, MAE: 0.0362, RMSE: 0.0555, R2: 0.9513), PNorm: 188.9595, GNorm: 0.4563
[177/299] timecost: 60.14, lr: 0.000061, Train: (LOSS: 0.0096, MAE: 0.0096, RMSE: 0.0155, R2: 0.9952), Valid: (LOSS: 0.0359, MAE: 0.0359, RMSE: 0.0565, R2: 0.9499), PNorm: 188.9637, GNorm: 0.4696
[178/299] timecost: 59.71, lr: 0.000061, Train: (LOSS: 0.0097, MAE: 0.0097, RMSE: 0.0157, R2: 0.9943), Valid: (LOSS: 0.0361, MAE: 0.0361, RMSE: 0.0562, R2: 0.9502), PNorm: 188.9693, GNorm: 0.4504
Epoch 00180: reducing learning rate of group 0 to 5.2201e-05.
[179/299] timecost: 60.19, lr: 0.000052, Train: (LOSS: 0.0097, MAE: 0.0097, RMSE: 0.0157, R2: 0.9948), Valid: (LOSS: 0.0350, MAE: 0.0350, RMSE: 0.0554, R2: 0.9513), PNorm: 188.9737, GNorm: 0.4654
[180/299] timecost: 60.75, lr: 0.000052, Train: (LOSS: 0.0091, MAE: 0.0091, RMSE: 0.0148, R2: 0.9955), Valid: (LOSS: 0.0362, MAE: 0.0362, RMSE: 0.0568, R2: 0.9494), PNorm: 188.9777, GNorm: 0.4034
[181/299] timecost: 62.11, lr: 0.000052, Train: (LOSS: 0.0091, MAE: 0.0091, RMSE: 0.0149, R2: 0.9953), Valid: (LOSS: 0.0351, MAE: 0.0351, RMSE: 0.0554, R2: 0.9515), PNorm: 188.9816, GNorm: 0.3863
[182/299] timecost: 62.34, lr: 0.000052, Train: (LOSS: 0.0087, MAE: 0.0087, RMSE: 0.0143, R2: 0.9954), Valid: (LOSS: 0.0351, MAE: 0.0351, RMSE: 0.0548, R2: 0.9536), PNorm: 188.9865, GNorm: 0.4743
[183/299] timecost: 62.68, lr: 0.000052, Train: (LOSS: 0.0084, MAE: 0.0084, RMSE: 0.0139, R2: 0.9957), Valid: (LOSS: 0.0356, MAE: 0.0356, RMSE: 0.0558, R2: 0.9517), PNorm: 188.9893, GNorm: 0.4267
[184/299] timecost: 62.25, lr: 0.000052, Train: (LOSS: 0.0084, MAE: 0.0084, RMSE: 0.0139, R2: 0.9952), Valid: (LOSS: 0.0356, MAE: 0.0356, RMSE: 0.0566, R2: 0.9491), PNorm: 188.9933, GNorm: 0.3650
[185/299] timecost: 61.88, lr: 0.000052, Train: (LOSS: 0.0086, MAE: 0.0086, RMSE: 0.0144, R2: 0.9955), Valid: (LOSS: 0.0355, MAE: 0.0355, RMSE: 0.0556, R2: 0.9513), PNorm: 188.9963, GNorm: 0.4868
[186/299] timecost: 62.75, lr: 0.000052, Train: (LOSS: 0.0082, MAE: 0.0082, RMSE: 0.0138, R2: 0.9955), Valid: (LOSS: 0.0367, MAE: 0.0367, RMSE: 0.0573, R2: 0.9486), PNorm: 189.0007, GNorm: 0.3662
[187/299] timecost: 62.05, lr: 0.000052, Train: (LOSS: 0.0083, MAE: 0.0083, RMSE: 0.0140, R2: 0.9955), Valid: (LOSS: 0.0361, MAE: 0.0361, RMSE: 0.0561, R2: 0.9506), PNorm: 189.0023, GNorm: 0.4420
[188/299] timecost: 62.09, lr: 0.000052, Train: (LOSS: 0.0086, MAE: 0.0086, RMSE: 0.0141, R2: 0.9951), Valid: (LOSS: 0.0362, MAE: 0.0362, RMSE: 0.0575, R2: 0.9481), PNorm: 189.0074, GNorm: 0.5000
[189/299] timecost: 62.28, lr: 0.000052, Train: (LOSS: 0.0084, MAE: 0.0084, RMSE: 0.0137, R2: 0.9952), Valid: (LOSS: 0.0358, MAE: 0.0358, RMSE: 0.0552, R2: 0.9527), PNorm: 189.0124, GNorm: 0.5000
[190/299] timecost: 62.37, lr: 0.000052, Train: (LOSS: 0.0083, MAE: 0.0083, RMSE: 0.0136, R2: 0.9952), Valid: (LOSS: 0.0355, MAE: 0.0355, RMSE: 0.0549, R2: 0.9513), PNorm: 189.0145, GNorm: 0.3932
[191/299] timecost: 62.17, lr: 0.000052, Train: (LOSS: 0.0086, MAE: 0.0086, RMSE: 0.0140, R2: 0.9957), Valid: (LOSS: 0.0350, MAE: 0.0350, RMSE: 0.0544, R2: 0.9533), PNorm: 189.0205, GNorm: 0.3815
[192/299] timecost: 61.52, lr: 0.000052, Train: (LOSS: 0.0084, MAE: 0.0084, RMSE: 0.0137, R2: 0.9958), Valid: (LOSS: 0.0355, MAE: 0.0355, RMSE: 0.0558, R2: 0.9505), PNorm: 189.0245, GNorm: 0.3826
[193/299] timecost: 61.62, lr: 0.000052, Train: (LOSS: 0.0080, MAE: 0.0080, RMSE: 0.0136, R2: 0.9957), Valid: (LOSS: 0.0361, MAE: 0.0361, RMSE: 0.0571, R2: 0.9484), PNorm: 189.0271, GNorm: 0.4374
[194/299] timecost: 61.71, lr: 0.000052, Train: (LOSS: 0.0081, MAE: 0.0081, RMSE: 0.0135, R2: 0.9954), Valid: (LOSS: 0.0357, MAE: 0.0357, RMSE: 0.0557, R2: 0.9518), PNorm: 189.0319, GNorm: 0.4513
Epoch 00196: reducing learning rate of group 0 to 4.4371e-05.
[195/299] timecost: 61.89, lr: 0.000044, Train: (LOSS: 0.0080, MAE: 0.0080, RMSE: 0.0135, R2: 0.9959), Valid: (LOSS: 0.0355, MAE: 0.0355, RMSE: 0.0558, R2: 0.9509), PNorm: 189.0356, GNorm: 0.4477
[196/299] timecost: 62.03, lr: 0.000044, Train: (LOSS: 0.0076, MAE: 0.0076, RMSE: 0.0128, R2: 0.9961), Valid: (LOSS: 0.0362, MAE: 0.0362, RMSE: 0.0556, R2: 0.9518), PNorm: 189.0383, GNorm: 0.4796
[197/299] timecost: 62.05, lr: 0.000044, Train: (LOSS: 0.0074, MAE: 0.0074, RMSE: 0.0125, R2: 0.9961), Valid: (LOSS: 0.0348, MAE: 0.0348, RMSE: 0.0544, R2: 0.9529), PNorm: 189.0404, GNorm: 0.5000
[198/299] timecost: 61.60, lr: 0.000044, Train: (LOSS: 0.0075, MAE: 0.0075, RMSE: 0.0126, R2: 0.9959), Valid: (LOSS: 0.0361, MAE: 0.0361, RMSE: 0.0567, R2: 0.9497), PNorm: 189.0439, GNorm: 0.5000
[199/299] timecost: 61.15, lr: 0.000044, Train: (LOSS: 0.0071, MAE: 0.0071, RMSE: 0.0121, R2: 0.9961), Valid: (LOSS: 0.0354, MAE: 0.0354, RMSE: 0.0539, R2: 0.9553), PNorm: 189.0466, GNorm: 0.3190
[200/299] timecost: 61.31, lr: 0.000044, Train: (LOSS: 0.0074, MAE: 0.0074, RMSE: 0.0125, R2: 0.9960), Valid: (LOSS: 0.0353, MAE: 0.0353, RMSE: 0.0553, R2: 0.9517), PNorm: 189.0487, GNorm: 0.5000
[201/299] timecost: 61.42, lr: 0.000044, Train: (LOSS: 0.0071, MAE: 0.0071, RMSE: 0.0122, R2: 0.9963), Valid: (LOSS: 0.0344, MAE: 0.0344, RMSE: 0.0539, R2: 0.9544), PNorm: 189.0529, GNorm: 0.5000
[202/299] timecost: 61.18, lr: 0.000044, Train: (LOSS: 0.0071, MAE: 0.0071, RMSE: 0.0121, R2: 0.9959), Valid: (LOSS: 0.0358, MAE: 0.0358, RMSE: 0.0558, R2: 0.9510), PNorm: 189.0557, GNorm: 0.3081
[203/299] timecost: 61.22, lr: 0.000044, Train: (LOSS: 0.0072, MAE: 0.0072, RMSE: 0.0124, R2: 0.9959), Valid: (LOSS: 0.0354, MAE: 0.0354, RMSE: 0.0551, R2: 0.9525), PNorm: 189.0571, GNorm: 0.3530
[204/299] timecost: 61.37, lr: 0.000044, Train: (LOSS: 0.0071, MAE: 0.0071, RMSE: 0.0118, R2: 0.9959), Valid: (LOSS: 0.0358, MAE: 0.0358, RMSE: 0.0551, R2: 0.9523), PNorm: 189.0594, GNorm: 0.3587
[205/299] timecost: 60.86, lr: 0.000044, Train: (LOSS: 0.0069, MAE: 0.0069, RMSE: 0.0119, R2: 0.9962), Valid: (LOSS: 0.0353, MAE: 0.0353, RMSE: 0.0547, R2: 0.9528), PNorm: 189.0623, GNorm: 0.3784
[206/299] timecost: 62.03, lr: 0.000044, Train: (LOSS: 0.0073, MAE: 0.0073, RMSE: 0.0124, R2: 0.9960), Valid: (LOSS: 0.0348, MAE: 0.0348, RMSE: 0.0549, R2: 0.9514), PNorm: 189.0656, GNorm: 0.4421
[207/299] timecost: 62.65, lr: 0.000044, Train: (LOSS: 0.0070, MAE: 0.0070, RMSE: 0.0120, R2: 0.9968), Valid: (LOSS: 0.0357, MAE: 0.0357, RMSE: 0.0561, R2: 0.9494), PNorm: 189.0686, GNorm: 0.4129
[208/299] timecost: 62.50, lr: 0.000044, Train: (LOSS: 0.0071, MAE: 0.0071, RMSE: 0.0121, R2: 0.9959), Valid: (LOSS: 0.0357, MAE: 0.0357, RMSE: 0.0552, R2: 0.9517), PNorm: 189.0725, GNorm: 0.4921
[209/299] timecost: 62.44, lr: 0.000044, Train: (LOSS: 0.0073, MAE: 0.0073, RMSE: 0.0123, R2: 0.9956), Valid: (LOSS: 0.0357, MAE: 0.0357, RMSE: 0.0560, R2: 0.9504), PNorm: 189.0748, GNorm: 0.4098
[210/299] timecost: 62.33, lr: 0.000044, Train: (LOSS: 0.0072, MAE: 0.0072, RMSE: 0.0123, R2: 0.9960), Valid: (LOSS: 0.0352, MAE: 0.0352, RMSE: 0.0547, R2: 0.9523), PNorm: 189.0786, GNorm: 0.4973
Epoch 00212: reducing learning rate of group 0 to 3.7715e-05.
[211/299] timecost: 62.12, lr: 0.000038, Train: (LOSS: 0.0071, MAE: 0.0071, RMSE: 0.0121, R2: 0.9954), Valid: (LOSS: 0.0349, MAE: 0.0349, RMSE: 0.0545, R2: 0.9527), PNorm: 189.0808, GNorm: 0.4442
[212/299] timecost: 62.00, lr: 0.000038, Train: (LOSS: 0.0065, MAE: 0.0065, RMSE: 0.0115, R2: 0.9965), Valid: (LOSS: 0.0351, MAE: 0.0351, RMSE: 0.0557, R2: 0.9505), PNorm: 189.0834, GNorm: 0.4543
[213/299] timecost: 62.63, lr: 0.000038, Train: (LOSS: 0.0064, MAE: 0.0064, RMSE: 0.0110, R2: 0.9968), Valid: (LOSS: 0.0356, MAE: 0.0356, RMSE: 0.0560, R2: 0.9504), PNorm: 189.0861, GNorm: 0.5000
[214/299] timecost: 61.88, lr: 0.000038, Train: (LOSS: 0.0066, MAE: 0.0066, RMSE: 0.0115, R2: 0.9962), Valid: (LOSS: 0.0353, MAE: 0.0353, RMSE: 0.0553, R2: 0.9520), PNorm: 189.0881, GNorm: 0.4742
[215/299] timecost: 62.16, lr: 0.000038, Train: (LOSS: 0.0064, MAE: 0.0064, RMSE: 0.0109, R2: 0.9966), Valid: (LOSS: 0.0360, MAE: 0.0360, RMSE: 0.0563, R2: 0.9498), PNorm: 189.0896, GNorm: 0.5000
[216/299] timecost: 62.04, lr: 0.000038, Train: (LOSS: 0.0061, MAE: 0.0061, RMSE: 0.0110, R2: 0.9965), Valid: (LOSS: 0.0359, MAE: 0.0359, RMSE: 0.0560, R2: 0.9504), PNorm: 189.0908, GNorm: 0.5000
[217/299] timecost: 61.83, lr: 0.000038, Train: (LOSS: 0.0062, MAE: 0.0062, RMSE: 0.0109, R2: 0.9958), Valid: (LOSS: 0.0355, MAE: 0.0355, RMSE: 0.0548, R2: 0.9527), PNorm: 189.0936, GNorm: 0.4423
[218/299] timecost: 58.79, lr: 0.000038, Train: (LOSS: 0.0061, MAE: 0.0061, RMSE: 0.0108, R2: 0.9963), Valid: (LOSS: 0.0353, MAE: 0.0353, RMSE: 0.0549, R2: 0.9526), PNorm: 189.0963, GNorm: 0.3556
[219/299] timecost: 59.06, lr: 0.000038, Train: (LOSS: 0.0060, MAE: 0.0060, RMSE: 0.0109, R2: 0.9966), Valid: (LOSS: 0.0347, MAE: 0.0347, RMSE: 0.0543, R2: 0.9535), PNorm: 189.0984, GNorm: 0.4435
[220/299] timecost: 58.76, lr: 0.000038, Train: (LOSS: 0.0063, MAE: 0.0063, RMSE: 0.0111, R2: 0.9964), Valid: (LOSS: 0.0356, MAE: 0.0356, RMSE: 0.0562, R2: 0.9501), PNorm: 189.1002, GNorm: 0.4399
[221/299] timecost: 59.44, lr: 0.000038, Train: (LOSS: 0.0062, MAE: 0.0062, RMSE: 0.0110, R2: 0.9962), Valid: (LOSS: 0.0358, MAE: 0.0358, RMSE: 0.0559, R2: 0.9506), PNorm: 189.1022, GNorm: 0.5000
[222/299] timecost: 59.28, lr: 0.000038, Train: (LOSS: 0.0060, MAE: 0.0060, RMSE: 0.0107, R2: 0.9967), Valid: (LOSS: 0.0347, MAE: 0.0347, RMSE: 0.0541, R2: 0.9541), PNorm: 189.1044, GNorm: 0.3570
[223/299] timecost: 59.23, lr: 0.000038, Train: (LOSS: 0.0059, MAE: 0.0059, RMSE: 0.0106, R2: 0.9967), Valid: (LOSS: 0.0357, MAE: 0.0357, RMSE: 0.0562, R2: 0.9497), PNorm: 189.1055, GNorm: 0.4261
[224/299] timecost: 60.67, lr: 0.000038, Train: (LOSS: 0.0061, MAE: 0.0061, RMSE: 0.0108, R2: 0.9963), Valid: (LOSS: 0.0351, MAE: 0.0351, RMSE: 0.0549, R2: 0.9525), PNorm: 189.1081, GNorm: 0.5000
[225/299] timecost: 64.41, lr: 0.000038, Train: (LOSS: 0.0063, MAE: 0.0063, RMSE: 0.0110, R2: 0.9964), Valid: (LOSS: 0.0358, MAE: 0.0358, RMSE: 0.0558, R2: 0.9508), PNorm: 189.1104, GNorm: 0.4194
[226/299] timecost: 58.90, lr: 0.000038, Train: (LOSS: 0.0061, MAE: 0.0061, RMSE: 0.0107, R2: 0.9963), Valid: (LOSS: 0.0357, MAE: 0.0357, RMSE: 0.0561, R2: 0.9505), PNorm: 189.1126, GNorm: 0.3709
Epoch 00228: reducing learning rate of group 0 to 3.2058e-05.
[227/299] timecost: 58.98, lr: 0.000032, Train: (LOSS: 0.0060, MAE: 0.0060, RMSE: 0.0108, R2: 0.9964), Valid: (LOSS: 0.0351, MAE: 0.0351, RMSE: 0.0557, R2: 0.9510), PNorm: 189.1158, GNorm: 0.3583
[228/299] timecost: 59.03, lr: 0.000032, Train: (LOSS: 0.0058, MAE: 0.0058, RMSE: 0.0104, R2: 0.9969), Valid: (LOSS: 0.0354, MAE: 0.0354, RMSE: 0.0556, R2: 0.9507), PNorm: 189.1177, GNorm: 0.3730
[229/299] timecost: 59.11, lr: 0.000032, Train: (LOSS: 0.0054, MAE: 0.0054, RMSE: 0.0098, R2: 0.9970), Valid: (LOSS: 0.0351, MAE: 0.0351, RMSE: 0.0549, R2: 0.9521), PNorm: 189.1194, GNorm: 0.4683
[230/299] timecost: 58.97, lr: 0.000032, Train: (LOSS: 0.0054, MAE: 0.0054, RMSE: 0.0099, R2: 0.9968), Valid: (LOSS: 0.0353, MAE: 0.0353, RMSE: 0.0554, R2: 0.9514), PNorm: 189.1217, GNorm: 0.5000
[231/299] timecost: 58.70, lr: 0.000032, Train: (LOSS: 0.0054, MAE: 0.0054, RMSE: 0.0100, R2: 0.9965), Valid: (LOSS: 0.0348, MAE: 0.0348, RMSE: 0.0550, R2: 0.9515), PNorm: 189.1232, GNorm: 0.5000
[232/299] timecost: 59.27, lr: 0.000032, Train: (LOSS: 0.0056, MAE: 0.0056, RMSE: 0.0101, R2: 0.9970), Valid: (LOSS: 0.0353, MAE: 0.0353, RMSE: 0.0551, R2: 0.9517), PNorm: 189.1250, GNorm: 0.4803
[233/299] timecost: 59.43, lr: 0.000032, Train: (LOSS: 0.0054, MAE: 0.0054, RMSE: 0.0100, R2: 0.9964), Valid: (LOSS: 0.0353, MAE: 0.0353, RMSE: 0.0551, R2: 0.9523), PNorm: 189.1270, GNorm: 0.4484
[234/299] timecost: 59.34, lr: 0.000032, Train: (LOSS: 0.0056, MAE: 0.0056, RMSE: 0.0102, R2: 0.9964), Valid: (LOSS: 0.0351, MAE: 0.0351, RMSE: 0.0555, R2: 0.9507), PNorm: 189.1284, GNorm: 0.4168
[235/299] timecost: 59.21, lr: 0.000032, Train: (LOSS: 0.0052, MAE: 0.0052, RMSE: 0.0094, R2: 0.9966), Valid: (LOSS: 0.0354, MAE: 0.0354, RMSE: 0.0558, R2: 0.9503), PNorm: 189.1302, GNorm: 0.4436
[236/299] timecost: 59.08, lr: 0.000032, Train: (LOSS: 0.0053, MAE: 0.0053, RMSE: 0.0097, R2: 0.9960), Valid: (LOSS: 0.0350, MAE: 0.0350, RMSE: 0.0543, R2: 0.9536), PNorm: 189.1322, GNorm: 0.5000
[237/299] timecost: 59.32, lr: 0.000032, Train: (LOSS: 0.0053, MAE: 0.0053, RMSE: 0.0096, R2: 0.9966), Valid: (LOSS: 0.0358, MAE: 0.0358, RMSE: 0.0563, R2: 0.9499), PNorm: 189.1335, GNorm: 0.4444
[238/299] timecost: 59.38, lr: 0.000032, Train: (LOSS: 0.0052, MAE: 0.0052, RMSE: 0.0099, R2: 0.9972), Valid: (LOSS: 0.0355, MAE: 0.0355, RMSE: 0.0554, R2: 0.9516), PNorm: 189.1344, GNorm: 0.4367
[239/299] timecost: 59.26, lr: 0.000032, Train: (LOSS: 0.0053, MAE: 0.0053, RMSE: 0.0099, R2: 0.9966), Valid: (LOSS: 0.0354, MAE: 0.0354, RMSE: 0.0554, R2: 0.9517), PNorm: 189.1363, GNorm: 0.4024
[240/299] timecost: 59.22, lr: 0.000032, Train: (LOSS: 0.0052, MAE: 0.0052, RMSE: 0.0097, R2: 0.9973), Valid: (LOSS: 0.0361, MAE: 0.0361, RMSE: 0.0569, R2: 0.9488), PNorm: 189.1381, GNorm: 0.5000
[241/299] timecost: 59.14, lr: 0.000032, Train: (LOSS: 0.0055, MAE: 0.0055, RMSE: 0.0100, R2: 0.9967), Valid: (LOSS: 0.0354, MAE: 0.0354, RMSE: 0.0552, R2: 0.9521), PNorm: 189.1399, GNorm: 0.4492
[242/299] timecost: 62.71, lr: 0.000032, Train: (LOSS: 0.0052, MAE: 0.0052, RMSE: 0.0096, R2: 0.9970), Valid: (LOSS: 0.0356, MAE: 0.0356, RMSE: 0.0564, R2: 0.9498), PNorm: 189.1414, GNorm: 0.4262
Epoch 00244: reducing learning rate of group 0 to 2.7249e-05.
[243/299] timecost: 63.88, lr: 0.000027, Train: (LOSS: 0.0054, MAE: 0.0054, RMSE: 0.0098, R2: 0.9970), Valid: (LOSS: 0.0357, MAE: 0.0357, RMSE: 0.0558, R2: 0.9506), PNorm: 189.1427, GNorm: 0.4167
[244/299] timecost: 60.30, lr: 0.000027, Train: (LOSS: 0.0049, MAE: 0.0049, RMSE: 0.0094, R2: 0.9971), Valid: (LOSS: 0.0355, MAE: 0.0355, RMSE: 0.0561, R2: 0.9500), PNorm: 189.1445, GNorm: 0.5000
[245/299] timecost: 60.35, lr: 0.000027, Train: (LOSS: 0.0047, MAE: 0.0047, RMSE: 0.0091, R2: 0.9974), Valid: (LOSS: 0.0349, MAE: 0.0349, RMSE: 0.0546, R2: 0.9524), PNorm: 189.1452, GNorm: 0.3698
[246/299] timecost: 62.69, lr: 0.000027, Train: (LOSS: 0.0048, MAE: 0.0048, RMSE: 0.0091, R2: 0.9972), Valid: (LOSS: 0.0350, MAE: 0.0350, RMSE: 0.0555, R2: 0.9514), PNorm: 189.1463, GNorm: 0.3522
[247/299] timecost: 64.20, lr: 0.000027, Train: (LOSS: 0.0047, MAE: 0.0047, RMSE: 0.0091, R2: 0.9971), Valid: (LOSS: 0.0354, MAE: 0.0354, RMSE: 0.0554, R2: 0.9517), PNorm: 189.1473, GNorm: 0.5000
[248/299] timecost: 63.10, lr: 0.000027, Train: (LOSS: 0.0048, MAE: 0.0048, RMSE: 0.0092, R2: 0.9972), Valid: (LOSS: 0.0354, MAE: 0.0354, RMSE: 0.0555, R2: 0.9511), PNorm: 189.1480, GNorm: 0.5000
[249/299] timecost: 63.59, lr: 0.000027, Train: (LOSS: 0.0046, MAE: 0.0046, RMSE: 0.0089, R2: 0.9976), Valid: (LOSS: 0.0354, MAE: 0.0354, RMSE: 0.0554, R2: 0.9518), PNorm: 189.1491, GNorm: 0.4243
[250/299] timecost: 63.59, lr: 0.000027, Train: (LOSS: 0.0048, MAE: 0.0048, RMSE: 0.0093, R2: 0.9968), Valid: (LOSS: 0.0354, MAE: 0.0354, RMSE: 0.0556, R2: 0.9512), PNorm: 189.1507, GNorm: 0.5000
[251/299] timecost: 63.47, lr: 0.000027, Train: (LOSS: 0.0048, MAE: 0.0048, RMSE: 0.0092, R2: 0.9971), Valid: (LOSS: 0.0355, MAE: 0.0355, RMSE: 0.0560, R2: 0.9505), PNorm: 189.1514, GNorm: 0.3869
[252/299] timecost: 62.61, lr: 0.000027, Train: (LOSS: 0.0047, MAE: 0.0047, RMSE: 0.0090, R2: 0.9973), Valid: (LOSS: 0.0358, MAE: 0.0358, RMSE: 0.0562, R2: 0.9503), PNorm: 189.1534, GNorm: 0.3577
[253/299] timecost: 59.96, lr: 0.000027, Train: (LOSS: 0.0048, MAE: 0.0048, RMSE: 0.0092, R2: 0.9972), Valid: (LOSS: 0.0358, MAE: 0.0358, RMSE: 0.0557, R2: 0.9512), PNorm: 189.1539, GNorm: 0.5000
[254/299] timecost: 59.27, lr: 0.000027, Train: (LOSS: 0.0048, MAE: 0.0048, RMSE: 0.0091, R2: 0.9964), Valid: (LOSS: 0.0355, MAE: 0.0355, RMSE: 0.0552, R2: 0.9519), PNorm: 189.1555, GNorm: 0.4332
[255/299] timecost: 63.02, lr: 0.000027, Train: (LOSS: 0.0049, MAE: 0.0049, RMSE: 0.0093, R2: 0.9970), Valid: (LOSS: 0.0355, MAE: 0.0355, RMSE: 0.0549, R2: 0.9527), PNorm: 189.1566, GNorm: 0.3760
[256/299] timecost: 63.92, lr: 0.000027, Train: (LOSS: 0.0046, MAE: 0.0046, RMSE: 0.0090, R2: 0.9971), Valid: (LOSS: 0.0355, MAE: 0.0355, RMSE: 0.0559, R2: 0.9505), PNorm: 189.1574, GNorm: 0.4384
[257/299] timecost: 64.14, lr: 0.000027, Train: (LOSS: 0.0045, MAE: 0.0045, RMSE: 0.0088, R2: 0.9971), Valid: (LOSS: 0.0352, MAE: 0.0352, RMSE: 0.0550, R2: 0.9521), PNorm: 189.1593, GNorm: 0.3999
[258/299] timecost: 63.68, lr: 0.000027, Train: (LOSS: 0.0047, MAE: 0.0047, RMSE: 0.0091, R2: 0.9973), Valid: (LOSS: 0.0357, MAE: 0.0357, RMSE: 0.0551, R2: 0.9523), PNorm: 189.1601, GNorm: 0.5000
Epoch 00260: reducing learning rate of group 0 to 2.3162e-05.
[259/299] timecost: 64.06, lr: 0.000023, Train: (LOSS: 0.0045, MAE: 0.0045, RMSE: 0.0088, R2: 0.9972), Valid: (LOSS: 0.0351, MAE: 0.0351, RMSE: 0.0551, R2: 0.9521), PNorm: 189.1617, GNorm: 0.3935
[260/299] timecost: 63.69, lr: 0.000023, Train: (LOSS: 0.0045, MAE: 0.0045, RMSE: 0.0089, R2: 0.9974), Valid: (LOSS: 0.0356, MAE: 0.0356, RMSE: 0.0555, R2: 0.9514), PNorm: 189.1631, GNorm: 0.4211
[261/299] timecost: 63.97, lr: 0.000023, Train: (LOSS: 0.0045, MAE: 0.0045, RMSE: 0.0086, R2: 0.9971), Valid: (LOSS: 0.0357, MAE: 0.0357, RMSE: 0.0553, R2: 0.9522), PNorm: 189.1639, GNorm: 0.4597
[262/299] timecost: 63.79, lr: 0.000023, Train: (LOSS: 0.0044, MAE: 0.0044, RMSE: 0.0087, R2: 0.9969), Valid: (LOSS: 0.0355, MAE: 0.0355, RMSE: 0.0557, R2: 0.9510), PNorm: 189.1650, GNorm: 0.3800
[263/299] timecost: 63.76, lr: 0.000023, Train: (LOSS: 0.0043, MAE: 0.0043, RMSE: 0.0086, R2: 0.9973), Valid: (LOSS: 0.0354, MAE: 0.0354, RMSE: 0.0553, R2: 0.9515), PNorm: 189.1658, GNorm: 0.3766
[264/299] timecost: 63.67, lr: 0.000023, Train: (LOSS: 0.0043, MAE: 0.0043, RMSE: 0.0085, R2: 0.9965), Valid: (LOSS: 0.0357, MAE: 0.0357, RMSE: 0.0555, R2: 0.9515), PNorm: 189.1663, GNorm: 0.5000
[265/299] timecost: 59.09, lr: 0.000023, Train: (LOSS: 0.0042, MAE: 0.0042, RMSE: 0.0083, R2: 0.9975), Valid: (LOSS: 0.0356, MAE: 0.0356, RMSE: 0.0555, R2: 0.9514), PNorm: 189.1670, GNorm: 0.4353
[266/299] timecost: 59.49, lr: 0.000023, Train: (LOSS: 0.0042, MAE: 0.0042, RMSE: 0.0084, R2: 0.9970), Valid: (LOSS: 0.0360, MAE: 0.0360, RMSE: 0.0558, R2: 0.9511), PNorm: 189.1681, GNorm: 0.4916
[267/299] timecost: 59.26, lr: 0.000023, Train: (LOSS: 0.0042, MAE: 0.0042, RMSE: 0.0084, R2: 0.9973), Valid: (LOSS: 0.0359, MAE: 0.0359, RMSE: 0.0560, R2: 0.9506), PNorm: 189.1693, GNorm: 0.4186
[268/299] timecost: 58.92, lr: 0.000023, Train: (LOSS: 0.0044, MAE: 0.0044, RMSE: 0.0085, R2: 0.9973), Valid: (LOSS: 0.0357, MAE: 0.0357, RMSE: 0.0559, R2: 0.9503), PNorm: 189.1701, GNorm: 0.3577
[269/299] timecost: 59.02, lr: 0.000023, Train: (LOSS: 0.0040, MAE: 0.0040, RMSE: 0.0082, R2: 0.9966), Valid: (LOSS: 0.0358, MAE: 0.0358, RMSE: 0.0557, R2: 0.9512), PNorm: 189.1714, GNorm: 0.3766
[270/299] timecost: 59.10, lr: 0.000023, Train: (LOSS: 0.0041, MAE: 0.0041, RMSE: 0.0083, R2: 0.9974), Valid: (LOSS: 0.0356, MAE: 0.0356, RMSE: 0.0555, R2: 0.9513), PNorm: 189.1722, GNorm: 0.3829
[271/299] timecost: 58.85, lr: 0.000023, Train: (LOSS: 0.0042, MAE: 0.0042, RMSE: 0.0085, R2: 0.9973), Valid: (LOSS: 0.0356, MAE: 0.0356, RMSE: 0.0563, R2: 0.9497), PNorm: 189.1729, GNorm: 0.5000
[272/299] timecost: 59.07, lr: 0.000023, Train: (LOSS: 0.0041, MAE: 0.0041, RMSE: 0.0080, R2: 0.9971), Valid: (LOSS: 0.0356, MAE: 0.0356, RMSE: 0.0554, R2: 0.9512), PNorm: 189.1745, GNorm: 0.5000
[273/299] timecost: 58.75, lr: 0.000023, Train: (LOSS: 0.0041, MAE: 0.0041, RMSE: 0.0084, R2: 0.9973), Valid: (LOSS: 0.0355, MAE: 0.0355, RMSE: 0.0549, R2: 0.9526), PNorm: 189.1753, GNorm: 0.5000
[274/299] timecost: 59.38, lr: 0.000023, Train: (LOSS: 0.0042, MAE: 0.0042, RMSE: 0.0084, R2: 0.9975), Valid: (LOSS: 0.0361, MAE: 0.0361, RMSE: 0.0564, R2: 0.9502), PNorm: 189.1760, GNorm: 0.3406
Epoch 00276: reducing learning rate of group 0 to 1.9687e-05.
[275/299] timecost: 59.34, lr: 0.000020, Train: (LOSS: 0.0042, MAE: 0.0042, RMSE: 0.0083, R2: 0.9968), Valid: (LOSS: 0.0354, MAE: 0.0354, RMSE: 0.0550, R2: 0.9523), PNorm: 189.1773, GNorm: 0.4673
[276/299] timecost: 59.42, lr: 0.000020, Train: (LOSS: 0.0040, MAE: 0.0040, RMSE: 0.0079, R2: 0.9969), Valid: (LOSS: 0.0359, MAE: 0.0359, RMSE: 0.0556, R2: 0.9517), PNorm: 189.1783, GNorm: 0.4682
[277/299] timecost: 59.44, lr: 0.000020, Train: (LOSS: 0.0037, MAE: 0.0037, RMSE: 0.0079, R2: 0.9971), Valid: (LOSS: 0.0358, MAE: 0.0358, RMSE: 0.0562, R2: 0.9500), PNorm: 189.1788, GNorm: 0.4945
[278/299] timecost: 58.93, lr: 0.000020, Train: (LOSS: 0.0037, MAE: 0.0037, RMSE: 0.0078, R2: 0.9974), Valid: (LOSS: 0.0360, MAE: 0.0360, RMSE: 0.0556, R2: 0.9516), PNorm: 189.1797, GNorm: 0.4168
[279/299] timecost: 58.92, lr: 0.000020, Train: (LOSS: 0.0037, MAE: 0.0037, RMSE: 0.0078, R2: 0.9976), Valid: (LOSS: 0.0361, MAE: 0.0361, RMSE: 0.0562, R2: 0.9502), PNorm: 189.1802, GNorm: 0.5000
[280/299] timecost: 58.95, lr: 0.000020, Train: (LOSS: 0.0037, MAE: 0.0037, RMSE: 0.0076, R2: 0.9975), Valid: (LOSS: 0.0356, MAE: 0.0356, RMSE: 0.0559, R2: 0.9507), PNorm: 189.1804, GNorm: 0.5000
[281/299] timecost: 59.01, lr: 0.000020, Train: (LOSS: 0.0037, MAE: 0.0037, RMSE: 0.0078, R2: 0.9970), Valid: (LOSS: 0.0359, MAE: 0.0359, RMSE: 0.0562, R2: 0.9503), PNorm: 189.1817, GNorm: 0.4396
[282/299] timecost: 59.15, lr: 0.000020, Train: (LOSS: 0.0037, MAE: 0.0037, RMSE: 0.0075, R2: 0.9975), Valid: (LOSS: 0.0357, MAE: 0.0357, RMSE: 0.0560, R2: 0.9506), PNorm: 189.1819, GNorm: 0.4202
[283/299] timecost: 59.19, lr: 0.000020, Train: (LOSS: 0.0038, MAE: 0.0038, RMSE: 0.0080, R2: 0.9968), Valid: (LOSS: 0.0356, MAE: 0.0356, RMSE: 0.0558, R2: 0.9510), PNorm: 189.1829, GNorm: 0.3150
[284/299] timecost: 58.99, lr: 0.000020, Train: (LOSS: 0.0036, MAE: 0.0036, RMSE: 0.0078, R2: 0.9970), Valid: (LOSS: 0.0351, MAE: 0.0351, RMSE: 0.0553, R2: 0.9517), PNorm: 189.1834, GNorm: 0.5000
[285/299] timecost: 59.06, lr: 0.000020, Train: (LOSS: 0.0037, MAE: 0.0037, RMSE: 0.0078, R2: 0.9973), Valid: (LOSS: 0.0354, MAE: 0.0354, RMSE: 0.0556, R2: 0.9513), PNorm: 189.1844, GNorm: 0.4563
[286/299] timecost: 58.95, lr: 0.000020, Train: (LOSS: 0.0036, MAE: 0.0036, RMSE: 0.0077, R2: 0.9968), Valid: (LOSS: 0.0356, MAE: 0.0356, RMSE: 0.0556, R2: 0.9513), PNorm: 189.1856, GNorm: 0.3881
[287/299] timecost: 59.12, lr: 0.000020, Train: (LOSS: 0.0039, MAE: 0.0039, RMSE: 0.0079, R2: 0.9973), Valid: (LOSS: 0.0355, MAE: 0.0355, RMSE: 0.0555, R2: 0.9517), PNorm: 189.1861, GNorm: 0.5000
[288/299] timecost: 58.76, lr: 0.000020, Train: (LOSS: 0.0036, MAE: 0.0036, RMSE: 0.0076, R2: 0.9973), Valid: (LOSS: 0.0357, MAE: 0.0357, RMSE: 0.0561, R2: 0.9504), PNorm: 189.1868, GNorm: 0.4501
[289/299] timecost: 59.21, lr: 0.000020, Train: (LOSS: 0.0036, MAE: 0.0036, RMSE: 0.0077, R2: 0.9974), Valid: (LOSS: 0.0356, MAE: 0.0356, RMSE: 0.0555, R2: 0.9514), PNorm: 189.1872, GNorm: 0.4708
[290/299] timecost: 59.19, lr: 0.000020, Train: (LOSS: 0.0037, MAE: 0.0037, RMSE: 0.0077, R2: 0.9973), Valid: (LOSS: 0.0356, MAE: 0.0356, RMSE: 0.0561, R2: 0.9503), PNorm: 189.1881, GNorm: 0.3868
Epoch 00292: reducing learning rate of group 0 to 1.6734e-05.
[291/299] timecost: 59.29, lr: 0.000017, Train: (LOSS: 0.0037, MAE: 0.0037, RMSE: 0.0077, R2: 0.9972), Valid: (LOSS: 0.0360, MAE: 0.0360, RMSE: 0.0560, R2: 0.9505), PNorm: 189.1888, GNorm: 0.3597
[292/299] timecost: 61.88, lr: 0.000017, Train: (LOSS: 0.0035, MAE: 0.0035, RMSE: 0.0076, R2: 0.9973), Valid: (LOSS: 0.0354, MAE: 0.0354, RMSE: 0.0553, R2: 0.9518), PNorm: 189.1892, GNorm: 0.3989
[293/299] timecost: 60.10, lr: 0.000017, Train: (LOSS: 0.0034, MAE: 0.0034, RMSE: 0.0075, R2: 0.9977), Valid: (LOSS: 0.0359, MAE: 0.0359, RMSE: 0.0560, R2: 0.9505), PNorm: 189.1899, GNorm: 0.5000
[294/299] timecost: 64.05, lr: 0.000017, Train: (LOSS: 0.0033, MAE: 0.0033, RMSE: 0.0074, R2: 0.9974), Valid: (LOSS: 0.0358, MAE: 0.0358, RMSE: 0.0559, R2: 0.9508), PNorm: 189.1901, GNorm: 0.5000
[295/299] timecost: 63.87, lr: 0.000017, Train: (LOSS: 0.0034, MAE: 0.0034, RMSE: 0.0074, R2: 0.9975), Valid: (LOSS: 0.0359, MAE: 0.0359, RMSE: 0.0562, R2: 0.9500), PNorm: 189.1907, GNorm: 0.3852
[296/299] timecost: 63.80, lr: 0.000017, Train: (LOSS: 0.0033, MAE: 0.0033, RMSE: 0.0073, R2: 0.9972), Valid: (LOSS: 0.0356, MAE: 0.0356, RMSE: 0.0555, R2: 0.9514), PNorm: 189.1911, GNorm: 0.3559
[297/299] timecost: 60.95, lr: 0.000017, Train: (LOSS: 0.0033, MAE: 0.0033, RMSE: 0.0074, R2: 0.9966), Valid: (LOSS: 0.0359, MAE: 0.0359, RMSE: 0.0560, R2: 0.9507), PNorm: 189.1920, GNorm: 0.5000
[298/299] timecost: 59.42, lr: 0.000017, Train: (LOSS: 0.0034, MAE: 0.0034, RMSE: 0.0073, R2: 0.9978), Valid: (LOSS: 0.0360, MAE: 0.0360, RMSE: 0.0559, R2: 0.9508), PNorm: 189.1925, GNorm: 0.4897
[299/299] timecost: 60.43, lr: 0.000017, Train: (LOSS: 0.0033, MAE: 0.0033, RMSE: 0.0074, R2: 0.9973), Valid: (LOSS: 0.0355, MAE: 0.0355, RMSE: 0.0556, R2: 0.9514), PNorm: 189.1930, GNorm: 0.5000
==========Training End==========
==========Test Best Model==========
================Final Results=======================
mse: 0.0360 +- 0.0000:
rmse: 0.0566 +- 0.0000:
mae: 0.0360 +- 0.0000:
r2: 0.9481 +- 0.0000:
tensor([[0.1444, 0.1066],
        [0.0000, 0.0000],
        [0.0000, 0.0000],
        ...,
        [0.0000, 0.0000],
        [0.5453, 0.4354],
        [0.0000, 0.0000]], device='cuda:0')
