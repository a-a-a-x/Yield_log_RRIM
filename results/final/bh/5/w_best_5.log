cuda available with GPU: Tesla V100-PCIE-16GB
==========Load Seed==========
set_random_seed
0
==========Training Start==========
Training Graphs:  2491
Valid Graphs:  277
Test Graphs:  1187
============Loading pretrained weights to generate initialization============
============Creating new layers============
============Creating Model============
Training Graphs Batches:  78
Valid Graphs Batches:  9
Test Graphs Batches:  37
[0/299] timecost: 64.29, lr: 0.000030, Train: (LOSS: 0.2369, MAE: 0.2369, RMSE: 0.2792, R2: -0.0858), Valid: (LOSS: 0.2261, MAE: 0.2261, RMSE: 0.2578, R2: 0.0677), PNorm: 174.3257, GNorm: 0.3671
[1/299] timecost: 63.37, lr: 0.000030, Train: (LOSS: 0.2245, MAE: 0.2245, RMSE: 0.2675, R2: -0.0163), Valid: (LOSS: 0.2190, MAE: 0.2190, RMSE: 0.2524, R2: 0.0968), PNorm: 173.7057, GNorm: 1.4805
[2/299] timecost: 63.76, lr: 0.000030, Train: (LOSS: 0.2042, MAE: 0.2042, RMSE: 0.2483, R2: 0.1382), Valid: (LOSS: 0.1986, MAE: 0.1986, RMSE: 0.2401, R2: 0.1612), PNorm: 173.2578, GNorm: 0.4705
[3/299] timecost: 64.40, lr: 0.000030, Train: (LOSS: 0.1856, MAE: 0.1856, RMSE: 0.2325, R2: 0.2395), Valid: (LOSS: 0.1830, MAE: 0.1830, RMSE: 0.2312, R2: 0.1843), PNorm: 172.8832, GNorm: 4.6244
[4/299] timecost: 64.77, lr: 0.000030, Train: (LOSS: 0.1633, MAE: 0.1633, RMSE: 0.2102, R2: 0.3665), Valid: (LOSS: 0.1445, MAE: 0.1445, RMSE: 0.1944, R2: 0.4298), PNorm: 172.5580, GNorm: 1.2032
[5/299] timecost: 64.07, lr: 0.000030, Train: (LOSS: 0.1456, MAE: 0.1456, RMSE: 0.1929, R2: 0.4670), Valid: (LOSS: 0.1331, MAE: 0.1331, RMSE: 0.1817, R2: 0.5089), PNorm: 172.2486, GNorm: 3.5132
[6/299] timecost: 63.79, lr: 0.000030, Train: (LOSS: 0.1326, MAE: 0.1326, RMSE: 0.1796, R2: 0.5494), Valid: (LOSS: 0.1314, MAE: 0.1314, RMSE: 0.1756, R2: 0.5269), PNorm: 171.9441, GNorm: 0.6923
[7/299] timecost: 64.01, lr: 0.000030, Train: (LOSS: 0.1306, MAE: 0.1306, RMSE: 0.1779, R2: 0.5367), Valid: (LOSS: 0.1303, MAE: 0.1303, RMSE: 0.1747, R2: 0.5460), PNorm: 171.6446, GNorm: 3.0297
[8/299] timecost: 63.32, lr: 0.000030, Train: (LOSS: 0.1296, MAE: 0.1296, RMSE: 0.1772, R2: 0.5442), Valid: (LOSS: 0.1218, MAE: 0.1218, RMSE: 0.1711, R2: 0.5598), PNorm: 171.3547, GNorm: 1.5445
[9/299] timecost: 63.36, lr: 0.000030, Train: (LOSS: 0.1276, MAE: 0.1276, RMSE: 0.1767, R2: 0.5380), Valid: (LOSS: 0.1102, MAE: 0.1102, RMSE: 0.1580, R2: 0.6193), PNorm: 171.0712, GNorm: 1.3663
[10/299] timecost: 63.86, lr: 0.000030, Train: (LOSS: 0.1274, MAE: 0.1274, RMSE: 0.1756, R2: 0.5566), Valid: (LOSS: 0.1107, MAE: 0.1107, RMSE: 0.1592, R2: 0.6089), PNorm: 170.7946, GNorm: 3.2455
[11/299] timecost: 63.64, lr: 0.000030, Train: (LOSS: 0.1181, MAE: 0.1181, RMSE: 0.1648, R2: 0.6101), Valid: (LOSS: 0.1099, MAE: 0.1099, RMSE: 0.1548, R2: 0.6474), PNorm: 170.5249, GNorm: 3.2918
[12/299] timecost: 63.52, lr: 0.000030, Train: (LOSS: 0.1181, MAE: 0.1181, RMSE: 0.1671, R2: 0.6017), Valid: (LOSS: 0.1063, MAE: 0.1063, RMSE: 0.1604, R2: 0.6009), PNorm: 170.2619, GNorm: 0.9876
[13/299] timecost: 63.98, lr: 0.000030, Train: (LOSS: 0.1141, MAE: 0.1141, RMSE: 0.1629, R2: 0.6220), Valid: (LOSS: 0.1058, MAE: 0.1058, RMSE: 0.1608, R2: 0.5966), PNorm: 170.0045, GNorm: 1.7721
[14/299] timecost: 63.56, lr: 0.000030, Train: (LOSS: 0.1113, MAE: 0.1113, RMSE: 0.1597, R2: 0.6249), Valid: (LOSS: 0.1158, MAE: 0.1158, RMSE: 0.1578, R2: 0.6288), PNorm: 169.7507, GNorm: 1.6098
[15/299] timecost: 63.46, lr: 0.000030, Train: (LOSS: 0.1161, MAE: 0.1161, RMSE: 0.1652, R2: 0.6060), Valid: (LOSS: 0.1074, MAE: 0.1074, RMSE: 0.1531, R2: 0.6533), PNorm: 169.5026, GNorm: 1.3616
[16/299] timecost: 63.64, lr: 0.000030, Train: (LOSS: 0.1097, MAE: 0.1097, RMSE: 0.1574, R2: 0.6446), Valid: (LOSS: 0.1045, MAE: 0.1045, RMSE: 0.1510, R2: 0.6682), PNorm: 169.2563, GNorm: 2.4288
[17/299] timecost: 65.06, lr: 0.000030, Train: (LOSS: 0.1030, MAE: 0.1030, RMSE: 0.1491, R2: 0.6774), Valid: (LOSS: 0.0957, MAE: 0.0957, RMSE: 0.1425, R2: 0.6879), PNorm: 169.0125, GNorm: 1.4936
[18/299] timecost: 66.10, lr: 0.000030, Train: (LOSS: 0.0999, MAE: 0.0999, RMSE: 0.1457, R2: 0.6922), Valid: (LOSS: 0.0974, MAE: 0.0974, RMSE: 0.1421, R2: 0.6957), PNorm: 168.7720, GNorm: 1.0614
[19/299] timecost: 65.82, lr: 0.000030, Train: (LOSS: 0.0980, MAE: 0.0980, RMSE: 0.1450, R2: 0.6951), Valid: (LOSS: 0.0891, MAE: 0.0891, RMSE: 0.1334, R2: 0.7323), PNorm: 168.5353, GNorm: 2.2055
[20/299] timecost: 65.87, lr: 0.000030, Train: (LOSS: 0.0947, MAE: 0.0947, RMSE: 0.1386, R2: 0.7177), Valid: (LOSS: 0.1035, MAE: 0.1035, RMSE: 0.1438, R2: 0.6693), PNorm: 168.3015, GNorm: 3.4212
[21/299] timecost: 65.39, lr: 0.000030, Train: (LOSS: 0.0917, MAE: 0.0917, RMSE: 0.1353, R2: 0.7349), Valid: (LOSS: 0.0967, MAE: 0.0967, RMSE: 0.1426, R2: 0.7058), PNorm: 168.0703, GNorm: 2.3976
[22/299] timecost: 64.26, lr: 0.000030, Train: (LOSS: 0.0894, MAE: 0.0894, RMSE: 0.1327, R2: 0.7374), Valid: (LOSS: 0.0928, MAE: 0.0928, RMSE: 0.1346, R2: 0.7304), PNorm: 167.8422, GNorm: 2.7471
[23/299] timecost: 63.38, lr: 0.000030, Train: (LOSS: 0.0918, MAE: 0.0918, RMSE: 0.1347, R2: 0.7387), Valid: (LOSS: 0.0826, MAE: 0.0826, RMSE: 0.1264, R2: 0.7645), PNorm: 167.6177, GNorm: 1.1744
[24/299] timecost: 63.68, lr: 0.000030, Train: (LOSS: 0.0825, MAE: 0.0825, RMSE: 0.1250, R2: 0.7703), Valid: (LOSS: 0.0961, MAE: 0.0961, RMSE: 0.1522, R2: 0.6379), PNorm: 167.3955, GNorm: 4.6472
[25/299] timecost: 64.01, lr: 0.000030, Train: (LOSS: 0.0874, MAE: 0.0874, RMSE: 0.1294, R2: 0.7481), Valid: (LOSS: 0.0812, MAE: 0.0812, RMSE: 0.1317, R2: 0.7440), PNorm: 167.1760, GNorm: 1.6096
[26/299] timecost: 67.81, lr: 0.000030, Train: (LOSS: 0.0827, MAE: 0.0827, RMSE: 0.1256, R2: 0.7740), Valid: (LOSS: 0.0818, MAE: 0.0818, RMSE: 0.1188, R2: 0.7775), PNorm: 166.9580, GNorm: 2.1939
[27/299] timecost: 68.08, lr: 0.000030, Train: (LOSS: 0.0782, MAE: 0.0782, RMSE: 0.1175, R2: 0.7959), Valid: (LOSS: 0.0936, MAE: 0.0936, RMSE: 0.1350, R2: 0.7296), PNorm: 166.7423, GNorm: 2.5096
[28/299] timecost: 68.09, lr: 0.000030, Train: (LOSS: 0.0792, MAE: 0.0792, RMSE: 0.1210, R2: 0.7808), Valid: (LOSS: 0.0830, MAE: 0.0830, RMSE: 0.1216, R2: 0.7774), PNorm: 166.5286, GNorm: 0.8414
[29/299] timecost: 63.85, lr: 0.000030, Train: (LOSS: 0.0746, MAE: 0.0746, RMSE: 0.1150, R2: 0.8028), Valid: (LOSS: 0.0901, MAE: 0.0901, RMSE: 0.1359, R2: 0.7329), PNorm: 166.3181, GNorm: 2.0621
[30/299] timecost: 63.38, lr: 0.000030, Train: (LOSS: 0.0718, MAE: 0.0718, RMSE: 0.1108, R2: 0.8143), Valid: (LOSS: 0.0819, MAE: 0.0819, RMSE: 0.1252, R2: 0.7604), PNorm: 166.1085, GNorm: 2.8088
[31/299] timecost: 63.77, lr: 0.000030, Train: (LOSS: 0.0712, MAE: 0.0712, RMSE: 0.1105, R2: 0.8154), Valid: (LOSS: 0.0806, MAE: 0.0806, RMSE: 0.1295, R2: 0.7525), PNorm: 165.9015, GNorm: 1.6051
[32/299] timecost: 63.14, lr: 0.000030, Train: (LOSS: 0.0695, MAE: 0.0695, RMSE: 0.1086, R2: 0.8209), Valid: (LOSS: 0.0697, MAE: 0.0697, RMSE: 0.1122, R2: 0.8071), PNorm: 165.6960, GNorm: 1.9063
[33/299] timecost: 63.33, lr: 0.000030, Train: (LOSS: 0.0672, MAE: 0.0672, RMSE: 0.1051, R2: 0.8292), Valid: (LOSS: 0.0684, MAE: 0.0684, RMSE: 0.1051, R2: 0.8241), PNorm: 165.4926, GNorm: 5.7417
[34/299] timecost: 63.64, lr: 0.000030, Train: (LOSS: 0.0685, MAE: 0.0685, RMSE: 0.1061, R2: 0.8319), Valid: (LOSS: 0.0822, MAE: 0.0822, RMSE: 0.1276, R2: 0.7586), PNorm: 165.2919, GNorm: 3.0811
[35/299] timecost: 63.50, lr: 0.000030, Train: (LOSS: 0.0645, MAE: 0.0645, RMSE: 0.1023, R2: 0.8428), Valid: (LOSS: 0.0684, MAE: 0.0684, RMSE: 0.1105, R2: 0.8112), PNorm: 165.0925, GNorm: 2.2491
[36/299] timecost: 63.54, lr: 0.000030, Train: (LOSS: 0.0624, MAE: 0.0624, RMSE: 0.0984, R2: 0.8525), Valid: (LOSS: 0.0674, MAE: 0.0674, RMSE: 0.1053, R2: 0.8234), PNorm: 164.8945, GNorm: 1.6523
[37/299] timecost: 63.48, lr: 0.000030, Train: (LOSS: 0.0609, MAE: 0.0609, RMSE: 0.0978, R2: 0.8557), Valid: (LOSS: 0.0648, MAE: 0.0648, RMSE: 0.1038, R2: 0.8307), PNorm: 164.6982, GNorm: 1.5084
[38/299] timecost: 64.52, lr: 0.000030, Train: (LOSS: 0.0629, MAE: 0.0629, RMSE: 0.0983, R2: 0.8487), Valid: (LOSS: 0.0718, MAE: 0.0718, RMSE: 0.1125, R2: 0.8042), PNorm: 164.5040, GNorm: 1.5540
[39/299] timecost: 65.51, lr: 0.000030, Train: (LOSS: 0.0592, MAE: 0.0592, RMSE: 0.0946, R2: 0.8618), Valid: (LOSS: 0.0655, MAE: 0.0655, RMSE: 0.1091, R2: 0.8059), PNorm: 164.3107, GNorm: 4.6373
[40/299] timecost: 65.35, lr: 0.000030, Train: (LOSS: 0.0573, MAE: 0.0573, RMSE: 0.0905, R2: 0.8731), Valid: (LOSS: 0.0614, MAE: 0.0614, RMSE: 0.0945, R2: 0.8560), PNorm: 164.1195, GNorm: 1.6404
[41/299] timecost: 65.57, lr: 0.000030, Train: (LOSS: 0.0551, MAE: 0.0551, RMSE: 0.0880, R2: 0.8784), Valid: (LOSS: 0.0627, MAE: 0.0627, RMSE: 0.1027, R2: 0.8264), PNorm: 163.9290, GNorm: 1.1802
[42/299] timecost: 65.50, lr: 0.000030, Train: (LOSS: 0.0551, MAE: 0.0551, RMSE: 0.0876, R2: 0.8802), Valid: (LOSS: 0.0582, MAE: 0.0582, RMSE: 0.0922, R2: 0.8619), PNorm: 163.7403, GNorm: 1.5172
[43/299] timecost: 65.09, lr: 0.000030, Train: (LOSS: 0.0531, MAE: 0.0531, RMSE: 0.0828, R2: 0.8932), Valid: (LOSS: 0.0607, MAE: 0.0607, RMSE: 0.0959, R2: 0.8570), PNorm: 163.5526, GNorm: 1.2962
[44/299] timecost: 65.64, lr: 0.000030, Train: (LOSS: 0.0521, MAE: 0.0521, RMSE: 0.0834, R2: 0.8942), Valid: (LOSS: 0.0555, MAE: 0.0555, RMSE: 0.0891, R2: 0.8709), PNorm: 163.3667, GNorm: 1.1027
[45/299] timecost: 66.75, lr: 0.000030, Train: (LOSS: 0.0561, MAE: 0.0561, RMSE: 0.0882, R2: 0.8793), Valid: (LOSS: 0.0606, MAE: 0.0606, RMSE: 0.0965, R2: 0.8547), PNorm: 163.1825, GNorm: 1.4170
[46/299] timecost: 67.14, lr: 0.000030, Train: (LOSS: 0.0518, MAE: 0.0518, RMSE: 0.0814, R2: 0.8961), Valid: (LOSS: 0.0557, MAE: 0.0557, RMSE: 0.0866, R2: 0.8800), PNorm: 163.0000, GNorm: 1.4164
[47/299] timecost: 67.07, lr: 0.000030, Train: (LOSS: 0.0493, MAE: 0.0493, RMSE: 0.0789, R2: 0.9051), Valid: (LOSS: 0.0579, MAE: 0.0579, RMSE: 0.0932, R2: 0.8598), PNorm: 162.8173, GNorm: 2.3312
[48/299] timecost: 66.77, lr: 0.000030, Train: (LOSS: 0.0472, MAE: 0.0472, RMSE: 0.0746, R2: 0.9128), Valid: (LOSS: 0.0573, MAE: 0.0573, RMSE: 0.0953, R2: 0.8558), PNorm: 162.6356, GNorm: 1.6494
[49/299] timecost: 64.90, lr: 0.000030, Train: (LOSS: 0.0477, MAE: 0.0477, RMSE: 0.0747, R2: 0.9097), Valid: (LOSS: 0.0543, MAE: 0.0543, RMSE: 0.0871, R2: 0.8787), PNorm: 162.4548, GNorm: 2.0257
[50/299] timecost: 63.09, lr: 0.000030, Train: (LOSS: 0.0493, MAE: 0.0493, RMSE: 0.0775, R2: 0.9059), Valid: (LOSS: 0.0557, MAE: 0.0557, RMSE: 0.0905, R2: 0.8606), PNorm: 162.2757, GNorm: 1.2173
[51/299] timecost: 63.18, lr: 0.000030, Train: (LOSS: 0.0470, MAE: 0.0470, RMSE: 0.0770, R2: 0.9097), Valid: (LOSS: 0.0540, MAE: 0.0540, RMSE: 0.0902, R2: 0.8668), PNorm: 162.0968, GNorm: 1.2850
[52/299] timecost: 63.63, lr: 0.000030, Train: (LOSS: 0.0444, MAE: 0.0444, RMSE: 0.0716, R2: 0.9233), Valid: (LOSS: 0.0523, MAE: 0.0523, RMSE: 0.0856, R2: 0.8789), PNorm: 161.9178, GNorm: 1.4371
[53/299] timecost: 66.77, lr: 0.000030, Train: (LOSS: 0.0435, MAE: 0.0435, RMSE: 0.0697, R2: 0.9245), Valid: (LOSS: 0.0570, MAE: 0.0570, RMSE: 0.0882, R2: 0.8799), PNorm: 161.7395, GNorm: 1.3539
[54/299] timecost: 63.21, lr: 0.000030, Train: (LOSS: 0.0446, MAE: 0.0446, RMSE: 0.0714, R2: 0.9225), Valid: (LOSS: 0.0506, MAE: 0.0506, RMSE: 0.0864, R2: 0.8749), PNorm: 161.5619, GNorm: 1.4593
[55/299] timecost: 63.13, lr: 0.000030, Train: (LOSS: 0.0424, MAE: 0.0424, RMSE: 0.0668, R2: 0.9281), Valid: (LOSS: 0.0476, MAE: 0.0476, RMSE: 0.0794, R2: 0.8964), PNorm: 161.3846, GNorm: 1.7363
[56/299] timecost: 62.92, lr: 0.000030, Train: (LOSS: 0.0419, MAE: 0.0419, RMSE: 0.0658, R2: 0.9288), Valid: (LOSS: 0.0565, MAE: 0.0565, RMSE: 0.0964, R2: 0.8467), PNorm: 161.2077, GNorm: 1.0796
[57/299] timecost: 63.15, lr: 0.000030, Train: (LOSS: 0.0420, MAE: 0.0420, RMSE: 0.0668, R2: 0.9294), Valid: (LOSS: 0.0505, MAE: 0.0505, RMSE: 0.0850, R2: 0.8791), PNorm: 161.0316, GNorm: 1.1478
[58/299] timecost: 62.97, lr: 0.000030, Train: (LOSS: 0.0438, MAE: 0.0438, RMSE: 0.0691, R2: 0.9262), Valid: (LOSS: 0.0536, MAE: 0.0536, RMSE: 0.0884, R2: 0.8748), PNorm: 160.8554, GNorm: 1.3320
[59/299] timecost: 63.71, lr: 0.000030, Train: (LOSS: 0.0424, MAE: 0.0424, RMSE: 0.0667, R2: 0.9276), Valid: (LOSS: 0.0487, MAE: 0.0487, RMSE: 0.0827, R2: 0.8868), PNorm: 160.6798, GNorm: 1.1473
[60/299] timecost: 63.70, lr: 0.000030, Train: (LOSS: 0.0399, MAE: 0.0399, RMSE: 0.0642, R2: 0.9364), Valid: (LOSS: 0.0484, MAE: 0.0484, RMSE: 0.0818, R2: 0.8884), PNorm: 160.5039, GNorm: 1.4938
[61/299] timecost: 62.66, lr: 0.000030, Train: (LOSS: 0.0378, MAE: 0.0378, RMSE: 0.0620, R2: 0.9398), Valid: (LOSS: 0.0477, MAE: 0.0477, RMSE: 0.0801, R2: 0.8940), PNorm: 160.3281, GNorm: 1.3492
[62/299] timecost: 62.89, lr: 0.000030, Train: (LOSS: 0.0414, MAE: 0.0414, RMSE: 0.0661, R2: 0.9307), Valid: (LOSS: 0.0511, MAE: 0.0511, RMSE: 0.0865, R2: 0.8733), PNorm: 160.1529, GNorm: 1.8894
[63/299] timecost: 64.94, lr: 0.000030, Train: (LOSS: 0.0409, MAE: 0.0409, RMSE: 0.0641, R2: 0.9335), Valid: (LOSS: 0.0464, MAE: 0.0464, RMSE: 0.0803, R2: 0.8929), PNorm: 159.9774, GNorm: 1.1891
[64/299] timecost: 66.91, lr: 0.000030, Train: (LOSS: 0.0385, MAE: 0.0385, RMSE: 0.0626, R2: 0.9363), Valid: (LOSS: 0.0492, MAE: 0.0492, RMSE: 0.0787, R2: 0.9018), PNorm: 159.8011, GNorm: 1.2093
[65/299] timecost: 67.13, lr: 0.000030, Train: (LOSS: 0.0390, MAE: 0.0390, RMSE: 0.0621, R2: 0.9381), Valid: (LOSS: 0.0462, MAE: 0.0462, RMSE: 0.0761, R2: 0.9057), PNorm: 159.6254, GNorm: 1.3233
[66/299] timecost: 66.87, lr: 0.000030, Train: (LOSS: 0.0391, MAE: 0.0391, RMSE: 0.0633, R2: 0.9335), Valid: (LOSS: 0.0474, MAE: 0.0474, RMSE: 0.0799, R2: 0.8962), PNorm: 159.4498, GNorm: 1.1785
[67/299] timecost: 67.21, lr: 0.000030, Train: (LOSS: 0.0368, MAE: 0.0368, RMSE: 0.0586, R2: 0.9446), Valid: (LOSS: 0.0465, MAE: 0.0465, RMSE: 0.0780, R2: 0.8982), PNorm: 159.2734, GNorm: 1.3053
[68/299] timecost: 66.91, lr: 0.000030, Train: (LOSS: 0.0357, MAE: 0.0357, RMSE: 0.0580, R2: 0.9464), Valid: (LOSS: 0.0506, MAE: 0.0506, RMSE: 0.0835, R2: 0.8844), PNorm: 159.0958, GNorm: 1.3882
[69/299] timecost: 66.95, lr: 0.000030, Train: (LOSS: 0.0366, MAE: 0.0366, RMSE: 0.0589, R2: 0.9457), Valid: (LOSS: 0.0470, MAE: 0.0470, RMSE: 0.0770, R2: 0.9070), PNorm: 158.9186, GNorm: 1.2343
[70/299] timecost: 66.62, lr: 0.000030, Train: (LOSS: 0.0350, MAE: 0.0350, RMSE: 0.0573, R2: 0.9477), Valid: (LOSS: 0.0435, MAE: 0.0435, RMSE: 0.0742, R2: 0.9069), PNorm: 158.7400, GNorm: 1.1361
[71/299] timecost: 66.88, lr: 0.000030, Train: (LOSS: 0.0353, MAE: 0.0353, RMSE: 0.0572, R2: 0.9462), Valid: (LOSS: 0.0505, MAE: 0.0505, RMSE: 0.0809, R2: 0.8937), PNorm: 158.5610, GNorm: 1.6049
[72/299] timecost: 67.74, lr: 0.000030, Train: (LOSS: 0.0346, MAE: 0.0346, RMSE: 0.0558, R2: 0.9510), Valid: (LOSS: 0.0480, MAE: 0.0480, RMSE: 0.0804, R2: 0.8945), PNorm: 158.3815, GNorm: 1.3987
[73/299] timecost: 65.43, lr: 0.000030, Train: (LOSS: 0.0347, MAE: 0.0347, RMSE: 0.0547, R2: 0.9483), Valid: (LOSS: 0.0468, MAE: 0.0468, RMSE: 0.0779, R2: 0.9022), PNorm: 158.2017, GNorm: 1.3272
[74/299] timecost: 63.08, lr: 0.000030, Train: (LOSS: 0.0344, MAE: 0.0344, RMSE: 0.0564, R2: 0.9467), Valid: (LOSS: 0.0463, MAE: 0.0463, RMSE: 0.0773, R2: 0.8986), PNorm: 158.0203, GNorm: 1.0261
[75/299] timecost: 62.97, lr: 0.000030, Train: (LOSS: 0.0340, MAE: 0.0340, RMSE: 0.0548, R2: 0.9504), Valid: (LOSS: 0.0468, MAE: 0.0468, RMSE: 0.0810, R2: 0.8913), PNorm: 157.8383, GNorm: 1.2200
[76/299] timecost: 62.85, lr: 0.000030, Train: (LOSS: 0.0342, MAE: 0.0342, RMSE: 0.0561, R2: 0.9497), Valid: (LOSS: 0.0472, MAE: 0.0472, RMSE: 0.0779, R2: 0.9046), PNorm: 157.6557, GNorm: 1.3657
[77/299] timecost: 62.82, lr: 0.000030, Train: (LOSS: 0.0347, MAE: 0.0347, RMSE: 0.0560, R2: 0.9480), Valid: (LOSS: 0.0432, MAE: 0.0432, RMSE: 0.0743, R2: 0.9091), PNorm: 157.4731, GNorm: 1.1031
[78/299] timecost: 63.13, lr: 0.000030, Train: (LOSS: 0.0326, MAE: 0.0326, RMSE: 0.0531, R2: 0.9544), Valid: (LOSS: 0.0434, MAE: 0.0434, RMSE: 0.0758, R2: 0.9042), PNorm: 157.2881, GNorm: 0.9678
[79/299] timecost: 63.18, lr: 0.000030, Train: (LOSS: 0.0329, MAE: 0.0329, RMSE: 0.0534, R2: 0.9547), Valid: (LOSS: 0.0496, MAE: 0.0496, RMSE: 0.0850, R2: 0.8819), PNorm: 157.1030, GNorm: 1.3379
[80/299] timecost: 63.23, lr: 0.000030, Train: (LOSS: 0.0323, MAE: 0.0323, RMSE: 0.0525, R2: 0.9538), Valid: (LOSS: 0.0471, MAE: 0.0471, RMSE: 0.0804, R2: 0.8927), PNorm: 156.9176, GNorm: 1.4369
[81/299] timecost: 63.14, lr: 0.000030, Train: (LOSS: 0.0311, MAE: 0.0311, RMSE: 0.0501, R2: 0.9590), Valid: (LOSS: 0.0413, MAE: 0.0413, RMSE: 0.0677, R2: 0.9228), PNorm: 156.7298, GNorm: 1.0532
[82/299] timecost: 63.22, lr: 0.000030, Train: (LOSS: 0.0307, MAE: 0.0307, RMSE: 0.0504, R2: 0.9570), Valid: (LOSS: 0.0460, MAE: 0.0460, RMSE: 0.0782, R2: 0.8988), PNorm: 156.5415, GNorm: 1.4365
[83/299] timecost: 66.80, lr: 0.000030, Train: (LOSS: 0.0322, MAE: 0.0322, RMSE: 0.0517, R2: 0.9551), Valid: (LOSS: 0.0435, MAE: 0.0435, RMSE: 0.0750, R2: 0.9072), PNorm: 156.3520, GNorm: 1.3468
[84/299] timecost: 67.49, lr: 0.000030, Train: (LOSS: 0.0303, MAE: 0.0303, RMSE: 0.0495, R2: 0.9592), Valid: (LOSS: 0.0448, MAE: 0.0448, RMSE: 0.0772, R2: 0.9001), PNorm: 156.1615, GNorm: 1.0597
[85/299] timecost: 67.50, lr: 0.000030, Train: (LOSS: 0.0316, MAE: 0.0316, RMSE: 0.0517, R2: 0.9573), Valid: (LOSS: 0.0435, MAE: 0.0435, RMSE: 0.0708, R2: 0.9177), PNorm: 155.9702, GNorm: 1.1482
[86/299] timecost: 65.99, lr: 0.000030, Train: (LOSS: 0.0307, MAE: 0.0307, RMSE: 0.0499, R2: 0.9578), Valid: (LOSS: 0.0436, MAE: 0.0436, RMSE: 0.0739, R2: 0.9099), PNorm: 155.7774, GNorm: 1.3465
[87/299] timecost: 63.05, lr: 0.000030, Train: (LOSS: 0.0300, MAE: 0.0300, RMSE: 0.0494, R2: 0.9589), Valid: (LOSS: 0.0430, MAE: 0.0430, RMSE: 0.0764, R2: 0.8975), PNorm: 155.5836, GNorm: 1.5862
[88/299] timecost: 63.12, lr: 0.000030, Train: (LOSS: 0.0304, MAE: 0.0304, RMSE: 0.0493, R2: 0.9606), Valid: (LOSS: 0.0469, MAE: 0.0469, RMSE: 0.0793, R2: 0.8954), PNorm: 155.3884, GNorm: 1.5669
[89/299] timecost: 63.99, lr: 0.000030, Train: (LOSS: 0.0302, MAE: 0.0302, RMSE: 0.0499, R2: 0.9588), Valid: (LOSS: 0.0424, MAE: 0.0424, RMSE: 0.0699, R2: 0.9184), PNorm: 155.1922, GNorm: 0.9196
[90/299] timecost: 64.36, lr: 0.000030, Train: (LOSS: 0.0306, MAE: 0.0306, RMSE: 0.0489, R2: 0.9582), Valid: (LOSS: 0.0469, MAE: 0.0469, RMSE: 0.0828, R2: 0.8854), PNorm: 154.9943, GNorm: 1.3763
[91/299] timecost: 64.61, lr: 0.000030, Train: (LOSS: 0.0300, MAE: 0.0300, RMSE: 0.0488, R2: 0.9599), Valid: (LOSS: 0.0425, MAE: 0.0425, RMSE: 0.0709, R2: 0.9148), PNorm: 154.7960, GNorm: 1.3364
[92/299] timecost: 64.98, lr: 0.000030, Train: (LOSS: 0.0304, MAE: 0.0304, RMSE: 0.0502, R2: 0.9583), Valid: (LOSS: 0.0444, MAE: 0.0444, RMSE: 0.0735, R2: 0.9130), PNorm: 154.5969, GNorm: 1.0105
[93/299] timecost: 65.32, lr: 0.000030, Train: (LOSS: 0.0307, MAE: 0.0307, RMSE: 0.0495, R2: 0.9588), Valid: (LOSS: 0.0419, MAE: 0.0419, RMSE: 0.0705, R2: 0.9176), PNorm: 154.3966, GNorm: 1.4650
[94/299] timecost: 65.24, lr: 0.000030, Train: (LOSS: 0.0314, MAE: 0.0314, RMSE: 0.0511, R2: 0.9572), Valid: (LOSS: 0.0456, MAE: 0.0456, RMSE: 0.0768, R2: 0.8980), PNorm: 154.1952, GNorm: 1.0173
[95/299] timecost: 65.04, lr: 0.000030, Train: (LOSS: 0.0295, MAE: 0.0295, RMSE: 0.0477, R2: 0.9613), Valid: (LOSS: 0.0476, MAE: 0.0476, RMSE: 0.0796, R2: 0.8964), PNorm: 153.9923, GNorm: 0.9709
[96/299] timecost: 66.86, lr: 0.000030, Train: (LOSS: 0.0291, MAE: 0.0291, RMSE: 0.0478, R2: 0.9620), Valid: (LOSS: 0.0430, MAE: 0.0430, RMSE: 0.0764, R2: 0.9030), PNorm: 153.7893, GNorm: 0.8909
[97/299] timecost: 66.00, lr: 0.000030, Train: (LOSS: 0.0281, MAE: 0.0281, RMSE: 0.0471, R2: 0.9620), Valid: (LOSS: 0.0452, MAE: 0.0452, RMSE: 0.0778, R2: 0.9021), PNorm: 153.5845, GNorm: 1.4529
[98/299] timecost: 65.88, lr: 0.000030, Train: (LOSS: 0.0285, MAE: 0.0285, RMSE: 0.0478, R2: 0.9619), Valid: (LOSS: 0.0438, MAE: 0.0438, RMSE: 0.0763, R2: 0.9035), PNorm: 153.3784, GNorm: 1.1282
[99/299] timecost: 66.14, lr: 0.000030, Train: (LOSS: 0.0272, MAE: 0.0272, RMSE: 0.0450, R2: 0.9640), Valid: (LOSS: 0.0437, MAE: 0.0437, RMSE: 0.0743, R2: 0.9061), PNorm: 153.1710, GNorm: 0.7392
[100/299] timecost: 66.50, lr: 0.000030, Train: (LOSS: 0.0274, MAE: 0.0274, RMSE: 0.0456, R2: 0.9640), Valid: (LOSS: 0.0421, MAE: 0.0421, RMSE: 0.0706, R2: 0.9192), PNorm: 152.9628, GNorm: 0.9095
[101/299] timecost: 66.11, lr: 0.000030, Train: (LOSS: 0.0265, MAE: 0.0265, RMSE: 0.0451, R2: 0.9629), Valid: (LOSS: 0.0435, MAE: 0.0435, RMSE: 0.0746, R2: 0.9046), PNorm: 152.7530, GNorm: 0.9297
Epoch 00103: reducing learning rate of group 0 to 2.7000e-05.
[102/299] timecost: 65.85, lr: 0.000027, Train: (LOSS: 0.0286, MAE: 0.0286, RMSE: 0.0479, R2: 0.9600), Valid: (LOSS: 0.0451, MAE: 0.0451, RMSE: 0.0768, R2: 0.9017), PNorm: 152.5433, GNorm: 0.9732
[103/299] timecost: 66.21, lr: 0.000027, Train: (LOSS: 0.0265, MAE: 0.0265, RMSE: 0.0438, R2: 0.9670), Valid: (LOSS: 0.0442, MAE: 0.0442, RMSE: 0.0749, R2: 0.9070), PNorm: 152.3529, GNorm: 1.4701
[104/299] timecost: 66.03, lr: 0.000027, Train: (LOSS: 0.0256, MAE: 0.0256, RMSE: 0.0436, R2: 0.9675), Valid: (LOSS: 0.0442, MAE: 0.0442, RMSE: 0.0769, R2: 0.9013), PNorm: 152.1611, GNorm: 1.1685
[105/299] timecost: 66.38, lr: 0.000027, Train: (LOSS: 0.0268, MAE: 0.0268, RMSE: 0.0452, R2: 0.9651), Valid: (LOSS: 0.0466, MAE: 0.0466, RMSE: 0.0791, R2: 0.8997), PNorm: 151.9685, GNorm: 1.1116
[106/299] timecost: 65.98, lr: 0.000027, Train: (LOSS: 0.0253, MAE: 0.0253, RMSE: 0.0419, R2: 0.9690), Valid: (LOSS: 0.0438, MAE: 0.0438, RMSE: 0.0754, R2: 0.9043), PNorm: 151.7741, GNorm: 0.9654
[107/299] timecost: 66.58, lr: 0.000027, Train: (LOSS: 0.0245, MAE: 0.0245, RMSE: 0.0415, R2: 0.9685), Valid: (LOSS: 0.0413, MAE: 0.0413, RMSE: 0.0712, R2: 0.9176), PNorm: 151.5789, GNorm: 0.9626
[108/299] timecost: 66.21, lr: 0.000027, Train: (LOSS: 0.0255, MAE: 0.0255, RMSE: 0.0438, R2: 0.9659), Valid: (LOSS: 0.0449, MAE: 0.0449, RMSE: 0.0799, R2: 0.8937), PNorm: 151.3824, GNorm: 1.1413
[109/299] timecost: 65.04, lr: 0.000027, Train: (LOSS: 0.0246, MAE: 0.0246, RMSE: 0.0416, R2: 0.9686), Valid: (LOSS: 0.0415, MAE: 0.0415, RMSE: 0.0708, R2: 0.9166), PNorm: 151.1854, GNorm: 1.3190
[110/299] timecost: 62.92, lr: 0.000027, Train: (LOSS: 0.0248, MAE: 0.0248, RMSE: 0.0420, R2: 0.9694), Valid: (LOSS: 0.0403, MAE: 0.0403, RMSE: 0.0687, R2: 0.9228), PNorm: 150.9863, GNorm: 0.8858
[111/299] timecost: 62.66, lr: 0.000027, Train: (LOSS: 0.0245, MAE: 0.0245, RMSE: 0.0403, R2: 0.9687), Valid: (LOSS: 0.0398, MAE: 0.0398, RMSE: 0.0689, R2: 0.9224), PNorm: 150.7874, GNorm: 1.0440
[112/299] timecost: 62.61, lr: 0.000027, Train: (LOSS: 0.0241, MAE: 0.0241, RMSE: 0.0402, R2: 0.9702), Valid: (LOSS: 0.0394, MAE: 0.0394, RMSE: 0.0690, R2: 0.9218), PNorm: 150.5872, GNorm: 0.9901
[113/299] timecost: 62.69, lr: 0.000027, Train: (LOSS: 0.0245, MAE: 0.0245, RMSE: 0.0417, R2: 0.9696), Valid: (LOSS: 0.0413, MAE: 0.0413, RMSE: 0.0712, R2: 0.9162), PNorm: 150.3867, GNorm: 1.4107
[114/299] timecost: 62.91, lr: 0.000027, Train: (LOSS: 0.0233, MAE: 0.0233, RMSE: 0.0396, R2: 0.9720), Valid: (LOSS: 0.0395, MAE: 0.0395, RMSE: 0.0669, R2: 0.9246), PNorm: 150.1855, GNorm: 0.9902
[115/299] timecost: 63.15, lr: 0.000027, Train: (LOSS: 0.0245, MAE: 0.0245, RMSE: 0.0417, R2: 0.9689), Valid: (LOSS: 0.0419, MAE: 0.0419, RMSE: 0.0700, R2: 0.9204), PNorm: 149.9835, GNorm: 1.2572
[116/299] timecost: 62.64, lr: 0.000027, Train: (LOSS: 0.0247, MAE: 0.0247, RMSE: 0.0415, R2: 0.9705), Valid: (LOSS: 0.0412, MAE: 0.0412, RMSE: 0.0709, R2: 0.9168), PNorm: 149.7815, GNorm: 1.6829
[117/299] timecost: 62.51, lr: 0.000027, Train: (LOSS: 0.0232, MAE: 0.0232, RMSE: 0.0392, R2: 0.9706), Valid: (LOSS: 0.0393, MAE: 0.0393, RMSE: 0.0692, R2: 0.9226), PNorm: 149.5781, GNorm: 1.2920
[118/299] timecost: 63.04, lr: 0.000027, Train: (LOSS: 0.0235, MAE: 0.0235, RMSE: 0.0405, R2: 0.9701), Valid: (LOSS: 0.0411, MAE: 0.0411, RMSE: 0.0696, R2: 0.9217), PNorm: 149.3749, GNorm: 1.1836
[119/299] timecost: 63.00, lr: 0.000027, Train: (LOSS: 0.0233, MAE: 0.0233, RMSE: 0.0397, R2: 0.9699), Valid: (LOSS: 0.0430, MAE: 0.0430, RMSE: 0.0752, R2: 0.9086), PNorm: 149.1706, GNorm: 0.7384
[120/299] timecost: 63.07, lr: 0.000027, Train: (LOSS: 0.0237, MAE: 0.0237, RMSE: 0.0399, R2: 0.9713), Valid: (LOSS: 0.0398, MAE: 0.0398, RMSE: 0.0683, R2: 0.9221), PNorm: 148.9669, GNorm: 1.0131
[121/299] timecost: 65.05, lr: 0.000027, Train: (LOSS: 0.0233, MAE: 0.0233, RMSE: 0.0392, R2: 0.9720), Valid: (LOSS: 0.0399, MAE: 0.0399, RMSE: 0.0691, R2: 0.9215), PNorm: 148.7622, GNorm: 0.9199
[122/299] timecost: 62.85, lr: 0.000027, Train: (LOSS: 0.0228, MAE: 0.0228, RMSE: 0.0379, R2: 0.9728), Valid: (LOSS: 0.0403, MAE: 0.0403, RMSE: 0.0672, R2: 0.9258), PNorm: 148.5579, GNorm: 1.2185
[123/299] timecost: 63.10, lr: 0.000027, Train: (LOSS: 0.0232, MAE: 0.0232, RMSE: 0.0392, R2: 0.9727), Valid: (LOSS: 0.0413, MAE: 0.0413, RMSE: 0.0740, R2: 0.9112), PNorm: 148.3530, GNorm: 1.0295
[124/299] timecost: 63.12, lr: 0.000027, Train: (LOSS: 0.0228, MAE: 0.0228, RMSE: 0.0386, R2: 0.9732), Valid: (LOSS: 0.0408, MAE: 0.0408, RMSE: 0.0687, R2: 0.9224), PNorm: 148.1480, GNorm: 0.9020
[125/299] timecost: 62.66, lr: 0.000027, Train: (LOSS: 0.0226, MAE: 0.0226, RMSE: 0.0386, R2: 0.9723), Valid: (LOSS: 0.0399, MAE: 0.0399, RMSE: 0.0683, R2: 0.9237), PNorm: 147.9433, GNorm: 1.7125
[126/299] timecost: 62.76, lr: 0.000027, Train: (LOSS: 0.0227, MAE: 0.0227, RMSE: 0.0373, R2: 0.9732), Valid: (LOSS: 0.0402, MAE: 0.0402, RMSE: 0.0711, R2: 0.9200), PNorm: 147.7382, GNorm: 0.8586
[127/299] timecost: 62.55, lr: 0.000027, Train: (LOSS: 0.0223, MAE: 0.0223, RMSE: 0.0374, R2: 0.9738), Valid: (LOSS: 0.0400, MAE: 0.0400, RMSE: 0.0660, R2: 0.9285), PNorm: 147.5336, GNorm: 1.1395
[128/299] timecost: 62.68, lr: 0.000027, Train: (LOSS: 0.0229, MAE: 0.0229, RMSE: 0.0387, R2: 0.9730), Valid: (LOSS: 0.0391, MAE: 0.0391, RMSE: 0.0664, R2: 0.9267), PNorm: 147.3301, GNorm: 1.0612
[129/299] timecost: 62.65, lr: 0.000027, Train: (LOSS: 0.0223, MAE: 0.0223, RMSE: 0.0383, R2: 0.9711), Valid: (LOSS: 0.0403, MAE: 0.0403, RMSE: 0.0716, R2: 0.9146), PNorm: 147.1259, GNorm: 1.1017
[130/299] timecost: 62.71, lr: 0.000027, Train: (LOSS: 0.0217, MAE: 0.0217, RMSE: 0.0366, R2: 0.9749), Valid: (LOSS: 0.0381, MAE: 0.0381, RMSE: 0.0651, R2: 0.9288), PNorm: 146.9226, GNorm: 1.1565
[131/299] timecost: 62.74, lr: 0.000027, Train: (LOSS: 0.0215, MAE: 0.0215, RMSE: 0.0360, R2: 0.9758), Valid: (LOSS: 0.0381, MAE: 0.0381, RMSE: 0.0660, R2: 0.9286), PNorm: 146.7194, GNorm: 1.4867
[132/299] timecost: 62.80, lr: 0.000027, Train: (LOSS: 0.0220, MAE: 0.0220, RMSE: 0.0362, R2: 0.9744), Valid: (LOSS: 0.0385, MAE: 0.0385, RMSE: 0.0646, R2: 0.9324), PNorm: 146.5172, GNorm: 1.4537
[133/299] timecost: 62.81, lr: 0.000027, Train: (LOSS: 0.0213, MAE: 0.0213, RMSE: 0.0356, R2: 0.9751), Valid: (LOSS: 0.0422, MAE: 0.0422, RMSE: 0.0732, R2: 0.9118), PNorm: 146.3149, GNorm: 0.8512
[134/299] timecost: 63.06, lr: 0.000027, Train: (LOSS: 0.0206, MAE: 0.0206, RMSE: 0.0356, R2: 0.9768), Valid: (LOSS: 0.0372, MAE: 0.0372, RMSE: 0.0644, R2: 0.9313), PNorm: 146.1134, GNorm: 1.1968
[135/299] timecost: 62.88, lr: 0.000027, Train: (LOSS: 0.0208, MAE: 0.0208, RMSE: 0.0342, R2: 0.9767), Valid: (LOSS: 0.0383, MAE: 0.0383, RMSE: 0.0682, R2: 0.9231), PNorm: 145.9128, GNorm: 1.1909
[136/299] timecost: 66.41, lr: 0.000027, Train: (LOSS: 0.0205, MAE: 0.0205, RMSE: 0.0343, R2: 0.9796), Valid: (LOSS: 0.0383, MAE: 0.0383, RMSE: 0.0654, R2: 0.9309), PNorm: 145.7132, GNorm: 1.0554
[137/299] timecost: 66.99, lr: 0.000027, Train: (LOSS: 0.0203, MAE: 0.0203, RMSE: 0.0349, R2: 0.9776), Valid: (LOSS: 0.0395, MAE: 0.0395, RMSE: 0.0686, R2: 0.9242), PNorm: 145.5146, GNorm: 0.6507
[138/299] timecost: 67.21, lr: 0.000027, Train: (LOSS: 0.0206, MAE: 0.0206, RMSE: 0.0348, R2: 0.9785), Valid: (LOSS: 0.0393, MAE: 0.0393, RMSE: 0.0669, R2: 0.9256), PNorm: 145.3168, GNorm: 0.9175
[139/299] timecost: 67.20, lr: 0.000027, Train: (LOSS: 0.0195, MAE: 0.0195, RMSE: 0.0339, R2: 0.9787), Valid: (LOSS: 0.0394, MAE: 0.0394, RMSE: 0.0642, R2: 0.9313), PNorm: 145.1193, GNorm: 0.8035
[140/299] timecost: 67.10, lr: 0.000027, Train: (LOSS: 0.0203, MAE: 0.0203, RMSE: 0.0345, R2: 0.9775), Valid: (LOSS: 0.0381, MAE: 0.0381, RMSE: 0.0660, R2: 0.9262), PNorm: 144.9238, GNorm: 0.8044
[141/299] timecost: 63.53, lr: 0.000027, Train: (LOSS: 0.0196, MAE: 0.0196, RMSE: 0.0330, R2: 0.9788), Valid: (LOSS: 0.0419, MAE: 0.0419, RMSE: 0.0713, R2: 0.9148), PNorm: 144.7286, GNorm: 0.8751
[142/299] timecost: 63.34, lr: 0.000027, Train: (LOSS: 0.0198, MAE: 0.0198, RMSE: 0.0331, R2: 0.9792), Valid: (LOSS: 0.0389, MAE: 0.0389, RMSE: 0.0643, R2: 0.9330), PNorm: 144.5364, GNorm: 0.7995
[143/299] timecost: 63.05, lr: 0.000027, Train: (LOSS: 0.0204, MAE: 0.0204, RMSE: 0.0337, R2: 0.9793), Valid: (LOSS: 0.0376, MAE: 0.0376, RMSE: 0.0647, R2: 0.9320), PNorm: 144.3440, GNorm: 1.1451
[144/299] timecost: 62.97, lr: 0.000027, Train: (LOSS: 0.0196, MAE: 0.0196, RMSE: 0.0329, R2: 0.9803), Valid: (LOSS: 0.0385, MAE: 0.0385, RMSE: 0.0645, R2: 0.9317), PNorm: 144.1535, GNorm: 1.1911
[145/299] timecost: 65.48, lr: 0.000027, Train: (LOSS: 0.0189, MAE: 0.0189, RMSE: 0.0324, R2: 0.9801), Valid: (LOSS: 0.0378, MAE: 0.0378, RMSE: 0.0660, R2: 0.9287), PNorm: 143.9641, GNorm: 1.1816
[146/299] timecost: 67.49, lr: 0.000027, Train: (LOSS: 0.0194, MAE: 0.0194, RMSE: 0.0329, R2: 0.9804), Valid: (LOSS: 0.0382, MAE: 0.0382, RMSE: 0.0647, R2: 0.9303), PNorm: 143.7765, GNorm: 0.9334
[147/299] timecost: 67.33, lr: 0.000027, Train: (LOSS: 0.0194, MAE: 0.0194, RMSE: 0.0323, R2: 0.9796), Valid: (LOSS: 0.0361, MAE: 0.0361, RMSE: 0.0611, R2: 0.9374), PNorm: 143.5906, GNorm: 0.9313
[148/299] timecost: 66.27, lr: 0.000027, Train: (LOSS: 0.0188, MAE: 0.0188, RMSE: 0.0319, R2: 0.9798), Valid: (LOSS: 0.0382, MAE: 0.0382, RMSE: 0.0661, R2: 0.9281), PNorm: 143.4060, GNorm: 0.6992
[149/299] timecost: 63.14, lr: 0.000027, Train: (LOSS: 0.0193, MAE: 0.0193, RMSE: 0.0322, R2: 0.9803), Valid: (LOSS: 0.0390, MAE: 0.0390, RMSE: 0.0662, R2: 0.9274), PNorm: 143.2232, GNorm: 0.7356
[150/299] timecost: 63.13, lr: 0.000027, Train: (LOSS: 0.0189, MAE: 0.0189, RMSE: 0.0320, R2: 0.9811), Valid: (LOSS: 0.0355, MAE: 0.0355, RMSE: 0.0594, R2: 0.9421), PNorm: 143.0426, GNorm: 1.4046
[151/299] timecost: 63.00, lr: 0.000027, Train: (LOSS: 0.0190, MAE: 0.0190, RMSE: 0.0319, R2: 0.9804), Valid: (LOSS: 0.0366, MAE: 0.0366, RMSE: 0.0607, R2: 0.9391), PNorm: 142.8632, GNorm: 0.8725
[152/299] timecost: 62.96, lr: 0.000027, Train: (LOSS: 0.0186, MAE: 0.0186, RMSE: 0.0307, R2: 0.9804), Valid: (LOSS: 0.0387, MAE: 0.0387, RMSE: 0.0667, R2: 0.9262), PNorm: 142.6856, GNorm: 1.4025
[153/299] timecost: 62.76, lr: 0.000027, Train: (LOSS: 0.0182, MAE: 0.0182, RMSE: 0.0307, R2: 0.9812), Valid: (LOSS: 0.0409, MAE: 0.0409, RMSE: 0.0696, R2: 0.9216), PNorm: 142.5105, GNorm: 1.3323
[154/299] timecost: 62.79, lr: 0.000027, Train: (LOSS: 0.0179, MAE: 0.0179, RMSE: 0.0306, R2: 0.9815), Valid: (LOSS: 0.0379, MAE: 0.0379, RMSE: 0.0646, R2: 0.9325), PNorm: 142.3358, GNorm: 0.8919
[155/299] timecost: 63.21, lr: 0.000027, Train: (LOSS: 0.0180, MAE: 0.0180, RMSE: 0.0302, R2: 0.9811), Valid: (LOSS: 0.0374, MAE: 0.0374, RMSE: 0.0628, R2: 0.9351), PNorm: 142.1632, GNorm: 1.0755
[156/299] timecost: 62.93, lr: 0.000027, Train: (LOSS: 0.0178, MAE: 0.0178, RMSE: 0.0302, R2: 0.9820), Valid: (LOSS: 0.0396, MAE: 0.0396, RMSE: 0.0698, R2: 0.9195), PNorm: 141.9933, GNorm: 0.8868
[157/299] timecost: 62.87, lr: 0.000027, Train: (LOSS: 0.0184, MAE: 0.0184, RMSE: 0.0305, R2: 0.9815), Valid: (LOSS: 0.0389, MAE: 0.0389, RMSE: 0.0652, R2: 0.9332), PNorm: 141.8254, GNorm: 0.9627
[158/299] timecost: 64.95, lr: 0.000027, Train: (LOSS: 0.0184, MAE: 0.0184, RMSE: 0.0312, R2: 0.9809), Valid: (LOSS: 0.0402, MAE: 0.0402, RMSE: 0.0709, R2: 0.9172), PNorm: 141.6589, GNorm: 0.9576
[159/299] timecost: 64.86, lr: 0.000027, Train: (LOSS: 0.0184, MAE: 0.0184, RMSE: 0.0308, R2: 0.9801), Valid: (LOSS: 0.0394, MAE: 0.0394, RMSE: 0.0657, R2: 0.9307), PNorm: 141.4954, GNorm: 1.3496
[160/299] timecost: 65.04, lr: 0.000027, Train: (LOSS: 0.0176, MAE: 0.0176, RMSE: 0.0293, R2: 0.9828), Valid: (LOSS: 0.0367, MAE: 0.0367, RMSE: 0.0619, R2: 0.9384), PNorm: 141.3329, GNorm: 0.8057
[161/299] timecost: 64.03, lr: 0.000027, Train: (LOSS: 0.0174, MAE: 0.0174, RMSE: 0.0297, R2: 0.9822), Valid: (LOSS: 0.0385, MAE: 0.0385, RMSE: 0.0670, R2: 0.9272), PNorm: 141.1726, GNorm: 1.0276
[162/299] timecost: 63.12, lr: 0.000027, Train: (LOSS: 0.0185, MAE: 0.0185, RMSE: 0.0308, R2: 0.9815), Valid: (LOSS: 0.0372, MAE: 0.0372, RMSE: 0.0619, R2: 0.9372), PNorm: 141.0148, GNorm: 1.0038
[163/299] timecost: 63.03, lr: 0.000027, Train: (LOSS: 0.0179, MAE: 0.0179, RMSE: 0.0300, R2: 0.9830), Valid: (LOSS: 0.0388, MAE: 0.0388, RMSE: 0.0665, R2: 0.9275), PNorm: 140.8593, GNorm: 0.9617
[164/299] timecost: 63.20, lr: 0.000027, Train: (LOSS: 0.0173, MAE: 0.0173, RMSE: 0.0293, R2: 0.9823), Valid: (LOSS: 0.0361, MAE: 0.0361, RMSE: 0.0615, R2: 0.9376), PNorm: 140.7061, GNorm: 0.7263
[165/299] timecost: 63.70, lr: 0.000027, Train: (LOSS: 0.0168, MAE: 0.0168, RMSE: 0.0288, R2: 0.9835), Valid: (LOSS: 0.0392, MAE: 0.0392, RMSE: 0.0652, R2: 0.9327), PNorm: 140.5546, GNorm: 0.8012
[166/299] timecost: 63.89, lr: 0.000027, Train: (LOSS: 0.0176, MAE: 0.0176, RMSE: 0.0299, R2: 0.9819), Valid: (LOSS: 0.0378, MAE: 0.0378, RMSE: 0.0642, R2: 0.9332), PNorm: 140.4058, GNorm: 1.0193
[167/299] timecost: 63.97, lr: 0.000027, Train: (LOSS: 0.0168, MAE: 0.0168, RMSE: 0.0286, R2: 0.9826), Valid: (LOSS: 0.0356, MAE: 0.0356, RMSE: 0.0590, R2: 0.9430), PNorm: 140.2582, GNorm: 1.0112
[168/299] timecost: 63.64, lr: 0.000027, Train: (LOSS: 0.0177, MAE: 0.0177, RMSE: 0.0288, R2: 0.9831), Valid: (LOSS: 0.0379, MAE: 0.0379, RMSE: 0.0646, R2: 0.9314), PNorm: 140.1124, GNorm: 1.1481
[169/299] timecost: 63.08, lr: 0.000027, Train: (LOSS: 0.0174, MAE: 0.0174, RMSE: 0.0293, R2: 0.9825), Valid: (LOSS: 0.0373, MAE: 0.0373, RMSE: 0.0654, R2: 0.9304), PNorm: 139.9702, GNorm: 1.1182
[170/299] timecost: 62.75, lr: 0.000027, Train: (LOSS: 0.0163, MAE: 0.0163, RMSE: 0.0276, R2: 0.9850), Valid: (LOSS: 0.0363, MAE: 0.0363, RMSE: 0.0620, R2: 0.9352), PNorm: 139.8284, GNorm: 0.7308
Epoch 00172: reducing learning rate of group 0 to 2.4300e-05.
[171/299] timecost: 62.96, lr: 0.000024, Train: (LOSS: 0.0163, MAE: 0.0163, RMSE: 0.0278, R2: 0.9845), Valid: (LOSS: 0.0384, MAE: 0.0384, RMSE: 0.0645, R2: 0.9316), PNorm: 139.6891, GNorm: 0.6467
[172/299] timecost: 62.67, lr: 0.000024, Train: (LOSS: 0.0163, MAE: 0.0163, RMSE: 0.0278, R2: 0.9840), Valid: (LOSS: 0.0372, MAE: 0.0372, RMSE: 0.0635, R2: 0.9342), PNorm: 139.5654, GNorm: 1.2371
[173/299] timecost: 62.73, lr: 0.000024, Train: (LOSS: 0.0151, MAE: 0.0151, RMSE: 0.0267, R2: 0.9855), Valid: (LOSS: 0.0358, MAE: 0.0358, RMSE: 0.0616, R2: 0.9370), PNorm: 139.4426, GNorm: 0.7731
[174/299] timecost: 62.63, lr: 0.000024, Train: (LOSS: 0.0163, MAE: 0.0163, RMSE: 0.0279, R2: 0.9839), Valid: (LOSS: 0.0373, MAE: 0.0373, RMSE: 0.0628, R2: 0.9358), PNorm: 139.3222, GNorm: 0.8715
[175/299] timecost: 63.34, lr: 0.000024, Train: (LOSS: 0.0149, MAE: 0.0149, RMSE: 0.0255, R2: 0.9860), Valid: (LOSS: 0.0370, MAE: 0.0370, RMSE: 0.0620, R2: 0.9384), PNorm: 139.2024, GNorm: 0.8321
[176/299] timecost: 63.54, lr: 0.000024, Train: (LOSS: 0.0160, MAE: 0.0160, RMSE: 0.0272, R2: 0.9850), Valid: (LOSS: 0.0380, MAE: 0.0380, RMSE: 0.0640, R2: 0.9330), PNorm: 139.0841, GNorm: 0.8661
[177/299] timecost: 66.11, lr: 0.000024, Train: (LOSS: 0.0147, MAE: 0.0147, RMSE: 0.0260, R2: 0.9864), Valid: (LOSS: 0.0375, MAE: 0.0375, RMSE: 0.0642, R2: 0.9329), PNorm: 138.9664, GNorm: 0.7322
[178/299] timecost: 66.39, lr: 0.000024, Train: (LOSS: 0.0152, MAE: 0.0152, RMSE: 0.0258, R2: 0.9856), Valid: (LOSS: 0.0361, MAE: 0.0361, RMSE: 0.0601, R2: 0.9401), PNorm: 138.8512, GNorm: 0.9123
[179/299] timecost: 65.41, lr: 0.000024, Train: (LOSS: 0.0148, MAE: 0.0148, RMSE: 0.0252, R2: 0.9853), Valid: (LOSS: 0.0369, MAE: 0.0369, RMSE: 0.0625, R2: 0.9361), PNorm: 138.7366, GNorm: 0.9696
[180/299] timecost: 66.45, lr: 0.000024, Train: (LOSS: 0.0155, MAE: 0.0155, RMSE: 0.0261, R2: 0.9861), Valid: (LOSS: 0.0372, MAE: 0.0372, RMSE: 0.0633, R2: 0.9344), PNorm: 138.6231, GNorm: 0.9816
[181/299] timecost: 65.52, lr: 0.000024, Train: (LOSS: 0.0147, MAE: 0.0147, RMSE: 0.0252, R2: 0.9844), Valid: (LOSS: 0.0361, MAE: 0.0361, RMSE: 0.0616, R2: 0.9369), PNorm: 138.5114, GNorm: 1.0615
[182/299] timecost: 65.45, lr: 0.000024, Train: (LOSS: 0.0152, MAE: 0.0152, RMSE: 0.0262, R2: 0.9861), Valid: (LOSS: 0.0375, MAE: 0.0375, RMSE: 0.0631, R2: 0.9343), PNorm: 138.4016, GNorm: 0.7175
[183/299] timecost: 65.05, lr: 0.000024, Train: (LOSS: 0.0154, MAE: 0.0154, RMSE: 0.0259, R2: 0.9868), Valid: (LOSS: 0.0389, MAE: 0.0389, RMSE: 0.0641, R2: 0.9349), PNorm: 138.2932, GNorm: 1.0376
[184/299] timecost: 65.29, lr: 0.000024, Train: (LOSS: 0.0150, MAE: 0.0150, RMSE: 0.0257, R2: 0.9861), Valid: (LOSS: 0.0366, MAE: 0.0366, RMSE: 0.0617, R2: 0.9382), PNorm: 138.1865, GNorm: 0.9280
[185/299] timecost: 64.86, lr: 0.000024, Train: (LOSS: 0.0147, MAE: 0.0147, RMSE: 0.0251, R2: 0.9861), Valid: (LOSS: 0.0370, MAE: 0.0370, RMSE: 0.0616, R2: 0.9378), PNorm: 138.0800, GNorm: 0.8773
[186/299] timecost: 64.77, lr: 0.000024, Train: (LOSS: 0.0147, MAE: 0.0147, RMSE: 0.0253, R2: 0.9867), Valid: (LOSS: 0.0387, MAE: 0.0387, RMSE: 0.0647, R2: 0.9326), PNorm: 137.9750, GNorm: 1.0733
[187/299] timecost: 65.39, lr: 0.000024, Train: (LOSS: 0.0145, MAE: 0.0145, RMSE: 0.0249, R2: 0.9857), Valid: (LOSS: 0.0381, MAE: 0.0381, RMSE: 0.0634, R2: 0.9350), PNorm: 137.8721, GNorm: 0.9234
[188/299] timecost: 65.08, lr: 0.000024, Train: (LOSS: 0.0143, MAE: 0.0143, RMSE: 0.0252, R2: 0.9867), Valid: (LOSS: 0.0361, MAE: 0.0361, RMSE: 0.0617, R2: 0.9371), PNorm: 137.7698, GNorm: 0.9217
[189/299] timecost: 65.09, lr: 0.000024, Train: (LOSS: 0.0146, MAE: 0.0146, RMSE: 0.0255, R2: 0.9854), Valid: (LOSS: 0.0373, MAE: 0.0373, RMSE: 0.0624, R2: 0.9362), PNorm: 137.6689, GNorm: 0.9360
[190/299] timecost: 65.26, lr: 0.000024, Train: (LOSS: 0.0152, MAE: 0.0152, RMSE: 0.0259, R2: 0.9857), Valid: (LOSS: 0.0374, MAE: 0.0374, RMSE: 0.0634, R2: 0.9343), PNorm: 137.5695, GNorm: 0.8690
[191/299] timecost: 65.61, lr: 0.000024, Train: (LOSS: 0.0147, MAE: 0.0147, RMSE: 0.0254, R2: 0.9863), Valid: (LOSS: 0.0366, MAE: 0.0366, RMSE: 0.0615, R2: 0.9395), PNorm: 137.4711, GNorm: 1.0589
Epoch 00193: reducing learning rate of group 0 to 2.1870e-05.
[192/299] timecost: 65.49, lr: 0.000022, Train: (LOSS: 0.0148, MAE: 0.0148, RMSE: 0.0259, R2: 0.9861), Valid: (LOSS: 0.0367, MAE: 0.0367, RMSE: 0.0599, R2: 0.9411), PNorm: 137.3746, GNorm: 1.0047
[193/299] timecost: 65.82, lr: 0.000022, Train: (LOSS: 0.0139, MAE: 0.0139, RMSE: 0.0242, R2: 0.9876), Valid: (LOSS: 0.0377, MAE: 0.0377, RMSE: 0.0618, R2: 0.9387), PNorm: 137.2876, GNorm: 0.6980
[194/299] timecost: 65.30, lr: 0.000022, Train: (LOSS: 0.0133, MAE: 0.0133, RMSE: 0.0234, R2: 0.9885), Valid: (LOSS: 0.0359, MAE: 0.0359, RMSE: 0.0622, R2: 0.9366), PNorm: 137.2014, GNorm: 0.8724
[195/299] timecost: 65.32, lr: 0.000022, Train: (LOSS: 0.0136, MAE: 0.0136, RMSE: 0.0240, R2: 0.9866), Valid: (LOSS: 0.0364, MAE: 0.0364, RMSE: 0.0621, R2: 0.9367), PNorm: 137.1165, GNorm: 0.7646
[196/299] timecost: 65.04, lr: 0.000022, Train: (LOSS: 0.0138, MAE: 0.0138, RMSE: 0.0239, R2: 0.9868), Valid: (LOSS: 0.0379, MAE: 0.0379, RMSE: 0.0612, R2: 0.9388), PNorm: 137.0316, GNorm: 0.6682
[197/299] timecost: 63.93, lr: 0.000022, Train: (LOSS: 0.0132, MAE: 0.0132, RMSE: 0.0233, R2: 0.9883), Valid: (LOSS: 0.0378, MAE: 0.0378, RMSE: 0.0629, R2: 0.9359), PNorm: 136.9480, GNorm: 0.7623
[198/299] timecost: 63.08, lr: 0.000022, Train: (LOSS: 0.0138, MAE: 0.0138, RMSE: 0.0242, R2: 0.9868), Valid: (LOSS: 0.0373, MAE: 0.0373, RMSE: 0.0633, R2: 0.9363), PNorm: 136.8659, GNorm: 0.7953
[199/299] timecost: 62.99, lr: 0.000022, Train: (LOSS: 0.0129, MAE: 0.0129, RMSE: 0.0230, R2: 0.9883), Valid: (LOSS: 0.0371, MAE: 0.0371, RMSE: 0.0638, R2: 0.9336), PNorm: 136.7833, GNorm: 0.7778
[200/299] timecost: 63.03, lr: 0.000022, Train: (LOSS: 0.0136, MAE: 0.0136, RMSE: 0.0232, R2: 0.9876), Valid: (LOSS: 0.0369, MAE: 0.0369, RMSE: 0.0613, R2: 0.9389), PNorm: 136.7014, GNorm: 0.6131
[201/299] timecost: 63.42, lr: 0.000022, Train: (LOSS: 0.0130, MAE: 0.0130, RMSE: 0.0225, R2: 0.9871), Valid: (LOSS: 0.0373, MAE: 0.0373, RMSE: 0.0614, R2: 0.9395), PNorm: 136.6210, GNorm: 1.0703
[202/299] timecost: 63.01, lr: 0.000022, Train: (LOSS: 0.0130, MAE: 0.0130, RMSE: 0.0230, R2: 0.9879), Valid: (LOSS: 0.0363, MAE: 0.0363, RMSE: 0.0610, R2: 0.9392), PNorm: 136.5409, GNorm: 0.8296
[203/299] timecost: 62.95, lr: 0.000022, Train: (LOSS: 0.0132, MAE: 0.0132, RMSE: 0.0230, R2: 0.9876), Valid: (LOSS: 0.0367, MAE: 0.0367, RMSE: 0.0604, R2: 0.9401), PNorm: 136.4609, GNorm: 0.8499
[204/299] timecost: 64.70, lr: 0.000022, Train: (LOSS: 0.0126, MAE: 0.0126, RMSE: 0.0229, R2: 0.9883), Valid: (LOSS: 0.0375, MAE: 0.0375, RMSE: 0.0634, R2: 0.9345), PNorm: 136.3820, GNorm: 0.9930
[205/299] timecost: 64.94, lr: 0.000022, Train: (LOSS: 0.0124, MAE: 0.0124, RMSE: 0.0225, R2: 0.9882), Valid: (LOSS: 0.0367, MAE: 0.0367, RMSE: 0.0600, R2: 0.9421), PNorm: 136.3036, GNorm: 1.1528
[206/299] timecost: 64.70, lr: 0.000022, Train: (LOSS: 0.0130, MAE: 0.0130, RMSE: 0.0224, R2: 0.9870), Valid: (LOSS: 0.0367, MAE: 0.0367, RMSE: 0.0606, R2: 0.9403), PNorm: 136.2259, GNorm: 0.8278
[207/299] timecost: 64.90, lr: 0.000022, Train: (LOSS: 0.0122, MAE: 0.0122, RMSE: 0.0220, R2: 0.9887), Valid: (LOSS: 0.0367, MAE: 0.0367, RMSE: 0.0634, R2: 0.9340), PNorm: 136.1484, GNorm: 0.8532
[208/299] timecost: 65.11, lr: 0.000022, Train: (LOSS: 0.0125, MAE: 0.0125, RMSE: 0.0225, R2: 0.9875), Valid: (LOSS: 0.0368, MAE: 0.0368, RMSE: 0.0611, R2: 0.9378), PNorm: 136.0720, GNorm: 0.7358
[209/299] timecost: 64.90, lr: 0.000022, Train: (LOSS: 0.0125, MAE: 0.0125, RMSE: 0.0224, R2: 0.9887), Valid: (LOSS: 0.0358, MAE: 0.0358, RMSE: 0.0586, R2: 0.9432), PNorm: 135.9952, GNorm: 1.0766
[210/299] timecost: 64.75, lr: 0.000022, Train: (LOSS: 0.0126, MAE: 0.0126, RMSE: 0.0217, R2: 0.9881), Valid: (LOSS: 0.0363, MAE: 0.0363, RMSE: 0.0623, R2: 0.9365), PNorm: 135.9198, GNorm: 1.0329
[211/299] timecost: 64.89, lr: 0.000022, Train: (LOSS: 0.0124, MAE: 0.0124, RMSE: 0.0225, R2: 0.9885), Valid: (LOSS: 0.0364, MAE: 0.0364, RMSE: 0.0623, R2: 0.9360), PNorm: 135.8445, GNorm: 0.8265
[212/299] timecost: 65.90, lr: 0.000022, Train: (LOSS: 0.0123, MAE: 0.0123, RMSE: 0.0218, R2: 0.9889), Valid: (LOSS: 0.0364, MAE: 0.0364, RMSE: 0.0632, R2: 0.9360), PNorm: 135.7698, GNorm: 0.7882
Epoch 00214: reducing learning rate of group 0 to 1.9683e-05.
[213/299] timecost: 66.55, lr: 0.000020, Train: (LOSS: 0.0127, MAE: 0.0127, RMSE: 0.0225, R2: 0.9880), Valid: (LOSS: 0.0365, MAE: 0.0365, RMSE: 0.0611, R2: 0.9389), PNorm: 135.6955, GNorm: 0.7480
[214/299] timecost: 65.74, lr: 0.000020, Train: (LOSS: 0.0118, MAE: 0.0118, RMSE: 0.0216, R2: 0.9888), Valid: (LOSS: 0.0360, MAE: 0.0360, RMSE: 0.0604, R2: 0.9400), PNorm: 135.6293, GNorm: 0.7534
[215/299] timecost: 66.08, lr: 0.000020, Train: (LOSS: 0.0117, MAE: 0.0117, RMSE: 0.0208, R2: 0.9891), Valid: (LOSS: 0.0370, MAE: 0.0370, RMSE: 0.0609, R2: 0.9400), PNorm: 135.5627, GNorm: 0.7607
[216/299] timecost: 66.18, lr: 0.000020, Train: (LOSS: 0.0118, MAE: 0.0118, RMSE: 0.0214, R2: 0.9879), Valid: (LOSS: 0.0371, MAE: 0.0371, RMSE: 0.0618, R2: 0.9368), PNorm: 135.4966, GNorm: 1.1450
[217/299] timecost: 63.89, lr: 0.000020, Train: (LOSS: 0.0117, MAE: 0.0117, RMSE: 0.0207, R2: 0.9899), Valid: (LOSS: 0.0355, MAE: 0.0355, RMSE: 0.0580, R2: 0.9448), PNorm: 135.4314, GNorm: 0.8242
[218/299] timecost: 64.33, lr: 0.000020, Train: (LOSS: 0.0117, MAE: 0.0117, RMSE: 0.0208, R2: 0.9887), Valid: (LOSS: 0.0360, MAE: 0.0360, RMSE: 0.0597, R2: 0.9426), PNorm: 135.3663, GNorm: 0.8976
[219/299] timecost: 63.25, lr: 0.000020, Train: (LOSS: 0.0115, MAE: 0.0115, RMSE: 0.0206, R2: 0.9894), Valid: (LOSS: 0.0355, MAE: 0.0355, RMSE: 0.0583, R2: 0.9437), PNorm: 135.3009, GNorm: 1.4357
[220/299] timecost: 65.56, lr: 0.000020, Train: (LOSS: 0.0117, MAE: 0.0117, RMSE: 0.0212, R2: 0.9887), Valid: (LOSS: 0.0362, MAE: 0.0362, RMSE: 0.0598, R2: 0.9414), PNorm: 135.2357, GNorm: 0.7387
[221/299] timecost: 65.71, lr: 0.000020, Train: (LOSS: 0.0123, MAE: 0.0123, RMSE: 0.0214, R2: 0.9893), Valid: (LOSS: 0.0370, MAE: 0.0370, RMSE: 0.0631, R2: 0.9353), PNorm: 135.1714, GNorm: 0.9138
[222/299] timecost: 65.45, lr: 0.000020, Train: (LOSS: 0.0117, MAE: 0.0117, RMSE: 0.0211, R2: 0.9896), Valid: (LOSS: 0.0369, MAE: 0.0369, RMSE: 0.0596, R2: 0.9416), PNorm: 135.1069, GNorm: 0.7537
[223/299] timecost: 64.58, lr: 0.000020, Train: (LOSS: 0.0109, MAE: 0.0109, RMSE: 0.0200, R2: 0.9885), Valid: (LOSS: 0.0352, MAE: 0.0352, RMSE: 0.0581, R2: 0.9450), PNorm: 135.0434, GNorm: 0.6892
[224/299] timecost: 63.47, lr: 0.000020, Train: (LOSS: 0.0109, MAE: 0.0109, RMSE: 0.0200, R2: 0.9888), Valid: (LOSS: 0.0368, MAE: 0.0368, RMSE: 0.0577, R2: 0.9460), PNorm: 134.9789, GNorm: 0.8292
[225/299] timecost: 63.72, lr: 0.000020, Train: (LOSS: 0.0113, MAE: 0.0113, RMSE: 0.0205, R2: 0.9893), Valid: (LOSS: 0.0351, MAE: 0.0351, RMSE: 0.0578, R2: 0.9434), PNorm: 134.9153, GNorm: 1.1653
[226/299] timecost: 64.31, lr: 0.000020, Train: (LOSS: 0.0112, MAE: 0.0112, RMSE: 0.0204, R2: 0.9885), Valid: (LOSS: 0.0356, MAE: 0.0356, RMSE: 0.0599, R2: 0.9417), PNorm: 134.8514, GNorm: 0.9741
[227/299] timecost: 65.45, lr: 0.000020, Train: (LOSS: 0.0115, MAE: 0.0115, RMSE: 0.0202, R2: 0.9892), Valid: (LOSS: 0.0365, MAE: 0.0365, RMSE: 0.0579, R2: 0.9454), PNorm: 134.7882, GNorm: 0.5997
[228/299] timecost: 66.47, lr: 0.000020, Train: (LOSS: 0.0121, MAE: 0.0121, RMSE: 0.0212, R2: 0.9892), Valid: (LOSS: 0.0358, MAE: 0.0358, RMSE: 0.0587, R2: 0.9446), PNorm: 134.7248, GNorm: 1.2338
[229/299] timecost: 66.15, lr: 0.000020, Train: (LOSS: 0.0115, MAE: 0.0115, RMSE: 0.0209, R2: 0.9897), Valid: (LOSS: 0.0361, MAE: 0.0361, RMSE: 0.0602, R2: 0.9411), PNorm: 134.6620, GNorm: 0.7401
[230/299] timecost: 64.38, lr: 0.000020, Train: (LOSS: 0.0116, MAE: 0.0116, RMSE: 0.0213, R2: 0.9891), Valid: (LOSS: 0.0345, MAE: 0.0345, RMSE: 0.0572, R2: 0.9463), PNorm: 134.5996, GNorm: 0.8779
[231/299] timecost: 65.21, lr: 0.000020, Train: (LOSS: 0.0113, MAE: 0.0113, RMSE: 0.0208, R2: 0.9896), Valid: (LOSS: 0.0362, MAE: 0.0362, RMSE: 0.0609, R2: 0.9406), PNorm: 134.5366, GNorm: 1.0203
[232/299] timecost: 66.50, lr: 0.000020, Train: (LOSS: 0.0111, MAE: 0.0111, RMSE: 0.0201, R2: 0.9901), Valid: (LOSS: 0.0371, MAE: 0.0371, RMSE: 0.0605, R2: 0.9405), PNorm: 134.4749, GNorm: 0.7936
[233/299] timecost: 66.17, lr: 0.000020, Train: (LOSS: 0.0109, MAE: 0.0109, RMSE: 0.0199, R2: 0.9900), Valid: (LOSS: 0.0374, MAE: 0.0374, RMSE: 0.0634, R2: 0.9338), PNorm: 134.4120, GNorm: 0.8359
[234/299] timecost: 65.92, lr: 0.000020, Train: (LOSS: 0.0114, MAE: 0.0114, RMSE: 0.0204, R2: 0.9894), Valid: (LOSS: 0.0367, MAE: 0.0367, RMSE: 0.0595, R2: 0.9417), PNorm: 134.3500, GNorm: 0.8569
[235/299] timecost: 65.29, lr: 0.000020, Train: (LOSS: 0.0111, MAE: 0.0111, RMSE: 0.0198, R2: 0.9905), Valid: (LOSS: 0.0359, MAE: 0.0359, RMSE: 0.0605, R2: 0.9394), PNorm: 134.2883, GNorm: 0.8440
[236/299] timecost: 63.28, lr: 0.000020, Train: (LOSS: 0.0108, MAE: 0.0108, RMSE: 0.0200, R2: 0.9901), Valid: (LOSS: 0.0350, MAE: 0.0350, RMSE: 0.0583, R2: 0.9429), PNorm: 134.2262, GNorm: 1.0664
[237/299] timecost: 62.86, lr: 0.000020, Train: (LOSS: 0.0113, MAE: 0.0113, RMSE: 0.0202, R2: 0.9903), Valid: (LOSS: 0.0347, MAE: 0.0347, RMSE: 0.0574, R2: 0.9463), PNorm: 134.1647, GNorm: 1.0268
[238/299] timecost: 62.90, lr: 0.000020, Train: (LOSS: 0.0112, MAE: 0.0112, RMSE: 0.0203, R2: 0.9907), Valid: (LOSS: 0.0359, MAE: 0.0359, RMSE: 0.0589, R2: 0.9437), PNorm: 134.1035, GNorm: 0.7533
[239/299] timecost: 64.79, lr: 0.000020, Train: (LOSS: 0.0117, MAE: 0.0117, RMSE: 0.0207, R2: 0.9894), Valid: (LOSS: 0.0362, MAE: 0.0362, RMSE: 0.0581, R2: 0.9442), PNorm: 134.0428, GNorm: 1.0751
[240/299] timecost: 65.20, lr: 0.000020, Train: (LOSS: 0.0110, MAE: 0.0110, RMSE: 0.0199, R2: 0.9897), Valid: (LOSS: 0.0370, MAE: 0.0370, RMSE: 0.0597, R2: 0.9419), PNorm: 133.9818, GNorm: 0.8056
[241/299] timecost: 64.95, lr: 0.000020, Train: (LOSS: 0.0106, MAE: 0.0106, RMSE: 0.0190, R2: 0.9905), Valid: (LOSS: 0.0375, MAE: 0.0375, RMSE: 0.0604, R2: 0.9407), PNorm: 133.9208, GNorm: 0.7767
[242/299] timecost: 65.06, lr: 0.000020, Train: (LOSS: 0.0113, MAE: 0.0113, RMSE: 0.0203, R2: 0.9903), Valid: (LOSS: 0.0372, MAE: 0.0372, RMSE: 0.0613, R2: 0.9390), PNorm: 133.8597, GNorm: 0.9027
[243/299] timecost: 65.07, lr: 0.000020, Train: (LOSS: 0.0108, MAE: 0.0108, RMSE: 0.0193, R2: 0.9897), Valid: (LOSS: 0.0358, MAE: 0.0358, RMSE: 0.0594, R2: 0.9413), PNorm: 133.7993, GNorm: 0.7698
[244/299] timecost: 65.45, lr: 0.000020, Train: (LOSS: 0.0106, MAE: 0.0106, RMSE: 0.0187, R2: 0.9917), Valid: (LOSS: 0.0358, MAE: 0.0358, RMSE: 0.0585, R2: 0.9438), PNorm: 133.7384, GNorm: 0.7002
[245/299] timecost: 65.21, lr: 0.000020, Train: (LOSS: 0.0105, MAE: 0.0105, RMSE: 0.0192, R2: 0.9898), Valid: (LOSS: 0.0366, MAE: 0.0366, RMSE: 0.0593, R2: 0.9430), PNorm: 133.6775, GNorm: 0.7344
[246/299] timecost: 65.71, lr: 0.000020, Train: (LOSS: 0.0107, MAE: 0.0107, RMSE: 0.0196, R2: 0.9904), Valid: (LOSS: 0.0367, MAE: 0.0367, RMSE: 0.0597, R2: 0.9428), PNorm: 133.6174, GNorm: 1.0127
[247/299] timecost: 65.93, lr: 0.000020, Train: (LOSS: 0.0105, MAE: 0.0105, RMSE: 0.0194, R2: 0.9906), Valid: (LOSS: 0.0372, MAE: 0.0372, RMSE: 0.0597, R2: 0.9422), PNorm: 133.5561, GNorm: 1.0635
[248/299] timecost: 65.44, lr: 0.000020, Train: (LOSS: 0.0105, MAE: 0.0105, RMSE: 0.0191, R2: 0.9906), Valid: (LOSS: 0.0373, MAE: 0.0373, RMSE: 0.0603, R2: 0.9405), PNorm: 133.4954, GNorm: 0.7990
[249/299] timecost: 65.37, lr: 0.000020, Train: (LOSS: 0.0105, MAE: 0.0105, RMSE: 0.0194, R2: 0.9904), Valid: (LOSS: 0.0360, MAE: 0.0360, RMSE: 0.0588, R2: 0.9435), PNorm: 133.4354, GNorm: 1.4028
[250/299] timecost: 65.61, lr: 0.000020, Train: (LOSS: 0.0109, MAE: 0.0109, RMSE: 0.0193, R2: 0.9906), Valid: (LOSS: 0.0363, MAE: 0.0363, RMSE: 0.0595, R2: 0.9421), PNorm: 133.3750, GNorm: 0.8352
Epoch 00252: reducing learning rate of group 0 to 1.7715e-05.
[251/299] timecost: 64.94, lr: 0.000018, Train: (LOSS: 0.0103, MAE: 0.0103, RMSE: 0.0183, R2: 0.9909), Valid: (LOSS: 0.0356, MAE: 0.0356, RMSE: 0.0597, R2: 0.9417), PNorm: 133.3146, GNorm: 0.7938
[252/299] timecost: 65.45, lr: 0.000018, Train: (LOSS: 0.0108, MAE: 0.0108, RMSE: 0.0193, R2: 0.9909), Valid: (LOSS: 0.0361, MAE: 0.0361, RMSE: 0.0587, R2: 0.9440), PNorm: 133.2607, GNorm: 0.8861
[253/299] timecost: 65.43, lr: 0.000018, Train: (LOSS: 0.0097, MAE: 0.0097, RMSE: 0.0184, R2: 0.9916), Valid: (LOSS: 0.0353, MAE: 0.0353, RMSE: 0.0574, R2: 0.9460), PNorm: 133.2065, GNorm: 0.8264
[254/299] timecost: 65.03, lr: 0.000018, Train: (LOSS: 0.0097, MAE: 0.0097, RMSE: 0.0181, R2: 0.9913), Valid: (LOSS: 0.0367, MAE: 0.0367, RMSE: 0.0598, R2: 0.9421), PNorm: 133.1523, GNorm: 0.9590
[255/299] timecost: 65.07, lr: 0.000018, Train: (LOSS: 0.0102, MAE: 0.0102, RMSE: 0.0185, R2: 0.9908), Valid: (LOSS: 0.0358, MAE: 0.0358, RMSE: 0.0596, R2: 0.9427), PNorm: 133.0983, GNorm: 1.2174
[256/299] timecost: 64.59, lr: 0.000018, Train: (LOSS: 0.0099, MAE: 0.0099, RMSE: 0.0182, R2: 0.9918), Valid: (LOSS: 0.0354, MAE: 0.0354, RMSE: 0.0572, R2: 0.9461), PNorm: 133.0440, GNorm: 0.8134
[257/299] timecost: 65.07, lr: 0.000018, Train: (LOSS: 0.0094, MAE: 0.0094, RMSE: 0.0178, R2: 0.9922), Valid: (LOSS: 0.0348, MAE: 0.0348, RMSE: 0.0552, R2: 0.9493), PNorm: 132.9893, GNorm: 0.9726
[258/299] timecost: 65.73, lr: 0.000018, Train: (LOSS: 0.0098, MAE: 0.0098, RMSE: 0.0181, R2: 0.9912), Valid: (LOSS: 0.0347, MAE: 0.0347, RMSE: 0.0574, R2: 0.9436), PNorm: 132.9358, GNorm: 0.7162
[259/299] timecost: 65.63, lr: 0.000018, Train: (LOSS: 0.0097, MAE: 0.0097, RMSE: 0.0177, R2: 0.9915), Valid: (LOSS: 0.0382, MAE: 0.0382, RMSE: 0.0623, R2: 0.9378), PNorm: 132.8817, GNorm: 0.8518
[260/299] timecost: 65.41, lr: 0.000018, Train: (LOSS: 0.0100, MAE: 0.0100, RMSE: 0.0186, R2: 0.9903), Valid: (LOSS: 0.0359, MAE: 0.0359, RMSE: 0.0592, R2: 0.9437), PNorm: 132.8274, GNorm: 0.6917
[261/299] timecost: 66.52, lr: 0.000018, Train: (LOSS: 0.0101, MAE: 0.0101, RMSE: 0.0180, R2: 0.9918), Valid: (LOSS: 0.0350, MAE: 0.0350, RMSE: 0.0559, R2: 0.9474), PNorm: 132.7741, GNorm: 0.6402
[262/299] timecost: 66.01, lr: 0.000018, Train: (LOSS: 0.0095, MAE: 0.0095, RMSE: 0.0178, R2: 0.9911), Valid: (LOSS: 0.0350, MAE: 0.0350, RMSE: 0.0561, R2: 0.9481), PNorm: 132.7199, GNorm: 0.9553
[263/299] timecost: 66.70, lr: 0.000018, Train: (LOSS: 0.0099, MAE: 0.0099, RMSE: 0.0181, R2: 0.9912), Valid: (LOSS: 0.0352, MAE: 0.0352, RMSE: 0.0567, R2: 0.9470), PNorm: 132.6660, GNorm: 0.9097
[264/299] timecost: 66.30, lr: 0.000018, Train: (LOSS: 0.0094, MAE: 0.0094, RMSE: 0.0176, R2: 0.9924), Valid: (LOSS: 0.0355, MAE: 0.0355, RMSE: 0.0575, R2: 0.9464), PNorm: 132.6122, GNorm: 0.7953
[265/299] timecost: 66.36, lr: 0.000018, Train: (LOSS: 0.0094, MAE: 0.0094, RMSE: 0.0175, R2: 0.9923), Valid: (LOSS: 0.0367, MAE: 0.0367, RMSE: 0.0593, R2: 0.9437), PNorm: 132.5584, GNorm: 0.7234
[266/299] timecost: 65.86, lr: 0.000018, Train: (LOSS: 0.0096, MAE: 0.0096, RMSE: 0.0177, R2: 0.9914), Valid: (LOSS: 0.0352, MAE: 0.0352, RMSE: 0.0562, R2: 0.9479), PNorm: 132.5049, GNorm: 0.7923
[267/299] timecost: 66.56, lr: 0.000018, Train: (LOSS: 0.0096, MAE: 0.0096, RMSE: 0.0173, R2: 0.9923), Valid: (LOSS: 0.0355, MAE: 0.0355, RMSE: 0.0570, R2: 0.9464), PNorm: 132.4515, GNorm: 0.8404
[268/299] timecost: 65.88, lr: 0.000018, Train: (LOSS: 0.0091, MAE: 0.0091, RMSE: 0.0173, R2: 0.9921), Valid: (LOSS: 0.0357, MAE: 0.0357, RMSE: 0.0570, R2: 0.9475), PNorm: 132.3979, GNorm: 0.7069
[269/299] timecost: 66.05, lr: 0.000018, Train: (LOSS: 0.0091, MAE: 0.0091, RMSE: 0.0167, R2: 0.9917), Valid: (LOSS: 0.0356, MAE: 0.0356, RMSE: 0.0584, R2: 0.9446), PNorm: 132.3441, GNorm: 0.7341
[270/299] timecost: 66.16, lr: 0.000018, Train: (LOSS: 0.0095, MAE: 0.0095, RMSE: 0.0174, R2: 0.9924), Valid: (LOSS: 0.0352, MAE: 0.0352, RMSE: 0.0575, R2: 0.9468), PNorm: 132.2907, GNorm: 0.8180
[271/299] timecost: 65.73, lr: 0.000018, Train: (LOSS: 0.0092, MAE: 0.0092, RMSE: 0.0174, R2: 0.9925), Valid: (LOSS: 0.0364, MAE: 0.0364, RMSE: 0.0585, R2: 0.9454), PNorm: 132.2369, GNorm: 0.7723
Epoch 00273: reducing learning rate of group 0 to 1.5943e-05.
[272/299] timecost: 65.67, lr: 0.000016, Train: (LOSS: 0.0095, MAE: 0.0095, RMSE: 0.0172, R2: 0.9920), Valid: (LOSS: 0.0363, MAE: 0.0363, RMSE: 0.0612, R2: 0.9393), PNorm: 132.1837, GNorm: 0.7534
[273/299] timecost: 63.30, lr: 0.000016, Train: (LOSS: 0.0093, MAE: 0.0093, RMSE: 0.0170, R2: 0.9924), Valid: (LOSS: 0.0352, MAE: 0.0352, RMSE: 0.0564, R2: 0.9479), PNorm: 132.1361, GNorm: 0.7175
[274/299] timecost: 63.06, lr: 0.000016, Train: (LOSS: 0.0089, MAE: 0.0089, RMSE: 0.0161, R2: 0.9934), Valid: (LOSS: 0.0347, MAE: 0.0347, RMSE: 0.0575, R2: 0.9464), PNorm: 132.0884, GNorm: 0.7945
[275/299] timecost: 63.12, lr: 0.000016, Train: (LOSS: 0.0085, MAE: 0.0085, RMSE: 0.0160, R2: 0.9930), Valid: (LOSS: 0.0352, MAE: 0.0352, RMSE: 0.0562, R2: 0.9493), PNorm: 132.0401, GNorm: 0.8180
[276/299] timecost: 63.45, lr: 0.000016, Train: (LOSS: 0.0087, MAE: 0.0087, RMSE: 0.0167, R2: 0.9927), Valid: (LOSS: 0.0353, MAE: 0.0353, RMSE: 0.0568, R2: 0.9470), PNorm: 131.9924, GNorm: 0.9603
[277/299] timecost: 62.93, lr: 0.000016, Train: (LOSS: 0.0091, MAE: 0.0091, RMSE: 0.0166, R2: 0.9926), Valid: (LOSS: 0.0361, MAE: 0.0361, RMSE: 0.0585, R2: 0.9441), PNorm: 131.9448, GNorm: 0.6507
[278/299] timecost: 62.92, lr: 0.000016, Train: (LOSS: 0.0085, MAE: 0.0085, RMSE: 0.0157, R2: 0.9924), Valid: (LOSS: 0.0347, MAE: 0.0347, RMSE: 0.0558, R2: 0.9494), PNorm: 131.8972, GNorm: 0.6368
[279/299] timecost: 62.90, lr: 0.000016, Train: (LOSS: 0.0084, MAE: 0.0084, RMSE: 0.0160, R2: 0.9931), Valid: (LOSS: 0.0351, MAE: 0.0351, RMSE: 0.0571, R2: 0.9470), PNorm: 131.8485, GNorm: 0.7359
[280/299] timecost: 63.10, lr: 0.000016, Train: (LOSS: 0.0083, MAE: 0.0083, RMSE: 0.0161, R2: 0.9927), Valid: (LOSS: 0.0343, MAE: 0.0343, RMSE: 0.0545, R2: 0.9510), PNorm: 131.8010, GNorm: 0.6397
[281/299] timecost: 63.23, lr: 0.000016, Train: (LOSS: 0.0086, MAE: 0.0086, RMSE: 0.0163, R2: 0.9922), Valid: (LOSS: 0.0351, MAE: 0.0351, RMSE: 0.0558, R2: 0.9490), PNorm: 131.7527, GNorm: 0.7302
[282/299] timecost: 65.77, lr: 0.000016, Train: (LOSS: 0.0087, MAE: 0.0087, RMSE: 0.0160, R2: 0.9931), Valid: (LOSS: 0.0357, MAE: 0.0357, RMSE: 0.0572, R2: 0.9464), PNorm: 131.7049, GNorm: 1.0268
[283/299] timecost: 65.79, lr: 0.000016, Train: (LOSS: 0.0084, MAE: 0.0084, RMSE: 0.0154, R2: 0.9934), Valid: (LOSS: 0.0357, MAE: 0.0357, RMSE: 0.0574, R2: 0.9470), PNorm: 131.6573, GNorm: 0.8739
[284/299] timecost: 66.36, lr: 0.000016, Train: (LOSS: 0.0088, MAE: 0.0088, RMSE: 0.0156, R2: 0.9938), Valid: (LOSS: 0.0355, MAE: 0.0355, RMSE: 0.0568, R2: 0.9469), PNorm: 131.6101, GNorm: 0.7262
[285/299] timecost: 66.19, lr: 0.000016, Train: (LOSS: 0.0083, MAE: 0.0083, RMSE: 0.0157, R2: 0.9926), Valid: (LOSS: 0.0345, MAE: 0.0345, RMSE: 0.0545, R2: 0.9511), PNorm: 131.5622, GNorm: 0.8153
[286/299] timecost: 66.03, lr: 0.000016, Train: (LOSS: 0.0085, MAE: 0.0085, RMSE: 0.0162, R2: 0.9933), Valid: (LOSS: 0.0354, MAE: 0.0354, RMSE: 0.0558, R2: 0.9498), PNorm: 131.5139, GNorm: 0.8902
[287/299] timecost: 66.34, lr: 0.000016, Train: (LOSS: 0.0084, MAE: 0.0084, RMSE: 0.0154, R2: 0.9928), Valid: (LOSS: 0.0360, MAE: 0.0360, RMSE: 0.0595, R2: 0.9433), PNorm: 131.4666, GNorm: 1.3194
[288/299] timecost: 66.45, lr: 0.000016, Train: (LOSS: 0.0085, MAE: 0.0085, RMSE: 0.0157, R2: 0.9933), Valid: (LOSS: 0.0358, MAE: 0.0358, RMSE: 0.0567, R2: 0.9467), PNorm: 131.4190, GNorm: 0.9837
[289/299] timecost: 66.00, lr: 0.000016, Train: (LOSS: 0.0086, MAE: 0.0086, RMSE: 0.0159, R2: 0.9931), Valid: (LOSS: 0.0348, MAE: 0.0348, RMSE: 0.0565, R2: 0.9488), PNorm: 131.3719, GNorm: 1.0736
[290/299] timecost: 66.40, lr: 0.000016, Train: (LOSS: 0.0083, MAE: 0.0083, RMSE: 0.0157, R2: 0.9932), Valid: (LOSS: 0.0345, MAE: 0.0345, RMSE: 0.0543, R2: 0.9514), PNorm: 131.3242, GNorm: 1.1315
[291/299] timecost: 66.25, lr: 0.000016, Train: (LOSS: 0.0089, MAE: 0.0089, RMSE: 0.0160, R2: 0.9937), Valid: (LOSS: 0.0359, MAE: 0.0359, RMSE: 0.0557, R2: 0.9497), PNorm: 131.2768, GNorm: 1.3805
[292/299] timecost: 65.90, lr: 0.000016, Train: (LOSS: 0.0084, MAE: 0.0084, RMSE: 0.0156, R2: 0.9927), Valid: (LOSS: 0.0343, MAE: 0.0343, RMSE: 0.0544, R2: 0.9527), PNorm: 131.2297, GNorm: 1.1986
Epoch 00294: reducing learning rate of group 0 to 1.4349e-05.
[293/299] timecost: 65.55, lr: 0.000014, Train: (LOSS: 0.0082, MAE: 0.0082, RMSE: 0.0151, R2: 0.9937), Valid: (LOSS: 0.0351, MAE: 0.0351, RMSE: 0.0557, R2: 0.9494), PNorm: 131.1823, GNorm: 0.8019
[294/299] timecost: 65.89, lr: 0.000014, Train: (LOSS: 0.0081, MAE: 0.0081, RMSE: 0.0150, R2: 0.9929), Valid: (LOSS: 0.0341, MAE: 0.0341, RMSE: 0.0533, R2: 0.9534), PNorm: 131.1399, GNorm: 0.8439
[295/299] timecost: 66.48, lr: 0.000014, Train: (LOSS: 0.0077, MAE: 0.0077, RMSE: 0.0148, R2: 0.9942), Valid: (LOSS: 0.0345, MAE: 0.0345, RMSE: 0.0546, R2: 0.9515), PNorm: 131.0971, GNorm: 0.6944
[296/299] timecost: 65.73, lr: 0.000014, Train: (LOSS: 0.0078, MAE: 0.0078, RMSE: 0.0150, R2: 0.9943), Valid: (LOSS: 0.0343, MAE: 0.0343, RMSE: 0.0545, R2: 0.9510), PNorm: 131.0550, GNorm: 1.1085
[297/299] timecost: 65.96, lr: 0.000014, Train: (LOSS: 0.0076, MAE: 0.0076, RMSE: 0.0145, R2: 0.9936), Valid: (LOSS: 0.0356, MAE: 0.0356, RMSE: 0.0567, R2: 0.9488), PNorm: 131.0126, GNorm: 0.7689
[298/299] timecost: 65.74, lr: 0.000014, Train: (LOSS: 0.0080, MAE: 0.0080, RMSE: 0.0150, R2: 0.9942), Valid: (LOSS: 0.0344, MAE: 0.0344, RMSE: 0.0531, R2: 0.9544), PNorm: 130.9705, GNorm: 0.9594
[299/299] timecost: 65.12, lr: 0.000014, Train: (LOSS: 0.0080, MAE: 0.0080, RMSE: 0.0149, R2: 0.9940), Valid: (LOSS: 0.0343, MAE: 0.0343, RMSE: 0.0541, R2: 0.9518), PNorm: 130.9281, GNorm: 0.8059
==========Training End==========
==========Test Best Model==========
================Final Results=======================
mse: 0.0346 +- 0.0000:
rmse: 0.0525 +- 0.0000:
mae: 0.0346 +- 0.0000:
r2: 0.9545 +- 0.0000:
tensor([[0.1104, 0.1066],
        [0.0000, 0.0000],
        [0.0000, 0.0000],
        ...,
        [0.0000, 0.0000],
        [0.5808, 0.4354],
        [0.0000, 0.0000]], device='cuda:0')
