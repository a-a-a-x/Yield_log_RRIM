cuda available with GPU: Tesla V100-PCIE-16GB
==========Load Seed==========
set_random_seed
0
==========Training Start==========
Training Graphs:  2491
Valid Graphs:  277
Test Graphs:  1187
============Loading pretrained weights to generate initialization============
============Creating new layers============
============Creating Model============
Training Graphs Batches:  78
Valid Graphs Batches:  9
Test Graphs Batches:  37
[0/299] timecost: 69.13, lr: 0.000030, Train: (LOSS: 0.2315, MAE: 0.2315, RMSE: 0.2733, R2: -0.0694), Valid: (LOSS: 0.2239, MAE: 0.2239, RMSE: 0.2600, R2: 0.0874), PNorm: 174.3676, GNorm: 0.5405
[1/299] timecost: 66.83, lr: 0.000030, Train: (LOSS: 0.2164, MAE: 0.2164, RMSE: 0.2600, R2: 0.0494), Valid: (LOSS: 0.2135, MAE: 0.2135, RMSE: 0.2537, R2: 0.1321), PNorm: 173.6858, GNorm: 0.4347
[2/299] timecost: 67.03, lr: 0.000030, Train: (LOSS: 0.2062, MAE: 0.2062, RMSE: 0.2491, R2: 0.1192), Valid: (LOSS: 0.1993, MAE: 0.1993, RMSE: 0.2405, R2: 0.2199), PNorm: 173.1628, GNorm: 0.8932
[3/299] timecost: 67.62, lr: 0.000030, Train: (LOSS: 0.1927, MAE: 0.1927, RMSE: 0.2361, R2: 0.1909), Valid: (LOSS: 0.2045, MAE: 0.2045, RMSE: 0.2368, R2: 0.2369), PNorm: 172.7488, GNorm: 1.9331
[4/299] timecost: 68.48, lr: 0.000030, Train: (LOSS: 0.1697, MAE: 0.1697, RMSE: 0.2168, R2: 0.3137), Valid: (LOSS: 0.1455, MAE: 0.1455, RMSE: 0.1823, R2: 0.5347), PNorm: 172.4072, GNorm: 3.8833
[5/299] timecost: 68.44, lr: 0.000030, Train: (LOSS: 0.1483, MAE: 0.1483, RMSE: 0.1954, R2: 0.4304), Valid: (LOSS: 0.1614, MAE: 0.1614, RMSE: 0.2079, R2: 0.3830), PNorm: 172.1139, GNorm: 2.4791
[6/299] timecost: 68.38, lr: 0.000030, Train: (LOSS: 0.1376, MAE: 0.1376, RMSE: 0.1848, R2: 0.4965), Valid: (LOSS: 0.1471, MAE: 0.1471, RMSE: 0.1986, R2: 0.4600), PNorm: 171.8469, GNorm: 1.6993
[7/299] timecost: 67.59, lr: 0.000030, Train: (LOSS: 0.1311, MAE: 0.1311, RMSE: 0.1783, R2: 0.5382), Valid: (LOSS: 0.1167, MAE: 0.1167, RMSE: 0.1573, R2: 0.6483), PNorm: 171.5925, GNorm: 2.2970
[8/299] timecost: 68.95, lr: 0.000030, Train: (LOSS: 0.1257, MAE: 0.1257, RMSE: 0.1737, R2: 0.5459), Valid: (LOSS: 0.1210, MAE: 0.1210, RMSE: 0.1553, R2: 0.6586), PNorm: 171.3534, GNorm: 1.6797
[9/299] timecost: 67.72, lr: 0.000030, Train: (LOSS: 0.1256, MAE: 0.1256, RMSE: 0.1728, R2: 0.5369), Valid: (LOSS: 0.1231, MAE: 0.1231, RMSE: 0.1653, R2: 0.6156), PNorm: 171.1258, GNorm: 2.7705
[10/299] timecost: 67.01, lr: 0.000030, Train: (LOSS: 0.1182, MAE: 0.1182, RMSE: 0.1639, R2: 0.5989), Valid: (LOSS: 0.1167, MAE: 0.1167, RMSE: 0.1573, R2: 0.6506), PNorm: 170.9101, GNorm: 2.5050
[11/299] timecost: 67.40, lr: 0.000030, Train: (LOSS: 0.1226, MAE: 0.1226, RMSE: 0.1699, R2: 0.5612), Valid: (LOSS: 0.1181, MAE: 0.1181, RMSE: 0.1552, R2: 0.6644), PNorm: 170.7055, GNorm: 3.2326
[12/299] timecost: 66.91, lr: 0.000030, Train: (LOSS: 0.1205, MAE: 0.1205, RMSE: 0.1671, R2: 0.5800), Valid: (LOSS: 0.1115, MAE: 0.1115, RMSE: 0.1480, R2: 0.6958), PNorm: 170.5097, GNorm: 1.9179
[13/299] timecost: 66.83, lr: 0.000030, Train: (LOSS: 0.1152, MAE: 0.1152, RMSE: 0.1617, R2: 0.6046), Valid: (LOSS: 0.1033, MAE: 0.1033, RMSE: 0.1440, R2: 0.7154), PNorm: 170.3218, GNorm: 2.7379
[14/299] timecost: 66.29, lr: 0.000030, Train: (LOSS: 0.1141, MAE: 0.1141, RMSE: 0.1605, R2: 0.6061), Valid: (LOSS: 0.1237, MAE: 0.1237, RMSE: 0.1655, R2: 0.6291), PNorm: 170.1395, GNorm: 1.2927
[15/299] timecost: 66.74, lr: 0.000030, Train: (LOSS: 0.1095, MAE: 0.1095, RMSE: 0.1551, R2: 0.6474), Valid: (LOSS: 0.0985, MAE: 0.0985, RMSE: 0.1396, R2: 0.7356), PNorm: 169.9626, GNorm: 5.2921
[16/299] timecost: 66.52, lr: 0.000030, Train: (LOSS: 0.1075, MAE: 0.1075, RMSE: 0.1551, R2: 0.6350), Valid: (LOSS: 0.1009, MAE: 0.1009, RMSE: 0.1426, R2: 0.7160), PNorm: 169.7889, GNorm: 1.4245
[17/299] timecost: 66.35, lr: 0.000030, Train: (LOSS: 0.1027, MAE: 0.1027, RMSE: 0.1469, R2: 0.6741), Valid: (LOSS: 0.1081, MAE: 0.1081, RMSE: 0.1493, R2: 0.6924), PNorm: 169.6217, GNorm: 1.6552
[18/299] timecost: 66.63, lr: 0.000030, Train: (LOSS: 0.1018, MAE: 0.1018, RMSE: 0.1461, R2: 0.6770), Valid: (LOSS: 0.0973, MAE: 0.0973, RMSE: 0.1389, R2: 0.7291), PNorm: 169.4596, GNorm: 1.3423
[19/299] timecost: 66.50, lr: 0.000030, Train: (LOSS: 0.1013, MAE: 0.1013, RMSE: 0.1480, R2: 0.6773), Valid: (LOSS: 0.1042, MAE: 0.1042, RMSE: 0.1538, R2: 0.6791), PNorm: 169.3018, GNorm: 1.3737
[20/299] timecost: 66.41, lr: 0.000030, Train: (LOSS: 0.0959, MAE: 0.0959, RMSE: 0.1396, R2: 0.7010), Valid: (LOSS: 0.0881, MAE: 0.0881, RMSE: 0.1283, R2: 0.7745), PNorm: 169.1463, GNorm: 1.2877
[21/299] timecost: 66.30, lr: 0.000030, Train: (LOSS: 0.0957, MAE: 0.0957, RMSE: 0.1390, R2: 0.7019), Valid: (LOSS: 0.1163, MAE: 0.1163, RMSE: 0.1678, R2: 0.6025), PNorm: 168.9933, GNorm: 3.7537
[22/299] timecost: 65.99, lr: 0.000030, Train: (LOSS: 0.0948, MAE: 0.0948, RMSE: 0.1393, R2: 0.7031), Valid: (LOSS: 0.0963, MAE: 0.0963, RMSE: 0.1404, R2: 0.7288), PNorm: 168.8436, GNorm: 1.8706
[23/299] timecost: 65.07, lr: 0.000030, Train: (LOSS: 0.0934, MAE: 0.0934, RMSE: 0.1374, R2: 0.7122), Valid: (LOSS: 0.1003, MAE: 0.1003, RMSE: 0.1480, R2: 0.7040), PNorm: 168.6952, GNorm: 1.3996
[24/299] timecost: 63.42, lr: 0.000030, Train: (LOSS: 0.0900, MAE: 0.0900, RMSE: 0.1328, R2: 0.7341), Valid: (LOSS: 0.0886, MAE: 0.0886, RMSE: 0.1286, R2: 0.7732), PNorm: 168.5497, GNorm: 4.8132
[25/299] timecost: 63.47, lr: 0.000030, Train: (LOSS: 0.0854, MAE: 0.0854, RMSE: 0.1262, R2: 0.7625), Valid: (LOSS: 0.0796, MAE: 0.0796, RMSE: 0.1193, R2: 0.8034), PNorm: 168.4047, GNorm: 2.6747
[26/299] timecost: 63.17, lr: 0.000030, Train: (LOSS: 0.0804, MAE: 0.0804, RMSE: 0.1181, R2: 0.7905), Valid: (LOSS: 0.0821, MAE: 0.0821, RMSE: 0.1218, R2: 0.7983), PNorm: 168.2623, GNorm: 1.2243
[27/299] timecost: 62.76, lr: 0.000030, Train: (LOSS: 0.0802, MAE: 0.0802, RMSE: 0.1187, R2: 0.7941), Valid: (LOSS: 0.0848, MAE: 0.0848, RMSE: 0.1274, R2: 0.7803), PNorm: 168.1215, GNorm: 1.5286
[28/299] timecost: 62.81, lr: 0.000030, Train: (LOSS: 0.0784, MAE: 0.0784, RMSE: 0.1156, R2: 0.7938), Valid: (LOSS: 0.0862, MAE: 0.0862, RMSE: 0.1240, R2: 0.7863), PNorm: 167.9823, GNorm: 1.6956
[29/299] timecost: 65.01, lr: 0.000030, Train: (LOSS: 0.0798, MAE: 0.0798, RMSE: 0.1179, R2: 0.7836), Valid: (LOSS: 0.0802, MAE: 0.0802, RMSE: 0.1215, R2: 0.7950), PNorm: 167.8458, GNorm: 2.0635
[30/299] timecost: 65.46, lr: 0.000030, Train: (LOSS: 0.0783, MAE: 0.0783, RMSE: 0.1170, R2: 0.7944), Valid: (LOSS: 0.0854, MAE: 0.0854, RMSE: 0.1276, R2: 0.7749), PNorm: 167.7102, GNorm: 1.3531
[31/299] timecost: 65.78, lr: 0.000030, Train: (LOSS: 0.0764, MAE: 0.0764, RMSE: 0.1147, R2: 0.7965), Valid: (LOSS: 0.0870, MAE: 0.0870, RMSE: 0.1287, R2: 0.7667), PNorm: 167.5741, GNorm: 1.2949
[32/299] timecost: 65.74, lr: 0.000030, Train: (LOSS: 0.0746, MAE: 0.0746, RMSE: 0.1134, R2: 0.8116), Valid: (LOSS: 0.0790, MAE: 0.0790, RMSE: 0.1193, R2: 0.8041), PNorm: 167.4399, GNorm: 1.3652
[33/299] timecost: 65.85, lr: 0.000030, Train: (LOSS: 0.0756, MAE: 0.0756, RMSE: 0.1128, R2: 0.8061), Valid: (LOSS: 0.0806, MAE: 0.0806, RMSE: 0.1222, R2: 0.7975), PNorm: 167.3079, GNorm: 1.2761
[34/299] timecost: 65.59, lr: 0.000030, Train: (LOSS: 0.0731, MAE: 0.0731, RMSE: 0.1126, R2: 0.8106), Valid: (LOSS: 0.0763, MAE: 0.0763, RMSE: 0.1167, R2: 0.8149), PNorm: 167.1754, GNorm: 3.2263
[35/299] timecost: 64.83, lr: 0.000030, Train: (LOSS: 0.0757, MAE: 0.0757, RMSE: 0.1139, R2: 0.7990), Valid: (LOSS: 0.0821, MAE: 0.0821, RMSE: 0.1215, R2: 0.7966), PNorm: 167.0456, GNorm: 1.4716
[36/299] timecost: 64.64, lr: 0.000030, Train: (LOSS: 0.0694, MAE: 0.0694, RMSE: 0.1065, R2: 0.8261), Valid: (LOSS: 0.0780, MAE: 0.0780, RMSE: 0.1150, R2: 0.8200), PNorm: 166.9153, GNorm: 1.2633
[37/299] timecost: 64.93, lr: 0.000030, Train: (LOSS: 0.0687, MAE: 0.0687, RMSE: 0.1065, R2: 0.8232), Valid: (LOSS: 0.0740, MAE: 0.0740, RMSE: 0.1151, R2: 0.8196), PNorm: 166.7850, GNorm: 1.2340
[38/299] timecost: 65.32, lr: 0.000030, Train: (LOSS: 0.0662, MAE: 0.0662, RMSE: 0.1030, R2: 0.8358), Valid: (LOSS: 0.0735, MAE: 0.0735, RMSE: 0.1102, R2: 0.8329), PNorm: 166.6552, GNorm: 1.6538
[39/299] timecost: 65.23, lr: 0.000030, Train: (LOSS: 0.0646, MAE: 0.0646, RMSE: 0.1009, R2: 0.8394), Valid: (LOSS: 0.0741, MAE: 0.0741, RMSE: 0.1117, R2: 0.8291), PNorm: 166.5274, GNorm: 2.1699
[40/299] timecost: 65.65, lr: 0.000030, Train: (LOSS: 0.0654, MAE: 0.0654, RMSE: 0.1005, R2: 0.8411), Valid: (LOSS: 0.0816, MAE: 0.0816, RMSE: 0.1207, R2: 0.7933), PNorm: 166.4008, GNorm: 2.8314
[41/299] timecost: 65.07, lr: 0.000030, Train: (LOSS: 0.0642, MAE: 0.0642, RMSE: 0.0992, R2: 0.8418), Valid: (LOSS: 0.0727, MAE: 0.0727, RMSE: 0.1127, R2: 0.8279), PNorm: 166.2765, GNorm: 1.1035
[42/299] timecost: 64.93, lr: 0.000030, Train: (LOSS: 0.0608, MAE: 0.0608, RMSE: 0.0958, R2: 0.8550), Valid: (LOSS: 0.0713, MAE: 0.0713, RMSE: 0.1065, R2: 0.8402), PNorm: 166.1518, GNorm: 1.3698
[43/299] timecost: 65.86, lr: 0.000030, Train: (LOSS: 0.0586, MAE: 0.0586, RMSE: 0.0913, R2: 0.8717), Valid: (LOSS: 0.0751, MAE: 0.0751, RMSE: 0.1108, R2: 0.8260), PNorm: 166.0276, GNorm: 1.1590
[44/299] timecost: 65.42, lr: 0.000030, Train: (LOSS: 0.0576, MAE: 0.0576, RMSE: 0.0902, R2: 0.8749), Valid: (LOSS: 0.0713, MAE: 0.0713, RMSE: 0.1082, R2: 0.8396), PNorm: 165.9048, GNorm: 4.9183
[45/299] timecost: 64.71, lr: 0.000030, Train: (LOSS: 0.0572, MAE: 0.0572, RMSE: 0.0904, R2: 0.8741), Valid: (LOSS: 0.0680, MAE: 0.0680, RMSE: 0.1065, R2: 0.8429), PNorm: 165.7823, GNorm: 1.8063
[46/299] timecost: 65.37, lr: 0.000030, Train: (LOSS: 0.0578, MAE: 0.0578, RMSE: 0.0906, R2: 0.8683), Valid: (LOSS: 0.0646, MAE: 0.0646, RMSE: 0.1020, R2: 0.8561), PNorm: 165.6617, GNorm: 1.2411
[47/299] timecost: 65.30, lr: 0.000030, Train: (LOSS: 0.0527, MAE: 0.0527, RMSE: 0.0827, R2: 0.8946), Valid: (LOSS: 0.0608, MAE: 0.0608, RMSE: 0.0968, R2: 0.8686), PNorm: 165.5409, GNorm: 1.7869
[48/299] timecost: 65.68, lr: 0.000030, Train: (LOSS: 0.0536, MAE: 0.0536, RMSE: 0.0861, R2: 0.8833), Valid: (LOSS: 0.0612, MAE: 0.0612, RMSE: 0.0955, R2: 0.8708), PNorm: 165.4207, GNorm: 2.6411
[49/299] timecost: 65.59, lr: 0.000030, Train: (LOSS: 0.0525, MAE: 0.0525, RMSE: 0.0810, R2: 0.8943), Valid: (LOSS: 0.0674, MAE: 0.0674, RMSE: 0.1053, R2: 0.8462), PNorm: 165.3014, GNorm: 1.2167
[50/299] timecost: 65.67, lr: 0.000030, Train: (LOSS: 0.0519, MAE: 0.0519, RMSE: 0.0797, R2: 0.8985), Valid: (LOSS: 0.0601, MAE: 0.0601, RMSE: 0.0933, R2: 0.8760), PNorm: 165.1824, GNorm: 1.3871
[51/299] timecost: 65.19, lr: 0.000030, Train: (LOSS: 0.0493, MAE: 0.0493, RMSE: 0.0777, R2: 0.9040), Valid: (LOSS: 0.0557, MAE: 0.0557, RMSE: 0.0884, R2: 0.8890), PNorm: 165.0639, GNorm: 1.4049
[52/299] timecost: 65.74, lr: 0.000030, Train: (LOSS: 0.0493, MAE: 0.0493, RMSE: 0.0779, R2: 0.9037), Valid: (LOSS: 0.0638, MAE: 0.0638, RMSE: 0.0976, R2: 0.8626), PNorm: 164.9457, GNorm: 1.2849
[53/299] timecost: 66.74, lr: 0.000030, Train: (LOSS: 0.0470, MAE: 0.0470, RMSE: 0.0737, R2: 0.9144), Valid: (LOSS: 0.0580, MAE: 0.0580, RMSE: 0.0928, R2: 0.8759), PNorm: 164.8278, GNorm: 0.8494
[54/299] timecost: 63.19, lr: 0.000030, Train: (LOSS: 0.0454, MAE: 0.0454, RMSE: 0.0722, R2: 0.9186), Valid: (LOSS: 0.0576, MAE: 0.0576, RMSE: 0.0916, R2: 0.8776), PNorm: 164.7092, GNorm: 1.4473
[55/299] timecost: 63.24, lr: 0.000030, Train: (LOSS: 0.0455, MAE: 0.0455, RMSE: 0.0733, R2: 0.9180), Valid: (LOSS: 0.0643, MAE: 0.0643, RMSE: 0.0976, R2: 0.8669), PNorm: 164.5909, GNorm: 1.3949
[56/299] timecost: 62.93, lr: 0.000030, Train: (LOSS: 0.0453, MAE: 0.0453, RMSE: 0.0718, R2: 0.9152), Valid: (LOSS: 0.0581, MAE: 0.0581, RMSE: 0.0926, R2: 0.8773), PNorm: 164.4734, GNorm: 1.2488
[57/299] timecost: 63.41, lr: 0.000030, Train: (LOSS: 0.0450, MAE: 0.0450, RMSE: 0.0716, R2: 0.9170), Valid: (LOSS: 0.0563, MAE: 0.0563, RMSE: 0.0910, R2: 0.8834), PNorm: 164.3556, GNorm: 1.1333
[58/299] timecost: 65.28, lr: 0.000030, Train: (LOSS: 0.0442, MAE: 0.0442, RMSE: 0.0701, R2: 0.9216), Valid: (LOSS: 0.0586, MAE: 0.0586, RMSE: 0.0912, R2: 0.8796), PNorm: 164.2379, GNorm: 1.5001
[59/299] timecost: 65.77, lr: 0.000030, Train: (LOSS: 0.0437, MAE: 0.0437, RMSE: 0.0685, R2: 0.9245), Valid: (LOSS: 0.0557, MAE: 0.0557, RMSE: 0.0880, R2: 0.8898), PNorm: 164.1196, GNorm: 2.2348
[60/299] timecost: 66.49, lr: 0.000030, Train: (LOSS: 0.0434, MAE: 0.0434, RMSE: 0.0696, R2: 0.9232), Valid: (LOSS: 0.0510, MAE: 0.0510, RMSE: 0.0819, R2: 0.9028), PNorm: 164.0025, GNorm: 1.4385
[61/299] timecost: 66.48, lr: 0.000030, Train: (LOSS: 0.0437, MAE: 0.0437, RMSE: 0.0698, R2: 0.9210), Valid: (LOSS: 0.0571, MAE: 0.0571, RMSE: 0.0902, R2: 0.8857), PNorm: 163.8858, GNorm: 1.0851
[62/299] timecost: 66.66, lr: 0.000030, Train: (LOSS: 0.0419, MAE: 0.0419, RMSE: 0.0666, R2: 0.9288), Valid: (LOSS: 0.0531, MAE: 0.0531, RMSE: 0.0877, R2: 0.8891), PNorm: 163.7691, GNorm: 1.8387
[63/299] timecost: 66.31, lr: 0.000030, Train: (LOSS: 0.0405, MAE: 0.0405, RMSE: 0.0644, R2: 0.9357), Valid: (LOSS: 0.0545, MAE: 0.0545, RMSE: 0.0878, R2: 0.8878), PNorm: 163.6525, GNorm: 1.1728
[64/299] timecost: 63.22, lr: 0.000030, Train: (LOSS: 0.0433, MAE: 0.0433, RMSE: 0.0672, R2: 0.9260), Valid: (LOSS: 0.0520, MAE: 0.0520, RMSE: 0.0823, R2: 0.9016), PNorm: 163.5375, GNorm: 1.5527
[65/299] timecost: 63.01, lr: 0.000030, Train: (LOSS: 0.0396, MAE: 0.0396, RMSE: 0.0627, R2: 0.9379), Valid: (LOSS: 0.0531, MAE: 0.0531, RMSE: 0.0824, R2: 0.9030), PNorm: 163.4212, GNorm: 1.1531
[66/299] timecost: 63.18, lr: 0.000030, Train: (LOSS: 0.0393, MAE: 0.0393, RMSE: 0.0615, R2: 0.9360), Valid: (LOSS: 0.0526, MAE: 0.0526, RMSE: 0.0826, R2: 0.9005), PNorm: 163.3056, GNorm: 1.7373
[67/299] timecost: 64.15, lr: 0.000030, Train: (LOSS: 0.0384, MAE: 0.0384, RMSE: 0.0606, R2: 0.9398), Valid: (LOSS: 0.0526, MAE: 0.0526, RMSE: 0.0829, R2: 0.9017), PNorm: 163.1893, GNorm: 1.5951
[68/299] timecost: 65.17, lr: 0.000030, Train: (LOSS: 0.0384, MAE: 0.0384, RMSE: 0.0609, R2: 0.9390), Valid: (LOSS: 0.0524, MAE: 0.0524, RMSE: 0.0856, R2: 0.8948), PNorm: 163.0729, GNorm: 1.8987
[69/299] timecost: 65.18, lr: 0.000030, Train: (LOSS: 0.0382, MAE: 0.0382, RMSE: 0.0592, R2: 0.9426), Valid: (LOSS: 0.0503, MAE: 0.0503, RMSE: 0.0808, R2: 0.9039), PNorm: 162.9565, GNorm: 1.2589
[70/299] timecost: 65.45, lr: 0.000030, Train: (LOSS: 0.0380, MAE: 0.0380, RMSE: 0.0602, R2: 0.9427), Valid: (LOSS: 0.0507, MAE: 0.0507, RMSE: 0.0792, R2: 0.9087), PNorm: 162.8402, GNorm: 1.8270
[71/299] timecost: 65.65, lr: 0.000030, Train: (LOSS: 0.0361, MAE: 0.0361, RMSE: 0.0575, R2: 0.9454), Valid: (LOSS: 0.0511, MAE: 0.0511, RMSE: 0.0803, R2: 0.9049), PNorm: 162.7249, GNorm: 1.6406
[72/299] timecost: 66.02, lr: 0.000030, Train: (LOSS: 0.0356, MAE: 0.0356, RMSE: 0.0556, R2: 0.9502), Valid: (LOSS: 0.0534, MAE: 0.0534, RMSE: 0.0824, R2: 0.9002), PNorm: 162.6082, GNorm: 1.3309
[73/299] timecost: 65.64, lr: 0.000030, Train: (LOSS: 0.0369, MAE: 0.0369, RMSE: 0.0586, R2: 0.9458), Valid: (LOSS: 0.0523, MAE: 0.0523, RMSE: 0.0807, R2: 0.9046), PNorm: 162.4928, GNorm: 1.4806
[74/299] timecost: 65.19, lr: 0.000030, Train: (LOSS: 0.0366, MAE: 0.0366, RMSE: 0.0564, R2: 0.9464), Valid: (LOSS: 0.0506, MAE: 0.0506, RMSE: 0.0807, R2: 0.9041), PNorm: 162.3779, GNorm: 1.4958
[75/299] timecost: 64.09, lr: 0.000030, Train: (LOSS: 0.0340, MAE: 0.0340, RMSE: 0.0541, R2: 0.9511), Valid: (LOSS: 0.0518, MAE: 0.0518, RMSE: 0.0826, R2: 0.9009), PNorm: 162.2619, GNorm: 1.3621
[76/299] timecost: 63.69, lr: 0.000030, Train: (LOSS: 0.0346, MAE: 0.0346, RMSE: 0.0560, R2: 0.9475), Valid: (LOSS: 0.0487, MAE: 0.0487, RMSE: 0.0779, R2: 0.9127), PNorm: 162.1450, GNorm: 0.8412
[77/299] timecost: 64.01, lr: 0.000030, Train: (LOSS: 0.0336, MAE: 0.0336, RMSE: 0.0532, R2: 0.9518), Valid: (LOSS: 0.0537, MAE: 0.0537, RMSE: 0.0873, R2: 0.8914), PNorm: 162.0292, GNorm: 1.3865
[78/299] timecost: 63.77, lr: 0.000030, Train: (LOSS: 0.0342, MAE: 0.0342, RMSE: 0.0545, R2: 0.9514), Valid: (LOSS: 0.0507, MAE: 0.0507, RMSE: 0.0801, R2: 0.9069), PNorm: 161.9134, GNorm: 1.5905
[79/299] timecost: 63.68, lr: 0.000030, Train: (LOSS: 0.0347, MAE: 0.0347, RMSE: 0.0552, R2: 0.9507), Valid: (LOSS: 0.0478, MAE: 0.0478, RMSE: 0.0741, R2: 0.9202), PNorm: 161.7978, GNorm: 1.0855
[80/299] timecost: 63.74, lr: 0.000030, Train: (LOSS: 0.0321, MAE: 0.0321, RMSE: 0.0509, R2: 0.9560), Valid: (LOSS: 0.0489, MAE: 0.0489, RMSE: 0.0769, R2: 0.9154), PNorm: 161.6797, GNorm: 1.6502
[81/299] timecost: 63.86, lr: 0.000030, Train: (LOSS: 0.0321, MAE: 0.0321, RMSE: 0.0520, R2: 0.9547), Valid: (LOSS: 0.0507, MAE: 0.0507, RMSE: 0.0791, R2: 0.9070), PNorm: 161.5624, GNorm: 0.9858
[82/299] timecost: 63.74, lr: 0.000030, Train: (LOSS: 0.0321, MAE: 0.0321, RMSE: 0.0523, R2: 0.9549), Valid: (LOSS: 0.0493, MAE: 0.0493, RMSE: 0.0773, R2: 0.9121), PNorm: 161.4458, GNorm: 0.9318
[83/299] timecost: 65.09, lr: 0.000030, Train: (LOSS: 0.0330, MAE: 0.0330, RMSE: 0.0523, R2: 0.9520), Valid: (LOSS: 0.0475, MAE: 0.0475, RMSE: 0.0741, R2: 0.9171), PNorm: 161.3287, GNorm: 1.3478
[84/299] timecost: 66.41, lr: 0.000030, Train: (LOSS: 0.0328, MAE: 0.0328, RMSE: 0.0522, R2: 0.9542), Valid: (LOSS: 0.0501, MAE: 0.0501, RMSE: 0.0793, R2: 0.9108), PNorm: 161.2125, GNorm: 0.9922
[85/299] timecost: 67.23, lr: 0.000030, Train: (LOSS: 0.0333, MAE: 0.0333, RMSE: 0.0527, R2: 0.9532), Valid: (LOSS: 0.0546, MAE: 0.0546, RMSE: 0.0847, R2: 0.8964), PNorm: 161.0970, GNorm: 1.0525
[86/299] timecost: 66.95, lr: 0.000030, Train: (LOSS: 0.0331, MAE: 0.0331, RMSE: 0.0518, R2: 0.9576), Valid: (LOSS: 0.0444, MAE: 0.0444, RMSE: 0.0700, R2: 0.9269), PNorm: 160.9803, GNorm: 1.4238
[87/299] timecost: 66.96, lr: 0.000030, Train: (LOSS: 0.0330, MAE: 0.0330, RMSE: 0.0524, R2: 0.9539), Valid: (LOSS: 0.0505, MAE: 0.0505, RMSE: 0.0786, R2: 0.9093), PNorm: 160.8644, GNorm: 1.5255
[88/299] timecost: 67.34, lr: 0.000030, Train: (LOSS: 0.0327, MAE: 0.0327, RMSE: 0.0519, R2: 0.9545), Valid: (LOSS: 0.0467, MAE: 0.0467, RMSE: 0.0731, R2: 0.9203), PNorm: 160.7486, GNorm: 1.1659
[89/299] timecost: 66.81, lr: 0.000030, Train: (LOSS: 0.0328, MAE: 0.0328, RMSE: 0.0524, R2: 0.9559), Valid: (LOSS: 0.0476, MAE: 0.0476, RMSE: 0.0722, R2: 0.9222), PNorm: 160.6330, GNorm: 1.2474
[90/299] timecost: 66.88, lr: 0.000030, Train: (LOSS: 0.0306, MAE: 0.0306, RMSE: 0.0487, R2: 0.9588), Valid: (LOSS: 0.0462, MAE: 0.0462, RMSE: 0.0731, R2: 0.9202), PNorm: 160.5174, GNorm: 1.3613
[91/299] timecost: 67.39, lr: 0.000030, Train: (LOSS: 0.0308, MAE: 0.0308, RMSE: 0.0488, R2: 0.9595), Valid: (LOSS: 0.0458, MAE: 0.0458, RMSE: 0.0757, R2: 0.9161), PNorm: 160.4010, GNorm: 1.1119
[92/299] timecost: 67.23, lr: 0.000030, Train: (LOSS: 0.0292, MAE: 0.0292, RMSE: 0.0470, R2: 0.9615), Valid: (LOSS: 0.0483, MAE: 0.0483, RMSE: 0.0762, R2: 0.9127), PNorm: 160.2833, GNorm: 1.2893
[93/299] timecost: 66.88, lr: 0.000030, Train: (LOSS: 0.0315, MAE: 0.0315, RMSE: 0.0505, R2: 0.9570), Valid: (LOSS: 0.0492, MAE: 0.0492, RMSE: 0.0779, R2: 0.9102), PNorm: 160.1681, GNorm: 1.4579
[94/299] timecost: 67.03, lr: 0.000030, Train: (LOSS: 0.0307, MAE: 0.0307, RMSE: 0.0492, R2: 0.9584), Valid: (LOSS: 0.0451, MAE: 0.0451, RMSE: 0.0722, R2: 0.9232), PNorm: 160.0518, GNorm: 0.9072
[95/299] timecost: 67.21, lr: 0.000030, Train: (LOSS: 0.0300, MAE: 0.0300, RMSE: 0.0475, R2: 0.9631), Valid: (LOSS: 0.0487, MAE: 0.0487, RMSE: 0.0757, R2: 0.9167), PNorm: 159.9346, GNorm: 1.4726
[96/299] timecost: 66.78, lr: 0.000030, Train: (LOSS: 0.0308, MAE: 0.0308, RMSE: 0.0479, R2: 0.9604), Valid: (LOSS: 0.0504, MAE: 0.0504, RMSE: 0.0808, R2: 0.9064), PNorm: 159.8195, GNorm: 2.1533
[97/299] timecost: 66.31, lr: 0.000030, Train: (LOSS: 0.0306, MAE: 0.0306, RMSE: 0.0497, R2: 0.9589), Valid: (LOSS: 0.0435, MAE: 0.0435, RMSE: 0.0703, R2: 0.9241), PNorm: 159.7041, GNorm: 1.3303
[98/299] timecost: 67.06, lr: 0.000030, Train: (LOSS: 0.0287, MAE: 0.0287, RMSE: 0.0467, R2: 0.9640), Valid: (LOSS: 0.0454, MAE: 0.0454, RMSE: 0.0715, R2: 0.9245), PNorm: 159.5876, GNorm: 1.7405
[99/299] timecost: 66.37, lr: 0.000030, Train: (LOSS: 0.0280, MAE: 0.0280, RMSE: 0.0453, R2: 0.9649), Valid: (LOSS: 0.0460, MAE: 0.0460, RMSE: 0.0752, R2: 0.9165), PNorm: 159.4702, GNorm: 1.1042
[100/299] timecost: 65.61, lr: 0.000030, Train: (LOSS: 0.0283, MAE: 0.0283, RMSE: 0.0457, R2: 0.9636), Valid: (LOSS: 0.0460, MAE: 0.0460, RMSE: 0.0731, R2: 0.9216), PNorm: 159.3523, GNorm: 1.4350
[101/299] timecost: 65.13, lr: 0.000030, Train: (LOSS: 0.0276, MAE: 0.0276, RMSE: 0.0450, R2: 0.9661), Valid: (LOSS: 0.0459, MAE: 0.0459, RMSE: 0.0728, R2: 0.9217), PNorm: 159.2343, GNorm: 1.2981
[102/299] timecost: 65.47, lr: 0.000030, Train: (LOSS: 0.0287, MAE: 0.0287, RMSE: 0.0461, R2: 0.9652), Valid: (LOSS: 0.0462, MAE: 0.0462, RMSE: 0.0739, R2: 0.9195), PNorm: 159.1161, GNorm: 0.9605
[103/299] timecost: 64.29, lr: 0.000030, Train: (LOSS: 0.0289, MAE: 0.0289, RMSE: 0.0458, R2: 0.9641), Valid: (LOSS: 0.0499, MAE: 0.0499, RMSE: 0.0783, R2: 0.9112), PNorm: 158.9992, GNorm: 1.4851
[104/299] timecost: 65.08, lr: 0.000030, Train: (LOSS: 0.0266, MAE: 0.0266, RMSE: 0.0437, R2: 0.9683), Valid: (LOSS: 0.0449, MAE: 0.0449, RMSE: 0.0695, R2: 0.9282), PNorm: 158.8813, GNorm: 0.9386
[105/299] timecost: 65.65, lr: 0.000030, Train: (LOSS: 0.0273, MAE: 0.0273, RMSE: 0.0443, R2: 0.9680), Valid: (LOSS: 0.0463, MAE: 0.0463, RMSE: 0.0726, R2: 0.9242), PNorm: 158.7623, GNorm: 1.0231
[106/299] timecost: 64.95, lr: 0.000030, Train: (LOSS: 0.0266, MAE: 0.0266, RMSE: 0.0428, R2: 0.9687), Valid: (LOSS: 0.0432, MAE: 0.0432, RMSE: 0.0680, R2: 0.9306), PNorm: 158.6443, GNorm: 1.4711
[107/299] timecost: 63.93, lr: 0.000030, Train: (LOSS: 0.0271, MAE: 0.0271, RMSE: 0.0434, R2: 0.9672), Valid: (LOSS: 0.0441, MAE: 0.0441, RMSE: 0.0700, R2: 0.9272), PNorm: 158.5261, GNorm: 1.3481
[108/299] timecost: 63.71, lr: 0.000030, Train: (LOSS: 0.0275, MAE: 0.0275, RMSE: 0.0445, R2: 0.9673), Valid: (LOSS: 0.0419, MAE: 0.0419, RMSE: 0.0663, R2: 0.9342), PNorm: 158.4080, GNorm: 0.7281
[109/299] timecost: 63.89, lr: 0.000030, Train: (LOSS: 0.0260, MAE: 0.0260, RMSE: 0.0427, R2: 0.9693), Valid: (LOSS: 0.0433, MAE: 0.0433, RMSE: 0.0688, R2: 0.9284), PNorm: 158.2888, GNorm: 1.4224
[110/299] timecost: 63.61, lr: 0.000030, Train: (LOSS: 0.0276, MAE: 0.0276, RMSE: 0.0450, R2: 0.9649), Valid: (LOSS: 0.0465, MAE: 0.0465, RMSE: 0.0723, R2: 0.9243), PNorm: 158.1699, GNorm: 0.7181
[111/299] timecost: 63.46, lr: 0.000030, Train: (LOSS: 0.0266, MAE: 0.0266, RMSE: 0.0433, R2: 0.9670), Valid: (LOSS: 0.0439, MAE: 0.0439, RMSE: 0.0684, R2: 0.9309), PNorm: 158.0520, GNorm: 0.8031
[112/299] timecost: 62.96, lr: 0.000030, Train: (LOSS: 0.0256, MAE: 0.0256, RMSE: 0.0421, R2: 0.9696), Valid: (LOSS: 0.0446, MAE: 0.0446, RMSE: 0.0691, R2: 0.9292), PNorm: 157.9334, GNorm: 0.7759
[113/299] timecost: 63.08, lr: 0.000030, Train: (LOSS: 0.0248, MAE: 0.0248, RMSE: 0.0404, R2: 0.9720), Valid: (LOSS: 0.0435, MAE: 0.0435, RMSE: 0.0667, R2: 0.9330), PNorm: 157.8139, GNorm: 1.5768
[114/299] timecost: 64.29, lr: 0.000030, Train: (LOSS: 0.0249, MAE: 0.0249, RMSE: 0.0410, R2: 0.9714), Valid: (LOSS: 0.0428, MAE: 0.0428, RMSE: 0.0672, R2: 0.9312), PNorm: 157.6945, GNorm: 1.7538
[115/299] timecost: 66.15, lr: 0.000030, Train: (LOSS: 0.0258, MAE: 0.0258, RMSE: 0.0423, R2: 0.9685), Valid: (LOSS: 0.0414, MAE: 0.0414, RMSE: 0.0642, R2: 0.9379), PNorm: 157.5766, GNorm: 1.0410
[116/299] timecost: 66.42, lr: 0.000030, Train: (LOSS: 0.0257, MAE: 0.0257, RMSE: 0.0420, R2: 0.9681), Valid: (LOSS: 0.0423, MAE: 0.0423, RMSE: 0.0660, R2: 0.9350), PNorm: 157.4587, GNorm: 1.0090
[117/299] timecost: 65.35, lr: 0.000030, Train: (LOSS: 0.0259, MAE: 0.0259, RMSE: 0.0418, R2: 0.9689), Valid: (LOSS: 0.0426, MAE: 0.0426, RMSE: 0.0650, R2: 0.9381), PNorm: 157.3401, GNorm: 1.1707
[118/299] timecost: 65.40, lr: 0.000030, Train: (LOSS: 0.0268, MAE: 0.0268, RMSE: 0.0429, R2: 0.9650), Valid: (LOSS: 0.0414, MAE: 0.0414, RMSE: 0.0648, R2: 0.9374), PNorm: 157.2218, GNorm: 0.9638
[119/299] timecost: 65.39, lr: 0.000030, Train: (LOSS: 0.0257, MAE: 0.0257, RMSE: 0.0412, R2: 0.9697), Valid: (LOSS: 0.0439, MAE: 0.0439, RMSE: 0.0662, R2: 0.9371), PNorm: 157.1047, GNorm: 1.5381
[120/299] timecost: 65.21, lr: 0.000030, Train: (LOSS: 0.0249, MAE: 0.0249, RMSE: 0.0407, R2: 0.9710), Valid: (LOSS: 0.0455, MAE: 0.0455, RMSE: 0.0729, R2: 0.9198), PNorm: 156.9861, GNorm: 1.0904
[121/299] timecost: 64.96, lr: 0.000030, Train: (LOSS: 0.0252, MAE: 0.0252, RMSE: 0.0410, R2: 0.9708), Valid: (LOSS: 0.0435, MAE: 0.0435, RMSE: 0.0677, R2: 0.9318), PNorm: 156.8677, GNorm: 0.9838
[122/299] timecost: 65.37, lr: 0.000030, Train: (LOSS: 0.0254, MAE: 0.0254, RMSE: 0.0413, R2: 0.9712), Valid: (LOSS: 0.0428, MAE: 0.0428, RMSE: 0.0659, R2: 0.9353), PNorm: 156.7496, GNorm: 0.9343
[123/299] timecost: 65.22, lr: 0.000030, Train: (LOSS: 0.0242, MAE: 0.0242, RMSE: 0.0391, R2: 0.9737), Valid: (LOSS: 0.0421, MAE: 0.0421, RMSE: 0.0665, R2: 0.9341), PNorm: 156.6312, GNorm: 1.0595
[124/299] timecost: 65.32, lr: 0.000030, Train: (LOSS: 0.0240, MAE: 0.0240, RMSE: 0.0394, R2: 0.9737), Valid: (LOSS: 0.0389, MAE: 0.0389, RMSE: 0.0625, R2: 0.9410), PNorm: 156.5137, GNorm: 1.1530
[125/299] timecost: 64.40, lr: 0.000030, Train: (LOSS: 0.0239, MAE: 0.0239, RMSE: 0.0386, R2: 0.9732), Valid: (LOSS: 0.0409, MAE: 0.0409, RMSE: 0.0629, R2: 0.9402), PNorm: 156.3943, GNorm: 2.0310
[126/299] timecost: 64.38, lr: 0.000030, Train: (LOSS: 0.0228, MAE: 0.0228, RMSE: 0.0374, R2: 0.9768), Valid: (LOSS: 0.0418, MAE: 0.0418, RMSE: 0.0644, R2: 0.9367), PNorm: 156.2765, GNorm: 0.8512
[127/299] timecost: 64.17, lr: 0.000030, Train: (LOSS: 0.0231, MAE: 0.0231, RMSE: 0.0377, R2: 0.9750), Valid: (LOSS: 0.0427, MAE: 0.0427, RMSE: 0.0678, R2: 0.9297), PNorm: 156.1587, GNorm: 0.7813
[128/299] timecost: 63.57, lr: 0.000030, Train: (LOSS: 0.0223, MAE: 0.0223, RMSE: 0.0372, R2: 0.9752), Valid: (LOSS: 0.0432, MAE: 0.0432, RMSE: 0.0683, R2: 0.9275), PNorm: 156.0403, GNorm: 0.8788
[129/299] timecost: 64.61, lr: 0.000030, Train: (LOSS: 0.0242, MAE: 0.0242, RMSE: 0.0387, R2: 0.9729), Valid: (LOSS: 0.0437, MAE: 0.0437, RMSE: 0.0663, R2: 0.9346), PNorm: 155.9237, GNorm: 1.0562
[130/299] timecost: 64.70, lr: 0.000030, Train: (LOSS: 0.0238, MAE: 0.0238, RMSE: 0.0384, R2: 0.9750), Valid: (LOSS: 0.0403, MAE: 0.0403, RMSE: 0.0646, R2: 0.9374), PNorm: 155.8089, GNorm: 1.2568
[131/299] timecost: 64.41, lr: 0.000030, Train: (LOSS: 0.0224, MAE: 0.0224, RMSE: 0.0366, R2: 0.9766), Valid: (LOSS: 0.0412, MAE: 0.0412, RMSE: 0.0673, R2: 0.9305), PNorm: 155.6926, GNorm: 0.9661
[132/299] timecost: 63.17, lr: 0.000030, Train: (LOSS: 0.0223, MAE: 0.0223, RMSE: 0.0370, R2: 0.9756), Valid: (LOSS: 0.0436, MAE: 0.0436, RMSE: 0.0683, R2: 0.9283), PNorm: 155.5750, GNorm: 1.3348
[133/299] timecost: 62.81, lr: 0.000030, Train: (LOSS: 0.0237, MAE: 0.0237, RMSE: 0.0373, R2: 0.9745), Valid: (LOSS: 0.0401, MAE: 0.0401, RMSE: 0.0633, R2: 0.9412), PNorm: 155.4591, GNorm: 1.0047
[134/299] timecost: 64.04, lr: 0.000030, Train: (LOSS: 0.0221, MAE: 0.0221, RMSE: 0.0359, R2: 0.9767), Valid: (LOSS: 0.0420, MAE: 0.0420, RMSE: 0.0641, R2: 0.9383), PNorm: 155.3424, GNorm: 0.8572
[135/299] timecost: 63.05, lr: 0.000030, Train: (LOSS: 0.0214, MAE: 0.0214, RMSE: 0.0348, R2: 0.9778), Valid: (LOSS: 0.0411, MAE: 0.0411, RMSE: 0.0629, R2: 0.9386), PNorm: 155.2265, GNorm: 1.0669
[136/299] timecost: 63.05, lr: 0.000030, Train: (LOSS: 0.0214, MAE: 0.0214, RMSE: 0.0351, R2: 0.9789), Valid: (LOSS: 0.0389, MAE: 0.0389, RMSE: 0.0621, R2: 0.9429), PNorm: 155.1083, GNorm: 1.1035
[137/299] timecost: 62.92, lr: 0.000030, Train: (LOSS: 0.0227, MAE: 0.0227, RMSE: 0.0359, R2: 0.9768), Valid: (LOSS: 0.0428, MAE: 0.0428, RMSE: 0.0690, R2: 0.9298), PNorm: 154.9923, GNorm: 0.9154
[138/299] timecost: 62.80, lr: 0.000030, Train: (LOSS: 0.0224, MAE: 0.0224, RMSE: 0.0363, R2: 0.9752), Valid: (LOSS: 0.0401, MAE: 0.0401, RMSE: 0.0636, R2: 0.9390), PNorm: 154.8755, GNorm: 0.7438
[139/299] timecost: 62.67, lr: 0.000030, Train: (LOSS: 0.0214, MAE: 0.0214, RMSE: 0.0355, R2: 0.9781), Valid: (LOSS: 0.0392, MAE: 0.0392, RMSE: 0.0612, R2: 0.9429), PNorm: 154.7592, GNorm: 0.7913
[140/299] timecost: 63.64, lr: 0.000030, Train: (LOSS: 0.0208, MAE: 0.0208, RMSE: 0.0341, R2: 0.9790), Valid: (LOSS: 0.0393, MAE: 0.0393, RMSE: 0.0621, R2: 0.9418), PNorm: 154.6426, GNorm: 1.2815
[141/299] timecost: 63.36, lr: 0.000030, Train: (LOSS: 0.0207, MAE: 0.0207, RMSE: 0.0341, R2: 0.9789), Valid: (LOSS: 0.0383, MAE: 0.0383, RMSE: 0.0607, R2: 0.9432), PNorm: 154.5240, GNorm: 1.2526
[142/299] timecost: 62.76, lr: 0.000030, Train: (LOSS: 0.0208, MAE: 0.0208, RMSE: 0.0341, R2: 0.9789), Valid: (LOSS: 0.0398, MAE: 0.0398, RMSE: 0.0637, R2: 0.9367), PNorm: 154.4072, GNorm: 0.8874
[143/299] timecost: 63.19, lr: 0.000030, Train: (LOSS: 0.0215, MAE: 0.0215, RMSE: 0.0352, R2: 0.9775), Valid: (LOSS: 0.0435, MAE: 0.0435, RMSE: 0.0673, R2: 0.9327), PNorm: 154.2904, GNorm: 0.8982
[144/299] timecost: 62.88, lr: 0.000030, Train: (LOSS: 0.0233, MAE: 0.0233, RMSE: 0.0369, R2: 0.9757), Valid: (LOSS: 0.0431, MAE: 0.0431, RMSE: 0.0669, R2: 0.9310), PNorm: 154.1755, GNorm: 0.9411
[145/299] timecost: 62.94, lr: 0.000030, Train: (LOSS: 0.0217, MAE: 0.0217, RMSE: 0.0347, R2: 0.9786), Valid: (LOSS: 0.0375, MAE: 0.0375, RMSE: 0.0609, R2: 0.9435), PNorm: 154.0593, GNorm: 1.0418
[146/299] timecost: 63.17, lr: 0.000030, Train: (LOSS: 0.0205, MAE: 0.0205, RMSE: 0.0338, R2: 0.9794), Valid: (LOSS: 0.0413, MAE: 0.0413, RMSE: 0.0646, R2: 0.9363), PNorm: 153.9439, GNorm: 0.8399
[147/299] timecost: 63.39, lr: 0.000030, Train: (LOSS: 0.0205, MAE: 0.0205, RMSE: 0.0334, R2: 0.9794), Valid: (LOSS: 0.0393, MAE: 0.0393, RMSE: 0.0641, R2: 0.9385), PNorm: 153.8292, GNorm: 1.1390
[148/299] timecost: 63.10, lr: 0.000030, Train: (LOSS: 0.0214, MAE: 0.0214, RMSE: 0.0345, R2: 0.9773), Valid: (LOSS: 0.0386, MAE: 0.0386, RMSE: 0.0606, R2: 0.9437), PNorm: 153.7149, GNorm: 0.6921
[149/299] timecost: 62.76, lr: 0.000030, Train: (LOSS: 0.0207, MAE: 0.0207, RMSE: 0.0336, R2: 0.9806), Valid: (LOSS: 0.0385, MAE: 0.0385, RMSE: 0.0603, R2: 0.9451), PNorm: 153.6000, GNorm: 1.2159
[150/299] timecost: 63.08, lr: 0.000030, Train: (LOSS: 0.0193, MAE: 0.0193, RMSE: 0.0316, R2: 0.9819), Valid: (LOSS: 0.0374, MAE: 0.0374, RMSE: 0.0601, R2: 0.9447), PNorm: 153.4839, GNorm: 1.5012
[151/299] timecost: 62.84, lr: 0.000030, Train: (LOSS: 0.0203, MAE: 0.0203, RMSE: 0.0330, R2: 0.9811), Valid: (LOSS: 0.0379, MAE: 0.0379, RMSE: 0.0590, R2: 0.9462), PNorm: 153.3690, GNorm: 1.0907
[152/299] timecost: 62.62, lr: 0.000030, Train: (LOSS: 0.0196, MAE: 0.0196, RMSE: 0.0321, R2: 0.9806), Valid: (LOSS: 0.0385, MAE: 0.0385, RMSE: 0.0604, R2: 0.9436), PNorm: 153.2524, GNorm: 0.7898
[153/299] timecost: 62.62, lr: 0.000030, Train: (LOSS: 0.0204, MAE: 0.0204, RMSE: 0.0331, R2: 0.9808), Valid: (LOSS: 0.0394, MAE: 0.0394, RMSE: 0.0623, R2: 0.9408), PNorm: 153.1370, GNorm: 0.9510
[154/299] timecost: 63.06, lr: 0.000030, Train: (LOSS: 0.0196, MAE: 0.0196, RMSE: 0.0322, R2: 0.9811), Valid: (LOSS: 0.0390, MAE: 0.0390, RMSE: 0.0626, R2: 0.9395), PNorm: 153.0209, GNorm: 0.9827
[155/299] timecost: 62.78, lr: 0.000030, Train: (LOSS: 0.0196, MAE: 0.0196, RMSE: 0.0324, R2: 0.9798), Valid: (LOSS: 0.0384, MAE: 0.0384, RMSE: 0.0599, R2: 0.9460), PNorm: 152.9054, GNorm: 1.5913
[156/299] timecost: 62.71, lr: 0.000030, Train: (LOSS: 0.0198, MAE: 0.0198, RMSE: 0.0327, R2: 0.9800), Valid: (LOSS: 0.0401, MAE: 0.0401, RMSE: 0.0620, R2: 0.9418), PNorm: 152.7898, GNorm: 1.4821
[157/299] timecost: 62.92, lr: 0.000030, Train: (LOSS: 0.0189, MAE: 0.0189, RMSE: 0.0309, R2: 0.9824), Valid: (LOSS: 0.0373, MAE: 0.0373, RMSE: 0.0594, R2: 0.9462), PNorm: 152.6736, GNorm: 1.4050
[158/299] timecost: 63.13, lr: 0.000030, Train: (LOSS: 0.0187, MAE: 0.0187, RMSE: 0.0307, R2: 0.9824), Valid: (LOSS: 0.0371, MAE: 0.0371, RMSE: 0.0600, R2: 0.9455), PNorm: 152.5583, GNorm: 0.9878
[159/299] timecost: 65.01, lr: 0.000030, Train: (LOSS: 0.0184, MAE: 0.0184, RMSE: 0.0307, R2: 0.9821), Valid: (LOSS: 0.0405, MAE: 0.0405, RMSE: 0.0640, R2: 0.9380), PNorm: 152.4422, GNorm: 1.0918
[160/299] timecost: 65.95, lr: 0.000030, Train: (LOSS: 0.0189, MAE: 0.0189, RMSE: 0.0313, R2: 0.9825), Valid: (LOSS: 0.0383, MAE: 0.0383, RMSE: 0.0603, R2: 0.9446), PNorm: 152.3267, GNorm: 0.9159
[161/299] timecost: 67.41, lr: 0.000030, Train: (LOSS: 0.0183, MAE: 0.0183, RMSE: 0.0301, R2: 0.9819), Valid: (LOSS: 0.0378, MAE: 0.0378, RMSE: 0.0578, R2: 0.9478), PNorm: 152.2099, GNorm: 0.9140
[162/299] timecost: 68.02, lr: 0.000030, Train: (LOSS: 0.0186, MAE: 0.0186, RMSE: 0.0303, R2: 0.9833), Valid: (LOSS: 0.0373, MAE: 0.0373, RMSE: 0.0606, R2: 0.9431), PNorm: 152.0948, GNorm: 1.2680
[163/299] timecost: 67.55, lr: 0.000030, Train: (LOSS: 0.0185, MAE: 0.0185, RMSE: 0.0302, R2: 0.9830), Valid: (LOSS: 0.0375, MAE: 0.0375, RMSE: 0.0598, R2: 0.9445), PNorm: 151.9794, GNorm: 1.2286
[164/299] timecost: 67.75, lr: 0.000030, Train: (LOSS: 0.0191, MAE: 0.0191, RMSE: 0.0308, R2: 0.9829), Valid: (LOSS: 0.0392, MAE: 0.0392, RMSE: 0.0614, R2: 0.9430), PNorm: 151.8653, GNorm: 0.9517
[165/299] timecost: 67.71, lr: 0.000030, Train: (LOSS: 0.0179, MAE: 0.0179, RMSE: 0.0291, R2: 0.9838), Valid: (LOSS: 0.0388, MAE: 0.0388, RMSE: 0.0591, R2: 0.9454), PNorm: 151.7500, GNorm: 0.8975
[166/299] timecost: 66.38, lr: 0.000030, Train: (LOSS: 0.0184, MAE: 0.0184, RMSE: 0.0300, R2: 0.9831), Valid: (LOSS: 0.0372, MAE: 0.0372, RMSE: 0.0589, R2: 0.9449), PNorm: 151.6358, GNorm: 1.2249
[167/299] timecost: 66.35, lr: 0.000030, Train: (LOSS: 0.0178, MAE: 0.0178, RMSE: 0.0288, R2: 0.9846), Valid: (LOSS: 0.0354, MAE: 0.0354, RMSE: 0.0551, R2: 0.9530), PNorm: 151.5200, GNorm: 0.8846
[168/299] timecost: 65.84, lr: 0.000030, Train: (LOSS: 0.0174, MAE: 0.0174, RMSE: 0.0288, R2: 0.9844), Valid: (LOSS: 0.0393, MAE: 0.0393, RMSE: 0.0633, R2: 0.9385), PNorm: 151.4057, GNorm: 1.0278
[169/299] timecost: 65.28, lr: 0.000030, Train: (LOSS: 0.0181, MAE: 0.0181, RMSE: 0.0295, R2: 0.9837), Valid: (LOSS: 0.0376, MAE: 0.0376, RMSE: 0.0591, R2: 0.9473), PNorm: 151.2907, GNorm: 1.3548
[170/299] timecost: 66.88, lr: 0.000030, Train: (LOSS: 0.0179, MAE: 0.0179, RMSE: 0.0291, R2: 0.9848), Valid: (LOSS: 0.0361, MAE: 0.0361, RMSE: 0.0583, R2: 0.9492), PNorm: 151.1760, GNorm: 0.8426
[171/299] timecost: 66.58, lr: 0.000030, Train: (LOSS: 0.0181, MAE: 0.0181, RMSE: 0.0295, R2: 0.9841), Valid: (LOSS: 0.0371, MAE: 0.0371, RMSE: 0.0587, R2: 0.9471), PNorm: 151.0627, GNorm: 0.9496
[172/299] timecost: 66.47, lr: 0.000030, Train: (LOSS: 0.0169, MAE: 0.0169, RMSE: 0.0282, R2: 0.9836), Valid: (LOSS: 0.0377, MAE: 0.0377, RMSE: 0.0594, R2: 0.9459), PNorm: 150.9477, GNorm: 0.7230
[173/299] timecost: 67.02, lr: 0.000030, Train: (LOSS: 0.0177, MAE: 0.0177, RMSE: 0.0289, R2: 0.9845), Valid: (LOSS: 0.0354, MAE: 0.0354, RMSE: 0.0556, R2: 0.9534), PNorm: 150.8328, GNorm: 1.3087
[174/299] timecost: 66.19, lr: 0.000030, Train: (LOSS: 0.0187, MAE: 0.0187, RMSE: 0.0291, R2: 0.9842), Valid: (LOSS: 0.0375, MAE: 0.0375, RMSE: 0.0590, R2: 0.9466), PNorm: 150.7189, GNorm: 1.5385
[175/299] timecost: 66.72, lr: 0.000030, Train: (LOSS: 0.0176, MAE: 0.0176, RMSE: 0.0281, R2: 0.9824), Valid: (LOSS: 0.0384, MAE: 0.0384, RMSE: 0.0585, R2: 0.9469), PNorm: 150.6046, GNorm: 0.8658
[176/299] timecost: 66.63, lr: 0.000030, Train: (LOSS: 0.0176, MAE: 0.0176, RMSE: 0.0280, R2: 0.9850), Valid: (LOSS: 0.0384, MAE: 0.0384, RMSE: 0.0598, R2: 0.9472), PNorm: 150.4909, GNorm: 1.2258
[177/299] timecost: 66.93, lr: 0.000030, Train: (LOSS: 0.0170, MAE: 0.0170, RMSE: 0.0278, R2: 0.9843), Valid: (LOSS: 0.0375, MAE: 0.0375, RMSE: 0.0571, R2: 0.9500), PNorm: 150.3788, GNorm: 0.8815
[178/299] timecost: 66.40, lr: 0.000030, Train: (LOSS: 0.0163, MAE: 0.0163, RMSE: 0.0268, R2: 0.9858), Valid: (LOSS: 0.0360, MAE: 0.0360, RMSE: 0.0573, R2: 0.9481), PNorm: 150.2648, GNorm: 1.0987
[179/299] timecost: 65.78, lr: 0.000030, Train: (LOSS: 0.0158, MAE: 0.0158, RMSE: 0.0264, R2: 0.9868), Valid: (LOSS: 0.0368, MAE: 0.0368, RMSE: 0.0563, R2: 0.9510), PNorm: 150.1505, GNorm: 0.7068
[180/299] timecost: 65.93, lr: 0.000030, Train: (LOSS: 0.0157, MAE: 0.0157, RMSE: 0.0260, R2: 0.9844), Valid: (LOSS: 0.0382, MAE: 0.0382, RMSE: 0.0606, R2: 0.9435), PNorm: 150.0361, GNorm: 0.7422
[181/299] timecost: 65.48, lr: 0.000030, Train: (LOSS: 0.0172, MAE: 0.0172, RMSE: 0.0279, R2: 0.9844), Valid: (LOSS: 0.0385, MAE: 0.0385, RMSE: 0.0596, R2: 0.9472), PNorm: 149.9247, GNorm: 1.3796
[182/299] timecost: 65.60, lr: 0.000030, Train: (LOSS: 0.0166, MAE: 0.0166, RMSE: 0.0268, R2: 0.9867), Valid: (LOSS: 0.0373, MAE: 0.0373, RMSE: 0.0579, R2: 0.9489), PNorm: 149.8115, GNorm: 1.1857
[183/299] timecost: 66.25, lr: 0.000030, Train: (LOSS: 0.0170, MAE: 0.0170, RMSE: 0.0271, R2: 0.9846), Valid: (LOSS: 0.0372, MAE: 0.0372, RMSE: 0.0588, R2: 0.9453), PNorm: 149.6992, GNorm: 1.1694
[184/299] timecost: 66.11, lr: 0.000030, Train: (LOSS: 0.0161, MAE: 0.0161, RMSE: 0.0263, R2: 0.9860), Valid: (LOSS: 0.0364, MAE: 0.0364, RMSE: 0.0569, R2: 0.9493), PNorm: 149.5868, GNorm: 0.7802
[185/299] timecost: 66.22, lr: 0.000030, Train: (LOSS: 0.0159, MAE: 0.0159, RMSE: 0.0257, R2: 0.9872), Valid: (LOSS: 0.0359, MAE: 0.0359, RMSE: 0.0570, R2: 0.9486), PNorm: 149.4743, GNorm: 1.2618
[186/299] timecost: 66.18, lr: 0.000030, Train: (LOSS: 0.0158, MAE: 0.0158, RMSE: 0.0254, R2: 0.9867), Valid: (LOSS: 0.0383, MAE: 0.0383, RMSE: 0.0588, R2: 0.9472), PNorm: 149.3622, GNorm: 0.9173
[187/299] timecost: 66.32, lr: 0.000030, Train: (LOSS: 0.0168, MAE: 0.0168, RMSE: 0.0267, R2: 0.9857), Valid: (LOSS: 0.0356, MAE: 0.0356, RMSE: 0.0572, R2: 0.9490), PNorm: 149.2510, GNorm: 1.1025
Epoch 00189: reducing learning rate of group 0 to 2.7000e-05.
[188/299] timecost: 65.59, lr: 0.000027, Train: (LOSS: 0.0163, MAE: 0.0163, RMSE: 0.0263, R2: 0.9863), Valid: (LOSS: 0.0357, MAE: 0.0357, RMSE: 0.0532, R2: 0.9554), PNorm: 149.1398, GNorm: 1.0814
[189/299] timecost: 64.80, lr: 0.000027, Train: (LOSS: 0.0158, MAE: 0.0158, RMSE: 0.0253, R2: 0.9870), Valid: (LOSS: 0.0357, MAE: 0.0357, RMSE: 0.0553, R2: 0.9518), PNorm: 149.0391, GNorm: 0.9061
[190/299] timecost: 63.45, lr: 0.000027, Train: (LOSS: 0.0156, MAE: 0.0156, RMSE: 0.0253, R2: 0.9871), Valid: (LOSS: 0.0349, MAE: 0.0349, RMSE: 0.0553, R2: 0.9523), PNorm: 148.9390, GNorm: 0.8803
[191/299] timecost: 63.26, lr: 0.000027, Train: (LOSS: 0.0151, MAE: 0.0151, RMSE: 0.0247, R2: 0.9877), Valid: (LOSS: 0.0354, MAE: 0.0354, RMSE: 0.0549, R2: 0.9517), PNorm: 148.8387, GNorm: 0.7457
[192/299] timecost: 63.05, lr: 0.000027, Train: (LOSS: 0.0149, MAE: 0.0149, RMSE: 0.0247, R2: 0.9878), Valid: (LOSS: 0.0371, MAE: 0.0371, RMSE: 0.0559, R2: 0.9506), PNorm: 148.7388, GNorm: 0.5985
[193/299] timecost: 62.68, lr: 0.000027, Train: (LOSS: 0.0150, MAE: 0.0150, RMSE: 0.0246, R2: 0.9874), Valid: (LOSS: 0.0362, MAE: 0.0362, RMSE: 0.0569, R2: 0.9494), PNorm: 148.6378, GNorm: 1.4156
[194/299] timecost: 62.77, lr: 0.000027, Train: (LOSS: 0.0141, MAE: 0.0141, RMSE: 0.0239, R2: 0.9879), Valid: (LOSS: 0.0377, MAE: 0.0377, RMSE: 0.0576, R2: 0.9463), PNorm: 148.5374, GNorm: 0.9016
[195/299] timecost: 62.89, lr: 0.000027, Train: (LOSS: 0.0146, MAE: 0.0146, RMSE: 0.0234, R2: 0.9884), Valid: (LOSS: 0.0381, MAE: 0.0381, RMSE: 0.0592, R2: 0.9460), PNorm: 148.4369, GNorm: 0.7595
[196/299] timecost: 62.85, lr: 0.000027, Train: (LOSS: 0.0149, MAE: 0.0149, RMSE: 0.0240, R2: 0.9879), Valid: (LOSS: 0.0368, MAE: 0.0368, RMSE: 0.0551, R2: 0.9526), PNorm: 148.3360, GNorm: 0.9677
[197/299] timecost: 62.72, lr: 0.000027, Train: (LOSS: 0.0144, MAE: 0.0144, RMSE: 0.0234, R2: 0.9890), Valid: (LOSS: 0.0371, MAE: 0.0371, RMSE: 0.0569, R2: 0.9494), PNorm: 148.2360, GNorm: 0.9547
[198/299] timecost: 63.07, lr: 0.000027, Train: (LOSS: 0.0144, MAE: 0.0144, RMSE: 0.0232, R2: 0.9892), Valid: (LOSS: 0.0345, MAE: 0.0345, RMSE: 0.0544, R2: 0.9534), PNorm: 148.1358, GNorm: 0.8352
[199/299] timecost: 62.81, lr: 0.000027, Train: (LOSS: 0.0140, MAE: 0.0140, RMSE: 0.0231, R2: 0.9877), Valid: (LOSS: 0.0340, MAE: 0.0340, RMSE: 0.0520, R2: 0.9571), PNorm: 148.0343, GNorm: 1.3931
[200/299] timecost: 63.39, lr: 0.000027, Train: (LOSS: 0.0141, MAE: 0.0141, RMSE: 0.0229, R2: 0.9894), Valid: (LOSS: 0.0370, MAE: 0.0370, RMSE: 0.0561, R2: 0.9507), PNorm: 147.9337, GNorm: 1.3193
[201/299] timecost: 64.26, lr: 0.000027, Train: (LOSS: 0.0148, MAE: 0.0148, RMSE: 0.0238, R2: 0.9890), Valid: (LOSS: 0.0344, MAE: 0.0344, RMSE: 0.0539, R2: 0.9551), PNorm: 147.8344, GNorm: 0.7739
[202/299] timecost: 63.58, lr: 0.000027, Train: (LOSS: 0.0137, MAE: 0.0137, RMSE: 0.0219, R2: 0.9900), Valid: (LOSS: 0.0367, MAE: 0.0367, RMSE: 0.0552, R2: 0.9522), PNorm: 147.7350, GNorm: 1.0823
[203/299] timecost: 63.68, lr: 0.000027, Train: (LOSS: 0.0154, MAE: 0.0154, RMSE: 0.0240, R2: 0.9894), Valid: (LOSS: 0.0357, MAE: 0.0357, RMSE: 0.0551, R2: 0.9517), PNorm: 147.6356, GNorm: 0.8075
[204/299] timecost: 64.00, lr: 0.000027, Train: (LOSS: 0.0140, MAE: 0.0140, RMSE: 0.0219, R2: 0.9906), Valid: (LOSS: 0.0344, MAE: 0.0344, RMSE: 0.0529, R2: 0.9554), PNorm: 147.5374, GNorm: 0.8638
[205/299] timecost: 63.19, lr: 0.000027, Train: (LOSS: 0.0135, MAE: 0.0135, RMSE: 0.0215, R2: 0.9911), Valid: (LOSS: 0.0363, MAE: 0.0363, RMSE: 0.0537, R2: 0.9527), PNorm: 147.4378, GNorm: 0.8620
[206/299] timecost: 62.95, lr: 0.000027, Train: (LOSS: 0.0136, MAE: 0.0136, RMSE: 0.0213, R2: 0.9912), Valid: (LOSS: 0.0355, MAE: 0.0355, RMSE: 0.0533, R2: 0.9547), PNorm: 147.3386, GNorm: 1.9841
[207/299] timecost: 64.86, lr: 0.000027, Train: (LOSS: 0.0150, MAE: 0.0150, RMSE: 0.0231, R2: 0.9900), Valid: (LOSS: 0.0358, MAE: 0.0358, RMSE: 0.0539, R2: 0.9541), PNorm: 147.2405, GNorm: 1.5499
[208/299] timecost: 65.29, lr: 0.000027, Train: (LOSS: 0.0144, MAE: 0.0144, RMSE: 0.0222, R2: 0.9908), Valid: (LOSS: 0.0346, MAE: 0.0346, RMSE: 0.0535, R2: 0.9538), PNorm: 147.1431, GNorm: 0.6351
[209/299] timecost: 65.87, lr: 0.000027, Train: (LOSS: 0.0142, MAE: 0.0142, RMSE: 0.0222, R2: 0.9901), Valid: (LOSS: 0.0343, MAE: 0.0343, RMSE: 0.0518, R2: 0.9574), PNorm: 147.0464, GNorm: 0.8043
[210/299] timecost: 64.51, lr: 0.000027, Train: (LOSS: 0.0134, MAE: 0.0134, RMSE: 0.0211, R2: 0.9911), Valid: (LOSS: 0.0360, MAE: 0.0360, RMSE: 0.0544, R2: 0.9520), PNorm: 146.9481, GNorm: 0.8435
[211/299] timecost: 63.96, lr: 0.000027, Train: (LOSS: 0.0134, MAE: 0.0134, RMSE: 0.0210, R2: 0.9911), Valid: (LOSS: 0.0352, MAE: 0.0352, RMSE: 0.0535, R2: 0.9546), PNorm: 146.8489, GNorm: 1.1306
[212/299] timecost: 63.93, lr: 0.000027, Train: (LOSS: 0.0131, MAE: 0.0131, RMSE: 0.0207, R2: 0.9913), Valid: (LOSS: 0.0341, MAE: 0.0341, RMSE: 0.0527, R2: 0.9548), PNorm: 146.7507, GNorm: 0.9345
[213/299] timecost: 64.26, lr: 0.000027, Train: (LOSS: 0.0123, MAE: 0.0123, RMSE: 0.0197, R2: 0.9919), Valid: (LOSS: 0.0349, MAE: 0.0349, RMSE: 0.0534, R2: 0.9537), PNorm: 146.6526, GNorm: 0.7544
[214/299] timecost: 64.19, lr: 0.000027, Train: (LOSS: 0.0123, MAE: 0.0123, RMSE: 0.0197, R2: 0.9926), Valid: (LOSS: 0.0349, MAE: 0.0349, RMSE: 0.0540, R2: 0.9544), PNorm: 146.5544, GNorm: 1.0784
[215/299] timecost: 63.82, lr: 0.000027, Train: (LOSS: 0.0127, MAE: 0.0127, RMSE: 0.0204, R2: 0.9919), Valid: (LOSS: 0.0347, MAE: 0.0347, RMSE: 0.0537, R2: 0.9546), PNorm: 146.4547, GNorm: 0.8874
[216/299] timecost: 64.62, lr: 0.000027, Train: (LOSS: 0.0129, MAE: 0.0129, RMSE: 0.0202, R2: 0.9918), Valid: (LOSS: 0.0344, MAE: 0.0344, RMSE: 0.0524, R2: 0.9553), PNorm: 146.3573, GNorm: 0.7141
[217/299] timecost: 65.01, lr: 0.000027, Train: (LOSS: 0.0129, MAE: 0.0129, RMSE: 0.0203, R2: 0.9924), Valid: (LOSS: 0.0348, MAE: 0.0348, RMSE: 0.0526, R2: 0.9559), PNorm: 146.2592, GNorm: 1.0104
[218/299] timecost: 65.44, lr: 0.000027, Train: (LOSS: 0.0121, MAE: 0.0121, RMSE: 0.0196, R2: 0.9925), Valid: (LOSS: 0.0354, MAE: 0.0354, RMSE: 0.0526, R2: 0.9554), PNorm: 146.1603, GNorm: 0.8637
[219/299] timecost: 65.25, lr: 0.000027, Train: (LOSS: 0.0124, MAE: 0.0124, RMSE: 0.0196, R2: 0.9914), Valid: (LOSS: 0.0338, MAE: 0.0338, RMSE: 0.0518, R2: 0.9562), PNorm: 146.0619, GNorm: 1.1428
Epoch 00221: reducing learning rate of group 0 to 2.4300e-05.
[220/299] timecost: 65.15, lr: 0.000024, Train: (LOSS: 0.0127, MAE: 0.0127, RMSE: 0.0197, R2: 0.9926), Valid: (LOSS: 0.0348, MAE: 0.0348, RMSE: 0.0527, R2: 0.9542), PNorm: 145.9630, GNorm: 1.0489
[221/299] timecost: 65.12, lr: 0.000024, Train: (LOSS: 0.0129, MAE: 0.0129, RMSE: 0.0199, R2: 0.9925), Valid: (LOSS: 0.0332, MAE: 0.0332, RMSE: 0.0514, R2: 0.9578), PNorm: 145.8756, GNorm: 0.7418
[222/299] timecost: 64.38, lr: 0.000024, Train: (LOSS: 0.0118, MAE: 0.0118, RMSE: 0.0182, R2: 0.9936), Valid: (LOSS: 0.0337, MAE: 0.0337, RMSE: 0.0510, R2: 0.9587), PNorm: 145.7863, GNorm: 1.0286
[223/299] timecost: 65.30, lr: 0.000024, Train: (LOSS: 0.0116, MAE: 0.0116, RMSE: 0.0182, R2: 0.9933), Valid: (LOSS: 0.0342, MAE: 0.0342, RMSE: 0.0509, R2: 0.9579), PNorm: 145.6982, GNorm: 1.0627
[224/299] timecost: 67.77, lr: 0.000024, Train: (LOSS: 0.0117, MAE: 0.0117, RMSE: 0.0183, R2: 0.9928), Valid: (LOSS: 0.0331, MAE: 0.0331, RMSE: 0.0496, R2: 0.9597), PNorm: 145.6095, GNorm: 0.8506
[225/299] timecost: 67.20, lr: 0.000024, Train: (LOSS: 0.0122, MAE: 0.0122, RMSE: 0.0193, R2: 0.9928), Valid: (LOSS: 0.0331, MAE: 0.0331, RMSE: 0.0515, R2: 0.9562), PNorm: 145.5212, GNorm: 0.8005
[226/299] timecost: 66.56, lr: 0.000024, Train: (LOSS: 0.0109, MAE: 0.0109, RMSE: 0.0178, R2: 0.9933), Valid: (LOSS: 0.0343, MAE: 0.0343, RMSE: 0.0529, R2: 0.9549), PNorm: 145.4325, GNorm: 0.9113
[227/299] timecost: 67.03, lr: 0.000024, Train: (LOSS: 0.0109, MAE: 0.0109, RMSE: 0.0173, R2: 0.9939), Valid: (LOSS: 0.0338, MAE: 0.0338, RMSE: 0.0526, R2: 0.9539), PNorm: 145.3442, GNorm: 0.7810
[228/299] timecost: 66.94, lr: 0.000024, Train: (LOSS: 0.0113, MAE: 0.0113, RMSE: 0.0181, R2: 0.9933), Valid: (LOSS: 0.0338, MAE: 0.0338, RMSE: 0.0525, R2: 0.9548), PNorm: 145.2547, GNorm: 0.8964
[229/299] timecost: 66.56, lr: 0.000024, Train: (LOSS: 0.0115, MAE: 0.0115, RMSE: 0.0182, R2: 0.9932), Valid: (LOSS: 0.0350, MAE: 0.0350, RMSE: 0.0527, R2: 0.9557), PNorm: 145.1656, GNorm: 0.7994
[230/299] timecost: 67.14, lr: 0.000024, Train: (LOSS: 0.0119, MAE: 0.0119, RMSE: 0.0189, R2: 0.9924), Valid: (LOSS: 0.0351, MAE: 0.0351, RMSE: 0.0532, R2: 0.9561), PNorm: 145.0774, GNorm: 1.1076
[231/299] timecost: 66.76, lr: 0.000024, Train: (LOSS: 0.0116, MAE: 0.0116, RMSE: 0.0184, R2: 0.9931), Valid: (LOSS: 0.0346, MAE: 0.0346, RMSE: 0.0528, R2: 0.9545), PNorm: 144.9882, GNorm: 1.6539
[232/299] timecost: 67.21, lr: 0.000024, Train: (LOSS: 0.0115, MAE: 0.0115, RMSE: 0.0180, R2: 0.9934), Valid: (LOSS: 0.0349, MAE: 0.0349, RMSE: 0.0523, R2: 0.9566), PNorm: 144.8992, GNorm: 0.9757
[233/299] timecost: 66.85, lr: 0.000024, Train: (LOSS: 0.0112, MAE: 0.0112, RMSE: 0.0179, R2: 0.9935), Valid: (LOSS: 0.0341, MAE: 0.0341, RMSE: 0.0527, R2: 0.9546), PNorm: 144.8106, GNorm: 0.6649
[234/299] timecost: 66.81, lr: 0.000024, Train: (LOSS: 0.0105, MAE: 0.0105, RMSE: 0.0167, R2: 0.9944), Valid: (LOSS: 0.0352, MAE: 0.0352, RMSE: 0.0545, R2: 0.9521), PNorm: 144.7210, GNorm: 0.9380
[235/299] timecost: 66.60, lr: 0.000024, Train: (LOSS: 0.0108, MAE: 0.0108, RMSE: 0.0174, R2: 0.9935), Valid: (LOSS: 0.0357, MAE: 0.0357, RMSE: 0.0541, R2: 0.9524), PNorm: 144.6324, GNorm: 0.6657
[236/299] timecost: 66.42, lr: 0.000024, Train: (LOSS: 0.0113, MAE: 0.0113, RMSE: 0.0175, R2: 0.9938), Valid: (LOSS: 0.0350, MAE: 0.0350, RMSE: 0.0534, R2: 0.9539), PNorm: 144.5438, GNorm: 1.1150
[237/299] timecost: 66.88, lr: 0.000024, Train: (LOSS: 0.0115, MAE: 0.0115, RMSE: 0.0184, R2: 0.9931), Valid: (LOSS: 0.0344, MAE: 0.0344, RMSE: 0.0522, R2: 0.9547), PNorm: 144.4544, GNorm: 0.8130
[238/299] timecost: 66.97, lr: 0.000024, Train: (LOSS: 0.0115, MAE: 0.0115, RMSE: 0.0182, R2: 0.9932), Valid: (LOSS: 0.0344, MAE: 0.0344, RMSE: 0.0518, R2: 0.9569), PNorm: 144.3660, GNorm: 0.7625
[239/299] timecost: 66.58, lr: 0.000024, Train: (LOSS: 0.0116, MAE: 0.0116, RMSE: 0.0184, R2: 0.9928), Valid: (LOSS: 0.0333, MAE: 0.0333, RMSE: 0.0524, R2: 0.9552), PNorm: 144.2774, GNorm: 0.8544
[240/299] timecost: 66.89, lr: 0.000024, Train: (LOSS: 0.0115, MAE: 0.0115, RMSE: 0.0183, R2: 0.9940), Valid: (LOSS: 0.0340, MAE: 0.0340, RMSE: 0.0517, R2: 0.9564), PNorm: 144.1902, GNorm: 0.9359
[241/299] timecost: 67.63, lr: 0.000024, Train: (LOSS: 0.0108, MAE: 0.0108, RMSE: 0.0174, R2: 0.9943), Valid: (LOSS: 0.0331, MAE: 0.0331, RMSE: 0.0512, R2: 0.9591), PNorm: 144.1018, GNorm: 0.6715
Epoch 00243: reducing learning rate of group 0 to 2.1870e-05.
[242/299] timecost: 67.18, lr: 0.000022, Train: (LOSS: 0.0104, MAE: 0.0104, RMSE: 0.0167, R2: 0.9939), Valid: (LOSS: 0.0351, MAE: 0.0351, RMSE: 0.0539, R2: 0.9534), PNorm: 144.0137, GNorm: 0.9545
[243/299] timecost: 67.38, lr: 0.000022, Train: (LOSS: 0.0105, MAE: 0.0105, RMSE: 0.0167, R2: 0.9944), Valid: (LOSS: 0.0341, MAE: 0.0341, RMSE: 0.0526, R2: 0.9554), PNorm: 143.9345, GNorm: 0.8637
[244/299] timecost: 67.13, lr: 0.000022, Train: (LOSS: 0.0113, MAE: 0.0113, RMSE: 0.0178, R2: 0.9931), Valid: (LOSS: 0.0343, MAE: 0.0343, RMSE: 0.0516, R2: 0.9574), PNorm: 143.8555, GNorm: 1.1886
[245/299] timecost: 66.98, lr: 0.000022, Train: (LOSS: 0.0107, MAE: 0.0107, RMSE: 0.0171, R2: 0.9934), Valid: (LOSS: 0.0334, MAE: 0.0334, RMSE: 0.0513, R2: 0.9577), PNorm: 143.7767, GNorm: 0.6785
[246/299] timecost: 66.93, lr: 0.000022, Train: (LOSS: 0.0104, MAE: 0.0104, RMSE: 0.0163, R2: 0.9937), Valid: (LOSS: 0.0336, MAE: 0.0336, RMSE: 0.0511, R2: 0.9577), PNorm: 143.6974, GNorm: 1.4752
[247/299] timecost: 67.22, lr: 0.000022, Train: (LOSS: 0.0102, MAE: 0.0102, RMSE: 0.0166, R2: 0.9938), Valid: (LOSS: 0.0349, MAE: 0.0349, RMSE: 0.0531, R2: 0.9551), PNorm: 143.6184, GNorm: 0.9321
[248/299] timecost: 67.12, lr: 0.000022, Train: (LOSS: 0.0099, MAE: 0.0099, RMSE: 0.0162, R2: 0.9942), Valid: (LOSS: 0.0348, MAE: 0.0348, RMSE: 0.0529, R2: 0.9558), PNorm: 143.5396, GNorm: 1.1418
[249/299] timecost: 66.56, lr: 0.000022, Train: (LOSS: 0.0100, MAE: 0.0100, RMSE: 0.0159, R2: 0.9942), Valid: (LOSS: 0.0344, MAE: 0.0344, RMSE: 0.0505, R2: 0.9598), PNorm: 143.4606, GNorm: 1.0440
[250/299] timecost: 64.27, lr: 0.000022, Train: (LOSS: 0.0096, MAE: 0.0096, RMSE: 0.0156, R2: 0.9943), Valid: (LOSS: 0.0340, MAE: 0.0340, RMSE: 0.0519, R2: 0.9560), PNorm: 143.3814, GNorm: 1.1081
[251/299] timecost: 63.06, lr: 0.000022, Train: (LOSS: 0.0099, MAE: 0.0099, RMSE: 0.0161, R2: 0.9938), Valid: (LOSS: 0.0333, MAE: 0.0333, RMSE: 0.0518, R2: 0.9568), PNorm: 143.3027, GNorm: 0.9792
[252/299] timecost: 63.15, lr: 0.000022, Train: (LOSS: 0.0095, MAE: 0.0095, RMSE: 0.0155, R2: 0.9946), Valid: (LOSS: 0.0342, MAE: 0.0342, RMSE: 0.0517, R2: 0.9566), PNorm: 143.2226, GNorm: 0.7346
[253/299] timecost: 63.93, lr: 0.000022, Train: (LOSS: 0.0106, MAE: 0.0106, RMSE: 0.0173, R2: 0.9936), Valid: (LOSS: 0.0350, MAE: 0.0350, RMSE: 0.0522, R2: 0.9563), PNorm: 143.1436, GNorm: 0.7474
[254/299] timecost: 63.95, lr: 0.000022, Train: (LOSS: 0.0100, MAE: 0.0100, RMSE: 0.0160, R2: 0.9945), Valid: (LOSS: 0.0336, MAE: 0.0336, RMSE: 0.0513, R2: 0.9578), PNorm: 143.0647, GNorm: 0.6397
[255/299] timecost: 64.12, lr: 0.000022, Train: (LOSS: 0.0104, MAE: 0.0104, RMSE: 0.0168, R2: 0.9935), Valid: (LOSS: 0.0340, MAE: 0.0340, RMSE: 0.0528, R2: 0.9554), PNorm: 142.9857, GNorm: 0.8912
[256/299] timecost: 63.96, lr: 0.000022, Train: (LOSS: 0.0105, MAE: 0.0105, RMSE: 0.0168, R2: 0.9941), Valid: (LOSS: 0.0340, MAE: 0.0340, RMSE: 0.0528, R2: 0.9539), PNorm: 142.9074, GNorm: 1.1185
[257/299] timecost: 63.88, lr: 0.000022, Train: (LOSS: 0.0101, MAE: 0.0101, RMSE: 0.0163, R2: 0.9936), Valid: (LOSS: 0.0338, MAE: 0.0338, RMSE: 0.0512, R2: 0.9581), PNorm: 142.8292, GNorm: 0.9828
[258/299] timecost: 63.83, lr: 0.000022, Train: (LOSS: 0.0098, MAE: 0.0098, RMSE: 0.0157, R2: 0.9946), Valid: (LOSS: 0.0353, MAE: 0.0353, RMSE: 0.0551, R2: 0.9517), PNorm: 142.7510, GNorm: 1.1175
[259/299] timecost: 65.21, lr: 0.000022, Train: (LOSS: 0.0098, MAE: 0.0098, RMSE: 0.0159, R2: 0.9948), Valid: (LOSS: 0.0353, MAE: 0.0353, RMSE: 0.0534, R2: 0.9547), PNorm: 142.6731, GNorm: 1.3847
[260/299] timecost: 65.20, lr: 0.000022, Train: (LOSS: 0.0099, MAE: 0.0099, RMSE: 0.0161, R2: 0.9946), Valid: (LOSS: 0.0343, MAE: 0.0343, RMSE: 0.0533, R2: 0.9536), PNorm: 142.5938, GNorm: 0.6670
[261/299] timecost: 65.56, lr: 0.000022, Train: (LOSS: 0.0092, MAE: 0.0092, RMSE: 0.0153, R2: 0.9949), Valid: (LOSS: 0.0344, MAE: 0.0344, RMSE: 0.0529, R2: 0.9555), PNorm: 142.5154, GNorm: 0.6277
[262/299] timecost: 65.77, lr: 0.000022, Train: (LOSS: 0.0096, MAE: 0.0096, RMSE: 0.0154, R2: 0.9948), Valid: (LOSS: 0.0351, MAE: 0.0351, RMSE: 0.0531, R2: 0.9554), PNorm: 142.4377, GNorm: 1.4258
Epoch 00264: reducing learning rate of group 0 to 1.9683e-05.
[263/299] timecost: 64.77, lr: 0.000020, Train: (LOSS: 0.0096, MAE: 0.0096, RMSE: 0.0156, R2: 0.9938), Valid: (LOSS: 0.0337, MAE: 0.0337, RMSE: 0.0528, R2: 0.9552), PNorm: 142.3588, GNorm: 0.8073
[264/299] timecost: 65.30, lr: 0.000020, Train: (LOSS: 0.0095, MAE: 0.0095, RMSE: 0.0156, R2: 0.9939), Valid: (LOSS: 0.0350, MAE: 0.0350, RMSE: 0.0530, R2: 0.9541), PNorm: 142.2879, GNorm: 1.1468
[265/299] timecost: 65.30, lr: 0.000020, Train: (LOSS: 0.0088, MAE: 0.0088, RMSE: 0.0147, R2: 0.9946), Valid: (LOSS: 0.0341, MAE: 0.0341, RMSE: 0.0534, R2: 0.9537), PNorm: 142.2164, GNorm: 0.6250
[266/299] timecost: 62.88, lr: 0.000020, Train: (LOSS: 0.0091, MAE: 0.0091, RMSE: 0.0150, R2: 0.9947), Valid: (LOSS: 0.0336, MAE: 0.0336, RMSE: 0.0517, R2: 0.9565), PNorm: 142.1457, GNorm: 0.6904
[267/299] timecost: 62.78, lr: 0.000020, Train: (LOSS: 0.0088, MAE: 0.0088, RMSE: 0.0148, R2: 0.9951), Valid: (LOSS: 0.0348, MAE: 0.0348, RMSE: 0.0527, R2: 0.9547), PNorm: 142.0755, GNorm: 0.9692
[268/299] timecost: 62.88, lr: 0.000020, Train: (LOSS: 0.0086, MAE: 0.0086, RMSE: 0.0144, R2: 0.9953), Valid: (LOSS: 0.0354, MAE: 0.0354, RMSE: 0.0539, R2: 0.9533), PNorm: 142.0047, GNorm: 1.1781
[269/299] timecost: 63.03, lr: 0.000020, Train: (LOSS: 0.0085, MAE: 0.0085, RMSE: 0.0142, R2: 0.9949), Valid: (LOSS: 0.0353, MAE: 0.0353, RMSE: 0.0531, R2: 0.9552), PNorm: 141.9330, GNorm: 0.7249
[270/299] timecost: 63.08, lr: 0.000020, Train: (LOSS: 0.0087, MAE: 0.0087, RMSE: 0.0145, R2: 0.9949), Valid: (LOSS: 0.0348, MAE: 0.0348, RMSE: 0.0530, R2: 0.9548), PNorm: 141.8623, GNorm: 1.0240
[271/299] timecost: 63.01, lr: 0.000020, Train: (LOSS: 0.0089, MAE: 0.0089, RMSE: 0.0147, R2: 0.9950), Valid: (LOSS: 0.0346, MAE: 0.0346, RMSE: 0.0520, R2: 0.9562), PNorm: 141.7911, GNorm: 0.8594
[272/299] timecost: 63.06, lr: 0.000020, Train: (LOSS: 0.0089, MAE: 0.0089, RMSE: 0.0146, R2: 0.9956), Valid: (LOSS: 0.0348, MAE: 0.0348, RMSE: 0.0524, R2: 0.9566), PNorm: 141.7199, GNorm: 0.6134
[273/299] timecost: 65.66, lr: 0.000020, Train: (LOSS: 0.0087, MAE: 0.0087, RMSE: 0.0144, R2: 0.9951), Valid: (LOSS: 0.0343, MAE: 0.0343, RMSE: 0.0524, R2: 0.9556), PNorm: 141.6486, GNorm: 1.4857
[274/299] timecost: 66.46, lr: 0.000020, Train: (LOSS: 0.0090, MAE: 0.0090, RMSE: 0.0146, R2: 0.9952), Valid: (LOSS: 0.0338, MAE: 0.0338, RMSE: 0.0512, R2: 0.9576), PNorm: 141.5781, GNorm: 0.7229
[275/299] timecost: 66.22, lr: 0.000020, Train: (LOSS: 0.0087, MAE: 0.0087, RMSE: 0.0144, R2: 0.9953), Valid: (LOSS: 0.0354, MAE: 0.0354, RMSE: 0.0538, R2: 0.9541), PNorm: 141.5069, GNorm: 0.7055
[276/299] timecost: 66.26, lr: 0.000020, Train: (LOSS: 0.0096, MAE: 0.0096, RMSE: 0.0153, R2: 0.9945), Valid: (LOSS: 0.0348, MAE: 0.0348, RMSE: 0.0519, R2: 0.9568), PNorm: 141.4368, GNorm: 0.7694
[277/299] timecost: 65.72, lr: 0.000020, Train: (LOSS: 0.0088, MAE: 0.0088, RMSE: 0.0145, R2: 0.9946), Valid: (LOSS: 0.0344, MAE: 0.0344, RMSE: 0.0525, R2: 0.9551), PNorm: 141.3654, GNorm: 0.9073
[278/299] timecost: 66.14, lr: 0.000020, Train: (LOSS: 0.0084, MAE: 0.0084, RMSE: 0.0137, R2: 0.9955), Valid: (LOSS: 0.0342, MAE: 0.0342, RMSE: 0.0519, R2: 0.9573), PNorm: 141.2953, GNorm: 0.7958
[279/299] timecost: 65.79, lr: 0.000020, Train: (LOSS: 0.0082, MAE: 0.0082, RMSE: 0.0138, R2: 0.9959), Valid: (LOSS: 0.0350, MAE: 0.0350, RMSE: 0.0541, R2: 0.9531), PNorm: 141.2252, GNorm: 0.7491
[280/299] timecost: 66.05, lr: 0.000020, Train: (LOSS: 0.0086, MAE: 0.0086, RMSE: 0.0141, R2: 0.9949), Valid: (LOSS: 0.0349, MAE: 0.0349, RMSE: 0.0538, R2: 0.9535), PNorm: 141.1548, GNorm: 0.8881
[281/299] timecost: 65.86, lr: 0.000020, Train: (LOSS: 0.0082, MAE: 0.0082, RMSE: 0.0138, R2: 0.9949), Valid: (LOSS: 0.0352, MAE: 0.0352, RMSE: 0.0530, R2: 0.9551), PNorm: 141.0841, GNorm: 1.1434
[282/299] timecost: 65.59, lr: 0.000020, Train: (LOSS: 0.0083, MAE: 0.0083, RMSE: 0.0140, R2: 0.9951), Valid: (LOSS: 0.0350, MAE: 0.0350, RMSE: 0.0536, R2: 0.9544), PNorm: 141.0132, GNorm: 0.9220
[283/299] timecost: 66.11, lr: 0.000020, Train: (LOSS: 0.0086, MAE: 0.0086, RMSE: 0.0143, R2: 0.9947), Valid: (LOSS: 0.0341, MAE: 0.0341, RMSE: 0.0515, R2: 0.9568), PNorm: 140.9425, GNorm: 1.2563
Epoch 00285: reducing learning rate of group 0 to 1.7715e-05.
[284/299] timecost: 66.18, lr: 0.000018, Train: (LOSS: 0.0087, MAE: 0.0087, RMSE: 0.0144, R2: 0.9952), Valid: (LOSS: 0.0344, MAE: 0.0344, RMSE: 0.0521, R2: 0.9565), PNorm: 140.8718, GNorm: 0.8757
[285/299] timecost: 65.48, lr: 0.000018, Train: (LOSS: 0.0080, MAE: 0.0080, RMSE: 0.0137, R2: 0.9957), Valid: (LOSS: 0.0339, MAE: 0.0339, RMSE: 0.0516, R2: 0.9577), PNorm: 140.8085, GNorm: 1.0648
[286/299] timecost: 65.42, lr: 0.000018, Train: (LOSS: 0.0076, MAE: 0.0076, RMSE: 0.0130, R2: 0.9959), Valid: (LOSS: 0.0342, MAE: 0.0342, RMSE: 0.0526, R2: 0.9556), PNorm: 140.7448, GNorm: 0.7209
[287/299] timecost: 65.53, lr: 0.000018, Train: (LOSS: 0.0079, MAE: 0.0079, RMSE: 0.0135, R2: 0.9951), Valid: (LOSS: 0.0349, MAE: 0.0349, RMSE: 0.0534, R2: 0.9545), PNorm: 140.6807, GNorm: 0.6607
[288/299] timecost: 65.23, lr: 0.000018, Train: (LOSS: 0.0077, MAE: 0.0077, RMSE: 0.0132, R2: 0.9957), Valid: (LOSS: 0.0345, MAE: 0.0345, RMSE: 0.0525, R2: 0.9557), PNorm: 140.6166, GNorm: 1.2450
[289/299] timecost: 64.89, lr: 0.000018, Train: (LOSS: 0.0077, MAE: 0.0077, RMSE: 0.0131, R2: 0.9958), Valid: (LOSS: 0.0347, MAE: 0.0347, RMSE: 0.0518, R2: 0.9562), PNorm: 140.5533, GNorm: 0.5718
[290/299] timecost: 65.09, lr: 0.000018, Train: (LOSS: 0.0076, MAE: 0.0076, RMSE: 0.0131, R2: 0.9960), Valid: (LOSS: 0.0342, MAE: 0.0342, RMSE: 0.0520, R2: 0.9562), PNorm: 140.4891, GNorm: 0.7294
[291/299] timecost: 64.93, lr: 0.000018, Train: (LOSS: 0.0078, MAE: 0.0078, RMSE: 0.0133, R2: 0.9956), Valid: (LOSS: 0.0336, MAE: 0.0336, RMSE: 0.0502, R2: 0.9596), PNorm: 140.4250, GNorm: 0.9722
[292/299] timecost: 65.55, lr: 0.000018, Train: (LOSS: 0.0081, MAE: 0.0081, RMSE: 0.0133, R2: 0.9959), Valid: (LOSS: 0.0338, MAE: 0.0338, RMSE: 0.0505, R2: 0.9601), PNorm: 140.3611, GNorm: 1.4738
[293/299] timecost: 65.65, lr: 0.000018, Train: (LOSS: 0.0077, MAE: 0.0077, RMSE: 0.0130, R2: 0.9952), Valid: (LOSS: 0.0340, MAE: 0.0340, RMSE: 0.0515, R2: 0.9580), PNorm: 140.2970, GNorm: 0.9422
[294/299] timecost: 65.32, lr: 0.000018, Train: (LOSS: 0.0076, MAE: 0.0076, RMSE: 0.0124, R2: 0.9957), Valid: (LOSS: 0.0347, MAE: 0.0347, RMSE: 0.0514, R2: 0.9574), PNorm: 140.2330, GNorm: 0.7584
[295/299] timecost: 65.74, lr: 0.000018, Train: (LOSS: 0.0075, MAE: 0.0075, RMSE: 0.0128, R2: 0.9957), Valid: (LOSS: 0.0338, MAE: 0.0338, RMSE: 0.0510, R2: 0.9585), PNorm: 140.1690, GNorm: 0.9138
[296/299] timecost: 65.34, lr: 0.000018, Train: (LOSS: 0.0076, MAE: 0.0076, RMSE: 0.0131, R2: 0.9953), Valid: (LOSS: 0.0344, MAE: 0.0344, RMSE: 0.0526, R2: 0.9557), PNorm: 140.1056, GNorm: 1.2754
[297/299] timecost: 65.11, lr: 0.000018, Train: (LOSS: 0.0077, MAE: 0.0077, RMSE: 0.0126, R2: 0.9954), Valid: (LOSS: 0.0347, MAE: 0.0347, RMSE: 0.0518, R2: 0.9574), PNorm: 140.0423, GNorm: 1.4868
[298/299] timecost: 67.69, lr: 0.000018, Train: (LOSS: 0.0078, MAE: 0.0078, RMSE: 0.0132, R2: 0.9958), Valid: (LOSS: 0.0349, MAE: 0.0349, RMSE: 0.0536, R2: 0.9539), PNorm: 139.9786, GNorm: 1.0000
[299/299] timecost: 67.86, lr: 0.000018, Train: (LOSS: 0.0083, MAE: 0.0083, RMSE: 0.0138, R2: 0.9959), Valid: (LOSS: 0.0345, MAE: 0.0345, RMSE: 0.0526, R2: 0.9558), PNorm: 139.9156, GNorm: 0.7259
==========Training End==========
==========Test Best Model==========
================Final Results=======================
mse: 0.0371 +- 0.0000:
rmse: 0.0592 +- 0.0000:
mae: 0.0371 +- 0.0000:
r2: 0.9442 +- 0.0000:
tensor([[0.0000, 0.0000],
        [0.0000, 0.0000],
        [0.1403, 0.1828],
        ...,
        [0.0000, 0.0000],
        [0.0000, 0.0000],
        [0.0000, 0.0000]], device='cuda:0')
