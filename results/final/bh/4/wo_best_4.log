cuda available with GPU: Tesla V100-PCIE-16GB
==========Load Seed==========
set_random_seed
0
==========Training Start==========
Training Graphs:  2491
Valid Graphs:  277
Test Graphs:  1187
============Not pretrained weights used============
============Creating new layers============
============Creating Model============
Training Graphs Batches:  78
Valid Graphs Batches:  9
Test Graphs Batches:  37
[0/299] timecost: 61.10, lr: 0.000100, Train: (LOSS: 0.2105, MAE: 0.2105, RMSE: 0.2578, R2: 0.0304), Valid: (LOSS: 0.1757, MAE: 0.1757, RMSE: 0.2297, R2: 0.2844), PNorm: 187.0395, GNorm: 0.5000
[1/299] timecost: 59.25, lr: 0.000100, Train: (LOSS: 0.1749, MAE: 0.1749, RMSE: 0.2247, R2: 0.2741), Valid: (LOSS: 0.1664, MAE: 0.1664, RMSE: 0.2042, R2: 0.4350), PNorm: 187.0749, GNorm: 0.5000
[2/299] timecost: 59.14, lr: 0.000100, Train: (LOSS: 0.1555, MAE: 0.1555, RMSE: 0.2043, R2: 0.4013), Valid: (LOSS: 0.1622, MAE: 0.1622, RMSE: 0.2059, R2: 0.4094), PNorm: 187.1124, GNorm: 0.4681
[3/299] timecost: 58.93, lr: 0.000100, Train: (LOSS: 0.1390, MAE: 0.1390, RMSE: 0.1899, R2: 0.4736), Valid: (LOSS: 0.1571, MAE: 0.1571, RMSE: 0.2048, R2: 0.3999), PNorm: 187.1329, GNorm: 0.5000
[4/299] timecost: 59.10, lr: 0.000100, Train: (LOSS: 0.1335, MAE: 0.1335, RMSE: 0.1817, R2: 0.5020), Valid: (LOSS: 0.1511, MAE: 0.1511, RMSE: 0.2051, R2: 0.3980), PNorm: 187.1547, GNorm: 0.5000
[5/299] timecost: 58.97, lr: 0.000100, Train: (LOSS: 0.1290, MAE: 0.1290, RMSE: 0.1784, R2: 0.5255), Valid: (LOSS: 0.1270, MAE: 0.1270, RMSE: 0.1747, R2: 0.5586), PNorm: 187.1724, GNorm: 0.5000
[6/299] timecost: 59.33, lr: 0.000100, Train: (LOSS: 0.1231, MAE: 0.1231, RMSE: 0.1700, R2: 0.5663), Valid: (LOSS: 0.1342, MAE: 0.1342, RMSE: 0.1771, R2: 0.5521), PNorm: 187.1886, GNorm: 0.5000
[7/299] timecost: 59.31, lr: 0.000100, Train: (LOSS: 0.1220, MAE: 0.1220, RMSE: 0.1718, R2: 0.5535), Valid: (LOSS: 0.1308, MAE: 0.1308, RMSE: 0.1732, R2: 0.5732), PNorm: 187.2073, GNorm: 0.5000
[8/299] timecost: 58.79, lr: 0.000100, Train: (LOSS: 0.1195, MAE: 0.1195, RMSE: 0.1682, R2: 0.5779), Valid: (LOSS: 0.1245, MAE: 0.1245, RMSE: 0.1664, R2: 0.6051), PNorm: 187.2242, GNorm: 0.4486
[9/299] timecost: 58.89, lr: 0.000100, Train: (LOSS: 0.1208, MAE: 0.1208, RMSE: 0.1693, R2: 0.5821), Valid: (LOSS: 0.1238, MAE: 0.1238, RMSE: 0.1653, R2: 0.6134), PNorm: 187.2420, GNorm: 0.4596
[10/299] timecost: 59.56, lr: 0.000100, Train: (LOSS: 0.1192, MAE: 0.1192, RMSE: 0.1697, R2: 0.5596), Valid: (LOSS: 0.1261, MAE: 0.1261, RMSE: 0.1776, R2: 0.5599), PNorm: 187.2608, GNorm: 0.4827
[11/299] timecost: 59.13, lr: 0.000100, Train: (LOSS: 0.1160, MAE: 0.1160, RMSE: 0.1639, R2: 0.5997), Valid: (LOSS: 0.1152, MAE: 0.1152, RMSE: 0.1561, R2: 0.6543), PNorm: 187.2864, GNorm: 0.3550
[12/299] timecost: 59.37, lr: 0.000100, Train: (LOSS: 0.1129, MAE: 0.1129, RMSE: 0.1620, R2: 0.6103), Valid: (LOSS: 0.1092, MAE: 0.1092, RMSE: 0.1536, R2: 0.6681), PNorm: 187.3228, GNorm: 0.5000
[13/299] timecost: 59.19, lr: 0.000100, Train: (LOSS: 0.1061, MAE: 0.1061, RMSE: 0.1523, R2: 0.6480), Valid: (LOSS: 0.1174, MAE: 0.1174, RMSE: 0.1620, R2: 0.6334), PNorm: 187.3593, GNorm: 0.5000
[14/299] timecost: 58.92, lr: 0.000100, Train: (LOSS: 0.1013, MAE: 0.1013, RMSE: 0.1469, R2: 0.6742), Valid: (LOSS: 0.1086, MAE: 0.1086, RMSE: 0.1517, R2: 0.6739), PNorm: 187.3855, GNorm: 0.4594
[15/299] timecost: 59.15, lr: 0.000100, Train: (LOSS: 0.0953, MAE: 0.0953, RMSE: 0.1413, R2: 0.6934), Valid: (LOSS: 0.0992, MAE: 0.0992, RMSE: 0.1383, R2: 0.7360), PNorm: 187.4102, GNorm: 0.4526
[16/299] timecost: 58.97, lr: 0.000100, Train: (LOSS: 0.0946, MAE: 0.0946, RMSE: 0.1383, R2: 0.7062), Valid: (LOSS: 0.0951, MAE: 0.0951, RMSE: 0.1327, R2: 0.7612), PNorm: 187.4357, GNorm: 0.4309
[17/299] timecost: 58.77, lr: 0.000100, Train: (LOSS: 0.0878, MAE: 0.0878, RMSE: 0.1308, R2: 0.7432), Valid: (LOSS: 0.0932, MAE: 0.0932, RMSE: 0.1312, R2: 0.7618), PNorm: 187.4563, GNorm: 0.5000
[18/299] timecost: 58.75, lr: 0.000100, Train: (LOSS: 0.0847, MAE: 0.0847, RMSE: 0.1257, R2: 0.7602), Valid: (LOSS: 0.0854, MAE: 0.0854, RMSE: 0.1181, R2: 0.8053), PNorm: 187.4734, GNorm: 0.5000
[19/299] timecost: 58.29, lr: 0.000100, Train: (LOSS: 0.0825, MAE: 0.0825, RMSE: 0.1234, R2: 0.7737), Valid: (LOSS: 0.0833, MAE: 0.0833, RMSE: 0.1187, R2: 0.8065), PNorm: 187.4952, GNorm: 0.5000
[20/299] timecost: 58.76, lr: 0.000100, Train: (LOSS: 0.0778, MAE: 0.0778, RMSE: 0.1160, R2: 0.7994), Valid: (LOSS: 0.0779, MAE: 0.0779, RMSE: 0.1154, R2: 0.8113), PNorm: 187.5149, GNorm: 0.5000
[21/299] timecost: 58.83, lr: 0.000100, Train: (LOSS: 0.0735, MAE: 0.0735, RMSE: 0.1093, R2: 0.8081), Valid: (LOSS: 0.0738, MAE: 0.0738, RMSE: 0.1109, R2: 0.8240), PNorm: 187.5370, GNorm: 0.5000
[22/299] timecost: 62.38, lr: 0.000100, Train: (LOSS: 0.0675, MAE: 0.0675, RMSE: 0.1019, R2: 0.8387), Valid: (LOSS: 0.0688, MAE: 0.0688, RMSE: 0.1052, R2: 0.8340), PNorm: 187.5563, GNorm: 0.5000
[23/299] timecost: 60.53, lr: 0.000100, Train: (LOSS: 0.0636, MAE: 0.0636, RMSE: 0.0978, R2: 0.8528), Valid: (LOSS: 0.0696, MAE: 0.0696, RMSE: 0.1039, R2: 0.8534), PNorm: 187.5741, GNorm: 0.4358
[24/299] timecost: 60.35, lr: 0.000100, Train: (LOSS: 0.0584, MAE: 0.0584, RMSE: 0.0882, R2: 0.8770), Valid: (LOSS: 0.0749, MAE: 0.0749, RMSE: 0.1103, R2: 0.8319), PNorm: 187.5935, GNorm: 0.5000
[25/299] timecost: 59.69, lr: 0.000100, Train: (LOSS: 0.0586, MAE: 0.0586, RMSE: 0.0916, R2: 0.8677), Valid: (LOSS: 0.0635, MAE: 0.0635, RMSE: 0.0971, R2: 0.8694), PNorm: 187.6154, GNorm: 0.5000
[26/299] timecost: 59.29, lr: 0.000100, Train: (LOSS: 0.0552, MAE: 0.0552, RMSE: 0.0857, R2: 0.8831), Valid: (LOSS: 0.0673, MAE: 0.0673, RMSE: 0.1060, R2: 0.8340), PNorm: 187.6334, GNorm: 0.5000
[27/299] timecost: 59.62, lr: 0.000100, Train: (LOSS: 0.0531, MAE: 0.0531, RMSE: 0.0821, R2: 0.8909), Valid: (LOSS: 0.0568, MAE: 0.0568, RMSE: 0.0924, R2: 0.8698), PNorm: 187.6489, GNorm: 0.5000
[28/299] timecost: 60.85, lr: 0.000100, Train: (LOSS: 0.0529, MAE: 0.0529, RMSE: 0.0831, R2: 0.8891), Valid: (LOSS: 0.0567, MAE: 0.0567, RMSE: 0.0931, R2: 0.8696), PNorm: 187.6659, GNorm: 0.5000
[29/299] timecost: 60.87, lr: 0.000100, Train: (LOSS: 0.0514, MAE: 0.0514, RMSE: 0.0787, R2: 0.9032), Valid: (LOSS: 0.0554, MAE: 0.0554, RMSE: 0.0924, R2: 0.8691), PNorm: 187.6833, GNorm: 0.4698
[30/299] timecost: 59.77, lr: 0.000100, Train: (LOSS: 0.0483, MAE: 0.0483, RMSE: 0.0754, R2: 0.9064), Valid: (LOSS: 0.0584, MAE: 0.0584, RMSE: 0.0912, R2: 0.8784), PNorm: 187.7007, GNorm: 0.5000
[31/299] timecost: 58.95, lr: 0.000100, Train: (LOSS: 0.0466, MAE: 0.0466, RMSE: 0.0740, R2: 0.9084), Valid: (LOSS: 0.0582, MAE: 0.0582, RMSE: 0.0961, R2: 0.8644), PNorm: 187.7173, GNorm: 0.5000
[32/299] timecost: 58.55, lr: 0.000100, Train: (LOSS: 0.0459, MAE: 0.0459, RMSE: 0.0720, R2: 0.9128), Valid: (LOSS: 0.0565, MAE: 0.0565, RMSE: 0.0899, R2: 0.8819), PNorm: 187.7321, GNorm: 0.5000
[33/299] timecost: 60.27, lr: 0.000100, Train: (LOSS: 0.0445, MAE: 0.0445, RMSE: 0.0699, R2: 0.9245), Valid: (LOSS: 0.0548, MAE: 0.0548, RMSE: 0.0869, R2: 0.8939), PNorm: 187.7485, GNorm: 0.5000
[34/299] timecost: 59.79, lr: 0.000100, Train: (LOSS: 0.0461, MAE: 0.0461, RMSE: 0.0717, R2: 0.9157), Valid: (LOSS: 0.0533, MAE: 0.0533, RMSE: 0.0829, R2: 0.9039), PNorm: 187.7630, GNorm: 0.4871
[35/299] timecost: 59.35, lr: 0.000100, Train: (LOSS: 0.0420, MAE: 0.0420, RMSE: 0.0668, R2: 0.9290), Valid: (LOSS: 0.0530, MAE: 0.0530, RMSE: 0.0873, R2: 0.8830), PNorm: 187.7767, GNorm: 0.5000
[36/299] timecost: 59.48, lr: 0.000100, Train: (LOSS: 0.0412, MAE: 0.0412, RMSE: 0.0637, R2: 0.9346), Valid: (LOSS: 0.0546, MAE: 0.0546, RMSE: 0.0824, R2: 0.9031), PNorm: 187.7904, GNorm: 0.4448
[37/299] timecost: 59.71, lr: 0.000100, Train: (LOSS: 0.0405, MAE: 0.0405, RMSE: 0.0636, R2: 0.9370), Valid: (LOSS: 0.0528, MAE: 0.0528, RMSE: 0.0809, R2: 0.9097), PNorm: 187.8062, GNorm: 0.4733
[38/299] timecost: 59.65, lr: 0.000100, Train: (LOSS: 0.0398, MAE: 0.0398, RMSE: 0.0627, R2: 0.9349), Valid: (LOSS: 0.0493, MAE: 0.0493, RMSE: 0.0763, R2: 0.9183), PNorm: 187.8222, GNorm: 0.5000
[39/299] timecost: 59.46, lr: 0.000100, Train: (LOSS: 0.0389, MAE: 0.0389, RMSE: 0.0610, R2: 0.9414), Valid: (LOSS: 0.0521, MAE: 0.0521, RMSE: 0.0807, R2: 0.9048), PNorm: 187.8361, GNorm: 0.5000
[40/299] timecost: 59.23, lr: 0.000100, Train: (LOSS: 0.0386, MAE: 0.0386, RMSE: 0.0616, R2: 0.9382), Valid: (LOSS: 0.0463, MAE: 0.0463, RMSE: 0.0732, R2: 0.9248), PNorm: 187.8506, GNorm: 0.4915
[41/299] timecost: 61.83, lr: 0.000100, Train: (LOSS: 0.0374, MAE: 0.0374, RMSE: 0.0587, R2: 0.9438), Valid: (LOSS: 0.0530, MAE: 0.0530, RMSE: 0.0879, R2: 0.8854), PNorm: 187.8623, GNorm: 0.3987
[42/299] timecost: 62.43, lr: 0.000100, Train: (LOSS: 0.0377, MAE: 0.0377, RMSE: 0.0588, R2: 0.9453), Valid: (LOSS: 0.0466, MAE: 0.0466, RMSE: 0.0715, R2: 0.9284), PNorm: 187.8764, GNorm: 0.4577
[43/299] timecost: 62.21, lr: 0.000100, Train: (LOSS: 0.0362, MAE: 0.0362, RMSE: 0.0568, R2: 0.9497), Valid: (LOSS: 0.0476, MAE: 0.0476, RMSE: 0.0782, R2: 0.9063), PNorm: 187.8899, GNorm: 0.5000
[44/299] timecost: 62.31, lr: 0.000100, Train: (LOSS: 0.0351, MAE: 0.0351, RMSE: 0.0550, R2: 0.9508), Valid: (LOSS: 0.0479, MAE: 0.0479, RMSE: 0.0760, R2: 0.9197), PNorm: 187.9023, GNorm: 0.4013
[45/299] timecost: 62.33, lr: 0.000100, Train: (LOSS: 0.0337, MAE: 0.0337, RMSE: 0.0530, R2: 0.9550), Valid: (LOSS: 0.0477, MAE: 0.0477, RMSE: 0.0736, R2: 0.9244), PNorm: 187.9139, GNorm: 0.5000
[46/299] timecost: 62.32, lr: 0.000100, Train: (LOSS: 0.0336, MAE: 0.0336, RMSE: 0.0527, R2: 0.9556), Valid: (LOSS: 0.0515, MAE: 0.0515, RMSE: 0.0851, R2: 0.8873), PNorm: 187.9280, GNorm: 0.5000
[47/299] timecost: 62.40, lr: 0.000100, Train: (LOSS: 0.0337, MAE: 0.0337, RMSE: 0.0519, R2: 0.9541), Valid: (LOSS: 0.0446, MAE: 0.0446, RMSE: 0.0702, R2: 0.9283), PNorm: 187.9417, GNorm: 0.4763
[48/299] timecost: 62.24, lr: 0.000100, Train: (LOSS: 0.0331, MAE: 0.0331, RMSE: 0.0519, R2: 0.9560), Valid: (LOSS: 0.0481, MAE: 0.0481, RMSE: 0.0758, R2: 0.9171), PNorm: 187.9521, GNorm: 0.5000
[49/299] timecost: 62.27, lr: 0.000100, Train: (LOSS: 0.0335, MAE: 0.0335, RMSE: 0.0519, R2: 0.9549), Valid: (LOSS: 0.0496, MAE: 0.0496, RMSE: 0.0775, R2: 0.9152), PNorm: 187.9668, GNorm: 0.5000
[50/299] timecost: 62.28, lr: 0.000100, Train: (LOSS: 0.0328, MAE: 0.0328, RMSE: 0.0502, R2: 0.9586), Valid: (LOSS: 0.0469, MAE: 0.0469, RMSE: 0.0718, R2: 0.9251), PNorm: 187.9777, GNorm: 0.5000
[51/299] timecost: 62.18, lr: 0.000100, Train: (LOSS: 0.0315, MAE: 0.0315, RMSE: 0.0484, R2: 0.9616), Valid: (LOSS: 0.0489, MAE: 0.0489, RMSE: 0.0760, R2: 0.9168), PNorm: 187.9902, GNorm: 0.5000
[52/299] timecost: 62.43, lr: 0.000100, Train: (LOSS: 0.0305, MAE: 0.0305, RMSE: 0.0474, R2: 0.9635), Valid: (LOSS: 0.0489, MAE: 0.0489, RMSE: 0.0764, R2: 0.9189), PNorm: 188.0040, GNorm: 0.4641
[53/299] timecost: 62.35, lr: 0.000100, Train: (LOSS: 0.0307, MAE: 0.0307, RMSE: 0.0480, R2: 0.9625), Valid: (LOSS: 0.0450, MAE: 0.0450, RMSE: 0.0737, R2: 0.9219), PNorm: 188.0163, GNorm: 0.5000
[54/299] timecost: 61.40, lr: 0.000100, Train: (LOSS: 0.0303, MAE: 0.0303, RMSE: 0.0468, R2: 0.9619), Valid: (LOSS: 0.0481, MAE: 0.0481, RMSE: 0.0785, R2: 0.9102), PNorm: 188.0279, GNorm: 0.5000
[55/299] timecost: 58.92, lr: 0.000100, Train: (LOSS: 0.0300, MAE: 0.0300, RMSE: 0.0475, R2: 0.9625), Valid: (LOSS: 0.0477, MAE: 0.0477, RMSE: 0.0759, R2: 0.9185), PNorm: 188.0414, GNorm: 0.4992
[56/299] timecost: 59.78, lr: 0.000100, Train: (LOSS: 0.0293, MAE: 0.0293, RMSE: 0.0461, R2: 0.9652), Valid: (LOSS: 0.0463, MAE: 0.0463, RMSE: 0.0752, R2: 0.9205), PNorm: 188.0538, GNorm: 0.4904
[57/299] timecost: 59.99, lr: 0.000100, Train: (LOSS: 0.0292, MAE: 0.0292, RMSE: 0.0457, R2: 0.9663), Valid: (LOSS: 0.0472, MAE: 0.0472, RMSE: 0.0735, R2: 0.9231), PNorm: 188.0669, GNorm: 0.3908
[58/299] timecost: 59.32, lr: 0.000100, Train: (LOSS: 0.0281, MAE: 0.0281, RMSE: 0.0439, R2: 0.9673), Valid: (LOSS: 0.0487, MAE: 0.0487, RMSE: 0.0763, R2: 0.9133), PNorm: 188.0807, GNorm: 0.5000
[59/299] timecost: 60.36, lr: 0.000100, Train: (LOSS: 0.0289, MAE: 0.0289, RMSE: 0.0457, R2: 0.9629), Valid: (LOSS: 0.0477, MAE: 0.0477, RMSE: 0.0738, R2: 0.9236), PNorm: 188.0921, GNorm: 0.4716
[60/299] timecost: 60.13, lr: 0.000100, Train: (LOSS: 0.0276, MAE: 0.0276, RMSE: 0.0432, R2: 0.9690), Valid: (LOSS: 0.0473, MAE: 0.0473, RMSE: 0.0718, R2: 0.9265), PNorm: 188.1029, GNorm: 0.4723
[61/299] timecost: 60.69, lr: 0.000100, Train: (LOSS: 0.0285, MAE: 0.0285, RMSE: 0.0448, R2: 0.9671), Valid: (LOSS: 0.0451, MAE: 0.0451, RMSE: 0.0708, R2: 0.9294), PNorm: 188.1136, GNorm: 0.5000
[62/299] timecost: 58.99, lr: 0.000100, Train: (LOSS: 0.0273, MAE: 0.0273, RMSE: 0.0430, R2: 0.9690), Valid: (LOSS: 0.0476, MAE: 0.0476, RMSE: 0.0756, R2: 0.9175), PNorm: 188.1263, GNorm: 0.5000
Epoch 00064: reducing learning rate of group 0 to 8.5000e-05.
[63/299] timecost: 58.84, lr: 0.000085, Train: (LOSS: 0.0276, MAE: 0.0276, RMSE: 0.0430, R2: 0.9693), Valid: (LOSS: 0.0500, MAE: 0.0500, RMSE: 0.0776, R2: 0.9137), PNorm: 188.1360, GNorm: 0.4461
[64/299] timecost: 59.08, lr: 0.000085, Train: (LOSS: 0.0264, MAE: 0.0264, RMSE: 0.0418, R2: 0.9705), Valid: (LOSS: 0.0464, MAE: 0.0464, RMSE: 0.0731, R2: 0.9233), PNorm: 188.1461, GNorm: 0.5000
[65/299] timecost: 60.95, lr: 0.000085, Train: (LOSS: 0.0257, MAE: 0.0257, RMSE: 0.0402, R2: 0.9728), Valid: (LOSS: 0.0452, MAE: 0.0452, RMSE: 0.0729, R2: 0.9259), PNorm: 188.1541, GNorm: 0.4641
[66/299] timecost: 60.73, lr: 0.000085, Train: (LOSS: 0.0244, MAE: 0.0244, RMSE: 0.0390, R2: 0.9715), Valid: (LOSS: 0.0463, MAE: 0.0463, RMSE: 0.0753, R2: 0.9174), PNorm: 188.1602, GNorm: 0.5000
[67/299] timecost: 59.00, lr: 0.000085, Train: (LOSS: 0.0242, MAE: 0.0242, RMSE: 0.0394, R2: 0.9742), Valid: (LOSS: 0.0471, MAE: 0.0471, RMSE: 0.0764, R2: 0.9165), PNorm: 188.1697, GNorm: 0.4997
[68/299] timecost: 58.88, lr: 0.000085, Train: (LOSS: 0.0245, MAE: 0.0245, RMSE: 0.0393, R2: 0.9725), Valid: (LOSS: 0.0464, MAE: 0.0464, RMSE: 0.0763, R2: 0.9181), PNorm: 188.1774, GNorm: 0.5000
[69/299] timecost: 58.65, lr: 0.000085, Train: (LOSS: 0.0240, MAE: 0.0240, RMSE: 0.0380, R2: 0.9747), Valid: (LOSS: 0.0424, MAE: 0.0424, RMSE: 0.0683, R2: 0.9321), PNorm: 188.1877, GNorm: 0.3995
[70/299] timecost: 59.54, lr: 0.000085, Train: (LOSS: 0.0234, MAE: 0.0234, RMSE: 0.0378, R2: 0.9749), Valid: (LOSS: 0.0444, MAE: 0.0444, RMSE: 0.0719, R2: 0.9260), PNorm: 188.1951, GNorm: 0.5000
[71/299] timecost: 59.33, lr: 0.000085, Train: (LOSS: 0.0229, MAE: 0.0229, RMSE: 0.0363, R2: 0.9773), Valid: (LOSS: 0.0434, MAE: 0.0434, RMSE: 0.0694, R2: 0.9305), PNorm: 188.2040, GNorm: 0.4760
[72/299] timecost: 59.55, lr: 0.000085, Train: (LOSS: 0.0238, MAE: 0.0238, RMSE: 0.0372, R2: 0.9743), Valid: (LOSS: 0.0443, MAE: 0.0443, RMSE: 0.0691, R2: 0.9302), PNorm: 188.2142, GNorm: 0.5000
[73/299] timecost: 58.81, lr: 0.000085, Train: (LOSS: 0.0220, MAE: 0.0220, RMSE: 0.0352, R2: 0.9784), Valid: (LOSS: 0.0444, MAE: 0.0444, RMSE: 0.0711, R2: 0.9267), PNorm: 188.2217, GNorm: 0.4440
[74/299] timecost: 58.85, lr: 0.000085, Train: (LOSS: 0.0226, MAE: 0.0226, RMSE: 0.0361, R2: 0.9770), Valid: (LOSS: 0.0480, MAE: 0.0480, RMSE: 0.0752, R2: 0.9205), PNorm: 188.2287, GNorm: 0.4294
[75/299] timecost: 58.64, lr: 0.000085, Train: (LOSS: 0.0236, MAE: 0.0236, RMSE: 0.0375, R2: 0.9765), Valid: (LOSS: 0.0467, MAE: 0.0467, RMSE: 0.0744, R2: 0.9212), PNorm: 188.2403, GNorm: 0.5000
[76/299] timecost: 58.54, lr: 0.000085, Train: (LOSS: 0.0229, MAE: 0.0229, RMSE: 0.0367, R2: 0.9749), Valid: (LOSS: 0.0472, MAE: 0.0472, RMSE: 0.0761, R2: 0.9166), PNorm: 188.2496, GNorm: 0.5000
[77/299] timecost: 58.72, lr: 0.000085, Train: (LOSS: 0.0224, MAE: 0.0224, RMSE: 0.0359, R2: 0.9769), Valid: (LOSS: 0.0451, MAE: 0.0451, RMSE: 0.0719, R2: 0.9247), PNorm: 188.2593, GNorm: 0.5000
[78/299] timecost: 59.64, lr: 0.000085, Train: (LOSS: 0.0225, MAE: 0.0225, RMSE: 0.0357, R2: 0.9774), Valid: (LOSS: 0.0443, MAE: 0.0443, RMSE: 0.0706, R2: 0.9280), PNorm: 188.2698, GNorm: 0.5000
[79/299] timecost: 60.62, lr: 0.000085, Train: (LOSS: 0.0227, MAE: 0.0227, RMSE: 0.0359, R2: 0.9767), Valid: (LOSS: 0.0440, MAE: 0.0440, RMSE: 0.0734, R2: 0.9182), PNorm: 188.2794, GNorm: 0.4649
[80/299] timecost: 60.86, lr: 0.000085, Train: (LOSS: 0.0217, MAE: 0.0217, RMSE: 0.0348, R2: 0.9786), Valid: (LOSS: 0.0450, MAE: 0.0450, RMSE: 0.0730, R2: 0.9220), PNorm: 188.2881, GNorm: 0.5000
[81/299] timecost: 61.14, lr: 0.000085, Train: (LOSS: 0.0216, MAE: 0.0216, RMSE: 0.0354, R2: 0.9779), Valid: (LOSS: 0.0445, MAE: 0.0445, RMSE: 0.0715, R2: 0.9256), PNorm: 188.2988, GNorm: 0.5000
[82/299] timecost: 61.14, lr: 0.000085, Train: (LOSS: 0.0219, MAE: 0.0219, RMSE: 0.0354, R2: 0.9776), Valid: (LOSS: 0.0432, MAE: 0.0432, RMSE: 0.0688, R2: 0.9311), PNorm: 188.3078, GNorm: 0.5000
[83/299] timecost: 60.62, lr: 0.000085, Train: (LOSS: 0.0209, MAE: 0.0209, RMSE: 0.0336, R2: 0.9783), Valid: (LOSS: 0.0444, MAE: 0.0444, RMSE: 0.0713, R2: 0.9260), PNorm: 188.3171, GNorm: 0.5000
[84/299] timecost: 60.60, lr: 0.000085, Train: (LOSS: 0.0209, MAE: 0.0209, RMSE: 0.0335, R2: 0.9796), Valid: (LOSS: 0.0425, MAE: 0.0425, RMSE: 0.0711, R2: 0.9264), PNorm: 188.3259, GNorm: 0.4015
Epoch 00086: reducing learning rate of group 0 to 7.2250e-05.
[85/299] timecost: 60.79, lr: 0.000072, Train: (LOSS: 0.0202, MAE: 0.0202, RMSE: 0.0334, R2: 0.9801), Valid: (LOSS: 0.0431, MAE: 0.0431, RMSE: 0.0705, R2: 0.9286), PNorm: 188.3342, GNorm: 0.5000
[86/299] timecost: 60.90, lr: 0.000072, Train: (LOSS: 0.0202, MAE: 0.0202, RMSE: 0.0329, R2: 0.9805), Valid: (LOSS: 0.0430, MAE: 0.0430, RMSE: 0.0693, R2: 0.9285), PNorm: 188.3401, GNorm: 0.4728
[87/299] timecost: 60.72, lr: 0.000072, Train: (LOSS: 0.0191, MAE: 0.0191, RMSE: 0.0315, R2: 0.9801), Valid: (LOSS: 0.0476, MAE: 0.0476, RMSE: 0.0786, R2: 0.9117), PNorm: 188.3458, GNorm: 0.4843
[88/299] timecost: 60.48, lr: 0.000072, Train: (LOSS: 0.0192, MAE: 0.0192, RMSE: 0.0317, R2: 0.9797), Valid: (LOSS: 0.0453, MAE: 0.0453, RMSE: 0.0731, R2: 0.9229), PNorm: 188.3524, GNorm: 0.5000
[89/299] timecost: 60.52, lr: 0.000072, Train: (LOSS: 0.0194, MAE: 0.0194, RMSE: 0.0319, R2: 0.9792), Valid: (LOSS: 0.0439, MAE: 0.0439, RMSE: 0.0718, R2: 0.9250), PNorm: 188.3606, GNorm: 0.3979
[90/299] timecost: 59.44, lr: 0.000072, Train: (LOSS: 0.0191, MAE: 0.0191, RMSE: 0.0314, R2: 0.9804), Valid: (LOSS: 0.0428, MAE: 0.0428, RMSE: 0.0700, R2: 0.9284), PNorm: 188.3688, GNorm: 0.5000
[91/299] timecost: 59.85, lr: 0.000072, Train: (LOSS: 0.0186, MAE: 0.0186, RMSE: 0.0303, R2: 0.9830), Valid: (LOSS: 0.0454, MAE: 0.0454, RMSE: 0.0724, R2: 0.9247), PNorm: 188.3741, GNorm: 0.4960
[92/299] timecost: 59.90, lr: 0.000072, Train: (LOSS: 0.0179, MAE: 0.0179, RMSE: 0.0302, R2: 0.9831), Valid: (LOSS: 0.0442, MAE: 0.0442, RMSE: 0.0697, R2: 0.9283), PNorm: 188.3814, GNorm: 0.5000
[93/299] timecost: 59.90, lr: 0.000072, Train: (LOSS: 0.0186, MAE: 0.0186, RMSE: 0.0302, R2: 0.9834), Valid: (LOSS: 0.0449, MAE: 0.0449, RMSE: 0.0723, R2: 0.9230), PNorm: 188.3874, GNorm: 0.4310
[94/299] timecost: 59.45, lr: 0.000072, Train: (LOSS: 0.0182, MAE: 0.0182, RMSE: 0.0297, R2: 0.9829), Valid: (LOSS: 0.0442, MAE: 0.0442, RMSE: 0.0699, R2: 0.9287), PNorm: 188.3924, GNorm: 0.4045
[95/299] timecost: 60.28, lr: 0.000072, Train: (LOSS: 0.0176, MAE: 0.0176, RMSE: 0.0295, R2: 0.9835), Valid: (LOSS: 0.0447, MAE: 0.0447, RMSE: 0.0723, R2: 0.9233), PNorm: 188.3990, GNorm: 0.4765
[96/299] timecost: 60.07, lr: 0.000072, Train: (LOSS: 0.0176, MAE: 0.0176, RMSE: 0.0291, R2: 0.9833), Valid: (LOSS: 0.0437, MAE: 0.0437, RMSE: 0.0705, R2: 0.9262), PNorm: 188.4048, GNorm: 0.3889
[97/299] timecost: 61.08, lr: 0.000072, Train: (LOSS: 0.0178, MAE: 0.0178, RMSE: 0.0289, R2: 0.9838), Valid: (LOSS: 0.0463, MAE: 0.0463, RMSE: 0.0756, R2: 0.9174), PNorm: 188.4124, GNorm: 0.4150
[98/299] timecost: 62.26, lr: 0.000072, Train: (LOSS: 0.0174, MAE: 0.0174, RMSE: 0.0285, R2: 0.9847), Valid: (LOSS: 0.0442, MAE: 0.0442, RMSE: 0.0740, R2: 0.9203), PNorm: 188.4189, GNorm: 0.4872
[99/299] timecost: 60.71, lr: 0.000072, Train: (LOSS: 0.0173, MAE: 0.0173, RMSE: 0.0283, R2: 0.9843), Valid: (LOSS: 0.0427, MAE: 0.0427, RMSE: 0.0709, R2: 0.9261), PNorm: 188.4277, GNorm: 0.3981
[100/299] timecost: 58.45, lr: 0.000072, Train: (LOSS: 0.0173, MAE: 0.0173, RMSE: 0.0287, R2: 0.9846), Valid: (LOSS: 0.0434, MAE: 0.0434, RMSE: 0.0725, R2: 0.9231), PNorm: 188.4346, GNorm: 0.4042
Epoch 00102: reducing learning rate of group 0 to 6.1413e-05.
[101/299] timecost: 57.99, lr: 0.000061, Train: (LOSS: 0.0162, MAE: 0.0162, RMSE: 0.0270, R2: 0.9848), Valid: (LOSS: 0.0447, MAE: 0.0447, RMSE: 0.0739, R2: 0.9212), PNorm: 188.4397, GNorm: 0.4523
[102/299] timecost: 58.78, lr: 0.000061, Train: (LOSS: 0.0160, MAE: 0.0160, RMSE: 0.0270, R2: 0.9857), Valid: (LOSS: 0.0433, MAE: 0.0433, RMSE: 0.0725, R2: 0.9232), PNorm: 188.4453, GNorm: 0.4485
[103/299] timecost: 58.57, lr: 0.000061, Train: (LOSS: 0.0156, MAE: 0.0156, RMSE: 0.0265, R2: 0.9854), Valid: (LOSS: 0.0434, MAE: 0.0434, RMSE: 0.0699, R2: 0.9288), PNorm: 188.4502, GNorm: 0.5000
[104/299] timecost: 58.84, lr: 0.000061, Train: (LOSS: 0.0160, MAE: 0.0160, RMSE: 0.0271, R2: 0.9859), Valid: (LOSS: 0.0435, MAE: 0.0435, RMSE: 0.0715, R2: 0.9258), PNorm: 188.4557, GNorm: 0.5000
[105/299] timecost: 58.56, lr: 0.000061, Train: (LOSS: 0.0155, MAE: 0.0155, RMSE: 0.0265, R2: 0.9869), Valid: (LOSS: 0.0423, MAE: 0.0423, RMSE: 0.0688, R2: 0.9302), PNorm: 188.4596, GNorm: 0.5000
[106/299] timecost: 60.14, lr: 0.000061, Train: (LOSS: 0.0153, MAE: 0.0153, RMSE: 0.0259, R2: 0.9858), Valid: (LOSS: 0.0437, MAE: 0.0437, RMSE: 0.0699, R2: 0.9282), PNorm: 188.4656, GNorm: 0.5000
[107/299] timecost: 62.28, lr: 0.000061, Train: (LOSS: 0.0153, MAE: 0.0153, RMSE: 0.0255, R2: 0.9863), Valid: (LOSS: 0.0434, MAE: 0.0434, RMSE: 0.0705, R2: 0.9275), PNorm: 188.4697, GNorm: 0.4091
[108/299] timecost: 60.28, lr: 0.000061, Train: (LOSS: 0.0150, MAE: 0.0150, RMSE: 0.0254, R2: 0.9863), Valid: (LOSS: 0.0427, MAE: 0.0427, RMSE: 0.0696, R2: 0.9285), PNorm: 188.4751, GNorm: 0.4501
[109/299] timecost: 60.35, lr: 0.000061, Train: (LOSS: 0.0150, MAE: 0.0150, RMSE: 0.0255, R2: 0.9861), Valid: (LOSS: 0.0436, MAE: 0.0436, RMSE: 0.0721, R2: 0.9244), PNorm: 188.4801, GNorm: 0.4651
[110/299] timecost: 60.39, lr: 0.000061, Train: (LOSS: 0.0147, MAE: 0.0147, RMSE: 0.0259, R2: 0.9863), Valid: (LOSS: 0.0418, MAE: 0.0418, RMSE: 0.0681, R2: 0.9326), PNorm: 188.4866, GNorm: 0.5000
[111/299] timecost: 60.15, lr: 0.000061, Train: (LOSS: 0.0148, MAE: 0.0148, RMSE: 0.0258, R2: 0.9864), Valid: (LOSS: 0.0443, MAE: 0.0443, RMSE: 0.0710, R2: 0.9269), PNorm: 188.4897, GNorm: 0.4614
[112/299] timecost: 59.82, lr: 0.000061, Train: (LOSS: 0.0143, MAE: 0.0143, RMSE: 0.0251, R2: 0.9871), Valid: (LOSS: 0.0411, MAE: 0.0411, RMSE: 0.0685, R2: 0.9311), PNorm: 188.4936, GNorm: 0.5000
[113/299] timecost: 58.94, lr: 0.000061, Train: (LOSS: 0.0146, MAE: 0.0146, RMSE: 0.0248, R2: 0.9862), Valid: (LOSS: 0.0414, MAE: 0.0414, RMSE: 0.0677, R2: 0.9313), PNorm: 188.4994, GNorm: 0.4641
[114/299] timecost: 59.59, lr: 0.000061, Train: (LOSS: 0.0146, MAE: 0.0146, RMSE: 0.0248, R2: 0.9870), Valid: (LOSS: 0.0432, MAE: 0.0432, RMSE: 0.0703, R2: 0.9290), PNorm: 188.5038, GNorm: 0.4663
[115/299] timecost: 58.65, lr: 0.000061, Train: (LOSS: 0.0144, MAE: 0.0144, RMSE: 0.0246, R2: 0.9876), Valid: (LOSS: 0.0434, MAE: 0.0434, RMSE: 0.0697, R2: 0.9297), PNorm: 188.5108, GNorm: 0.4902
[116/299] timecost: 58.70, lr: 0.000061, Train: (LOSS: 0.0143, MAE: 0.0143, RMSE: 0.0243, R2: 0.9868), Valid: (LOSS: 0.0444, MAE: 0.0444, RMSE: 0.0711, R2: 0.9257), PNorm: 188.5159, GNorm: 0.5000
[117/299] timecost: 58.56, lr: 0.000061, Train: (LOSS: 0.0147, MAE: 0.0147, RMSE: 0.0249, R2: 0.9870), Valid: (LOSS: 0.0415, MAE: 0.0415, RMSE: 0.0673, R2: 0.9338), PNorm: 188.5214, GNorm: 0.5000
[118/299] timecost: 58.53, lr: 0.000061, Train: (LOSS: 0.0141, MAE: 0.0141, RMSE: 0.0239, R2: 0.9870), Valid: (LOSS: 0.0420, MAE: 0.0420, RMSE: 0.0684, R2: 0.9297), PNorm: 188.5276, GNorm: 0.3459
[119/299] timecost: 58.37, lr: 0.000061, Train: (LOSS: 0.0146, MAE: 0.0146, RMSE: 0.0246, R2: 0.9860), Valid: (LOSS: 0.0431, MAE: 0.0431, RMSE: 0.0687, R2: 0.9318), PNorm: 188.5329, GNorm: 0.5000
[120/299] timecost: 58.83, lr: 0.000061, Train: (LOSS: 0.0137, MAE: 0.0137, RMSE: 0.0239, R2: 0.9881), Valid: (LOSS: 0.0435, MAE: 0.0435, RMSE: 0.0701, R2: 0.9293), PNorm: 188.5375, GNorm: 0.4452
[121/299] timecost: 60.98, lr: 0.000061, Train: (LOSS: 0.0142, MAE: 0.0142, RMSE: 0.0244, R2: 0.9875), Valid: (LOSS: 0.0449, MAE: 0.0449, RMSE: 0.0714, R2: 0.9261), PNorm: 188.5438, GNorm: 0.3874
[122/299] timecost: 61.10, lr: 0.000061, Train: (LOSS: 0.0141, MAE: 0.0141, RMSE: 0.0244, R2: 0.9874), Valid: (LOSS: 0.0418, MAE: 0.0418, RMSE: 0.0685, R2: 0.9312), PNorm: 188.5485, GNorm: 0.5000
[123/299] timecost: 60.79, lr: 0.000061, Train: (LOSS: 0.0144, MAE: 0.0144, RMSE: 0.0244, R2: 0.9874), Valid: (LOSS: 0.0417, MAE: 0.0417, RMSE: 0.0684, R2: 0.9306), PNorm: 188.5552, GNorm: 0.4700
[124/299] timecost: 59.67, lr: 0.000061, Train: (LOSS: 0.0143, MAE: 0.0143, RMSE: 0.0247, R2: 0.9869), Valid: (LOSS: 0.0446, MAE: 0.0446, RMSE: 0.0709, R2: 0.9271), PNorm: 188.5627, GNorm: 0.4476
[125/299] timecost: 61.48, lr: 0.000061, Train: (LOSS: 0.0145, MAE: 0.0145, RMSE: 0.0248, R2: 0.9870), Valid: (LOSS: 0.0429, MAE: 0.0429, RMSE: 0.0711, R2: 0.9254), PNorm: 188.5662, GNorm: 0.4654
[126/299] timecost: 62.44, lr: 0.000061, Train: (LOSS: 0.0143, MAE: 0.0143, RMSE: 0.0244, R2: 0.9873), Valid: (LOSS: 0.0453, MAE: 0.0453, RMSE: 0.0736, R2: 0.9197), PNorm: 188.5745, GNorm: 0.4973
[127/299] timecost: 62.41, lr: 0.000061, Train: (LOSS: 0.0137, MAE: 0.0137, RMSE: 0.0232, R2: 0.9868), Valid: (LOSS: 0.0435, MAE: 0.0435, RMSE: 0.0714, R2: 0.9244), PNorm: 188.5813, GNorm: 0.3485
Epoch 00129: reducing learning rate of group 0 to 5.2201e-05.
[128/299] timecost: 62.30, lr: 0.000052, Train: (LOSS: 0.0134, MAE: 0.0134, RMSE: 0.0233, R2: 0.9885), Valid: (LOSS: 0.0442, MAE: 0.0442, RMSE: 0.0706, R2: 0.9270), PNorm: 188.5857, GNorm: 0.5000
[129/299] timecost: 62.40, lr: 0.000052, Train: (LOSS: 0.0132, MAE: 0.0132, RMSE: 0.0226, R2: 0.9873), Valid: (LOSS: 0.0419, MAE: 0.0419, RMSE: 0.0676, R2: 0.9314), PNorm: 188.5904, GNorm: 0.3699
[130/299] timecost: 62.40, lr: 0.000052, Train: (LOSS: 0.0125, MAE: 0.0125, RMSE: 0.0223, R2: 0.9888), Valid: (LOSS: 0.0415, MAE: 0.0415, RMSE: 0.0688, R2: 0.9298), PNorm: 188.5944, GNorm: 0.3941
[131/299] timecost: 62.31, lr: 0.000052, Train: (LOSS: 0.0123, MAE: 0.0123, RMSE: 0.0218, R2: 0.9899), Valid: (LOSS: 0.0423, MAE: 0.0423, RMSE: 0.0680, R2: 0.9319), PNorm: 188.5989, GNorm: 0.5000
[132/299] timecost: 62.12, lr: 0.000052, Train: (LOSS: 0.0123, MAE: 0.0123, RMSE: 0.0218, R2: 0.9894), Valid: (LOSS: 0.0436, MAE: 0.0436, RMSE: 0.0701, R2: 0.9269), PNorm: 188.6019, GNorm: 0.5000
[133/299] timecost: 59.32, lr: 0.000052, Train: (LOSS: 0.0127, MAE: 0.0127, RMSE: 0.0222, R2: 0.9896), Valid: (LOSS: 0.0429, MAE: 0.0429, RMSE: 0.0681, R2: 0.9315), PNorm: 188.6063, GNorm: 0.4800
[134/299] timecost: 58.68, lr: 0.000052, Train: (LOSS: 0.0124, MAE: 0.0124, RMSE: 0.0219, R2: 0.9889), Valid: (LOSS: 0.0425, MAE: 0.0425, RMSE: 0.0692, R2: 0.9302), PNorm: 188.6096, GNorm: 0.4770
[135/299] timecost: 59.32, lr: 0.000052, Train: (LOSS: 0.0120, MAE: 0.0120, RMSE: 0.0213, R2: 0.9887), Valid: (LOSS: 0.0416, MAE: 0.0416, RMSE: 0.0692, R2: 0.9285), PNorm: 188.6142, GNorm: 0.4648
[136/299] timecost: 59.27, lr: 0.000052, Train: (LOSS: 0.0115, MAE: 0.0115, RMSE: 0.0211, R2: 0.9887), Valid: (LOSS: 0.0418, MAE: 0.0418, RMSE: 0.0674, R2: 0.9318), PNorm: 188.6174, GNorm: 0.5000
[137/299] timecost: 59.35, lr: 0.000052, Train: (LOSS: 0.0122, MAE: 0.0122, RMSE: 0.0220, R2: 0.9879), Valid: (LOSS: 0.0416, MAE: 0.0416, RMSE: 0.0688, R2: 0.9294), PNorm: 188.6228, GNorm: 0.4925
[138/299] timecost: 59.45, lr: 0.000052, Train: (LOSS: 0.0121, MAE: 0.0121, RMSE: 0.0217, R2: 0.9904), Valid: (LOSS: 0.0426, MAE: 0.0426, RMSE: 0.0703, R2: 0.9275), PNorm: 188.6258, GNorm: 0.5000
[139/299] timecost: 59.69, lr: 0.000052, Train: (LOSS: 0.0120, MAE: 0.0120, RMSE: 0.0211, R2: 0.9890), Valid: (LOSS: 0.0411, MAE: 0.0411, RMSE: 0.0663, R2: 0.9350), PNorm: 188.6296, GNorm: 0.5000
[140/299] timecost: 61.55, lr: 0.000052, Train: (LOSS: 0.0119, MAE: 0.0119, RMSE: 0.0216, R2: 0.9896), Valid: (LOSS: 0.0418, MAE: 0.0418, RMSE: 0.0686, R2: 0.9301), PNorm: 188.6331, GNorm: 0.5000
[141/299] timecost: 58.65, lr: 0.000052, Train: (LOSS: 0.0114, MAE: 0.0114, RMSE: 0.0208, R2: 0.9906), Valid: (LOSS: 0.0429, MAE: 0.0429, RMSE: 0.0685, R2: 0.9308), PNorm: 188.6383, GNorm: 0.5000
[142/299] timecost: 59.16, lr: 0.000052, Train: (LOSS: 0.0122, MAE: 0.0122, RMSE: 0.0213, R2: 0.9903), Valid: (LOSS: 0.0428, MAE: 0.0428, RMSE: 0.0690, R2: 0.9291), PNorm: 188.6426, GNorm: 0.4590
[143/299] timecost: 58.87, lr: 0.000052, Train: (LOSS: 0.0114, MAE: 0.0114, RMSE: 0.0207, R2: 0.9899), Valid: (LOSS: 0.0428, MAE: 0.0428, RMSE: 0.0703, R2: 0.9279), PNorm: 188.6460, GNorm: 0.4585
Epoch 00145: reducing learning rate of group 0 to 4.4371e-05.
[144/299] timecost: 58.55, lr: 0.000044, Train: (LOSS: 0.0115, MAE: 0.0115, RMSE: 0.0206, R2: 0.9897), Valid: (LOSS: 0.0409, MAE: 0.0409, RMSE: 0.0682, R2: 0.9309), PNorm: 188.6486, GNorm: 0.4717
[145/299] timecost: 58.58, lr: 0.000044, Train: (LOSS: 0.0111, MAE: 0.0111, RMSE: 0.0207, R2: 0.9900), Valid: (LOSS: 0.0425, MAE: 0.0425, RMSE: 0.0699, R2: 0.9281), PNorm: 188.6526, GNorm: 0.5000
[146/299] timecost: 58.65, lr: 0.000044, Train: (LOSS: 0.0110, MAE: 0.0110, RMSE: 0.0199, R2: 0.9901), Valid: (LOSS: 0.0423, MAE: 0.0423, RMSE: 0.0691, R2: 0.9298), PNorm: 188.6555, GNorm: 0.4329
[147/299] timecost: 59.43, lr: 0.000044, Train: (LOSS: 0.0105, MAE: 0.0105, RMSE: 0.0199, R2: 0.9907), Valid: (LOSS: 0.0418, MAE: 0.0418, RMSE: 0.0674, R2: 0.9332), PNorm: 188.6586, GNorm: 0.5000
[148/299] timecost: 59.54, lr: 0.000044, Train: (LOSS: 0.0108, MAE: 0.0108, RMSE: 0.0200, R2: 0.9893), Valid: (LOSS: 0.0413, MAE: 0.0413, RMSE: 0.0683, R2: 0.9308), PNorm: 188.6614, GNorm: 0.3469
[149/299] timecost: 59.31, lr: 0.000044, Train: (LOSS: 0.0104, MAE: 0.0104, RMSE: 0.0198, R2: 0.9903), Valid: (LOSS: 0.0411, MAE: 0.0411, RMSE: 0.0678, R2: 0.9324), PNorm: 188.6656, GNorm: 0.5000
[150/299] timecost: 59.71, lr: 0.000044, Train: (LOSS: 0.0103, MAE: 0.0103, RMSE: 0.0193, R2: 0.9905), Valid: (LOSS: 0.0416, MAE: 0.0416, RMSE: 0.0679, R2: 0.9312), PNorm: 188.6685, GNorm: 0.5000
[151/299] timecost: 60.36, lr: 0.000044, Train: (LOSS: 0.0102, MAE: 0.0102, RMSE: 0.0190, R2: 0.9907), Valid: (LOSS: 0.0416, MAE: 0.0416, RMSE: 0.0685, R2: 0.9295), PNorm: 188.6707, GNorm: 0.4789
[152/299] timecost: 60.78, lr: 0.000044, Train: (LOSS: 0.0104, MAE: 0.0104, RMSE: 0.0198, R2: 0.9908), Valid: (LOSS: 0.0414, MAE: 0.0414, RMSE: 0.0685, R2: 0.9308), PNorm: 188.6731, GNorm: 0.5000
[153/299] timecost: 60.55, lr: 0.000044, Train: (LOSS: 0.0103, MAE: 0.0103, RMSE: 0.0193, R2: 0.9911), Valid: (LOSS: 0.0411, MAE: 0.0411, RMSE: 0.0665, R2: 0.9355), PNorm: 188.6767, GNorm: 0.4136
[154/299] timecost: 58.51, lr: 0.000044, Train: (LOSS: 0.0103, MAE: 0.0103, RMSE: 0.0196, R2: 0.9912), Valid: (LOSS: 0.0423, MAE: 0.0423, RMSE: 0.0690, R2: 0.9290), PNorm: 188.6806, GNorm: 0.4569
[155/299] timecost: 60.40, lr: 0.000044, Train: (LOSS: 0.0101, MAE: 0.0101, RMSE: 0.0194, R2: 0.9912), Valid: (LOSS: 0.0415, MAE: 0.0415, RMSE: 0.0678, R2: 0.9316), PNorm: 188.6828, GNorm: 0.4152
[156/299] timecost: 61.27, lr: 0.000044, Train: (LOSS: 0.0101, MAE: 0.0101, RMSE: 0.0187, R2: 0.9913), Valid: (LOSS: 0.0419, MAE: 0.0419, RMSE: 0.0679, R2: 0.9315), PNorm: 188.6861, GNorm: 0.3986
[157/299] timecost: 59.66, lr: 0.000044, Train: (LOSS: 0.0100, MAE: 0.0100, RMSE: 0.0191, R2: 0.9908), Valid: (LOSS: 0.0428, MAE: 0.0428, RMSE: 0.0692, R2: 0.9289), PNorm: 188.6902, GNorm: 0.5000
[158/299] timecost: 59.58, lr: 0.000044, Train: (LOSS: 0.0104, MAE: 0.0104, RMSE: 0.0194, R2: 0.9899), Valid: (LOSS: 0.0424, MAE: 0.0424, RMSE: 0.0698, R2: 0.9274), PNorm: 188.6928, GNorm: 0.5000
[159/299] timecost: 59.10, lr: 0.000044, Train: (LOSS: 0.0102, MAE: 0.0102, RMSE: 0.0188, R2: 0.9901), Valid: (LOSS: 0.0423, MAE: 0.0423, RMSE: 0.0684, R2: 0.9299), PNorm: 188.6952, GNorm: 0.5000
Epoch 00161: reducing learning rate of group 0 to 3.7715e-05.
[160/299] timecost: 62.48, lr: 0.000038, Train: (LOSS: 0.0101, MAE: 0.0101, RMSE: 0.0190, R2: 0.9906), Valid: (LOSS: 0.0415, MAE: 0.0415, RMSE: 0.0670, R2: 0.9335), PNorm: 188.6990, GNorm: 0.5000
[161/299] timecost: 62.39, lr: 0.000038, Train: (LOSS: 0.0098, MAE: 0.0098, RMSE: 0.0185, R2: 0.9893), Valid: (LOSS: 0.0427, MAE: 0.0427, RMSE: 0.0699, R2: 0.9280), PNorm: 188.7012, GNorm: 0.3924
[162/299] timecost: 62.41, lr: 0.000038, Train: (LOSS: 0.0093, MAE: 0.0093, RMSE: 0.0185, R2: 0.9916), Valid: (LOSS: 0.0424, MAE: 0.0424, RMSE: 0.0686, R2: 0.9308), PNorm: 188.7026, GNorm: 0.4186
[163/299] timecost: 62.57, lr: 0.000038, Train: (LOSS: 0.0092, MAE: 0.0092, RMSE: 0.0184, R2: 0.9912), Valid: (LOSS: 0.0412, MAE: 0.0412, RMSE: 0.0680, R2: 0.9309), PNorm: 188.7051, GNorm: 0.5000
[164/299] timecost: 62.57, lr: 0.000038, Train: (LOSS: 0.0091, MAE: 0.0091, RMSE: 0.0182, R2: 0.9905), Valid: (LOSS: 0.0431, MAE: 0.0431, RMSE: 0.0708, R2: 0.9259), PNorm: 188.7069, GNorm: 0.4670
[165/299] timecost: 62.30, lr: 0.000038, Train: (LOSS: 0.0096, MAE: 0.0096, RMSE: 0.0181, R2: 0.9911), Valid: (LOSS: 0.0419, MAE: 0.0419, RMSE: 0.0683, R2: 0.9303), PNorm: 188.7104, GNorm: 0.3175
[166/299] timecost: 59.42, lr: 0.000038, Train: (LOSS: 0.0094, MAE: 0.0094, RMSE: 0.0184, R2: 0.9910), Valid: (LOSS: 0.0416, MAE: 0.0416, RMSE: 0.0679, R2: 0.9317), PNorm: 188.7127, GNorm: 0.4129
[167/299] timecost: 58.77, lr: 0.000038, Train: (LOSS: 0.0091, MAE: 0.0091, RMSE: 0.0179, R2: 0.9908), Valid: (LOSS: 0.0417, MAE: 0.0417, RMSE: 0.0678, R2: 0.9310), PNorm: 188.7144, GNorm: 0.5000
[168/299] timecost: 58.65, lr: 0.000038, Train: (LOSS: 0.0088, MAE: 0.0088, RMSE: 0.0177, R2: 0.9919), Valid: (LOSS: 0.0414, MAE: 0.0414, RMSE: 0.0671, R2: 0.9333), PNorm: 188.7157, GNorm: 0.5000
[169/299] timecost: 58.97, lr: 0.000038, Train: (LOSS: 0.0088, MAE: 0.0088, RMSE: 0.0180, R2: 0.9913), Valid: (LOSS: 0.0423, MAE: 0.0423, RMSE: 0.0687, R2: 0.9299), PNorm: 188.7191, GNorm: 0.4089
[170/299] timecost: 59.03, lr: 0.000038, Train: (LOSS: 0.0091, MAE: 0.0091, RMSE: 0.0182, R2: 0.9911), Valid: (LOSS: 0.0423, MAE: 0.0423, RMSE: 0.0694, R2: 0.9286), PNorm: 188.7210, GNorm: 0.5000
[171/299] timecost: 61.59, lr: 0.000038, Train: (LOSS: 0.0093, MAE: 0.0093, RMSE: 0.0179, R2: 0.9911), Valid: (LOSS: 0.0420, MAE: 0.0420, RMSE: 0.0687, R2: 0.9293), PNorm: 188.7241, GNorm: 0.5000
[172/299] timecost: 58.96, lr: 0.000038, Train: (LOSS: 0.0090, MAE: 0.0090, RMSE: 0.0179, R2: 0.9919), Valid: (LOSS: 0.0420, MAE: 0.0420, RMSE: 0.0688, R2: 0.9297), PNorm: 188.7259, GNorm: 0.5000
[173/299] timecost: 58.79, lr: 0.000038, Train: (LOSS: 0.0089, MAE: 0.0089, RMSE: 0.0180, R2: 0.9910), Valid: (LOSS: 0.0416, MAE: 0.0416, RMSE: 0.0680, R2: 0.9312), PNorm: 188.7283, GNorm: 0.4088
[174/299] timecost: 58.44, lr: 0.000038, Train: (LOSS: 0.0089, MAE: 0.0089, RMSE: 0.0179, R2: 0.9918), Valid: (LOSS: 0.0423, MAE: 0.0423, RMSE: 0.0685, R2: 0.9300), PNorm: 188.7314, GNorm: 0.5000
[175/299] timecost: 58.44, lr: 0.000038, Train: (LOSS: 0.0090, MAE: 0.0090, RMSE: 0.0180, R2: 0.9902), Valid: (LOSS: 0.0413, MAE: 0.0413, RMSE: 0.0673, R2: 0.9323), PNorm: 188.7337, GNorm: 0.4996
Epoch 00177: reducing learning rate of group 0 to 3.2058e-05.
[176/299] timecost: 58.47, lr: 0.000032, Train: (LOSS: 0.0090, MAE: 0.0090, RMSE: 0.0178, R2: 0.9910), Valid: (LOSS: 0.0413, MAE: 0.0413, RMSE: 0.0683, R2: 0.9299), PNorm: 188.7358, GNorm: 0.3826
[177/299] timecost: 58.57, lr: 0.000032, Train: (LOSS: 0.0084, MAE: 0.0084, RMSE: 0.0172, R2: 0.9916), Valid: (LOSS: 0.0418, MAE: 0.0418, RMSE: 0.0688, R2: 0.9298), PNorm: 188.7381, GNorm: 0.5000
[178/299] timecost: 58.38, lr: 0.000032, Train: (LOSS: 0.0086, MAE: 0.0086, RMSE: 0.0179, R2: 0.9913), Valid: (LOSS: 0.0421, MAE: 0.0421, RMSE: 0.0687, R2: 0.9301), PNorm: 188.7396, GNorm: 0.4330
[179/299] timecost: 58.76, lr: 0.000032, Train: (LOSS: 0.0082, MAE: 0.0082, RMSE: 0.0171, R2: 0.9912), Valid: (LOSS: 0.0414, MAE: 0.0414, RMSE: 0.0676, R2: 0.9320), PNorm: 188.7413, GNorm: 0.4718
[180/299] timecost: 60.38, lr: 0.000032, Train: (LOSS: 0.0083, MAE: 0.0083, RMSE: 0.0169, R2: 0.9917), Valid: (LOSS: 0.0420, MAE: 0.0420, RMSE: 0.0689, R2: 0.9293), PNorm: 188.7423, GNorm: 0.5000
[181/299] timecost: 59.39, lr: 0.000032, Train: (LOSS: 0.0083, MAE: 0.0083, RMSE: 0.0167, R2: 0.9926), Valid: (LOSS: 0.0417, MAE: 0.0417, RMSE: 0.0677, R2: 0.9318), PNorm: 188.7442, GNorm: 0.3675
[182/299] timecost: 59.61, lr: 0.000032, Train: (LOSS: 0.0083, MAE: 0.0083, RMSE: 0.0175, R2: 0.9920), Valid: (LOSS: 0.0427, MAE: 0.0427, RMSE: 0.0688, R2: 0.9297), PNorm: 188.7459, GNorm: 0.3589
[183/299] timecost: 59.94, lr: 0.000032, Train: (LOSS: 0.0082, MAE: 0.0082, RMSE: 0.0173, R2: 0.9913), Valid: (LOSS: 0.0413, MAE: 0.0413, RMSE: 0.0678, R2: 0.9309), PNorm: 188.7481, GNorm: 0.5000
[184/299] timecost: 60.36, lr: 0.000032, Train: (LOSS: 0.0083, MAE: 0.0083, RMSE: 0.0170, R2: 0.9903), Valid: (LOSS: 0.0416, MAE: 0.0416, RMSE: 0.0679, R2: 0.9313), PNorm: 188.7495, GNorm: 0.5000
[185/299] timecost: 60.59, lr: 0.000032, Train: (LOSS: 0.0081, MAE: 0.0081, RMSE: 0.0167, R2: 0.9914), Valid: (LOSS: 0.0431, MAE: 0.0431, RMSE: 0.0700, R2: 0.9273), PNorm: 188.7514, GNorm: 0.5000
[186/299] timecost: 60.29, lr: 0.000032, Train: (LOSS: 0.0082, MAE: 0.0082, RMSE: 0.0170, R2: 0.9913), Valid: (LOSS: 0.0414, MAE: 0.0414, RMSE: 0.0675, R2: 0.9320), PNorm: 188.7536, GNorm: 0.3684
[187/299] timecost: 62.42, lr: 0.000032, Train: (LOSS: 0.0080, MAE: 0.0080, RMSE: 0.0167, R2: 0.9925), Valid: (LOSS: 0.0408, MAE: 0.0408, RMSE: 0.0671, R2: 0.9329), PNorm: 188.7546, GNorm: 0.4271
[188/299] timecost: 62.27, lr: 0.000032, Train: (LOSS: 0.0082, MAE: 0.0082, RMSE: 0.0170, R2: 0.9916), Valid: (LOSS: 0.0429, MAE: 0.0429, RMSE: 0.0694, R2: 0.9283), PNorm: 188.7563, GNorm: 0.4066
[189/299] timecost: 62.26, lr: 0.000032, Train: (LOSS: 0.0083, MAE: 0.0083, RMSE: 0.0168, R2: 0.9914), Valid: (LOSS: 0.0417, MAE: 0.0417, RMSE: 0.0680, R2: 0.9310), PNorm: 188.7594, GNorm: 0.4833
[190/299] timecost: 62.28, lr: 0.000032, Train: (LOSS: 0.0080, MAE: 0.0080, RMSE: 0.0167, R2: 0.9919), Valid: (LOSS: 0.0420, MAE: 0.0420, RMSE: 0.0685, R2: 0.9299), PNorm: 188.7614, GNorm: 0.3501
[191/299] timecost: 62.33, lr: 0.000032, Train: (LOSS: 0.0080, MAE: 0.0080, RMSE: 0.0164, R2: 0.9922), Valid: (LOSS: 0.0408, MAE: 0.0408, RMSE: 0.0676, R2: 0.9312), PNorm: 188.7622, GNorm: 0.5000
Epoch 00193: reducing learning rate of group 0 to 2.7249e-05.
[192/299] timecost: 62.41, lr: 0.000027, Train: (LOSS: 0.0078, MAE: 0.0078, RMSE: 0.0167, R2: 0.9913), Valid: (LOSS: 0.0416, MAE: 0.0416, RMSE: 0.0677, R2: 0.9311), PNorm: 188.7637, GNorm: 0.4516
[193/299] timecost: 62.33, lr: 0.000027, Train: (LOSS: 0.0076, MAE: 0.0076, RMSE: 0.0164, R2: 0.9924), Valid: (LOSS: 0.0418, MAE: 0.0418, RMSE: 0.0677, R2: 0.9319), PNorm: 188.7655, GNorm: 0.5000
[194/299] timecost: 62.27, lr: 0.000027, Train: (LOSS: 0.0076, MAE: 0.0076, RMSE: 0.0163, R2: 0.9919), Valid: (LOSS: 0.0419, MAE: 0.0419, RMSE: 0.0690, R2: 0.9281), PNorm: 188.7668, GNorm: 0.4708
[195/299] timecost: 62.36, lr: 0.000027, Train: (LOSS: 0.0073, MAE: 0.0073, RMSE: 0.0162, R2: 0.9919), Valid: (LOSS: 0.0410, MAE: 0.0410, RMSE: 0.0665, R2: 0.9335), PNorm: 188.7679, GNorm: 0.3830
[196/299] timecost: 62.28, lr: 0.000027, Train: (LOSS: 0.0072, MAE: 0.0072, RMSE: 0.0161, R2: 0.9920), Valid: (LOSS: 0.0417, MAE: 0.0417, RMSE: 0.0682, R2: 0.9305), PNorm: 188.7690, GNorm: 0.4250
[197/299] timecost: 60.99, lr: 0.000027, Train: (LOSS: 0.0072, MAE: 0.0072, RMSE: 0.0161, R2: 0.9927), Valid: (LOSS: 0.0421, MAE: 0.0421, RMSE: 0.0687, R2: 0.9293), PNorm: 188.7703, GNorm: 0.3252
[198/299] timecost: 60.10, lr: 0.000027, Train: (LOSS: 0.0073, MAE: 0.0073, RMSE: 0.0160, R2: 0.9918), Valid: (LOSS: 0.0420, MAE: 0.0420, RMSE: 0.0685, R2: 0.9297), PNorm: 188.7717, GNorm: 0.4560
[199/299] timecost: 60.40, lr: 0.000027, Train: (LOSS: 0.0074, MAE: 0.0074, RMSE: 0.0161, R2: 0.9931), Valid: (LOSS: 0.0421, MAE: 0.0421, RMSE: 0.0685, R2: 0.9298), PNorm: 188.7731, GNorm: 0.5000
[200/299] timecost: 60.45, lr: 0.000027, Train: (LOSS: 0.0073, MAE: 0.0073, RMSE: 0.0158, R2: 0.9916), Valid: (LOSS: 0.0424, MAE: 0.0424, RMSE: 0.0689, R2: 0.9290), PNorm: 188.7745, GNorm: 0.4449
[201/299] timecost: 60.22, lr: 0.000027, Train: (LOSS: 0.0072, MAE: 0.0072, RMSE: 0.0149, R2: 0.9922), Valid: (LOSS: 0.0416, MAE: 0.0416, RMSE: 0.0675, R2: 0.9320), PNorm: 188.7763, GNorm: 0.4089
[202/299] timecost: 59.48, lr: 0.000027, Train: (LOSS: 0.0074, MAE: 0.0074, RMSE: 0.0159, R2: 0.9920), Valid: (LOSS: 0.0418, MAE: 0.0418, RMSE: 0.0687, R2: 0.9298), PNorm: 188.7775, GNorm: 0.5000
[203/299] timecost: 59.48, lr: 0.000027, Train: (LOSS: 0.0073, MAE: 0.0073, RMSE: 0.0160, R2: 0.9920), Valid: (LOSS: 0.0415, MAE: 0.0415, RMSE: 0.0678, R2: 0.9311), PNorm: 188.7783, GNorm: 0.4757
[204/299] timecost: 60.24, lr: 0.000027, Train: (LOSS: 0.0075, MAE: 0.0075, RMSE: 0.0165, R2: 0.9912), Valid: (LOSS: 0.0425, MAE: 0.0425, RMSE: 0.0697, R2: 0.9275), PNorm: 188.7799, GNorm: 0.4622
[205/299] timecost: 60.25, lr: 0.000027, Train: (LOSS: 0.0073, MAE: 0.0073, RMSE: 0.0156, R2: 0.9923), Valid: (LOSS: 0.0412, MAE: 0.0412, RMSE: 0.0670, R2: 0.9326), PNorm: 188.7813, GNorm: 0.4385
[206/299] timecost: 60.39, lr: 0.000027, Train: (LOSS: 0.0074, MAE: 0.0074, RMSE: 0.0164, R2: 0.9924), Valid: (LOSS: 0.0411, MAE: 0.0411, RMSE: 0.0679, R2: 0.9312), PNorm: 188.7829, GNorm: 0.3904
[207/299] timecost: 60.73, lr: 0.000027, Train: (LOSS: 0.0074, MAE: 0.0074, RMSE: 0.0156, R2: 0.9920), Valid: (LOSS: 0.0414, MAE: 0.0414, RMSE: 0.0680, R2: 0.9308), PNorm: 188.7840, GNorm: 0.4403
Epoch 00209: reducing learning rate of group 0 to 2.3162e-05.
[208/299] timecost: 60.75, lr: 0.000023, Train: (LOSS: 0.0072, MAE: 0.0072, RMSE: 0.0163, R2: 0.9924), Valid: (LOSS: 0.0414, MAE: 0.0414, RMSE: 0.0676, R2: 0.9321), PNorm: 188.7856, GNorm: 0.5000
[209/299] timecost: 61.11, lr: 0.000023, Train: (LOSS: 0.0070, MAE: 0.0070, RMSE: 0.0158, R2: 0.9926), Valid: (LOSS: 0.0419, MAE: 0.0419, RMSE: 0.0691, R2: 0.9289), PNorm: 188.7864, GNorm: 0.3479
[210/299] timecost: 60.99, lr: 0.000023, Train: (LOSS: 0.0070, MAE: 0.0070, RMSE: 0.0154, R2: 0.9924), Valid: (LOSS: 0.0414, MAE: 0.0414, RMSE: 0.0675, R2: 0.9314), PNorm: 188.7882, GNorm: 0.5000
[211/299] timecost: 61.07, lr: 0.000023, Train: (LOSS: 0.0067, MAE: 0.0067, RMSE: 0.0157, R2: 0.9925), Valid: (LOSS: 0.0417, MAE: 0.0417, RMSE: 0.0675, R2: 0.9315), PNorm: 188.7889, GNorm: 0.4662
[212/299] timecost: 60.69, lr: 0.000023, Train: (LOSS: 0.0068, MAE: 0.0068, RMSE: 0.0155, R2: 0.9921), Valid: (LOSS: 0.0420, MAE: 0.0420, RMSE: 0.0685, R2: 0.9302), PNorm: 188.7894, GNorm: 0.4518
[213/299] timecost: 60.59, lr: 0.000023, Train: (LOSS: 0.0068, MAE: 0.0068, RMSE: 0.0156, R2: 0.9914), Valid: (LOSS: 0.0415, MAE: 0.0415, RMSE: 0.0676, R2: 0.9320), PNorm: 188.7911, GNorm: 0.3718
[214/299] timecost: 59.83, lr: 0.000023, Train: (LOSS: 0.0069, MAE: 0.0069, RMSE: 0.0156, R2: 0.9927), Valid: (LOSS: 0.0421, MAE: 0.0421, RMSE: 0.0684, R2: 0.9302), PNorm: 188.7917, GNorm: 0.5000
[215/299] timecost: 59.88, lr: 0.000023, Train: (LOSS: 0.0068, MAE: 0.0068, RMSE: 0.0158, R2: 0.9925), Valid: (LOSS: 0.0422, MAE: 0.0422, RMSE: 0.0687, R2: 0.9294), PNorm: 188.7928, GNorm: 0.4212
[216/299] timecost: 60.00, lr: 0.000023, Train: (LOSS: 0.0066, MAE: 0.0066, RMSE: 0.0154, R2: 0.9922), Valid: (LOSS: 0.0420, MAE: 0.0420, RMSE: 0.0681, R2: 0.9311), PNorm: 188.7940, GNorm: 0.4782
[217/299] timecost: 60.27, lr: 0.000023, Train: (LOSS: 0.0068, MAE: 0.0068, RMSE: 0.0154, R2: 0.9930), Valid: (LOSS: 0.0424, MAE: 0.0424, RMSE: 0.0692, R2: 0.9286), PNorm: 188.7955, GNorm: 0.5000
[218/299] timecost: 60.31, lr: 0.000023, Train: (LOSS: 0.0068, MAE: 0.0068, RMSE: 0.0155, R2: 0.9915), Valid: (LOSS: 0.0418, MAE: 0.0418, RMSE: 0.0684, R2: 0.9304), PNorm: 188.7964, GNorm: 0.5000
[219/299] timecost: 60.34, lr: 0.000023, Train: (LOSS: 0.0066, MAE: 0.0066, RMSE: 0.0148, R2: 0.9920), Valid: (LOSS: 0.0415, MAE: 0.0415, RMSE: 0.0680, R2: 0.9308), PNorm: 188.7970, GNorm: 0.5000
[220/299] timecost: 60.65, lr: 0.000023, Train: (LOSS: 0.0065, MAE: 0.0065, RMSE: 0.0150, R2: 0.9925), Valid: (LOSS: 0.0416, MAE: 0.0416, RMSE: 0.0678, R2: 0.9318), PNorm: 188.7981, GNorm: 0.4594
[221/299] timecost: 60.35, lr: 0.000023, Train: (LOSS: 0.0067, MAE: 0.0067, RMSE: 0.0155, R2: 0.9923), Valid: (LOSS: 0.0419, MAE: 0.0419, RMSE: 0.0677, R2: 0.9319), PNorm: 188.7996, GNorm: 0.4093
[222/299] timecost: 60.70, lr: 0.000023, Train: (LOSS: 0.0066, MAE: 0.0066, RMSE: 0.0155, R2: 0.9929), Valid: (LOSS: 0.0420, MAE: 0.0420, RMSE: 0.0685, R2: 0.9302), PNorm: 188.8005, GNorm: 0.4425
[223/299] timecost: 59.58, lr: 0.000023, Train: (LOSS: 0.0070, MAE: 0.0070, RMSE: 0.0156, R2: 0.9921), Valid: (LOSS: 0.0415, MAE: 0.0415, RMSE: 0.0671, R2: 0.9328), PNorm: 188.8017, GNorm: 0.5000
Epoch 00225: reducing learning rate of group 0 to 1.9687e-05.
[224/299] timecost: 59.30, lr: 0.000020, Train: (LOSS: 0.0067, MAE: 0.0067, RMSE: 0.0149, R2: 0.9933), Valid: (LOSS: 0.0423, MAE: 0.0423, RMSE: 0.0687, R2: 0.9301), PNorm: 188.8025, GNorm: 0.4165
[225/299] timecost: 58.60, lr: 0.000020, Train: (LOSS: 0.0064, MAE: 0.0064, RMSE: 0.0153, R2: 0.9932), Valid: (LOSS: 0.0417, MAE: 0.0417, RMSE: 0.0678, R2: 0.9316), PNorm: 188.8042, GNorm: 0.5000
[226/299] timecost: 61.50, lr: 0.000020, Train: (LOSS: 0.0062, MAE: 0.0062, RMSE: 0.0147, R2: 0.9927), Valid: (LOSS: 0.0418, MAE: 0.0418, RMSE: 0.0681, R2: 0.9310), PNorm: 188.8044, GNorm: 0.4463
[227/299] timecost: 60.84, lr: 0.000020, Train: (LOSS: 0.0064, MAE: 0.0064, RMSE: 0.0151, R2: 0.9923), Valid: (LOSS: 0.0419, MAE: 0.0419, RMSE: 0.0684, R2: 0.9305), PNorm: 188.8056, GNorm: 0.3599
[228/299] timecost: 61.04, lr: 0.000020, Train: (LOSS: 0.0061, MAE: 0.0061, RMSE: 0.0146, R2: 0.9928), Valid: (LOSS: 0.0417, MAE: 0.0417, RMSE: 0.0681, R2: 0.9311), PNorm: 188.8062, GNorm: 0.3986
[229/299] timecost: 60.98, lr: 0.000020, Train: (LOSS: 0.0060, MAE: 0.0060, RMSE: 0.0153, R2: 0.9926), Valid: (LOSS: 0.0421, MAE: 0.0421, RMSE: 0.0683, R2: 0.9304), PNorm: 188.8069, GNorm: 0.4595
[230/299] timecost: 60.70, lr: 0.000020, Train: (LOSS: 0.0061, MAE: 0.0061, RMSE: 0.0149, R2: 0.9926), Valid: (LOSS: 0.0419, MAE: 0.0419, RMSE: 0.0677, R2: 0.9323), PNorm: 188.8083, GNorm: 0.4585
[231/299] timecost: 60.13, lr: 0.000020, Train: (LOSS: 0.0062, MAE: 0.0062, RMSE: 0.0149, R2: 0.9910), Valid: (LOSS: 0.0412, MAE: 0.0412, RMSE: 0.0675, R2: 0.9321), PNorm: 188.8089, GNorm: 0.5000
[232/299] timecost: 59.80, lr: 0.000020, Train: (LOSS: 0.0061, MAE: 0.0061, RMSE: 0.0150, R2: 0.9921), Valid: (LOSS: 0.0417, MAE: 0.0417, RMSE: 0.0680, R2: 0.9312), PNorm: 188.8097, GNorm: 0.4548
[233/299] timecost: 59.51, lr: 0.000020, Train: (LOSS: 0.0060, MAE: 0.0060, RMSE: 0.0144, R2: 0.9933), Valid: (LOSS: 0.0415, MAE: 0.0415, RMSE: 0.0678, R2: 0.9312), PNorm: 188.8106, GNorm: 0.3901
[234/299] timecost: 59.30, lr: 0.000020, Train: (LOSS: 0.0061, MAE: 0.0061, RMSE: 0.0151, R2: 0.9931), Valid: (LOSS: 0.0422, MAE: 0.0422, RMSE: 0.0686, R2: 0.9301), PNorm: 188.8116, GNorm: 0.4124
[235/299] timecost: 59.45, lr: 0.000020, Train: (LOSS: 0.0060, MAE: 0.0060, RMSE: 0.0144, R2: 0.9917), Valid: (LOSS: 0.0418, MAE: 0.0418, RMSE: 0.0675, R2: 0.9324), PNorm: 188.8119, GNorm: 0.3993
[236/299] timecost: 59.66, lr: 0.000020, Train: (LOSS: 0.0060, MAE: 0.0060, RMSE: 0.0145, R2: 0.9917), Valid: (LOSS: 0.0419, MAE: 0.0419, RMSE: 0.0679, R2: 0.9316), PNorm: 188.8130, GNorm: 0.4602
[237/299] timecost: 60.02, lr: 0.000020, Train: (LOSS: 0.0059, MAE: 0.0059, RMSE: 0.0147, R2: 0.9935), Valid: (LOSS: 0.0424, MAE: 0.0424, RMSE: 0.0691, R2: 0.9288), PNorm: 188.8136, GNorm: 0.4376
[238/299] timecost: 59.02, lr: 0.000020, Train: (LOSS: 0.0060, MAE: 0.0060, RMSE: 0.0148, R2: 0.9926), Valid: (LOSS: 0.0418, MAE: 0.0418, RMSE: 0.0670, R2: 0.9330), PNorm: 188.8148, GNorm: 0.3726
[239/299] timecost: 59.76, lr: 0.000020, Train: (LOSS: 0.0059, MAE: 0.0059, RMSE: 0.0143, R2: 0.9923), Valid: (LOSS: 0.0416, MAE: 0.0416, RMSE: 0.0677, R2: 0.9315), PNorm: 188.8157, GNorm: 0.5000
Epoch 00241: reducing learning rate of group 0 to 1.6734e-05.
[240/299] timecost: 59.95, lr: 0.000017, Train: (LOSS: 0.0060, MAE: 0.0060, RMSE: 0.0142, R2: 0.9932), Valid: (LOSS: 0.0415, MAE: 0.0415, RMSE: 0.0674, R2: 0.9323), PNorm: 188.8166, GNorm: 0.4260
[241/299] timecost: 61.08, lr: 0.000017, Train: (LOSS: 0.0059, MAE: 0.0059, RMSE: 0.0146, R2: 0.9932), Valid: (LOSS: 0.0418, MAE: 0.0418, RMSE: 0.0677, R2: 0.9313), PNorm: 188.8173, GNorm: 0.4471
[242/299] timecost: 59.06, lr: 0.000017, Train: (LOSS: 0.0056, MAE: 0.0056, RMSE: 0.0145, R2: 0.9932), Valid: (LOSS: 0.0421, MAE: 0.0421, RMSE: 0.0681, R2: 0.9309), PNorm: 188.8180, GNorm: 0.5000
[243/299] timecost: 58.60, lr: 0.000017, Train: (LOSS: 0.0056, MAE: 0.0056, RMSE: 0.0137, R2: 0.9932), Valid: (LOSS: 0.0418, MAE: 0.0418, RMSE: 0.0674, R2: 0.9317), PNorm: 188.8186, GNorm: 0.4111
[244/299] timecost: 58.25, lr: 0.000017, Train: (LOSS: 0.0057, MAE: 0.0057, RMSE: 0.0143, R2: 0.9928), Valid: (LOSS: 0.0419, MAE: 0.0419, RMSE: 0.0676, R2: 0.9318), PNorm: 188.8194, GNorm: 0.5000
[245/299] timecost: 59.62, lr: 0.000017, Train: (LOSS: 0.0056, MAE: 0.0056, RMSE: 0.0144, R2: 0.9919), Valid: (LOSS: 0.0421, MAE: 0.0421, RMSE: 0.0676, R2: 0.9314), PNorm: 188.8201, GNorm: 0.5000
[246/299] timecost: 59.15, lr: 0.000017, Train: (LOSS: 0.0056, MAE: 0.0056, RMSE: 0.0141, R2: 0.9931), Valid: (LOSS: 0.0422, MAE: 0.0422, RMSE: 0.0684, R2: 0.9304), PNorm: 188.8205, GNorm: 0.5000
[247/299] timecost: 59.25, lr: 0.000017, Train: (LOSS: 0.0056, MAE: 0.0056, RMSE: 0.0142, R2: 0.9927), Valid: (LOSS: 0.0423, MAE: 0.0423, RMSE: 0.0685, R2: 0.9296), PNorm: 188.8217, GNorm: 0.5000
[248/299] timecost: 58.39, lr: 0.000017, Train: (LOSS: 0.0056, MAE: 0.0056, RMSE: 0.0145, R2: 0.9925), Valid: (LOSS: 0.0422, MAE: 0.0422, RMSE: 0.0688, R2: 0.9291), PNorm: 188.8223, GNorm: 0.3997
[249/299] timecost: 59.54, lr: 0.000017, Train: (LOSS: 0.0056, MAE: 0.0056, RMSE: 0.0143, R2: 0.9929), Valid: (LOSS: 0.0420, MAE: 0.0420, RMSE: 0.0676, R2: 0.9317), PNorm: 188.8235, GNorm: 0.4399
[250/299] timecost: 60.79, lr: 0.000017, Train: (LOSS: 0.0056, MAE: 0.0056, RMSE: 0.0141, R2: 0.9927), Valid: (LOSS: 0.0419, MAE: 0.0419, RMSE: 0.0677, R2: 0.9314), PNorm: 188.8237, GNorm: 0.4661
[251/299] timecost: 60.10, lr: 0.000017, Train: (LOSS: 0.0056, MAE: 0.0056, RMSE: 0.0144, R2: 0.9931), Valid: (LOSS: 0.0419, MAE: 0.0419, RMSE: 0.0673, R2: 0.9324), PNorm: 188.8242, GNorm: 0.4580
[252/299] timecost: 60.19, lr: 0.000017, Train: (LOSS: 0.0056, MAE: 0.0056, RMSE: 0.0138, R2: 0.9937), Valid: (LOSS: 0.0420, MAE: 0.0420, RMSE: 0.0676, R2: 0.9318), PNorm: 188.8252, GNorm: 0.4585
[253/299] timecost: 60.11, lr: 0.000017, Train: (LOSS: 0.0055, MAE: 0.0055, RMSE: 0.0139, R2: 0.9931), Valid: (LOSS: 0.0418, MAE: 0.0418, RMSE: 0.0673, R2: 0.9324), PNorm: 188.8257, GNorm: 0.5000
[254/299] timecost: 59.99, lr: 0.000017, Train: (LOSS: 0.0056, MAE: 0.0056, RMSE: 0.0141, R2: 0.9925), Valid: (LOSS: 0.0417, MAE: 0.0417, RMSE: 0.0671, R2: 0.9330), PNorm: 188.8268, GNorm: 0.4809
[255/299] timecost: 60.09, lr: 0.000017, Train: (LOSS: 0.0057, MAE: 0.0057, RMSE: 0.0143, R2: 0.9927), Valid: (LOSS: 0.0419, MAE: 0.0419, RMSE: 0.0675, R2: 0.9320), PNorm: 188.8273, GNorm: 0.4515
Epoch 00257: reducing learning rate of group 0 to 1.4224e-05.
[256/299] timecost: 60.38, lr: 0.000014, Train: (LOSS: 0.0056, MAE: 0.0056, RMSE: 0.0143, R2: 0.9930), Valid: (LOSS: 0.0417, MAE: 0.0417, RMSE: 0.0671, R2: 0.9332), PNorm: 188.8280, GNorm: 0.5000
[257/299] timecost: 60.32, lr: 0.000014, Train: (LOSS: 0.0054, MAE: 0.0054, RMSE: 0.0140, R2: 0.9937), Valid: (LOSS: 0.0420, MAE: 0.0420, RMSE: 0.0676, R2: 0.9320), PNorm: 188.8283, GNorm: 0.4353
[258/299] timecost: 60.04, lr: 0.000014, Train: (LOSS: 0.0053, MAE: 0.0053, RMSE: 0.0136, R2: 0.9927), Valid: (LOSS: 0.0418, MAE: 0.0418, RMSE: 0.0672, R2: 0.9330), PNorm: 188.8287, GNorm: 0.4627
[259/299] timecost: 59.73, lr: 0.000014, Train: (LOSS: 0.0052, MAE: 0.0052, RMSE: 0.0134, R2: 0.9926), Valid: (LOSS: 0.0419, MAE: 0.0419, RMSE: 0.0677, R2: 0.9317), PNorm: 188.8294, GNorm: 0.4515
[260/299] timecost: 59.38, lr: 0.000014, Train: (LOSS: 0.0052, MAE: 0.0052, RMSE: 0.0139, R2: 0.9933), Valid: (LOSS: 0.0419, MAE: 0.0419, RMSE: 0.0674, R2: 0.9320), PNorm: 188.8303, GNorm: 0.5000
[261/299] timecost: 58.83, lr: 0.000014, Train: (LOSS: 0.0052, MAE: 0.0052, RMSE: 0.0138, R2: 0.9940), Valid: (LOSS: 0.0421, MAE: 0.0421, RMSE: 0.0679, R2: 0.9319), PNorm: 188.8308, GNorm: 0.4517
[262/299] timecost: 58.75, lr: 0.000014, Train: (LOSS: 0.0051, MAE: 0.0051, RMSE: 0.0130, R2: 0.9926), Valid: (LOSS: 0.0421, MAE: 0.0421, RMSE: 0.0673, R2: 0.9323), PNorm: 188.8315, GNorm: 0.5000
[263/299] timecost: 60.18, lr: 0.000014, Train: (LOSS: 0.0052, MAE: 0.0052, RMSE: 0.0136, R2: 0.9936), Valid: (LOSS: 0.0422, MAE: 0.0422, RMSE: 0.0676, R2: 0.9321), PNorm: 188.8319, GNorm: 0.3962
[264/299] timecost: 60.78, lr: 0.000014, Train: (LOSS: 0.0051, MAE: 0.0051, RMSE: 0.0137, R2: 0.9929), Valid: (LOSS: 0.0420, MAE: 0.0420, RMSE: 0.0677, R2: 0.9321), PNorm: 188.8328, GNorm: 0.4764
[265/299] timecost: 60.71, lr: 0.000014, Train: (LOSS: 0.0051, MAE: 0.0051, RMSE: 0.0140, R2: 0.9934), Valid: (LOSS: 0.0424, MAE: 0.0424, RMSE: 0.0684, R2: 0.9302), PNorm: 188.8333, GNorm: 0.5000
[266/299] timecost: 60.45, lr: 0.000014, Train: (LOSS: 0.0051, MAE: 0.0051, RMSE: 0.0133, R2: 0.9928), Valid: (LOSS: 0.0421, MAE: 0.0421, RMSE: 0.0678, R2: 0.9316), PNorm: 188.8340, GNorm: 0.5000
[267/299] timecost: 60.71, lr: 0.000014, Train: (LOSS: 0.0051, MAE: 0.0051, RMSE: 0.0138, R2: 0.9935), Valid: (LOSS: 0.0418, MAE: 0.0418, RMSE: 0.0672, R2: 0.9325), PNorm: 188.8348, GNorm: 0.5000
[268/299] timecost: 62.13, lr: 0.000014, Train: (LOSS: 0.0052, MAE: 0.0052, RMSE: 0.0135, R2: 0.9935), Valid: (LOSS: 0.0416, MAE: 0.0416, RMSE: 0.0673, R2: 0.9324), PNorm: 188.8350, GNorm: 0.4832
[269/299] timecost: 62.41, lr: 0.000014, Train: (LOSS: 0.0052, MAE: 0.0052, RMSE: 0.0133, R2: 0.9934), Valid: (LOSS: 0.0416, MAE: 0.0416, RMSE: 0.0669, R2: 0.9333), PNorm: 188.8357, GNorm: 0.5000
[270/299] timecost: 62.36, lr: 0.000014, Train: (LOSS: 0.0051, MAE: 0.0051, RMSE: 0.0134, R2: 0.9934), Valid: (LOSS: 0.0418, MAE: 0.0418, RMSE: 0.0669, R2: 0.9332), PNorm: 188.8364, GNorm: 0.4585
[271/299] timecost: 62.52, lr: 0.000014, Train: (LOSS: 0.0051, MAE: 0.0051, RMSE: 0.0138, R2: 0.9936), Valid: (LOSS: 0.0419, MAE: 0.0419, RMSE: 0.0673, R2: 0.9328), PNorm: 188.8366, GNorm: 0.4463
Epoch 00273: reducing learning rate of group 0 to 1.2091e-05.
[272/299] timecost: 62.48, lr: 0.000012, Train: (LOSS: 0.0050, MAE: 0.0050, RMSE: 0.0133, R2: 0.9919), Valid: (LOSS: 0.0417, MAE: 0.0417, RMSE: 0.0669, R2: 0.9333), PNorm: 188.8375, GNorm: 0.4607
[273/299] timecost: 61.46, lr: 0.000012, Train: (LOSS: 0.0049, MAE: 0.0049, RMSE: 0.0135, R2: 0.9920), Valid: (LOSS: 0.0419, MAE: 0.0419, RMSE: 0.0675, R2: 0.9320), PNorm: 188.8377, GNorm: 0.5000
[274/299] timecost: 60.83, lr: 0.000012, Train: (LOSS: 0.0049, MAE: 0.0049, RMSE: 0.0132, R2: 0.9931), Valid: (LOSS: 0.0421, MAE: 0.0421, RMSE: 0.0675, R2: 0.9322), PNorm: 188.8384, GNorm: 0.4256
[275/299] timecost: 60.22, lr: 0.000012, Train: (LOSS: 0.0048, MAE: 0.0048, RMSE: 0.0123, R2: 0.9939), Valid: (LOSS: 0.0420, MAE: 0.0420, RMSE: 0.0678, R2: 0.9314), PNorm: 188.8388, GNorm: 0.4824
[276/299] timecost: 60.53, lr: 0.000012, Train: (LOSS: 0.0049, MAE: 0.0049, RMSE: 0.0130, R2: 0.9927), Valid: (LOSS: 0.0419, MAE: 0.0419, RMSE: 0.0673, R2: 0.9326), PNorm: 188.8392, GNorm: 0.5000
[277/299] timecost: 59.91, lr: 0.000012, Train: (LOSS: 0.0049, MAE: 0.0049, RMSE: 0.0129, R2: 0.9934), Valid: (LOSS: 0.0421, MAE: 0.0421, RMSE: 0.0677, R2: 0.9317), PNorm: 188.8397, GNorm: 0.3561
[278/299] timecost: 60.19, lr: 0.000012, Train: (LOSS: 0.0048, MAE: 0.0048, RMSE: 0.0129, R2: 0.9931), Valid: (LOSS: 0.0420, MAE: 0.0420, RMSE: 0.0675, R2: 0.9321), PNorm: 188.8398, GNorm: 0.4201
[279/299] timecost: 60.68, lr: 0.000012, Train: (LOSS: 0.0049, MAE: 0.0049, RMSE: 0.0133, R2: 0.9934), Valid: (LOSS: 0.0425, MAE: 0.0425, RMSE: 0.0680, R2: 0.9312), PNorm: 188.8405, GNorm: 0.5000
[280/299] timecost: 59.68, lr: 0.000012, Train: (LOSS: 0.0047, MAE: 0.0047, RMSE: 0.0126, R2: 0.9922), Valid: (LOSS: 0.0422, MAE: 0.0422, RMSE: 0.0673, R2: 0.9324), PNorm: 188.8411, GNorm: 0.4828
[281/299] timecost: 60.26, lr: 0.000012, Train: (LOSS: 0.0048, MAE: 0.0048, RMSE: 0.0133, R2: 0.9934), Valid: (LOSS: 0.0418, MAE: 0.0418, RMSE: 0.0671, R2: 0.9327), PNorm: 188.8415, GNorm: 0.4259
[282/299] timecost: 61.61, lr: 0.000012, Train: (LOSS: 0.0047, MAE: 0.0047, RMSE: 0.0128, R2: 0.9935), Valid: (LOSS: 0.0423, MAE: 0.0423, RMSE: 0.0674, R2: 0.9323), PNorm: 188.8420, GNorm: 0.5000
[283/299] timecost: 61.14, lr: 0.000012, Train: (LOSS: 0.0047, MAE: 0.0047, RMSE: 0.0130, R2: 0.9930), Valid: (LOSS: 0.0421, MAE: 0.0421, RMSE: 0.0674, R2: 0.9321), PNorm: 188.8424, GNorm: 0.4439
[284/299] timecost: 61.09, lr: 0.000012, Train: (LOSS: 0.0046, MAE: 0.0046, RMSE: 0.0130, R2: 0.9941), Valid: (LOSS: 0.0423, MAE: 0.0423, RMSE: 0.0677, R2: 0.9316), PNorm: 188.8432, GNorm: 0.5000
[285/299] timecost: 61.02, lr: 0.000012, Train: (LOSS: 0.0047, MAE: 0.0047, RMSE: 0.0129, R2: 0.9935), Valid: (LOSS: 0.0421, MAE: 0.0421, RMSE: 0.0674, R2: 0.9322), PNorm: 188.8431, GNorm: 0.5000
[286/299] timecost: 60.66, lr: 0.000012, Train: (LOSS: 0.0048, MAE: 0.0048, RMSE: 0.0126, R2: 0.9937), Valid: (LOSS: 0.0422, MAE: 0.0422, RMSE: 0.0675, R2: 0.9322), PNorm: 188.8436, GNorm: 0.5000
[287/299] timecost: 60.88, lr: 0.000012, Train: (LOSS: 0.0047, MAE: 0.0047, RMSE: 0.0127, R2: 0.9940), Valid: (LOSS: 0.0421, MAE: 0.0421, RMSE: 0.0669, R2: 0.9328), PNorm: 188.8442, GNorm: 0.4762
Epoch 00289: reducing learning rate of group 0 to 1.0277e-05.
[288/299] timecost: 60.22, lr: 0.000010, Train: (LOSS: 0.0047, MAE: 0.0047, RMSE: 0.0128, R2: 0.9935), Valid: (LOSS: 0.0420, MAE: 0.0420, RMSE: 0.0672, R2: 0.9324), PNorm: 188.8445, GNorm: 0.4883
[289/299] timecost: 60.55, lr: 0.000010, Train: (LOSS: 0.0046, MAE: 0.0046, RMSE: 0.0126, R2: 0.9933), Valid: (LOSS: 0.0419, MAE: 0.0419, RMSE: 0.0671, R2: 0.9331), PNorm: 188.8450, GNorm: 0.5000
[290/299] timecost: 60.25, lr: 0.000010, Train: (LOSS: 0.0045, MAE: 0.0045, RMSE: 0.0127, R2: 0.9936), Valid: (LOSS: 0.0418, MAE: 0.0418, RMSE: 0.0668, R2: 0.9333), PNorm: 188.8456, GNorm: 0.4407
[291/299] timecost: 60.32, lr: 0.000010, Train: (LOSS: 0.0044, MAE: 0.0044, RMSE: 0.0129, R2: 0.9937), Valid: (LOSS: 0.0420, MAE: 0.0420, RMSE: 0.0671, R2: 0.9328), PNorm: 188.8458, GNorm: 0.3642
[292/299] timecost: 59.38, lr: 0.000010, Train: (LOSS: 0.0046, MAE: 0.0046, RMSE: 0.0128, R2: 0.9936), Valid: (LOSS: 0.0421, MAE: 0.0421, RMSE: 0.0671, R2: 0.9327), PNorm: 188.8462, GNorm: 0.5000
[293/299] timecost: 59.47, lr: 0.000010, Train: (LOSS: 0.0045, MAE: 0.0045, RMSE: 0.0123, R2: 0.9940), Valid: (LOSS: 0.0418, MAE: 0.0418, RMSE: 0.0669, R2: 0.9329), PNorm: 188.8465, GNorm: 0.4322
[294/299] timecost: 59.61, lr: 0.000010, Train: (LOSS: 0.0044, MAE: 0.0044, RMSE: 0.0125, R2: 0.9937), Valid: (LOSS: 0.0418, MAE: 0.0418, RMSE: 0.0670, R2: 0.9328), PNorm: 188.8468, GNorm: 0.4090
[295/299] timecost: 59.20, lr: 0.000010, Train: (LOSS: 0.0044, MAE: 0.0044, RMSE: 0.0127, R2: 0.9935), Valid: (LOSS: 0.0420, MAE: 0.0420, RMSE: 0.0668, R2: 0.9333), PNorm: 188.8471, GNorm: 0.4020
[296/299] timecost: 59.81, lr: 0.000010, Train: (LOSS: 0.0045, MAE: 0.0045, RMSE: 0.0127, R2: 0.9942), Valid: (LOSS: 0.0420, MAE: 0.0420, RMSE: 0.0672, R2: 0.9325), PNorm: 188.8475, GNorm: 0.4217
[297/299] timecost: 61.80, lr: 0.000010, Train: (LOSS: 0.0044, MAE: 0.0044, RMSE: 0.0128, R2: 0.9935), Valid: (LOSS: 0.0419, MAE: 0.0419, RMSE: 0.0668, R2: 0.9330), PNorm: 188.8479, GNorm: 0.4815
[298/299] timecost: 62.27, lr: 0.000010, Train: (LOSS: 0.0045, MAE: 0.0045, RMSE: 0.0122, R2: 0.9936), Valid: (LOSS: 0.0418, MAE: 0.0418, RMSE: 0.0666, R2: 0.9337), PNorm: 188.8483, GNorm: 0.5000
[299/299] timecost: 62.33, lr: 0.000010, Train: (LOSS: 0.0044, MAE: 0.0044, RMSE: 0.0123, R2: 0.9931), Valid: (LOSS: 0.0419, MAE: 0.0419, RMSE: 0.0669, R2: 0.9330), PNorm: 188.8487, GNorm: 0.4313
==========Training End==========
==========Test Best Model==========
================Final Results=======================
mse: 0.0364 +- 0.0000:
rmse: 0.0612 +- 0.0000:
mae: 0.0364 +- 0.0000:
r2: 0.9383 +- 0.0000:
tensor([[0.0000, 0.0000],
        [0.0000, 0.0000],
        [0.1346, 0.1828],
        ...,
        [0.0000, 0.0000],
        [0.0000, 0.0000],
        [0.0000, 0.0000]], device='cuda:0')
