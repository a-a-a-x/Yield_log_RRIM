cuda available with GPU: Tesla V100-PCIE-16GB
==========Load Seed==========
set_random_seed
0
==========Training Start==========
Training Graphs:  2491
Valid Graphs:  277
Test Graphs:  1187
============Not pretrained weights used============
============Creating new layers============
============Creating Model============
Training Graphs Batches:  78
Valid Graphs Batches:  9
Test Graphs Batches:  37
[0/299] timecost: 66.12, lr: 0.000100, Train: (LOSS: 0.2072, MAE: 0.2072, RMSE: 0.2570, R2: 0.0368), Valid: (LOSS: 0.1755, MAE: 0.1755, RMSE: 0.2236, R2: 0.2711), PNorm: 187.0409, GNorm: 0.5000
[1/299] timecost: 61.64, lr: 0.000100, Train: (LOSS: 0.1691, MAE: 0.1691, RMSE: 0.2189, R2: 0.2883), Valid: (LOSS: 0.1647, MAE: 0.1647, RMSE: 0.2032, R2: 0.3944), PNorm: 187.0757, GNorm: 0.5000
[2/299] timecost: 62.38, lr: 0.000100, Train: (LOSS: 0.1556, MAE: 0.1556, RMSE: 0.2063, R2: 0.3777), Valid: (LOSS: 0.1817, MAE: 0.1817, RMSE: 0.2364, R2: 0.1682), PNorm: 187.1098, GNorm: 0.5000
[3/299] timecost: 62.44, lr: 0.000100, Train: (LOSS: 0.1472, MAE: 0.1472, RMSE: 0.1956, R2: 0.4454), Valid: (LOSS: 0.1437, MAE: 0.1437, RMSE: 0.1882, R2: 0.4758), PNorm: 187.1559, GNorm: 0.5000
[4/299] timecost: 61.99, lr: 0.000100, Train: (LOSS: 0.1381, MAE: 0.1381, RMSE: 0.1887, R2: 0.4814), Valid: (LOSS: 0.1444, MAE: 0.1444, RMSE: 0.1837, R2: 0.4983), PNorm: 187.2030, GNorm: 0.5000
[5/299] timecost: 62.16, lr: 0.000100, Train: (LOSS: 0.1303, MAE: 0.1303, RMSE: 0.1797, R2: 0.5230), Valid: (LOSS: 0.1401, MAE: 0.1401, RMSE: 0.1791, R2: 0.5233), PNorm: 187.2330, GNorm: 0.5000
[6/299] timecost: 62.36, lr: 0.000100, Train: (LOSS: 0.1267, MAE: 0.1267, RMSE: 0.1768, R2: 0.5267), Valid: (LOSS: 0.1462, MAE: 0.1462, RMSE: 0.2033, R2: 0.3893), PNorm: 187.2551, GNorm: 0.5000
[7/299] timecost: 62.61, lr: 0.000100, Train: (LOSS: 0.1213, MAE: 0.1213, RMSE: 0.1694, R2: 0.5798), Valid: (LOSS: 0.1324, MAE: 0.1324, RMSE: 0.1792, R2: 0.5246), PNorm: 187.2785, GNorm: 0.5000
[8/299] timecost: 62.68, lr: 0.000100, Train: (LOSS: 0.1177, MAE: 0.1177, RMSE: 0.1675, R2: 0.5838), Valid: (LOSS: 0.1277, MAE: 0.1277, RMSE: 0.1764, R2: 0.5454), PNorm: 187.3058, GNorm: 0.5000
[9/299] timecost: 64.42, lr: 0.000100, Train: (LOSS: 0.1235, MAE: 0.1235, RMSE: 0.1723, R2: 0.5586), Valid: (LOSS: 0.1241, MAE: 0.1241, RMSE: 0.1695, R2: 0.5765), PNorm: 187.3349, GNorm: 0.5000
[10/299] timecost: 64.54, lr: 0.000100, Train: (LOSS: 0.1182, MAE: 0.1182, RMSE: 0.1667, R2: 0.5888), Valid: (LOSS: 0.1322, MAE: 0.1322, RMSE: 0.1854, R2: 0.4973), PNorm: 187.3615, GNorm: 0.4894
[11/299] timecost: 63.66, lr: 0.000100, Train: (LOSS: 0.1157, MAE: 0.1157, RMSE: 0.1660, R2: 0.5998), Valid: (LOSS: 0.1281, MAE: 0.1281, RMSE: 0.1782, R2: 0.5309), PNorm: 187.3957, GNorm: 0.5000
[12/299] timecost: 62.60, lr: 0.000100, Train: (LOSS: 0.1032, MAE: 0.1032, RMSE: 0.1508, R2: 0.6550), Valid: (LOSS: 0.1077, MAE: 0.1077, RMSE: 0.1557, R2: 0.6435), PNorm: 187.4284, GNorm: 0.5000
[13/299] timecost: 63.69, lr: 0.000100, Train: (LOSS: 0.1020, MAE: 0.1020, RMSE: 0.1486, R2: 0.6716), Valid: (LOSS: 0.1148, MAE: 0.1148, RMSE: 0.1637, R2: 0.6079), PNorm: 187.4617, GNorm: 0.5000
[14/299] timecost: 64.21, lr: 0.000100, Train: (LOSS: 0.0867, MAE: 0.0867, RMSE: 0.1268, R2: 0.7636), Valid: (LOSS: 0.1057, MAE: 0.1057, RMSE: 0.1571, R2: 0.6353), PNorm: 187.4881, GNorm: 0.5000
[15/299] timecost: 64.86, lr: 0.000100, Train: (LOSS: 0.0850, MAE: 0.0850, RMSE: 0.1254, R2: 0.7635), Valid: (LOSS: 0.0953, MAE: 0.0953, RMSE: 0.1394, R2: 0.7126), PNorm: 187.5070, GNorm: 0.4931
[16/299] timecost: 64.96, lr: 0.000100, Train: (LOSS: 0.0800, MAE: 0.0800, RMSE: 0.1183, R2: 0.7890), Valid: (LOSS: 0.0988, MAE: 0.0988, RMSE: 0.1435, R2: 0.6821), PNorm: 187.5311, GNorm: 0.5000
[17/299] timecost: 65.53, lr: 0.000100, Train: (LOSS: 0.0778, MAE: 0.0778, RMSE: 0.1154, R2: 0.7956), Valid: (LOSS: 0.0877, MAE: 0.0877, RMSE: 0.1325, R2: 0.7330), PNorm: 187.5580, GNorm: 0.5000
[18/299] timecost: 64.81, lr: 0.000100, Train: (LOSS: 0.0696, MAE: 0.0696, RMSE: 0.1041, R2: 0.8330), Valid: (LOSS: 0.0743, MAE: 0.0743, RMSE: 0.1138, R2: 0.7996), PNorm: 187.5810, GNorm: 0.5000
[19/299] timecost: 64.67, lr: 0.000100, Train: (LOSS: 0.0645, MAE: 0.0645, RMSE: 0.0972, R2: 0.8550), Valid: (LOSS: 0.0835, MAE: 0.0835, RMSE: 0.1269, R2: 0.7556), PNorm: 187.6053, GNorm: 0.5000
[20/299] timecost: 64.58, lr: 0.000100, Train: (LOSS: 0.0637, MAE: 0.0637, RMSE: 0.0964, R2: 0.8555), Valid: (LOSS: 0.0757, MAE: 0.0757, RMSE: 0.1108, R2: 0.8101), PNorm: 187.6273, GNorm: 0.5000
[21/299] timecost: 61.68, lr: 0.000100, Train: (LOSS: 0.0595, MAE: 0.0595, RMSE: 0.0916, R2: 0.8677), Valid: (LOSS: 0.0738, MAE: 0.0738, RMSE: 0.1194, R2: 0.7770), PNorm: 187.6468, GNorm: 0.5000
[22/299] timecost: 64.98, lr: 0.000100, Train: (LOSS: 0.0576, MAE: 0.0576, RMSE: 0.0880, R2: 0.8801), Valid: (LOSS: 0.0681, MAE: 0.0681, RMSE: 0.1058, R2: 0.8245), PNorm: 187.6694, GNorm: 0.5000
[23/299] timecost: 65.24, lr: 0.000100, Train: (LOSS: 0.0545, MAE: 0.0545, RMSE: 0.0829, R2: 0.8958), Valid: (LOSS: 0.0709, MAE: 0.0709, RMSE: 0.1122, R2: 0.8009), PNorm: 187.6877, GNorm: 0.5000
[24/299] timecost: 65.06, lr: 0.000100, Train: (LOSS: 0.0538, MAE: 0.0538, RMSE: 0.0835, R2: 0.8855), Valid: (LOSS: 0.0654, MAE: 0.0654, RMSE: 0.1005, R2: 0.8391), PNorm: 187.7088, GNorm: 0.5000
[25/299] timecost: 64.90, lr: 0.000100, Train: (LOSS: 0.0499, MAE: 0.0499, RMSE: 0.0772, R2: 0.9043), Valid: (LOSS: 0.0617, MAE: 0.0617, RMSE: 0.0937, R2: 0.8618), PNorm: 187.7295, GNorm: 0.5000
[26/299] timecost: 64.57, lr: 0.000100, Train: (LOSS: 0.0495, MAE: 0.0495, RMSE: 0.0784, R2: 0.9022), Valid: (LOSS: 0.0643, MAE: 0.0643, RMSE: 0.0986, R2: 0.8464), PNorm: 187.7449, GNorm: 0.4349
[27/299] timecost: 61.08, lr: 0.000100, Train: (LOSS: 0.0471, MAE: 0.0471, RMSE: 0.0727, R2: 0.9152), Valid: (LOSS: 0.0622, MAE: 0.0622, RMSE: 0.0996, R2: 0.8461), PNorm: 187.7625, GNorm: 0.5000
[28/299] timecost: 60.21, lr: 0.000100, Train: (LOSS: 0.0482, MAE: 0.0482, RMSE: 0.0743, R2: 0.9131), Valid: (LOSS: 0.0594, MAE: 0.0594, RMSE: 0.0970, R2: 0.8497), PNorm: 187.7795, GNorm: 0.5000
[29/299] timecost: 63.69, lr: 0.000100, Train: (LOSS: 0.0437, MAE: 0.0437, RMSE: 0.0689, R2: 0.9232), Valid: (LOSS: 0.0540, MAE: 0.0540, RMSE: 0.0933, R2: 0.8577), PNorm: 187.7925, GNorm: 0.5000
[30/299] timecost: 64.51, lr: 0.000100, Train: (LOSS: 0.0438, MAE: 0.0438, RMSE: 0.0691, R2: 0.9229), Valid: (LOSS: 0.0568, MAE: 0.0568, RMSE: 0.0891, R2: 0.8745), PNorm: 187.8122, GNorm: 0.4961
[31/299] timecost: 63.68, lr: 0.000100, Train: (LOSS: 0.0430, MAE: 0.0430, RMSE: 0.0662, R2: 0.9297), Valid: (LOSS: 0.0557, MAE: 0.0557, RMSE: 0.0850, R2: 0.8897), PNorm: 187.8340, GNorm: 0.5000
[32/299] timecost: 63.33, lr: 0.000100, Train: (LOSS: 0.0418, MAE: 0.0418, RMSE: 0.0656, R2: 0.9298), Valid: (LOSS: 0.0532, MAE: 0.0532, RMSE: 0.0868, R2: 0.8785), PNorm: 187.8486, GNorm: 0.5000
[33/299] timecost: 63.48, lr: 0.000100, Train: (LOSS: 0.0412, MAE: 0.0412, RMSE: 0.0643, R2: 0.9324), Valid: (LOSS: 0.0556, MAE: 0.0556, RMSE: 0.0878, R2: 0.8756), PNorm: 187.8646, GNorm: 0.4829
[34/299] timecost: 63.58, lr: 0.000100, Train: (LOSS: 0.0394, MAE: 0.0394, RMSE: 0.0626, R2: 0.9355), Valid: (LOSS: 0.0512, MAE: 0.0512, RMSE: 0.0805, R2: 0.9009), PNorm: 187.8777, GNorm: 0.5000
[35/299] timecost: 63.34, lr: 0.000100, Train: (LOSS: 0.0397, MAE: 0.0397, RMSE: 0.0624, R2: 0.9381), Valid: (LOSS: 0.0577, MAE: 0.0577, RMSE: 0.0930, R2: 0.8594), PNorm: 187.8920, GNorm: 0.4103
[36/299] timecost: 63.33, lr: 0.000100, Train: (LOSS: 0.0409, MAE: 0.0409, RMSE: 0.0661, R2: 0.9292), Valid: (LOSS: 0.0519, MAE: 0.0519, RMSE: 0.0857, R2: 0.8784), PNorm: 187.9111, GNorm: 0.4734
[37/299] timecost: 64.34, lr: 0.000100, Train: (LOSS: 0.0371, MAE: 0.0371, RMSE: 0.0594, R2: 0.9421), Valid: (LOSS: 0.0493, MAE: 0.0493, RMSE: 0.0820, R2: 0.8937), PNorm: 187.9241, GNorm: 0.5000
[38/299] timecost: 65.32, lr: 0.000100, Train: (LOSS: 0.0388, MAE: 0.0388, RMSE: 0.0623, R2: 0.9380), Valid: (LOSS: 0.0532, MAE: 0.0532, RMSE: 0.0868, R2: 0.8787), PNorm: 187.9413, GNorm: 0.5000
[39/299] timecost: 65.09, lr: 0.000100, Train: (LOSS: 0.0372, MAE: 0.0372, RMSE: 0.0594, R2: 0.9411), Valid: (LOSS: 0.0539, MAE: 0.0539, RMSE: 0.0830, R2: 0.8931), PNorm: 187.9541, GNorm: 0.5000
[40/299] timecost: 65.35, lr: 0.000100, Train: (LOSS: 0.0366, MAE: 0.0366, RMSE: 0.0580, R2: 0.9435), Valid: (LOSS: 0.0517, MAE: 0.0517, RMSE: 0.0788, R2: 0.9034), PNorm: 187.9716, GNorm: 0.5000
[41/299] timecost: 61.51, lr: 0.000100, Train: (LOSS: 0.0351, MAE: 0.0351, RMSE: 0.0562, R2: 0.9461), Valid: (LOSS: 0.0451, MAE: 0.0451, RMSE: 0.0697, R2: 0.9251), PNorm: 187.9871, GNorm: 0.5000
[42/299] timecost: 60.43, lr: 0.000100, Train: (LOSS: 0.0357, MAE: 0.0357, RMSE: 0.0563, R2: 0.9496), Valid: (LOSS: 0.0535, MAE: 0.0535, RMSE: 0.0841, R2: 0.8901), PNorm: 187.9992, GNorm: 0.5000
[43/299] timecost: 60.63, lr: 0.000100, Train: (LOSS: 0.0344, MAE: 0.0344, RMSE: 0.0546, R2: 0.9493), Valid: (LOSS: 0.0490, MAE: 0.0490, RMSE: 0.0849, R2: 0.8787), PNorm: 188.0164, GNorm: 0.5000
[44/299] timecost: 63.40, lr: 0.000100, Train: (LOSS: 0.0322, MAE: 0.0322, RMSE: 0.0520, R2: 0.9515), Valid: (LOSS: 0.0522, MAE: 0.0522, RMSE: 0.0838, R2: 0.8886), PNorm: 188.0310, GNorm: 0.4066
[45/299] timecost: 62.12, lr: 0.000100, Train: (LOSS: 0.0333, MAE: 0.0333, RMSE: 0.0525, R2: 0.9531), Valid: (LOSS: 0.0525, MAE: 0.0525, RMSE: 0.0814, R2: 0.8980), PNorm: 188.0445, GNorm: 0.4564
[46/299] timecost: 59.91, lr: 0.000100, Train: (LOSS: 0.0327, MAE: 0.0327, RMSE: 0.0528, R2: 0.9543), Valid: (LOSS: 0.0536, MAE: 0.0536, RMSE: 0.0860, R2: 0.8793), PNorm: 188.0632, GNorm: 0.5000
[47/299] timecost: 59.56, lr: 0.000100, Train: (LOSS: 0.0322, MAE: 0.0322, RMSE: 0.0517, R2: 0.9520), Valid: (LOSS: 0.0480, MAE: 0.0480, RMSE: 0.0748, R2: 0.9113), PNorm: 188.0788, GNorm: 0.5000
[48/299] timecost: 59.90, lr: 0.000100, Train: (LOSS: 0.0322, MAE: 0.0322, RMSE: 0.0517, R2: 0.9538), Valid: (LOSS: 0.0474, MAE: 0.0474, RMSE: 0.0766, R2: 0.9085), PNorm: 188.0937, GNorm: 0.5000
[49/299] timecost: 59.88, lr: 0.000100, Train: (LOSS: 0.0319, MAE: 0.0319, RMSE: 0.0511, R2: 0.9548), Valid: (LOSS: 0.0496, MAE: 0.0496, RMSE: 0.0755, R2: 0.9099), PNorm: 188.1109, GNorm: 0.5000
[50/299] timecost: 62.83, lr: 0.000100, Train: (LOSS: 0.0311, MAE: 0.0311, RMSE: 0.0502, R2: 0.9575), Valid: (LOSS: 0.0485, MAE: 0.0485, RMSE: 0.0747, R2: 0.9123), PNorm: 188.1238, GNorm: 0.4938
[51/299] timecost: 64.70, lr: 0.000100, Train: (LOSS: 0.0307, MAE: 0.0307, RMSE: 0.0489, R2: 0.9590), Valid: (LOSS: 0.0471, MAE: 0.0471, RMSE: 0.0746, R2: 0.9102), PNorm: 188.1365, GNorm: 0.5000
[52/299] timecost: 64.70, lr: 0.000100, Train: (LOSS: 0.0305, MAE: 0.0305, RMSE: 0.0486, R2: 0.9595), Valid: (LOSS: 0.0460, MAE: 0.0460, RMSE: 0.0752, R2: 0.9101), PNorm: 188.1534, GNorm: 0.5000
[53/299] timecost: 64.39, lr: 0.000100, Train: (LOSS: 0.0305, MAE: 0.0305, RMSE: 0.0487, R2: 0.9582), Valid: (LOSS: 0.0473, MAE: 0.0473, RMSE: 0.0725, R2: 0.9190), PNorm: 188.1674, GNorm: 0.4562
[54/299] timecost: 63.88, lr: 0.000100, Train: (LOSS: 0.0297, MAE: 0.0297, RMSE: 0.0479, R2: 0.9622), Valid: (LOSS: 0.0451, MAE: 0.0451, RMSE: 0.0732, R2: 0.9113), PNorm: 188.1847, GNorm: 0.5000
[55/299] timecost: 64.04, lr: 0.000100, Train: (LOSS: 0.0281, MAE: 0.0281, RMSE: 0.0459, R2: 0.9636), Valid: (LOSS: 0.0466, MAE: 0.0466, RMSE: 0.0692, R2: 0.9261), PNorm: 188.1974, GNorm: 0.5000
[56/299] timecost: 63.94, lr: 0.000100, Train: (LOSS: 0.0285, MAE: 0.0285, RMSE: 0.0460, R2: 0.9621), Valid: (LOSS: 0.0458, MAE: 0.0458, RMSE: 0.0680, R2: 0.9274), PNorm: 188.2097, GNorm: 0.5000
Epoch 00058: reducing learning rate of group 0 to 8.5000e-05.
[57/299] timecost: 63.68, lr: 0.000085, Train: (LOSS: 0.0287, MAE: 0.0287, RMSE: 0.0469, R2: 0.9599), Valid: (LOSS: 0.0468, MAE: 0.0468, RMSE: 0.0724, R2: 0.9192), PNorm: 188.2271, GNorm: 0.5000
[58/299] timecost: 63.62, lr: 0.000085, Train: (LOSS: 0.0290, MAE: 0.0290, RMSE: 0.0476, R2: 0.9609), Valid: (LOSS: 0.0484, MAE: 0.0484, RMSE: 0.0714, R2: 0.9200), PNorm: 188.2384, GNorm: 0.5000
[59/299] timecost: 64.52, lr: 0.000085, Train: (LOSS: 0.0270, MAE: 0.0270, RMSE: 0.0442, R2: 0.9644), Valid: (LOSS: 0.0425, MAE: 0.0425, RMSE: 0.0649, R2: 0.9322), PNorm: 188.2480, GNorm: 0.4597
[60/299] timecost: 59.38, lr: 0.000085, Train: (LOSS: 0.0259, MAE: 0.0259, RMSE: 0.0435, R2: 0.9654), Valid: (LOSS: 0.0453, MAE: 0.0453, RMSE: 0.0697, R2: 0.9234), PNorm: 188.2583, GNorm: 0.4851
[61/299] timecost: 61.41, lr: 0.000085, Train: (LOSS: 0.0252, MAE: 0.0252, RMSE: 0.0433, R2: 0.9678), Valid: (LOSS: 0.0439, MAE: 0.0439, RMSE: 0.0671, R2: 0.9289), PNorm: 188.2728, GNorm: 0.5000
[62/299] timecost: 62.12, lr: 0.000085, Train: (LOSS: 0.0246, MAE: 0.0246, RMSE: 0.0420, R2: 0.9681), Valid: (LOSS: 0.0417, MAE: 0.0417, RMSE: 0.0645, R2: 0.9343), PNorm: 188.2818, GNorm: 0.4177
[63/299] timecost: 61.62, lr: 0.000085, Train: (LOSS: 0.0257, MAE: 0.0257, RMSE: 0.0438, R2: 0.9656), Valid: (LOSS: 0.0452, MAE: 0.0452, RMSE: 0.0722, R2: 0.9151), PNorm: 188.2918, GNorm: 0.4601
[64/299] timecost: 61.91, lr: 0.000085, Train: (LOSS: 0.0247, MAE: 0.0247, RMSE: 0.0421, R2: 0.9682), Valid: (LOSS: 0.0439, MAE: 0.0439, RMSE: 0.0674, R2: 0.9292), PNorm: 188.3051, GNorm: 0.5000
[65/299] timecost: 61.48, lr: 0.000085, Train: (LOSS: 0.0248, MAE: 0.0248, RMSE: 0.0422, R2: 0.9687), Valid: (LOSS: 0.0468, MAE: 0.0468, RMSE: 0.0775, R2: 0.8990), PNorm: 188.3167, GNorm: 0.4647
[66/299] timecost: 62.06, lr: 0.000085, Train: (LOSS: 0.0254, MAE: 0.0254, RMSE: 0.0421, R2: 0.9678), Valid: (LOSS: 0.0419, MAE: 0.0419, RMSE: 0.0645, R2: 0.9321), PNorm: 188.3280, GNorm: 0.3121
[67/299] timecost: 62.53, lr: 0.000085, Train: (LOSS: 0.0246, MAE: 0.0246, RMSE: 0.0407, R2: 0.9701), Valid: (LOSS: 0.0458, MAE: 0.0458, RMSE: 0.0700, R2: 0.9227), PNorm: 188.3389, GNorm: 0.5000
[68/299] timecost: 62.08, lr: 0.000085, Train: (LOSS: 0.0245, MAE: 0.0245, RMSE: 0.0421, R2: 0.9693), Valid: (LOSS: 0.0422, MAE: 0.0422, RMSE: 0.0664, R2: 0.9305), PNorm: 188.3516, GNorm: 0.3666
[69/299] timecost: 62.40, lr: 0.000085, Train: (LOSS: 0.0235, MAE: 0.0235, RMSE: 0.0401, R2: 0.9683), Valid: (LOSS: 0.0444, MAE: 0.0444, RMSE: 0.0678, R2: 0.9273), PNorm: 188.3628, GNorm: 0.5000
[70/299] timecost: 62.50, lr: 0.000085, Train: (LOSS: 0.0229, MAE: 0.0229, RMSE: 0.0395, R2: 0.9723), Valid: (LOSS: 0.0415, MAE: 0.0415, RMSE: 0.0642, R2: 0.9339), PNorm: 188.3729, GNorm: 0.5000
[71/299] timecost: 61.91, lr: 0.000085, Train: (LOSS: 0.0230, MAE: 0.0230, RMSE: 0.0386, R2: 0.9715), Valid: (LOSS: 0.0413, MAE: 0.0413, RMSE: 0.0667, R2: 0.9309), PNorm: 188.3857, GNorm: 0.3977
[72/299] timecost: 62.25, lr: 0.000085, Train: (LOSS: 0.0227, MAE: 0.0227, RMSE: 0.0390, R2: 0.9686), Valid: (LOSS: 0.0427, MAE: 0.0427, RMSE: 0.0688, R2: 0.9244), PNorm: 188.3955, GNorm: 0.4067
[73/299] timecost: 63.72, lr: 0.000085, Train: (LOSS: 0.0234, MAE: 0.0234, RMSE: 0.0401, R2: 0.9701), Valid: (LOSS: 0.0433, MAE: 0.0433, RMSE: 0.0685, R2: 0.9263), PNorm: 188.4085, GNorm: 0.4401
[74/299] timecost: 63.25, lr: 0.000085, Train: (LOSS: 0.0234, MAE: 0.0234, RMSE: 0.0402, R2: 0.9693), Valid: (LOSS: 0.0435, MAE: 0.0435, RMSE: 0.0675, R2: 0.9283), PNorm: 188.4198, GNorm: 0.5000
[75/299] timecost: 62.96, lr: 0.000085, Train: (LOSS: 0.0232, MAE: 0.0232, RMSE: 0.0384, R2: 0.9687), Valid: (LOSS: 0.0416, MAE: 0.0416, RMSE: 0.0640, R2: 0.9350), PNorm: 188.4315, GNorm: 0.4047
[76/299] timecost: 62.73, lr: 0.000085, Train: (LOSS: 0.0218, MAE: 0.0218, RMSE: 0.0376, R2: 0.9736), Valid: (LOSS: 0.0419, MAE: 0.0419, RMSE: 0.0633, R2: 0.9362), PNorm: 188.4384, GNorm: 0.3912
[77/299] timecost: 63.17, lr: 0.000085, Train: (LOSS: 0.0223, MAE: 0.0223, RMSE: 0.0382, R2: 0.9720), Valid: (LOSS: 0.0434, MAE: 0.0434, RMSE: 0.0694, R2: 0.9223), PNorm: 188.4503, GNorm: 0.5000
Epoch 00079: reducing learning rate of group 0 to 7.2250e-05.
[78/299] timecost: 63.03, lr: 0.000072, Train: (LOSS: 0.0225, MAE: 0.0225, RMSE: 0.0380, R2: 0.9720), Valid: (LOSS: 0.0428, MAE: 0.0428, RMSE: 0.0658, R2: 0.9319), PNorm: 188.4606, GNorm: 0.5000
[79/299] timecost: 62.85, lr: 0.000072, Train: (LOSS: 0.0214, MAE: 0.0214, RMSE: 0.0376, R2: 0.9738), Valid: (LOSS: 0.0427, MAE: 0.0427, RMSE: 0.0659, R2: 0.9323), PNorm: 188.4692, GNorm: 0.4215
[80/299] timecost: 63.05, lr: 0.000072, Train: (LOSS: 0.0208, MAE: 0.0208, RMSE: 0.0362, R2: 0.9739), Valid: (LOSS: 0.0417, MAE: 0.0417, RMSE: 0.0638, R2: 0.9360), PNorm: 188.4769, GNorm: 0.4489
[81/299] timecost: 62.75, lr: 0.000072, Train: (LOSS: 0.0203, MAE: 0.0203, RMSE: 0.0360, R2: 0.9751), Valid: (LOSS: 0.0425, MAE: 0.0425, RMSE: 0.0663, R2: 0.9312), PNorm: 188.4868, GNorm: 0.4918
[82/299] timecost: 62.59, lr: 0.000072, Train: (LOSS: 0.0197, MAE: 0.0197, RMSE: 0.0351, R2: 0.9767), Valid: (LOSS: 0.0446, MAE: 0.0446, RMSE: 0.0701, R2: 0.9244), PNorm: 188.4964, GNorm: 0.4932
[83/299] timecost: 62.86, lr: 0.000072, Train: (LOSS: 0.0197, MAE: 0.0197, RMSE: 0.0343, R2: 0.9777), Valid: (LOSS: 0.0416, MAE: 0.0416, RMSE: 0.0646, R2: 0.9353), PNorm: 188.5045, GNorm: 0.5000
[84/299] timecost: 62.81, lr: 0.000072, Train: (LOSS: 0.0193, MAE: 0.0193, RMSE: 0.0343, R2: 0.9764), Valid: (LOSS: 0.0445, MAE: 0.0445, RMSE: 0.0730, R2: 0.9132), PNorm: 188.5100, GNorm: 0.4690
[85/299] timecost: 62.01, lr: 0.000072, Train: (LOSS: 0.0196, MAE: 0.0196, RMSE: 0.0346, R2: 0.9763), Valid: (LOSS: 0.0418, MAE: 0.0418, RMSE: 0.0641, R2: 0.9352), PNorm: 188.5179, GNorm: 0.4370
[86/299] timecost: 60.94, lr: 0.000072, Train: (LOSS: 0.0196, MAE: 0.0196, RMSE: 0.0341, R2: 0.9755), Valid: (LOSS: 0.0420, MAE: 0.0420, RMSE: 0.0663, R2: 0.9321), PNorm: 188.5267, GNorm: 0.3386
[87/299] timecost: 61.57, lr: 0.000072, Train: (LOSS: 0.0191, MAE: 0.0191, RMSE: 0.0344, R2: 0.9778), Valid: (LOSS: 0.0405, MAE: 0.0405, RMSE: 0.0617, R2: 0.9399), PNorm: 188.5359, GNorm: 0.5000
[88/299] timecost: 60.49, lr: 0.000072, Train: (LOSS: 0.0188, MAE: 0.0188, RMSE: 0.0333, R2: 0.9764), Valid: (LOSS: 0.0405, MAE: 0.0405, RMSE: 0.0628, R2: 0.9388), PNorm: 188.5462, GNorm: 0.5000
[89/299] timecost: 62.58, lr: 0.000072, Train: (LOSS: 0.0192, MAE: 0.0192, RMSE: 0.0341, R2: 0.9758), Valid: (LOSS: 0.0423, MAE: 0.0423, RMSE: 0.0651, R2: 0.9340), PNorm: 188.5552, GNorm: 0.5000
[90/299] timecost: 63.41, lr: 0.000072, Train: (LOSS: 0.0190, MAE: 0.0190, RMSE: 0.0348, R2: 0.9782), Valid: (LOSS: 0.0427, MAE: 0.0427, RMSE: 0.0657, R2: 0.9344), PNorm: 188.5633, GNorm: 0.3807
[91/299] timecost: 63.82, lr: 0.000072, Train: (LOSS: 0.0186, MAE: 0.0186, RMSE: 0.0341, R2: 0.9784), Valid: (LOSS: 0.0408, MAE: 0.0408, RMSE: 0.0645, R2: 0.9350), PNorm: 188.5710, GNorm: 0.4005
[92/299] timecost: 64.20, lr: 0.000072, Train: (LOSS: 0.0192, MAE: 0.0192, RMSE: 0.0345, R2: 0.9772), Valid: (LOSS: 0.0430, MAE: 0.0430, RMSE: 0.0644, R2: 0.9360), PNorm: 188.5809, GNorm: 0.3411
[93/299] timecost: 62.78, lr: 0.000072, Train: (LOSS: 0.0180, MAE: 0.0180, RMSE: 0.0336, R2: 0.9784), Valid: (LOSS: 0.0426, MAE: 0.0426, RMSE: 0.0645, R2: 0.9354), PNorm: 188.5891, GNorm: 0.5000
[94/299] timecost: 62.93, lr: 0.000072, Train: (LOSS: 0.0188, MAE: 0.0188, RMSE: 0.0339, R2: 0.9773), Valid: (LOSS: 0.0425, MAE: 0.0425, RMSE: 0.0640, R2: 0.9365), PNorm: 188.5979, GNorm: 0.5000
[95/299] timecost: 62.89, lr: 0.000072, Train: (LOSS: 0.0186, MAE: 0.0186, RMSE: 0.0329, R2: 0.9779), Valid: (LOSS: 0.0438, MAE: 0.0438, RMSE: 0.0672, R2: 0.9303), PNorm: 188.6098, GNorm: 0.4412
[96/299] timecost: 62.74, lr: 0.000072, Train: (LOSS: 0.0183, MAE: 0.0183, RMSE: 0.0323, R2: 0.9794), Valid: (LOSS: 0.0420, MAE: 0.0420, RMSE: 0.0651, R2: 0.9353), PNorm: 188.6171, GNorm: 0.4980
[97/299] timecost: 62.54, lr: 0.000072, Train: (LOSS: 0.0185, MAE: 0.0185, RMSE: 0.0328, R2: 0.9777), Valid: (LOSS: 0.0445, MAE: 0.0445, RMSE: 0.0682, R2: 0.9291), PNorm: 188.6258, GNorm: 0.5000
[98/299] timecost: 62.59, lr: 0.000072, Train: (LOSS: 0.0180, MAE: 0.0180, RMSE: 0.0331, R2: 0.9785), Valid: (LOSS: 0.0429, MAE: 0.0429, RMSE: 0.0640, R2: 0.9377), PNorm: 188.6355, GNorm: 0.4905
[99/299] timecost: 63.02, lr: 0.000072, Train: (LOSS: 0.0181, MAE: 0.0181, RMSE: 0.0323, R2: 0.9796), Valid: (LOSS: 0.0430, MAE: 0.0430, RMSE: 0.0644, R2: 0.9373), PNorm: 188.6490, GNorm: 0.4188
[100/299] timecost: 59.92, lr: 0.000072, Train: (LOSS: 0.0177, MAE: 0.0177, RMSE: 0.0328, R2: 0.9783), Valid: (LOSS: 0.0417, MAE: 0.0417, RMSE: 0.0629, R2: 0.9400), PNorm: 188.6552, GNorm: 0.5000
[101/299] timecost: 59.62, lr: 0.000072, Train: (LOSS: 0.0177, MAE: 0.0177, RMSE: 0.0310, R2: 0.9781), Valid: (LOSS: 0.0421, MAE: 0.0421, RMSE: 0.0632, R2: 0.9395), PNorm: 188.6668, GNorm: 0.5000
[102/299] timecost: 59.96, lr: 0.000072, Train: (LOSS: 0.0181, MAE: 0.0181, RMSE: 0.0323, R2: 0.9787), Valid: (LOSS: 0.0391, MAE: 0.0391, RMSE: 0.0595, R2: 0.9451), PNorm: 188.6760, GNorm: 0.3651
[103/299] timecost: 59.73, lr: 0.000072, Train: (LOSS: 0.0175, MAE: 0.0175, RMSE: 0.0314, R2: 0.9812), Valid: (LOSS: 0.0412, MAE: 0.0412, RMSE: 0.0622, R2: 0.9418), PNorm: 188.6891, GNorm: 0.5000
[104/299] timecost: 61.89, lr: 0.000072, Train: (LOSS: 0.0173, MAE: 0.0173, RMSE: 0.0315, R2: 0.9805), Valid: (LOSS: 0.0413, MAE: 0.0413, RMSE: 0.0634, R2: 0.9390), PNorm: 188.6964, GNorm: 0.4713
[105/299] timecost: 60.24, lr: 0.000072, Train: (LOSS: 0.0174, MAE: 0.0174, RMSE: 0.0309, R2: 0.9814), Valid: (LOSS: 0.0430, MAE: 0.0430, RMSE: 0.0648, R2: 0.9374), PNorm: 188.7070, GNorm: 0.4824
[106/299] timecost: 59.80, lr: 0.000072, Train: (LOSS: 0.0165, MAE: 0.0165, RMSE: 0.0299, R2: 0.9827), Valid: (LOSS: 0.0432, MAE: 0.0432, RMSE: 0.0646, R2: 0.9384), PNorm: 188.7157, GNorm: 0.4443
[107/299] timecost: 60.28, lr: 0.000072, Train: (LOSS: 0.0180, MAE: 0.0180, RMSE: 0.0314, R2: 0.9808), Valid: (LOSS: 0.0429, MAE: 0.0429, RMSE: 0.0681, R2: 0.9288), PNorm: 188.7231, GNorm: 0.3935
[108/299] timecost: 60.24, lr: 0.000072, Train: (LOSS: 0.0166, MAE: 0.0166, RMSE: 0.0291, R2: 0.9840), Valid: (LOSS: 0.0441, MAE: 0.0441, RMSE: 0.0647, R2: 0.9383), PNorm: 188.7340, GNorm: 0.5000
[109/299] timecost: 61.90, lr: 0.000072, Train: (LOSS: 0.0169, MAE: 0.0169, RMSE: 0.0303, R2: 0.9833), Valid: (LOSS: 0.0417, MAE: 0.0417, RMSE: 0.0617, R2: 0.9431), PNorm: 188.7428, GNorm: 0.4960
[110/299] timecost: 62.04, lr: 0.000072, Train: (LOSS: 0.0164, MAE: 0.0164, RMSE: 0.0290, R2: 0.9836), Valid: (LOSS: 0.0408, MAE: 0.0408, RMSE: 0.0609, R2: 0.9439), PNorm: 188.7531, GNorm: 0.5000
[111/299] timecost: 62.02, lr: 0.000072, Train: (LOSS: 0.0160, MAE: 0.0160, RMSE: 0.0291, R2: 0.9847), Valid: (LOSS: 0.0440, MAE: 0.0440, RMSE: 0.0674, R2: 0.9293), PNorm: 188.7631, GNorm: 0.5000
[112/299] timecost: 61.67, lr: 0.000072, Train: (LOSS: 0.0157, MAE: 0.0157, RMSE: 0.0282, R2: 0.9846), Valid: (LOSS: 0.0440, MAE: 0.0440, RMSE: 0.0658, R2: 0.9368), PNorm: 188.7715, GNorm: 0.3912
[113/299] timecost: 62.33, lr: 0.000072, Train: (LOSS: 0.0164, MAE: 0.0164, RMSE: 0.0282, R2: 0.9842), Valid: (LOSS: 0.0413, MAE: 0.0413, RMSE: 0.0614, R2: 0.9433), PNorm: 188.7833, GNorm: 0.3575
[114/299] timecost: 64.21, lr: 0.000072, Train: (LOSS: 0.0160, MAE: 0.0160, RMSE: 0.0278, R2: 0.9852), Valid: (LOSS: 0.0443, MAE: 0.0443, RMSE: 0.0642, R2: 0.9399), PNorm: 188.7931, GNorm: 0.5000
[115/299] timecost: 64.23, lr: 0.000072, Train: (LOSS: 0.0161, MAE: 0.0161, RMSE: 0.0280, R2: 0.9851), Valid: (LOSS: 0.0399, MAE: 0.0399, RMSE: 0.0589, R2: 0.9480), PNorm: 188.8036, GNorm: 0.4400
[116/299] timecost: 64.05, lr: 0.000072, Train: (LOSS: 0.0165, MAE: 0.0165, RMSE: 0.0282, R2: 0.9848), Valid: (LOSS: 0.0442, MAE: 0.0442, RMSE: 0.0730, R2: 0.9071), PNorm: 188.8144, GNorm: 0.4235
[117/299] timecost: 64.17, lr: 0.000072, Train: (LOSS: 0.0167, MAE: 0.0167, RMSE: 0.0285, R2: 0.9848), Valid: (LOSS: 0.0430, MAE: 0.0430, RMSE: 0.0652, R2: 0.9362), PNorm: 188.8306, GNorm: 0.4385
Epoch 00119: reducing learning rate of group 0 to 6.1413e-05.
[118/299] timecost: 62.85, lr: 0.000061, Train: (LOSS: 0.0158, MAE: 0.0158, RMSE: 0.0273, R2: 0.9863), Valid: (LOSS: 0.0425, MAE: 0.0425, RMSE: 0.0615, R2: 0.9445), PNorm: 188.8430, GNorm: 0.4457
[119/299] timecost: 62.74, lr: 0.000061, Train: (LOSS: 0.0151, MAE: 0.0151, RMSE: 0.0261, R2: 0.9871), Valid: (LOSS: 0.0425, MAE: 0.0425, RMSE: 0.0641, R2: 0.9380), PNorm: 188.8494, GNorm: 0.5000
[120/299] timecost: 62.75, lr: 0.000061, Train: (LOSS: 0.0141, MAE: 0.0141, RMSE: 0.0249, R2: 0.9884), Valid: (LOSS: 0.0436, MAE: 0.0436, RMSE: 0.0642, R2: 0.9391), PNorm: 188.8571, GNorm: 0.5000
[121/299] timecost: 62.27, lr: 0.000061, Train: (LOSS: 0.0144, MAE: 0.0144, RMSE: 0.0256, R2: 0.9871), Valid: (LOSS: 0.0426, MAE: 0.0426, RMSE: 0.0616, R2: 0.9447), PNorm: 188.8653, GNorm: 0.4939
[122/299] timecost: 59.69, lr: 0.000061, Train: (LOSS: 0.0141, MAE: 0.0141, RMSE: 0.0252, R2: 0.9885), Valid: (LOSS: 0.0433, MAE: 0.0433, RMSE: 0.0669, R2: 0.9312), PNorm: 188.8723, GNorm: 0.5000
[123/299] timecost: 60.19, lr: 0.000061, Train: (LOSS: 0.0139, MAE: 0.0139, RMSE: 0.0242, R2: 0.9876), Valid: (LOSS: 0.0431, MAE: 0.0431, RMSE: 0.0629, R2: 0.9425), PNorm: 188.8815, GNorm: 0.3981
[124/299] timecost: 60.83, lr: 0.000061, Train: (LOSS: 0.0137, MAE: 0.0137, RMSE: 0.0238, R2: 0.9893), Valid: (LOSS: 0.0412, MAE: 0.0412, RMSE: 0.0593, R2: 0.9489), PNorm: 188.8888, GNorm: 0.5000
[125/299] timecost: 60.66, lr: 0.000061, Train: (LOSS: 0.0132, MAE: 0.0132, RMSE: 0.0237, R2: 0.9880), Valid: (LOSS: 0.0415, MAE: 0.0415, RMSE: 0.0606, R2: 0.9465), PNorm: 188.8958, GNorm: 0.4085
[126/299] timecost: 60.56, lr: 0.000061, Train: (LOSS: 0.0136, MAE: 0.0136, RMSE: 0.0235, R2: 0.9895), Valid: (LOSS: 0.0435, MAE: 0.0435, RMSE: 0.0629, R2: 0.9422), PNorm: 188.9037, GNorm: 0.5000
[127/299] timecost: 60.47, lr: 0.000061, Train: (LOSS: 0.0136, MAE: 0.0136, RMSE: 0.0236, R2: 0.9898), Valid: (LOSS: 0.0421, MAE: 0.0421, RMSE: 0.0608, R2: 0.9456), PNorm: 188.9135, GNorm: 0.3823
[128/299] timecost: 62.26, lr: 0.000061, Train: (LOSS: 0.0134, MAE: 0.0134, RMSE: 0.0227, R2: 0.9893), Valid: (LOSS: 0.0414, MAE: 0.0414, RMSE: 0.0587, R2: 0.9491), PNorm: 188.9204, GNorm: 0.5000
[129/299] timecost: 62.91, lr: 0.000061, Train: (LOSS: 0.0132, MAE: 0.0132, RMSE: 0.0224, R2: 0.9899), Valid: (LOSS: 0.0422, MAE: 0.0422, RMSE: 0.0627, R2: 0.9414), PNorm: 188.9299, GNorm: 0.4087
[130/299] timecost: 63.24, lr: 0.000061, Train: (LOSS: 0.0132, MAE: 0.0132, RMSE: 0.0217, R2: 0.9906), Valid: (LOSS: 0.0438, MAE: 0.0438, RMSE: 0.0667, R2: 0.9334), PNorm: 188.9362, GNorm: 0.5000
[131/299] timecost: 62.99, lr: 0.000061, Train: (LOSS: 0.0128, MAE: 0.0128, RMSE: 0.0208, R2: 0.9923), Valid: (LOSS: 0.0420, MAE: 0.0420, RMSE: 0.0628, R2: 0.9397), PNorm: 188.9459, GNorm: 0.5000
[132/299] timecost: 63.03, lr: 0.000061, Train: (LOSS: 0.0127, MAE: 0.0127, RMSE: 0.0218, R2: 0.9903), Valid: (LOSS: 0.0407, MAE: 0.0407, RMSE: 0.0587, R2: 0.9483), PNorm: 188.9515, GNorm: 0.4502
[133/299] timecost: 64.53, lr: 0.000061, Train: (LOSS: 0.0133, MAE: 0.0133, RMSE: 0.0219, R2: 0.9906), Valid: (LOSS: 0.0426, MAE: 0.0426, RMSE: 0.0637, R2: 0.9383), PNorm: 188.9623, GNorm: 0.4070
Epoch 00135: reducing learning rate of group 0 to 5.2201e-05.
[134/299] timecost: 64.59, lr: 0.000052, Train: (LOSS: 0.0124, MAE: 0.0124, RMSE: 0.0214, R2: 0.9905), Valid: (LOSS: 0.0417, MAE: 0.0417, RMSE: 0.0608, R2: 0.9450), PNorm: 188.9724, GNorm: 0.4201
[135/299] timecost: 64.65, lr: 0.000052, Train: (LOSS: 0.0119, MAE: 0.0119, RMSE: 0.0198, R2: 0.9926), Valid: (LOSS: 0.0412, MAE: 0.0412, RMSE: 0.0603, R2: 0.9466), PNorm: 188.9787, GNorm: 0.3581
[136/299] timecost: 64.08, lr: 0.000052, Train: (LOSS: 0.0115, MAE: 0.0115, RMSE: 0.0192, R2: 0.9927), Valid: (LOSS: 0.0406, MAE: 0.0406, RMSE: 0.0605, R2: 0.9468), PNorm: 188.9830, GNorm: 0.5000
[137/299] timecost: 61.95, lr: 0.000052, Train: (LOSS: 0.0108, MAE: 0.0108, RMSE: 0.0180, R2: 0.9935), Valid: (LOSS: 0.0406, MAE: 0.0406, RMSE: 0.0592, R2: 0.9481), PNorm: 188.9883, GNorm: 0.3703
[138/299] timecost: 62.48, lr: 0.000052, Train: (LOSS: 0.0110, MAE: 0.0110, RMSE: 0.0185, R2: 0.9934), Valid: (LOSS: 0.0408, MAE: 0.0408, RMSE: 0.0597, R2: 0.9478), PNorm: 188.9936, GNorm: 0.5000
[139/299] timecost: 64.01, lr: 0.000052, Train: (LOSS: 0.0107, MAE: 0.0107, RMSE: 0.0173, R2: 0.9943), Valid: (LOSS: 0.0407, MAE: 0.0407, RMSE: 0.0603, R2: 0.9473), PNorm: 189.0008, GNorm: 0.5000
[140/299] timecost: 63.77, lr: 0.000052, Train: (LOSS: 0.0107, MAE: 0.0107, RMSE: 0.0177, R2: 0.9935), Valid: (LOSS: 0.0415, MAE: 0.0415, RMSE: 0.0607, R2: 0.9461), PNorm: 189.0073, GNorm: 0.3854
[141/299] timecost: 63.60, lr: 0.000052, Train: (LOSS: 0.0101, MAE: 0.0101, RMSE: 0.0169, R2: 0.9942), Valid: (LOSS: 0.0408, MAE: 0.0408, RMSE: 0.0599, R2: 0.9468), PNorm: 189.0126, GNorm: 0.4476
[142/299] timecost: 62.95, lr: 0.000052, Train: (LOSS: 0.0107, MAE: 0.0107, RMSE: 0.0173, R2: 0.9946), Valid: (LOSS: 0.0415, MAE: 0.0415, RMSE: 0.0624, R2: 0.9430), PNorm: 189.0191, GNorm: 0.4561
[143/299] timecost: 62.48, lr: 0.000052, Train: (LOSS: 0.0103, MAE: 0.0103, RMSE: 0.0167, R2: 0.9948), Valid: (LOSS: 0.0407, MAE: 0.0407, RMSE: 0.0608, R2: 0.9440), PNorm: 189.0228, GNorm: 0.3941
[144/299] timecost: 63.04, lr: 0.000052, Train: (LOSS: 0.0107, MAE: 0.0107, RMSE: 0.0176, R2: 0.9936), Valid: (LOSS: 0.0406, MAE: 0.0406, RMSE: 0.0610, R2: 0.9447), PNorm: 189.0309, GNorm: 0.4687
[145/299] timecost: 62.65, lr: 0.000052, Train: (LOSS: 0.0101, MAE: 0.0101, RMSE: 0.0163, R2: 0.9951), Valid: (LOSS: 0.0413, MAE: 0.0413, RMSE: 0.0611, R2: 0.9453), PNorm: 189.0354, GNorm: 0.5000
[146/299] timecost: 62.98, lr: 0.000052, Train: (LOSS: 0.0102, MAE: 0.0102, RMSE: 0.0169, R2: 0.9941), Valid: (LOSS: 0.0412, MAE: 0.0412, RMSE: 0.0598, R2: 0.9468), PNorm: 189.0417, GNorm: 0.3693
[147/299] timecost: 62.80, lr: 0.000052, Train: (LOSS: 0.0098, MAE: 0.0098, RMSE: 0.0164, R2: 0.9946), Valid: (LOSS: 0.0406, MAE: 0.0406, RMSE: 0.0598, R2: 0.9478), PNorm: 189.0480, GNorm: 0.3711
[148/299] timecost: 62.91, lr: 0.000052, Train: (LOSS: 0.0096, MAE: 0.0096, RMSE: 0.0156, R2: 0.9956), Valid: (LOSS: 0.0401, MAE: 0.0401, RMSE: 0.0619, R2: 0.9439), PNorm: 189.0534, GNorm: 0.4502
[149/299] timecost: 63.62, lr: 0.000052, Train: (LOSS: 0.0097, MAE: 0.0097, RMSE: 0.0158, R2: 0.9950), Valid: (LOSS: 0.0426, MAE: 0.0426, RMSE: 0.0623, R2: 0.9434), PNorm: 189.0581, GNorm: 0.4192
Epoch 00151: reducing learning rate of group 0 to 4.4371e-05.
[150/299] timecost: 63.13, lr: 0.000044, Train: (LOSS: 0.0096, MAE: 0.0096, RMSE: 0.0154, R2: 0.9954), Valid: (LOSS: 0.0410, MAE: 0.0410, RMSE: 0.0604, R2: 0.9465), PNorm: 189.0621, GNorm: 0.3650
[151/299] timecost: 63.28, lr: 0.000044, Train: (LOSS: 0.0094, MAE: 0.0094, RMSE: 0.0152, R2: 0.9957), Valid: (LOSS: 0.0393, MAE: 0.0393, RMSE: 0.0580, R2: 0.9505), PNorm: 189.0682, GNorm: 0.5000
[152/299] timecost: 63.31, lr: 0.000044, Train: (LOSS: 0.0091, MAE: 0.0091, RMSE: 0.0146, R2: 0.9959), Valid: (LOSS: 0.0412, MAE: 0.0412, RMSE: 0.0600, R2: 0.9470), PNorm: 189.0723, GNorm: 0.4856
[153/299] timecost: 62.51, lr: 0.000044, Train: (LOSS: 0.0087, MAE: 0.0087, RMSE: 0.0140, R2: 0.9959), Valid: (LOSS: 0.0409, MAE: 0.0409, RMSE: 0.0619, R2: 0.9435), PNorm: 189.0755, GNorm: 0.5000
[154/299] timecost: 62.87, lr: 0.000044, Train: (LOSS: 0.0087, MAE: 0.0087, RMSE: 0.0143, R2: 0.9960), Valid: (LOSS: 0.0406, MAE: 0.0406, RMSE: 0.0595, R2: 0.9478), PNorm: 189.0798, GNorm: 0.5000
[155/299] timecost: 62.64, lr: 0.000044, Train: (LOSS: 0.0084, MAE: 0.0084, RMSE: 0.0140, R2: 0.9959), Valid: (LOSS: 0.0402, MAE: 0.0402, RMSE: 0.0590, R2: 0.9488), PNorm: 189.0831, GNorm: 0.5000
[156/299] timecost: 62.66, lr: 0.000044, Train: (LOSS: 0.0087, MAE: 0.0087, RMSE: 0.0143, R2: 0.9956), Valid: (LOSS: 0.0403, MAE: 0.0403, RMSE: 0.0601, R2: 0.9468), PNorm: 189.0862, GNorm: 0.4737
[157/299] timecost: 61.41, lr: 0.000044, Train: (LOSS: 0.0086, MAE: 0.0086, RMSE: 0.0140, R2: 0.9960), Valid: (LOSS: 0.0416, MAE: 0.0416, RMSE: 0.0612, R2: 0.9452), PNorm: 189.0926, GNorm: 0.3817
[158/299] timecost: 61.15, lr: 0.000044, Train: (LOSS: 0.0086, MAE: 0.0086, RMSE: 0.0144, R2: 0.9957), Valid: (LOSS: 0.0408, MAE: 0.0408, RMSE: 0.0609, R2: 0.9456), PNorm: 189.0959, GNorm: 0.5000
[159/299] timecost: 60.76, lr: 0.000044, Train: (LOSS: 0.0082, MAE: 0.0082, RMSE: 0.0135, R2: 0.9956), Valid: (LOSS: 0.0402, MAE: 0.0402, RMSE: 0.0602, R2: 0.9464), PNorm: 189.1000, GNorm: 0.4136
[160/299] timecost: 60.50, lr: 0.000044, Train: (LOSS: 0.0083, MAE: 0.0083, RMSE: 0.0137, R2: 0.9960), Valid: (LOSS: 0.0408, MAE: 0.0408, RMSE: 0.0597, R2: 0.9475), PNorm: 189.1029, GNorm: 0.5000
[161/299] timecost: 60.28, lr: 0.000044, Train: (LOSS: 0.0082, MAE: 0.0082, RMSE: 0.0135, R2: 0.9963), Valid: (LOSS: 0.0407, MAE: 0.0407, RMSE: 0.0611, R2: 0.9450), PNorm: 189.1074, GNorm: 0.5000
[162/299] timecost: 61.51, lr: 0.000044, Train: (LOSS: 0.0085, MAE: 0.0085, RMSE: 0.0139, R2: 0.9963), Valid: (LOSS: 0.0406, MAE: 0.0406, RMSE: 0.0600, R2: 0.9471), PNorm: 189.1110, GNorm: 0.5000
[163/299] timecost: 64.91, lr: 0.000044, Train: (LOSS: 0.0081, MAE: 0.0081, RMSE: 0.0132, R2: 0.9963), Valid: (LOSS: 0.0406, MAE: 0.0406, RMSE: 0.0590, R2: 0.9491), PNorm: 189.1152, GNorm: 0.5000
[164/299] timecost: 64.26, lr: 0.000044, Train: (LOSS: 0.0084, MAE: 0.0084, RMSE: 0.0140, R2: 0.9955), Valid: (LOSS: 0.0398, MAE: 0.0398, RMSE: 0.0581, R2: 0.9503), PNorm: 189.1202, GNorm: 0.3676
[165/299] timecost: 64.45, lr: 0.000044, Train: (LOSS: 0.0082, MAE: 0.0082, RMSE: 0.0137, R2: 0.9962), Valid: (LOSS: 0.0395, MAE: 0.0395, RMSE: 0.0586, R2: 0.9494), PNorm: 189.1236, GNorm: 0.4259
Epoch 00167: reducing learning rate of group 0 to 3.7715e-05.
[166/299] timecost: 65.16, lr: 0.000038, Train: (LOSS: 0.0082, MAE: 0.0082, RMSE: 0.0134, R2: 0.9963), Valid: (LOSS: 0.0403, MAE: 0.0403, RMSE: 0.0596, R2: 0.9479), PNorm: 189.1285, GNorm: 0.5000
[167/299] timecost: 64.56, lr: 0.000038, Train: (LOSS: 0.0079, MAE: 0.0079, RMSE: 0.0131, R2: 0.9963), Valid: (LOSS: 0.0392, MAE: 0.0392, RMSE: 0.0582, R2: 0.9503), PNorm: 189.1305, GNorm: 0.4237
[168/299] timecost: 63.80, lr: 0.000038, Train: (LOSS: 0.0076, MAE: 0.0076, RMSE: 0.0128, R2: 0.9966), Valid: (LOSS: 0.0401, MAE: 0.0401, RMSE: 0.0597, R2: 0.9471), PNorm: 189.1350, GNorm: 0.3770
[169/299] timecost: 61.58, lr: 0.000038, Train: (LOSS: 0.0075, MAE: 0.0075, RMSE: 0.0126, R2: 0.9965), Valid: (LOSS: 0.0402, MAE: 0.0402, RMSE: 0.0598, R2: 0.9465), PNorm: 189.1370, GNorm: 0.3272
[170/299] timecost: 61.33, lr: 0.000038, Train: (LOSS: 0.0074, MAE: 0.0074, RMSE: 0.0124, R2: 0.9967), Valid: (LOSS: 0.0398, MAE: 0.0398, RMSE: 0.0584, R2: 0.9502), PNorm: 189.1411, GNorm: 0.3679
[171/299] timecost: 60.68, lr: 0.000038, Train: (LOSS: 0.0069, MAE: 0.0069, RMSE: 0.0119, R2: 0.9963), Valid: (LOSS: 0.0396, MAE: 0.0396, RMSE: 0.0579, R2: 0.9509), PNorm: 189.1441, GNorm: 0.5000
[172/299] timecost: 62.50, lr: 0.000038, Train: (LOSS: 0.0071, MAE: 0.0071, RMSE: 0.0118, R2: 0.9969), Valid: (LOSS: 0.0406, MAE: 0.0406, RMSE: 0.0607, R2: 0.9462), PNorm: 189.1453, GNorm: 0.3162
[173/299] timecost: 62.81, lr: 0.000038, Train: (LOSS: 0.0071, MAE: 0.0071, RMSE: 0.0121, R2: 0.9964), Valid: (LOSS: 0.0403, MAE: 0.0403, RMSE: 0.0601, R2: 0.9460), PNorm: 189.1494, GNorm: 0.5000
[174/299] timecost: 62.65, lr: 0.000038, Train: (LOSS: 0.0071, MAE: 0.0071, RMSE: 0.0120, R2: 0.9966), Valid: (LOSS: 0.0406, MAE: 0.0406, RMSE: 0.0609, R2: 0.9457), PNorm: 189.1515, GNorm: 0.5000
[175/299] timecost: 60.65, lr: 0.000038, Train: (LOSS: 0.0073, MAE: 0.0073, RMSE: 0.0122, R2: 0.9967), Valid: (LOSS: 0.0393, MAE: 0.0393, RMSE: 0.0586, R2: 0.9499), PNorm: 189.1540, GNorm: 0.3947
[176/299] timecost: 59.98, lr: 0.000038, Train: (LOSS: 0.0070, MAE: 0.0070, RMSE: 0.0120, R2: 0.9966), Valid: (LOSS: 0.0397, MAE: 0.0397, RMSE: 0.0585, R2: 0.9497), PNorm: 189.1583, GNorm: 0.4739
[177/299] timecost: 60.07, lr: 0.000038, Train: (LOSS: 0.0071, MAE: 0.0071, RMSE: 0.0120, R2: 0.9967), Valid: (LOSS: 0.0392, MAE: 0.0392, RMSE: 0.0582, R2: 0.9500), PNorm: 189.1606, GNorm: 0.5000
[178/299] timecost: 60.58, lr: 0.000038, Train: (LOSS: 0.0072, MAE: 0.0072, RMSE: 0.0119, R2: 0.9968), Valid: (LOSS: 0.0405, MAE: 0.0405, RMSE: 0.0618, R2: 0.9437), PNorm: 189.1645, GNorm: 0.5000
[179/299] timecost: 61.86, lr: 0.000038, Train: (LOSS: 0.0071, MAE: 0.0071, RMSE: 0.0119, R2: 0.9969), Valid: (LOSS: 0.0398, MAE: 0.0398, RMSE: 0.0598, R2: 0.9474), PNorm: 189.1679, GNorm: 0.3690
[180/299] timecost: 63.17, lr: 0.000038, Train: (LOSS: 0.0071, MAE: 0.0071, RMSE: 0.0120, R2: 0.9969), Valid: (LOSS: 0.0396, MAE: 0.0396, RMSE: 0.0590, R2: 0.9486), PNorm: 189.1697, GNorm: 0.3657
[181/299] timecost: 64.85, lr: 0.000038, Train: (LOSS: 0.0071, MAE: 0.0071, RMSE: 0.0120, R2: 0.9968), Valid: (LOSS: 0.0399, MAE: 0.0399, RMSE: 0.0588, R2: 0.9486), PNorm: 189.1733, GNorm: 0.3987
Epoch 00183: reducing learning rate of group 0 to 3.2058e-05.
[182/299] timecost: 64.45, lr: 0.000032, Train: (LOSS: 0.0068, MAE: 0.0068, RMSE: 0.0114, R2: 0.9969), Valid: (LOSS: 0.0398, MAE: 0.0398, RMSE: 0.0585, R2: 0.9497), PNorm: 189.1768, GNorm: 0.5000
[183/299] timecost: 61.45, lr: 0.000032, Train: (LOSS: 0.0068, MAE: 0.0068, RMSE: 0.0122, R2: 0.9965), Valid: (LOSS: 0.0395, MAE: 0.0395, RMSE: 0.0583, R2: 0.9500), PNorm: 189.1788, GNorm: 0.5000
[184/299] timecost: 60.10, lr: 0.000032, Train: (LOSS: 0.0063, MAE: 0.0063, RMSE: 0.0110, R2: 0.9965), Valid: (LOSS: 0.0402, MAE: 0.0402, RMSE: 0.0597, R2: 0.9477), PNorm: 189.1815, GNorm: 0.4995
[185/299] timecost: 60.36, lr: 0.000032, Train: (LOSS: 0.0060, MAE: 0.0060, RMSE: 0.0107, R2: 0.9968), Valid: (LOSS: 0.0402, MAE: 0.0402, RMSE: 0.0596, R2: 0.9470), PNorm: 189.1821, GNorm: 0.5000
[186/299] timecost: 60.13, lr: 0.000032, Train: (LOSS: 0.0062, MAE: 0.0062, RMSE: 0.0108, R2: 0.9970), Valid: (LOSS: 0.0403, MAE: 0.0403, RMSE: 0.0596, R2: 0.9478), PNorm: 189.1857, GNorm: 0.4836
[187/299] timecost: 60.05, lr: 0.000032, Train: (LOSS: 0.0063, MAE: 0.0063, RMSE: 0.0108, R2: 0.9972), Valid: (LOSS: 0.0397, MAE: 0.0397, RMSE: 0.0584, R2: 0.9500), PNorm: 189.1875, GNorm: 0.4997
[188/299] timecost: 60.07, lr: 0.000032, Train: (LOSS: 0.0063, MAE: 0.0063, RMSE: 0.0108, R2: 0.9971), Valid: (LOSS: 0.0393, MAE: 0.0393, RMSE: 0.0576, R2: 0.9509), PNorm: 189.1899, GNorm: 0.5000
[189/299] timecost: 60.15, lr: 0.000032, Train: (LOSS: 0.0060, MAE: 0.0060, RMSE: 0.0106, R2: 0.9966), Valid: (LOSS: 0.0394, MAE: 0.0394, RMSE: 0.0583, R2: 0.9501), PNorm: 189.1918, GNorm: 0.4821
[190/299] timecost: 59.88, lr: 0.000032, Train: (LOSS: 0.0061, MAE: 0.0061, RMSE: 0.0106, R2: 0.9972), Valid: (LOSS: 0.0411, MAE: 0.0411, RMSE: 0.0634, R2: 0.9387), PNorm: 189.1946, GNorm: 0.5000
[191/299] timecost: 60.27, lr: 0.000032, Train: (LOSS: 0.0063, MAE: 0.0063, RMSE: 0.0109, R2: 0.9971), Valid: (LOSS: 0.0394, MAE: 0.0394, RMSE: 0.0586, R2: 0.9488), PNorm: 189.1969, GNorm: 0.4546
[192/299] timecost: 61.48, lr: 0.000032, Train: (LOSS: 0.0062, MAE: 0.0062, RMSE: 0.0109, R2: 0.9970), Valid: (LOSS: 0.0401, MAE: 0.0401, RMSE: 0.0594, R2: 0.9477), PNorm: 189.1992, GNorm: 0.4010
[193/299] timecost: 60.53, lr: 0.000032, Train: (LOSS: 0.0063, MAE: 0.0063, RMSE: 0.0109, R2: 0.9975), Valid: (LOSS: 0.0399, MAE: 0.0399, RMSE: 0.0607, R2: 0.9446), PNorm: 189.2010, GNorm: 0.5000
[194/299] timecost: 60.64, lr: 0.000032, Train: (LOSS: 0.0061, MAE: 0.0061, RMSE: 0.0106, R2: 0.9974), Valid: (LOSS: 0.0402, MAE: 0.0402, RMSE: 0.0592, R2: 0.9480), PNorm: 189.2039, GNorm: 0.4202
[195/299] timecost: 60.99, lr: 0.000032, Train: (LOSS: 0.0059, MAE: 0.0059, RMSE: 0.0104, R2: 0.9974), Valid: (LOSS: 0.0390, MAE: 0.0390, RMSE: 0.0569, R2: 0.9518), PNorm: 189.2067, GNorm: 0.4002
[196/299] timecost: 60.49, lr: 0.000032, Train: (LOSS: 0.0059, MAE: 0.0059, RMSE: 0.0103, R2: 0.9972), Valid: (LOSS: 0.0393, MAE: 0.0393, RMSE: 0.0587, R2: 0.9491), PNorm: 189.2090, GNorm: 0.4252
[197/299] timecost: 62.09, lr: 0.000032, Train: (LOSS: 0.0060, MAE: 0.0060, RMSE: 0.0105, R2: 0.9965), Valid: (LOSS: 0.0395, MAE: 0.0395, RMSE: 0.0593, R2: 0.9482), PNorm: 189.2108, GNorm: 0.4462
Epoch 00199: reducing learning rate of group 0 to 2.7249e-05.
[198/299] timecost: 63.39, lr: 0.000027, Train: (LOSS: 0.0062, MAE: 0.0062, RMSE: 0.0108, R2: 0.9971), Valid: (LOSS: 0.0393, MAE: 0.0393, RMSE: 0.0581, R2: 0.9505), PNorm: 189.2133, GNorm: 0.4351
[199/299] timecost: 61.40, lr: 0.000027, Train: (LOSS: 0.0057, MAE: 0.0057, RMSE: 0.0101, R2: 0.9971), Valid: (LOSS: 0.0401, MAE: 0.0401, RMSE: 0.0593, R2: 0.9483), PNorm: 189.2155, GNorm: 0.5000
[200/299] timecost: 62.47, lr: 0.000027, Train: (LOSS: 0.0055, MAE: 0.0055, RMSE: 0.0098, R2: 0.9972), Valid: (LOSS: 0.0393, MAE: 0.0393, RMSE: 0.0586, R2: 0.9496), PNorm: 189.2160, GNorm: 0.4676
[201/299] timecost: 64.63, lr: 0.000027, Train: (LOSS: 0.0056, MAE: 0.0056, RMSE: 0.0099, R2: 0.9971), Valid: (LOSS: 0.0393, MAE: 0.0393, RMSE: 0.0580, R2: 0.9502), PNorm: 189.2184, GNorm: 0.4272
[202/299] timecost: 62.42, lr: 0.000027, Train: (LOSS: 0.0053, MAE: 0.0053, RMSE: 0.0097, R2: 0.9976), Valid: (LOSS: 0.0400, MAE: 0.0400, RMSE: 0.0592, R2: 0.9481), PNorm: 189.2194, GNorm: 0.4614
[203/299] timecost: 60.32, lr: 0.000027, Train: (LOSS: 0.0053, MAE: 0.0053, RMSE: 0.0096, R2: 0.9971), Valid: (LOSS: 0.0393, MAE: 0.0393, RMSE: 0.0582, R2: 0.9500), PNorm: 189.2220, GNorm: 0.5000
[204/299] timecost: 60.44, lr: 0.000027, Train: (LOSS: 0.0054, MAE: 0.0054, RMSE: 0.0098, R2: 0.9975), Valid: (LOSS: 0.0390, MAE: 0.0390, RMSE: 0.0582, R2: 0.9501), PNorm: 189.2235, GNorm: 0.4168
[205/299] timecost: 60.10, lr: 0.000027, Train: (LOSS: 0.0054, MAE: 0.0054, RMSE: 0.0098, R2: 0.9970), Valid: (LOSS: 0.0393, MAE: 0.0393, RMSE: 0.0577, R2: 0.9507), PNorm: 189.2253, GNorm: 0.5000
[206/299] timecost: 59.74, lr: 0.000027, Train: (LOSS: 0.0053, MAE: 0.0053, RMSE: 0.0098, R2: 0.9974), Valid: (LOSS: 0.0388, MAE: 0.0388, RMSE: 0.0571, R2: 0.9518), PNorm: 189.2265, GNorm: 0.5000
[207/299] timecost: 59.84, lr: 0.000027, Train: (LOSS: 0.0052, MAE: 0.0052, RMSE: 0.0094, R2: 0.9978), Valid: (LOSS: 0.0396, MAE: 0.0396, RMSE: 0.0595, R2: 0.9473), PNorm: 189.2281, GNorm: 0.4373
[208/299] timecost: 59.83, lr: 0.000027, Train: (LOSS: 0.0055, MAE: 0.0055, RMSE: 0.0097, R2: 0.9974), Valid: (LOSS: 0.0389, MAE: 0.0389, RMSE: 0.0580, R2: 0.9498), PNorm: 189.2301, GNorm: 0.5000
[209/299] timecost: 60.24, lr: 0.000027, Train: (LOSS: 0.0052, MAE: 0.0052, RMSE: 0.0096, R2: 0.9976), Valid: (LOSS: 0.0388, MAE: 0.0388, RMSE: 0.0581, R2: 0.9503), PNorm: 189.2313, GNorm: 0.5000
[210/299] timecost: 63.73, lr: 0.000027, Train: (LOSS: 0.0053, MAE: 0.0053, RMSE: 0.0097, R2: 0.9974), Valid: (LOSS: 0.0395, MAE: 0.0395, RMSE: 0.0586, R2: 0.9493), PNorm: 189.2334, GNorm: 0.5000
[211/299] timecost: 64.57, lr: 0.000027, Train: (LOSS: 0.0052, MAE: 0.0052, RMSE: 0.0096, R2: 0.9974), Valid: (LOSS: 0.0395, MAE: 0.0395, RMSE: 0.0582, R2: 0.9500), PNorm: 189.2357, GNorm: 0.5000
[212/299] timecost: 64.23, lr: 0.000027, Train: (LOSS: 0.0053, MAE: 0.0053, RMSE: 0.0096, R2: 0.9974), Valid: (LOSS: 0.0392, MAE: 0.0392, RMSE: 0.0578, R2: 0.9510), PNorm: 189.2367, GNorm: 0.4486
[213/299] timecost: 64.47, lr: 0.000027, Train: (LOSS: 0.0054, MAE: 0.0054, RMSE: 0.0096, R2: 0.9973), Valid: (LOSS: 0.0391, MAE: 0.0391, RMSE: 0.0573, R2: 0.9516), PNorm: 189.2382, GNorm: 0.5000
Epoch 00215: reducing learning rate of group 0 to 2.3162e-05.
[214/299] timecost: 64.41, lr: 0.000023, Train: (LOSS: 0.0052, MAE: 0.0052, RMSE: 0.0095, R2: 0.9972), Valid: (LOSS: 0.0390, MAE: 0.0390, RMSE: 0.0578, R2: 0.9504), PNorm: 189.2403, GNorm: 0.3965
[215/299] timecost: 64.65, lr: 0.000023, Train: (LOSS: 0.0048, MAE: 0.0048, RMSE: 0.0091, R2: 0.9975), Valid: (LOSS: 0.0390, MAE: 0.0390, RMSE: 0.0577, R2: 0.9506), PNorm: 189.2410, GNorm: 0.4564
[216/299] timecost: 64.18, lr: 0.000023, Train: (LOSS: 0.0047, MAE: 0.0047, RMSE: 0.0087, R2: 0.9974), Valid: (LOSS: 0.0395, MAE: 0.0395, RMSE: 0.0584, R2: 0.9499), PNorm: 189.2422, GNorm: 0.3719
[217/299] timecost: 64.38, lr: 0.000023, Train: (LOSS: 0.0046, MAE: 0.0046, RMSE: 0.0087, R2: 0.9975), Valid: (LOSS: 0.0393, MAE: 0.0393, RMSE: 0.0587, R2: 0.9492), PNorm: 189.2432, GNorm: 0.5000
[218/299] timecost: 61.59, lr: 0.000023, Train: (LOSS: 0.0048, MAE: 0.0048, RMSE: 0.0091, R2: 0.9976), Valid: (LOSS: 0.0392, MAE: 0.0392, RMSE: 0.0580, R2: 0.9503), PNorm: 189.2445, GNorm: 0.4855
[219/299] timecost: 61.37, lr: 0.000023, Train: (LOSS: 0.0051, MAE: 0.0051, RMSE: 0.0092, R2: 0.9977), Valid: (LOSS: 0.0393, MAE: 0.0393, RMSE: 0.0582, R2: 0.9500), PNorm: 189.2474, GNorm: 0.4780
[220/299] timecost: 61.05, lr: 0.000023, Train: (LOSS: 0.0049, MAE: 0.0049, RMSE: 0.0090, R2: 0.9978), Valid: (LOSS: 0.0392, MAE: 0.0392, RMSE: 0.0585, R2: 0.9494), PNorm: 189.2478, GNorm: 0.4632
[221/299] timecost: 60.19, lr: 0.000023, Train: (LOSS: 0.0047, MAE: 0.0047, RMSE: 0.0090, R2: 0.9976), Valid: (LOSS: 0.0395, MAE: 0.0395, RMSE: 0.0585, R2: 0.9494), PNorm: 189.2489, GNorm: 0.3819
[222/299] timecost: 62.16, lr: 0.000023, Train: (LOSS: 0.0048, MAE: 0.0048, RMSE: 0.0089, R2: 0.9973), Valid: (LOSS: 0.0390, MAE: 0.0390, RMSE: 0.0583, R2: 0.9495), PNorm: 189.2504, GNorm: 0.4003
[223/299] timecost: 61.08, lr: 0.000023, Train: (LOSS: 0.0047, MAE: 0.0047, RMSE: 0.0088, R2: 0.9976), Valid: (LOSS: 0.0386, MAE: 0.0386, RMSE: 0.0567, R2: 0.9520), PNorm: 189.2515, GNorm: 0.5000
[224/299] timecost: 60.30, lr: 0.000023, Train: (LOSS: 0.0049, MAE: 0.0049, RMSE: 0.0089, R2: 0.9975), Valid: (LOSS: 0.0395, MAE: 0.0395, RMSE: 0.0582, R2: 0.9501), PNorm: 189.2534, GNorm: 0.5000
[225/299] timecost: 62.01, lr: 0.000023, Train: (LOSS: 0.0046, MAE: 0.0046, RMSE: 0.0087, R2: 0.9979), Valid: (LOSS: 0.0392, MAE: 0.0392, RMSE: 0.0578, R2: 0.9506), PNorm: 189.2541, GNorm: 0.5000
[226/299] timecost: 62.20, lr: 0.000023, Train: (LOSS: 0.0046, MAE: 0.0046, RMSE: 0.0087, R2: 0.9976), Valid: (LOSS: 0.0387, MAE: 0.0387, RMSE: 0.0573, R2: 0.9514), PNorm: 189.2557, GNorm: 0.4330
[227/299] timecost: 62.38, lr: 0.000023, Train: (LOSS: 0.0046, MAE: 0.0046, RMSE: 0.0087, R2: 0.9968), Valid: (LOSS: 0.0387, MAE: 0.0387, RMSE: 0.0575, R2: 0.9513), PNorm: 189.2559, GNorm: 0.4847
[228/299] timecost: 62.19, lr: 0.000023, Train: (LOSS: 0.0048, MAE: 0.0048, RMSE: 0.0088, R2: 0.9977), Valid: (LOSS: 0.0389, MAE: 0.0389, RMSE: 0.0577, R2: 0.9508), PNorm: 189.2582, GNorm: 0.5000
[229/299] timecost: 61.88, lr: 0.000023, Train: (LOSS: 0.0045, MAE: 0.0045, RMSE: 0.0086, R2: 0.9973), Valid: (LOSS: 0.0390, MAE: 0.0390, RMSE: 0.0579, R2: 0.9506), PNorm: 189.2592, GNorm: 0.5000
[230/299] timecost: 61.95, lr: 0.000023, Train: (LOSS: 0.0046, MAE: 0.0046, RMSE: 0.0088, R2: 0.9977), Valid: (LOSS: 0.0389, MAE: 0.0389, RMSE: 0.0578, R2: 0.9509), PNorm: 189.2603, GNorm: 0.4576
[231/299] timecost: 61.78, lr: 0.000023, Train: (LOSS: 0.0045, MAE: 0.0045, RMSE: 0.0085, R2: 0.9976), Valid: (LOSS: 0.0390, MAE: 0.0390, RMSE: 0.0579, R2: 0.9507), PNorm: 189.2624, GNorm: 0.3710
[232/299] timecost: 62.72, lr: 0.000023, Train: (LOSS: 0.0045, MAE: 0.0045, RMSE: 0.0085, R2: 0.9973), Valid: (LOSS: 0.0384, MAE: 0.0384, RMSE: 0.0568, R2: 0.9524), PNorm: 189.2632, GNorm: 0.3952
[233/299] timecost: 64.11, lr: 0.000023, Train: (LOSS: 0.0044, MAE: 0.0044, RMSE: 0.0086, R2: 0.9980), Valid: (LOSS: 0.0390, MAE: 0.0390, RMSE: 0.0578, R2: 0.9506), PNorm: 189.2642, GNorm: 0.4090
[234/299] timecost: 63.56, lr: 0.000023, Train: (LOSS: 0.0045, MAE: 0.0045, RMSE: 0.0086, R2: 0.9980), Valid: (LOSS: 0.0386, MAE: 0.0386, RMSE: 0.0568, R2: 0.9524), PNorm: 189.2654, GNorm: 0.3864
[235/299] timecost: 63.28, lr: 0.000023, Train: (LOSS: 0.0044, MAE: 0.0044, RMSE: 0.0083, R2: 0.9971), Valid: (LOSS: 0.0386, MAE: 0.0386, RMSE: 0.0573, R2: 0.9517), PNorm: 189.2668, GNorm: 0.5000
[236/299] timecost: 63.54, lr: 0.000023, Train: (LOSS: 0.0045, MAE: 0.0045, RMSE: 0.0084, R2: 0.9980), Valid: (LOSS: 0.0386, MAE: 0.0386, RMSE: 0.0571, R2: 0.9516), PNorm: 189.2683, GNorm: 0.4165
[237/299] timecost: 63.73, lr: 0.000023, Train: (LOSS: 0.0046, MAE: 0.0046, RMSE: 0.0085, R2: 0.9980), Valid: (LOSS: 0.0392, MAE: 0.0392, RMSE: 0.0580, R2: 0.9503), PNorm: 189.2696, GNorm: 0.5000
[238/299] timecost: 63.44, lr: 0.000023, Train: (LOSS: 0.0046, MAE: 0.0046, RMSE: 0.0087, R2: 0.9974), Valid: (LOSS: 0.0394, MAE: 0.0394, RMSE: 0.0584, R2: 0.9494), PNorm: 189.2718, GNorm: 0.5000
Epoch 00240: reducing learning rate of group 0 to 1.9687e-05.
[239/299] timecost: 62.91, lr: 0.000020, Train: (LOSS: 0.0045, MAE: 0.0045, RMSE: 0.0086, R2: 0.9979), Valid: (LOSS: 0.0389, MAE: 0.0389, RMSE: 0.0576, R2: 0.9510), PNorm: 189.2723, GNorm: 0.4104
[240/299] timecost: 63.19, lr: 0.000020, Train: (LOSS: 0.0042, MAE: 0.0042, RMSE: 0.0081, R2: 0.9978), Valid: (LOSS: 0.0384, MAE: 0.0384, RMSE: 0.0568, R2: 0.9522), PNorm: 189.2731, GNorm: 0.3285
[241/299] timecost: 63.10, lr: 0.000020, Train: (LOSS: 0.0040, MAE: 0.0040, RMSE: 0.0081, R2: 0.9974), Valid: (LOSS: 0.0389, MAE: 0.0389, RMSE: 0.0573, R2: 0.9516), PNorm: 189.2739, GNorm: 0.4468
[242/299] timecost: 62.93, lr: 0.000020, Train: (LOSS: 0.0041, MAE: 0.0041, RMSE: 0.0080, R2: 0.9974), Valid: (LOSS: 0.0390, MAE: 0.0390, RMSE: 0.0575, R2: 0.9512), PNorm: 189.2755, GNorm: 0.5000
[243/299] timecost: 62.96, lr: 0.000020, Train: (LOSS: 0.0043, MAE: 0.0043, RMSE: 0.0082, R2: 0.9978), Valid: (LOSS: 0.0387, MAE: 0.0387, RMSE: 0.0573, R2: 0.9515), PNorm: 189.2762, GNorm: 0.4259
[244/299] timecost: 62.97, lr: 0.000020, Train: (LOSS: 0.0041, MAE: 0.0041, RMSE: 0.0082, R2: 0.9977), Valid: (LOSS: 0.0386, MAE: 0.0386, RMSE: 0.0570, R2: 0.9521), PNorm: 189.2768, GNorm: 0.3821
[245/299] timecost: 62.46, lr: 0.000020, Train: (LOSS: 0.0041, MAE: 0.0041, RMSE: 0.0080, R2: 0.9973), Valid: (LOSS: 0.0390, MAE: 0.0390, RMSE: 0.0579, R2: 0.9505), PNorm: 189.2781, GNorm: 0.3734
[246/299] timecost: 62.80, lr: 0.000020, Train: (LOSS: 0.0039, MAE: 0.0039, RMSE: 0.0079, R2: 0.9980), Valid: (LOSS: 0.0389, MAE: 0.0389, RMSE: 0.0582, R2: 0.9501), PNorm: 189.2786, GNorm: 0.5000
[247/299] timecost: 63.13, lr: 0.000020, Train: (LOSS: 0.0040, MAE: 0.0040, RMSE: 0.0080, R2: 0.9972), Valid: (LOSS: 0.0391, MAE: 0.0391, RMSE: 0.0576, R2: 0.9509), PNorm: 189.2799, GNorm: 0.5000
[248/299] timecost: 62.73, lr: 0.000020, Train: (LOSS: 0.0041, MAE: 0.0041, RMSE: 0.0081, R2: 0.9979), Valid: (LOSS: 0.0391, MAE: 0.0391, RMSE: 0.0580, R2: 0.9501), PNorm: 189.2811, GNorm: 0.4564
[249/299] timecost: 63.27, lr: 0.000020, Train: (LOSS: 0.0040, MAE: 0.0040, RMSE: 0.0078, R2: 0.9982), Valid: (LOSS: 0.0392, MAE: 0.0392, RMSE: 0.0585, R2: 0.9494), PNorm: 189.2817, GNorm: 0.5000
[250/299] timecost: 63.18, lr: 0.000020, Train: (LOSS: 0.0041, MAE: 0.0041, RMSE: 0.0080, R2: 0.9981), Valid: (LOSS: 0.0392, MAE: 0.0392, RMSE: 0.0579, R2: 0.9504), PNorm: 189.2824, GNorm: 0.4987
[251/299] timecost: 63.34, lr: 0.000020, Train: (LOSS: 0.0041, MAE: 0.0041, RMSE: 0.0080, R2: 0.9972), Valid: (LOSS: 0.0388, MAE: 0.0388, RMSE: 0.0576, R2: 0.9511), PNorm: 189.2841, GNorm: 0.4031
[252/299] timecost: 62.99, lr: 0.000020, Train: (LOSS: 0.0039, MAE: 0.0039, RMSE: 0.0079, R2: 0.9979), Valid: (LOSS: 0.0393, MAE: 0.0393, RMSE: 0.0584, R2: 0.9498), PNorm: 189.2849, GNorm: 0.4947
[253/299] timecost: 63.15, lr: 0.000020, Train: (LOSS: 0.0040, MAE: 0.0040, RMSE: 0.0078, R2: 0.9975), Valid: (LOSS: 0.0391, MAE: 0.0391, RMSE: 0.0581, R2: 0.9499), PNorm: 189.2858, GNorm: 0.4081
[254/299] timecost: 63.38, lr: 0.000020, Train: (LOSS: 0.0040, MAE: 0.0040, RMSE: 0.0079, R2: 0.9979), Valid: (LOSS: 0.0392, MAE: 0.0392, RMSE: 0.0581, R2: 0.9503), PNorm: 189.2868, GNorm: 0.4092
Epoch 00256: reducing learning rate of group 0 to 1.6734e-05.
[255/299] timecost: 63.21, lr: 0.000017, Train: (LOSS: 0.0039, MAE: 0.0039, RMSE: 0.0077, R2: 0.9975), Valid: (LOSS: 0.0389, MAE: 0.0389, RMSE: 0.0577, R2: 0.9509), PNorm: 189.2882, GNorm: 0.3774
[256/299] timecost: 64.11, lr: 0.000017, Train: (LOSS: 0.0037, MAE: 0.0037, RMSE: 0.0077, R2: 0.9979), Valid: (LOSS: 0.0387, MAE: 0.0387, RMSE: 0.0573, R2: 0.9514), PNorm: 189.2883, GNorm: 0.3977
[257/299] timecost: 64.59, lr: 0.000017, Train: (LOSS: 0.0036, MAE: 0.0036, RMSE: 0.0074, R2: 0.9983), Valid: (LOSS: 0.0388, MAE: 0.0388, RMSE: 0.0573, R2: 0.9515), PNorm: 189.2890, GNorm: 0.3793
[258/299] timecost: 64.83, lr: 0.000017, Train: (LOSS: 0.0036, MAE: 0.0036, RMSE: 0.0071, R2: 0.9978), Valid: (LOSS: 0.0387, MAE: 0.0387, RMSE: 0.0576, R2: 0.9510), PNorm: 189.2897, GNorm: 0.5000
[259/299] timecost: 64.38, lr: 0.000017, Train: (LOSS: 0.0036, MAE: 0.0036, RMSE: 0.0075, R2: 0.9981), Valid: (LOSS: 0.0389, MAE: 0.0389, RMSE: 0.0576, R2: 0.9508), PNorm: 189.2909, GNorm: 0.5000
[260/299] timecost: 63.95, lr: 0.000017, Train: (LOSS: 0.0037, MAE: 0.0037, RMSE: 0.0076, R2: 0.9980), Valid: (LOSS: 0.0391, MAE: 0.0391, RMSE: 0.0579, R2: 0.9504), PNorm: 189.2914, GNorm: 0.5000
[261/299] timecost: 64.32, lr: 0.000017, Train: (LOSS: 0.0035, MAE: 0.0035, RMSE: 0.0074, R2: 0.9980), Valid: (LOSS: 0.0386, MAE: 0.0386, RMSE: 0.0573, R2: 0.9515), PNorm: 189.2920, GNorm: 0.4759
[262/299] timecost: 62.64, lr: 0.000017, Train: (LOSS: 0.0037, MAE: 0.0037, RMSE: 0.0076, R2: 0.9981), Valid: (LOSS: 0.0386, MAE: 0.0386, RMSE: 0.0572, R2: 0.9514), PNorm: 189.2930, GNorm: 0.3773
[263/299] timecost: 60.70, lr: 0.000017, Train: (LOSS: 0.0036, MAE: 0.0036, RMSE: 0.0076, R2: 0.9981), Valid: (LOSS: 0.0394, MAE: 0.0394, RMSE: 0.0581, R2: 0.9502), PNorm: 189.2935, GNorm: 0.4558
[264/299] timecost: 61.69, lr: 0.000017, Train: (LOSS: 0.0037, MAE: 0.0037, RMSE: 0.0074, R2: 0.9979), Valid: (LOSS: 0.0386, MAE: 0.0386, RMSE: 0.0572, R2: 0.9519), PNorm: 189.2941, GNorm: 0.4320
[265/299] timecost: 60.97, lr: 0.000017, Train: (LOSS: 0.0037, MAE: 0.0037, RMSE: 0.0075, R2: 0.9976), Valid: (LOSS: 0.0389, MAE: 0.0389, RMSE: 0.0576, R2: 0.9507), PNorm: 189.2954, GNorm: 0.3876
[266/299] timecost: 62.43, lr: 0.000017, Train: (LOSS: 0.0036, MAE: 0.0036, RMSE: 0.0074, R2: 0.9981), Valid: (LOSS: 0.0386, MAE: 0.0386, RMSE: 0.0571, R2: 0.9518), PNorm: 189.2957, GNorm: 0.4331
[267/299] timecost: 63.58, lr: 0.000017, Train: (LOSS: 0.0037, MAE: 0.0037, RMSE: 0.0076, R2: 0.9979), Valid: (LOSS: 0.0387, MAE: 0.0387, RMSE: 0.0573, R2: 0.9515), PNorm: 189.2962, GNorm: 0.5000
[268/299] timecost: 63.83, lr: 0.000017, Train: (LOSS: 0.0035, MAE: 0.0035, RMSE: 0.0074, R2: 0.9976), Valid: (LOSS: 0.0391, MAE: 0.0391, RMSE: 0.0575, R2: 0.9510), PNorm: 189.2973, GNorm: 0.5000
[269/299] timecost: 63.90, lr: 0.000017, Train: (LOSS: 0.0036, MAE: 0.0036, RMSE: 0.0074, R2: 0.9978), Valid: (LOSS: 0.0391, MAE: 0.0391, RMSE: 0.0574, R2: 0.9512), PNorm: 189.2982, GNorm: 0.3783
[270/299] timecost: 63.59, lr: 0.000017, Train: (LOSS: 0.0035, MAE: 0.0035, RMSE: 0.0074, R2: 0.9979), Valid: (LOSS: 0.0393, MAE: 0.0393, RMSE: 0.0578, R2: 0.9506), PNorm: 189.2982, GNorm: 0.3307
Epoch 00272: reducing learning rate of group 0 to 1.4224e-05.
[271/299] timecost: 63.25, lr: 0.000014, Train: (LOSS: 0.0036, MAE: 0.0036, RMSE: 0.0073, R2: 0.9980), Valid: (LOSS: 0.0391, MAE: 0.0391, RMSE: 0.0576, R2: 0.9509), PNorm: 189.2994, GNorm: 0.5000
[272/299] timecost: 60.49, lr: 0.000014, Train: (LOSS: 0.0034, MAE: 0.0034, RMSE: 0.0072, R2: 0.9981), Valid: (LOSS: 0.0390, MAE: 0.0390, RMSE: 0.0577, R2: 0.9507), PNorm: 189.3003, GNorm: 0.3313
[273/299] timecost: 60.43, lr: 0.000014, Train: (LOSS: 0.0032, MAE: 0.0032, RMSE: 0.0069, R2: 0.9981), Valid: (LOSS: 0.0389, MAE: 0.0389, RMSE: 0.0574, R2: 0.9512), PNorm: 189.3003, GNorm: 0.4720
[274/299] timecost: 59.71, lr: 0.000014, Train: (LOSS: 0.0033, MAE: 0.0033, RMSE: 0.0071, R2: 0.9977), Valid: (LOSS: 0.0390, MAE: 0.0390, RMSE: 0.0577, R2: 0.9509), PNorm: 189.3006, GNorm: 0.4500
[275/299] timecost: 60.90, lr: 0.000014, Train: (LOSS: 0.0032, MAE: 0.0032, RMSE: 0.0070, R2: 0.9981), Valid: (LOSS: 0.0391, MAE: 0.0391, RMSE: 0.0577, R2: 0.9509), PNorm: 189.3016, GNorm: 0.3739
[276/299] timecost: 61.04, lr: 0.000014, Train: (LOSS: 0.0032, MAE: 0.0032, RMSE: 0.0070, R2: 0.9978), Valid: (LOSS: 0.0392, MAE: 0.0392, RMSE: 0.0577, R2: 0.9507), PNorm: 189.3019, GNorm: 0.3872
[277/299] timecost: 59.99, lr: 0.000014, Train: (LOSS: 0.0032, MAE: 0.0032, RMSE: 0.0070, R2: 0.9982), Valid: (LOSS: 0.0389, MAE: 0.0389, RMSE: 0.0575, R2: 0.9513), PNorm: 189.3022, GNorm: 0.5000
[278/299] timecost: 60.38, lr: 0.000014, Train: (LOSS: 0.0033, MAE: 0.0033, RMSE: 0.0071, R2: 0.9982), Valid: (LOSS: 0.0389, MAE: 0.0389, RMSE: 0.0572, R2: 0.9516), PNorm: 189.3028, GNorm: 0.5000
[279/299] timecost: 61.17, lr: 0.000014, Train: (LOSS: 0.0033, MAE: 0.0033, RMSE: 0.0071, R2: 0.9979), Valid: (LOSS: 0.0389, MAE: 0.0389, RMSE: 0.0568, R2: 0.9524), PNorm: 189.3033, GNorm: 0.5000
[280/299] timecost: 61.53, lr: 0.000014, Train: (LOSS: 0.0032, MAE: 0.0032, RMSE: 0.0071, R2: 0.9983), Valid: (LOSS: 0.0389, MAE: 0.0389, RMSE: 0.0573, R2: 0.9514), PNorm: 189.3040, GNorm: 0.5000
[281/299] timecost: 60.93, lr: 0.000014, Train: (LOSS: 0.0032, MAE: 0.0032, RMSE: 0.0069, R2: 0.9984), Valid: (LOSS: 0.0387, MAE: 0.0387, RMSE: 0.0573, R2: 0.9515), PNorm: 189.3042, GNorm: 0.5000
[282/299] timecost: 60.42, lr: 0.000014, Train: (LOSS: 0.0032, MAE: 0.0032, RMSE: 0.0070, R2: 0.9974), Valid: (LOSS: 0.0387, MAE: 0.0387, RMSE: 0.0571, R2: 0.9519), PNorm: 189.3049, GNorm: 0.4448
[283/299] timecost: 60.13, lr: 0.000014, Train: (LOSS: 0.0031, MAE: 0.0031, RMSE: 0.0069, R2: 0.9975), Valid: (LOSS: 0.0390, MAE: 0.0390, RMSE: 0.0577, R2: 0.9510), PNorm: 189.3055, GNorm: 0.3739
[284/299] timecost: 60.30, lr: 0.000014, Train: (LOSS: 0.0031, MAE: 0.0031, RMSE: 0.0068, R2: 0.9982), Valid: (LOSS: 0.0389, MAE: 0.0389, RMSE: 0.0579, R2: 0.9507), PNorm: 189.3062, GNorm: 0.4059
[285/299] timecost: 59.89, lr: 0.000014, Train: (LOSS: 0.0032, MAE: 0.0032, RMSE: 0.0070, R2: 0.9982), Valid: (LOSS: 0.0387, MAE: 0.0387, RMSE: 0.0574, R2: 0.9513), PNorm: 189.3065, GNorm: 0.3050
[286/299] timecost: 59.88, lr: 0.000014, Train: (LOSS: 0.0032, MAE: 0.0032, RMSE: 0.0069, R2: 0.9982), Valid: (LOSS: 0.0387, MAE: 0.0387, RMSE: 0.0577, R2: 0.9509), PNorm: 189.3069, GNorm: 0.4821
Epoch 00288: reducing learning rate of group 0 to 1.2091e-05.
[287/299] timecost: 63.45, lr: 0.000012, Train: (LOSS: 0.0032, MAE: 0.0032, RMSE: 0.0068, R2: 0.9983), Valid: (LOSS: 0.0388, MAE: 0.0388, RMSE: 0.0576, R2: 0.9512), PNorm: 189.3074, GNorm: 0.4165
[288/299] timecost: 64.31, lr: 0.000012, Train: (LOSS: 0.0030, MAE: 0.0030, RMSE: 0.0067, R2: 0.9978), Valid: (LOSS: 0.0388, MAE: 0.0388, RMSE: 0.0574, R2: 0.9514), PNorm: 189.3080, GNorm: 0.5000
[289/299] timecost: 64.68, lr: 0.000012, Train: (LOSS: 0.0031, MAE: 0.0031, RMSE: 0.0068, R2: 0.9983), Valid: (LOSS: 0.0387, MAE: 0.0387, RMSE: 0.0576, R2: 0.9511), PNorm: 189.3082, GNorm: 0.3943
[290/299] timecost: 63.92, lr: 0.000012, Train: (LOSS: 0.0029, MAE: 0.0029, RMSE: 0.0066, R2: 0.9980), Valid: (LOSS: 0.0389, MAE: 0.0389, RMSE: 0.0576, R2: 0.9509), PNorm: 189.3090, GNorm: 0.5000
[291/299] timecost: 64.86, lr: 0.000012, Train: (LOSS: 0.0029, MAE: 0.0029, RMSE: 0.0066, R2: 0.9981), Valid: (LOSS: 0.0389, MAE: 0.0389, RMSE: 0.0573, R2: 0.9514), PNorm: 189.3094, GNorm: 0.5000
[292/299] timecost: 63.93, lr: 0.000012, Train: (LOSS: 0.0030, MAE: 0.0030, RMSE: 0.0067, R2: 0.9982), Valid: (LOSS: 0.0388, MAE: 0.0388, RMSE: 0.0574, R2: 0.9513), PNorm: 189.3099, GNorm: 0.4465
[293/299] timecost: 63.89, lr: 0.000012, Train: (LOSS: 0.0029, MAE: 0.0029, RMSE: 0.0066, R2: 0.9979), Valid: (LOSS: 0.0388, MAE: 0.0388, RMSE: 0.0570, R2: 0.9520), PNorm: 189.3103, GNorm: 0.3714
[294/299] timecost: 59.89, lr: 0.000012, Train: (LOSS: 0.0029, MAE: 0.0029, RMSE: 0.0066, R2: 0.9982), Valid: (LOSS: 0.0390, MAE: 0.0390, RMSE: 0.0572, R2: 0.9516), PNorm: 189.3107, GNorm: 0.3171
[295/299] timecost: 59.47, lr: 0.000012, Train: (LOSS: 0.0030, MAE: 0.0030, RMSE: 0.0068, R2: 0.9978), Valid: (LOSS: 0.0390, MAE: 0.0390, RMSE: 0.0576, R2: 0.9511), PNorm: 189.3111, GNorm: 0.4130
[296/299] timecost: 60.38, lr: 0.000012, Train: (LOSS: 0.0030, MAE: 0.0030, RMSE: 0.0068, R2: 0.9980), Valid: (LOSS: 0.0391, MAE: 0.0391, RMSE: 0.0575, R2: 0.9513), PNorm: 189.3110, GNorm: 0.4179
[297/299] timecost: 64.65, lr: 0.000012, Train: (LOSS: 0.0029, MAE: 0.0029, RMSE: 0.0065, R2: 0.9984), Valid: (LOSS: 0.0392, MAE: 0.0392, RMSE: 0.0577, R2: 0.9507), PNorm: 189.3118, GNorm: 0.5000
[298/299] timecost: 63.92, lr: 0.000012, Train: (LOSS: 0.0029, MAE: 0.0029, RMSE: 0.0066, R2: 0.9980), Valid: (LOSS: 0.0389, MAE: 0.0389, RMSE: 0.0576, R2: 0.9510), PNorm: 189.3120, GNorm: 0.3944
[299/299] timecost: 63.61, lr: 0.000012, Train: (LOSS: 0.0029, MAE: 0.0029, RMSE: 0.0066, R2: 0.9980), Valid: (LOSS: 0.0389, MAE: 0.0389, RMSE: 0.0576, R2: 0.9512), PNorm: 189.3123, GNorm: 0.3011
==========Training End==========
==========Test Best Model==========
================Final Results=======================
mse: 0.0344 +- 0.0000:
rmse: 0.0517 +- 0.0000:
mae: 0.0344 +- 0.0000:
r2: 0.9582 +- 0.0000:
tensor([[0.1019, 0.1066],
        [0.0000, 0.0000],
        [0.0000, 0.0000],
        ...,
        [0.0164, 0.0144],
        [0.4934, 0.4354],
        [0.0000, 0.0000]], device='cuda:0')
