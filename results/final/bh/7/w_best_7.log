cuda available with GPU: Tesla V100-PCIE-16GB
==========Load Seed==========
set_random_seed
0
==========Training Start==========
Training Graphs:  2491
Valid Graphs:  277
Test Graphs:  1187
============Loading pretrained weights to generate initialization============
============Creating new layers============
============Creating Model============
Training Graphs Batches:  78
Valid Graphs Batches:  9
Test Graphs Batches:  37
[0/299] timecost: 64.35, lr: 0.000030, Train: (LOSS: 0.2376, MAE: 0.2376, RMSE: 0.2834, R2: -0.1522), Valid: (LOSS: 0.3047, MAE: 0.3047, RMSE: 0.3802, R2: -1.1000), PNorm: 174.3936, GNorm: 1.5220
[1/299] timecost: 63.75, lr: 0.000030, Train: (LOSS: 0.2321, MAE: 0.2321, RMSE: 0.2805, R2: -0.1171), Valid: (LOSS: 0.2101, MAE: 0.2101, RMSE: 0.2494, R2: 0.0996), PNorm: 173.8859, GNorm: 0.1869
[2/299] timecost: 63.53, lr: 0.000030, Train: (LOSS: 0.2150, MAE: 0.2150, RMSE: 0.2593, R2: 0.0423), Valid: (LOSS: 0.2155, MAE: 0.2155, RMSE: 0.2669, R2: -0.0388), PNorm: 173.3831, GNorm: 1.8847
[3/299] timecost: 63.74, lr: 0.000030, Train: (LOSS: 0.2001, MAE: 0.2001, RMSE: 0.2439, R2: 0.1448), Valid: (LOSS: 0.2295, MAE: 0.2295, RMSE: 0.2641, R2: -0.0111), PNorm: 172.9203, GNorm: 2.2966
[4/299] timecost: 63.82, lr: 0.000030, Train: (LOSS: 0.1866, MAE: 0.1866, RMSE: 0.2322, R2: 0.2201), Valid: (LOSS: 0.1632, MAE: 0.1632, RMSE: 0.2037, R2: 0.3913), PNorm: 172.5073, GNorm: 2.0050
[5/299] timecost: 63.49, lr: 0.000030, Train: (LOSS: 0.1597, MAE: 0.1597, RMSE: 0.2068, R2: 0.3737), Valid: (LOSS: 0.1395, MAE: 0.1395, RMSE: 0.1825, R2: 0.5017), PNorm: 172.1338, GNorm: 3.0750
[6/299] timecost: 63.80, lr: 0.000030, Train: (LOSS: 0.1386, MAE: 0.1386, RMSE: 0.1859, R2: 0.4817), Valid: (LOSS: 0.1352, MAE: 0.1352, RMSE: 0.1767, R2: 0.5347), PNorm: 171.7687, GNorm: 2.2817
[7/299] timecost: 64.43, lr: 0.000030, Train: (LOSS: 0.1342, MAE: 0.1342, RMSE: 0.1815, R2: 0.5223), Valid: (LOSS: 0.1388, MAE: 0.1388, RMSE: 0.1799, R2: 0.5188), PNorm: 171.4123, GNorm: 0.9893
[8/299] timecost: 64.42, lr: 0.000030, Train: (LOSS: 0.1437, MAE: 0.1437, RMSE: 0.1931, R2: 0.4482), Valid: (LOSS: 0.1423, MAE: 0.1423, RMSE: 0.1866, R2: 0.4877), PNorm: 171.0665, GNorm: 3.4868
[9/299] timecost: 64.47, lr: 0.000030, Train: (LOSS: 0.1276, MAE: 0.1276, RMSE: 0.1753, R2: 0.5521), Valid: (LOSS: 0.1419, MAE: 0.1419, RMSE: 0.1899, R2: 0.4620), PNorm: 170.7244, GNorm: 4.9550
[10/299] timecost: 64.15, lr: 0.000030, Train: (LOSS: 0.1244, MAE: 0.1244, RMSE: 0.1733, R2: 0.5604), Valid: (LOSS: 0.1384, MAE: 0.1384, RMSE: 0.1868, R2: 0.4753), PNorm: 170.3877, GNorm: 1.4573
[11/299] timecost: 66.15, lr: 0.000030, Train: (LOSS: 0.1213, MAE: 0.1213, RMSE: 0.1699, R2: 0.5730), Valid: (LOSS: 0.1304, MAE: 0.1304, RMSE: 0.1810, R2: 0.5106), PNorm: 170.0576, GNorm: 6.5913
[12/299] timecost: 67.14, lr: 0.000030, Train: (LOSS: 0.1219, MAE: 0.1219, RMSE: 0.1699, R2: 0.5749), Valid: (LOSS: 0.1331, MAE: 0.1331, RMSE: 0.1850, R2: 0.4860), PNorm: 169.7363, GNorm: 1.9042
[13/299] timecost: 66.83, lr: 0.000030, Train: (LOSS: 0.1183, MAE: 0.1183, RMSE: 0.1662, R2: 0.5898), Valid: (LOSS: 0.1274, MAE: 0.1274, RMSE: 0.1762, R2: 0.5367), PNorm: 169.4231, GNorm: 4.2320
[14/299] timecost: 66.65, lr: 0.000030, Train: (LOSS: 0.1206, MAE: 0.1206, RMSE: 0.1697, R2: 0.5664), Valid: (LOSS: 0.1251, MAE: 0.1251, RMSE: 0.1738, R2: 0.5456), PNorm: 169.1172, GNorm: 2.5276
[15/299] timecost: 66.42, lr: 0.000030, Train: (LOSS: 0.1111, MAE: 0.1111, RMSE: 0.1609, R2: 0.6174), Valid: (LOSS: 0.1197, MAE: 0.1197, RMSE: 0.1673, R2: 0.5789), PNorm: 168.8162, GNorm: 0.8301
[16/299] timecost: 66.82, lr: 0.000030, Train: (LOSS: 0.1087, MAE: 0.1087, RMSE: 0.1575, R2: 0.6331), Valid: (LOSS: 0.1262, MAE: 0.1262, RMSE: 0.1771, R2: 0.5324), PNorm: 168.5209, GNorm: 2.9582
[17/299] timecost: 67.56, lr: 0.000030, Train: (LOSS: 0.1051, MAE: 0.1051, RMSE: 0.1524, R2: 0.6483), Valid: (LOSS: 0.1209, MAE: 0.1209, RMSE: 0.1720, R2: 0.5562), PNorm: 168.2315, GNorm: 1.8752
[18/299] timecost: 67.74, lr: 0.000030, Train: (LOSS: 0.1037, MAE: 0.1037, RMSE: 0.1490, R2: 0.6630), Valid: (LOSS: 0.1222, MAE: 0.1222, RMSE: 0.1726, R2: 0.5557), PNorm: 167.9482, GNorm: 2.2468
[19/299] timecost: 67.70, lr: 0.000030, Train: (LOSS: 0.1062, MAE: 0.1062, RMSE: 0.1546, R2: 0.6462), Valid: (LOSS: 0.1150, MAE: 0.1150, RMSE: 0.1599, R2: 0.6248), PNorm: 167.6716, GNorm: 1.3844
[20/299] timecost: 67.11, lr: 0.000030, Train: (LOSS: 0.1026, MAE: 0.1026, RMSE: 0.1503, R2: 0.6545), Valid: (LOSS: 0.1115, MAE: 0.1115, RMSE: 0.1544, R2: 0.6509), PNorm: 167.3988, GNorm: 1.9032
[21/299] timecost: 67.47, lr: 0.000030, Train: (LOSS: 0.0988, MAE: 0.0988, RMSE: 0.1452, R2: 0.6816), Valid: (LOSS: 0.1131, MAE: 0.1131, RMSE: 0.1664, R2: 0.5954), PNorm: 167.1298, GNorm: 3.8214
[22/299] timecost: 66.87, lr: 0.000030, Train: (LOSS: 0.0959, MAE: 0.0959, RMSE: 0.1402, R2: 0.7040), Valid: (LOSS: 0.1106, MAE: 0.1106, RMSE: 0.1580, R2: 0.6333), PNorm: 166.8640, GNorm: 1.1278
[23/299] timecost: 67.60, lr: 0.000030, Train: (LOSS: 0.0966, MAE: 0.0966, RMSE: 0.1436, R2: 0.6864), Valid: (LOSS: 0.1168, MAE: 0.1168, RMSE: 0.1614, R2: 0.6189), PNorm: 166.6030, GNorm: 2.1285
[24/299] timecost: 67.86, lr: 0.000030, Train: (LOSS: 0.0926, MAE: 0.0926, RMSE: 0.1381, R2: 0.7104), Valid: (LOSS: 0.1062, MAE: 0.1062, RMSE: 0.1547, R2: 0.6458), PNorm: 166.3467, GNorm: 2.6255
[25/299] timecost: 66.97, lr: 0.000030, Train: (LOSS: 0.0886, MAE: 0.0886, RMSE: 0.1339, R2: 0.7329), Valid: (LOSS: 0.0985, MAE: 0.0985, RMSE: 0.1439, R2: 0.6940), PNorm: 166.0941, GNorm: 1.1170
[26/299] timecost: 66.70, lr: 0.000030, Train: (LOSS: 0.0856, MAE: 0.0856, RMSE: 0.1298, R2: 0.7498), Valid: (LOSS: 0.0977, MAE: 0.0977, RMSE: 0.1441, R2: 0.6913), PNorm: 165.8454, GNorm: 4.2450
[27/299] timecost: 66.68, lr: 0.000030, Train: (LOSS: 0.0866, MAE: 0.0866, RMSE: 0.1303, R2: 0.7390), Valid: (LOSS: 0.0982, MAE: 0.0982, RMSE: 0.1475, R2: 0.6783), PNorm: 165.6002, GNorm: 1.6761
[28/299] timecost: 65.43, lr: 0.000030, Train: (LOSS: 0.0874, MAE: 0.0874, RMSE: 0.1312, R2: 0.7380), Valid: (LOSS: 0.1061, MAE: 0.1061, RMSE: 0.1494, R2: 0.6656), PNorm: 165.3590, GNorm: 1.4658
[29/299] timecost: 63.72, lr: 0.000030, Train: (LOSS: 0.0814, MAE: 0.0814, RMSE: 0.1242, R2: 0.7649), Valid: (LOSS: 0.0923, MAE: 0.0923, RMSE: 0.1352, R2: 0.7319), PNorm: 165.1194, GNorm: 1.7121
[30/299] timecost: 63.90, lr: 0.000030, Train: (LOSS: 0.0782, MAE: 0.0782, RMSE: 0.1194, R2: 0.7884), Valid: (LOSS: 0.0911, MAE: 0.0911, RMSE: 0.1351, R2: 0.7301), PNorm: 164.8821, GNorm: 1.6225
[31/299] timecost: 63.30, lr: 0.000030, Train: (LOSS: 0.0770, MAE: 0.0770, RMSE: 0.1202, R2: 0.7788), Valid: (LOSS: 0.0924, MAE: 0.0924, RMSE: 0.1368, R2: 0.7170), PNorm: 164.6487, GNorm: 1.0375
[32/299] timecost: 63.11, lr: 0.000030, Train: (LOSS: 0.0757, MAE: 0.0757, RMSE: 0.1163, R2: 0.7959), Valid: (LOSS: 0.1009, MAE: 0.1009, RMSE: 0.1471, R2: 0.6831), PNorm: 164.4186, GNorm: 1.5035
[33/299] timecost: 63.34, lr: 0.000030, Train: (LOSS: 0.0729, MAE: 0.0729, RMSE: 0.1121, R2: 0.8059), Valid: (LOSS: 0.0943, MAE: 0.0943, RMSE: 0.1361, R2: 0.7235), PNorm: 164.1904, GNorm: 1.5896
[34/299] timecost: 63.35, lr: 0.000030, Train: (LOSS: 0.0728, MAE: 0.0728, RMSE: 0.1110, R2: 0.8116), Valid: (LOSS: 0.0884, MAE: 0.0884, RMSE: 0.1322, R2: 0.7398), PNorm: 163.9648, GNorm: 1.6675
[35/299] timecost: 63.18, lr: 0.000030, Train: (LOSS: 0.0748, MAE: 0.0748, RMSE: 0.1152, R2: 0.7941), Valid: (LOSS: 0.0955, MAE: 0.0955, RMSE: 0.1392, R2: 0.7110), PNorm: 163.7438, GNorm: 1.5248
[36/299] timecost: 63.38, lr: 0.000030, Train: (LOSS: 0.0703, MAE: 0.0703, RMSE: 0.1082, R2: 0.8202), Valid: (LOSS: 0.0869, MAE: 0.0869, RMSE: 0.1311, R2: 0.7436), PNorm: 163.5240, GNorm: 1.6303
[37/299] timecost: 63.57, lr: 0.000030, Train: (LOSS: 0.0682, MAE: 0.0682, RMSE: 0.1074, R2: 0.8230), Valid: (LOSS: 0.0877, MAE: 0.0877, RMSE: 0.1300, R2: 0.7470), PNorm: 163.3055, GNorm: 2.0120
[38/299] timecost: 63.41, lr: 0.000030, Train: (LOSS: 0.0676, MAE: 0.0676, RMSE: 0.1060, R2: 0.8298), Valid: (LOSS: 0.0844, MAE: 0.0844, RMSE: 0.1303, R2: 0.7411), PNorm: 163.0905, GNorm: 2.2161
[39/299] timecost: 63.36, lr: 0.000030, Train: (LOSS: 0.0675, MAE: 0.0675, RMSE: 0.1037, R2: 0.8363), Valid: (LOSS: 0.0827, MAE: 0.0827, RMSE: 0.1253, R2: 0.7602), PNorm: 162.8774, GNorm: 0.9038
[40/299] timecost: 63.24, lr: 0.000030, Train: (LOSS: 0.0676, MAE: 0.0676, RMSE: 0.1054, R2: 0.8264), Valid: (LOSS: 0.0818, MAE: 0.0818, RMSE: 0.1244, R2: 0.7625), PNorm: 162.6658, GNorm: 1.9971
[41/299] timecost: 63.24, lr: 0.000030, Train: (LOSS: 0.0662, MAE: 0.0662, RMSE: 0.1031, R2: 0.8315), Valid: (LOSS: 0.0844, MAE: 0.0844, RMSE: 0.1314, R2: 0.7324), PNorm: 162.4570, GNorm: 2.9061
[42/299] timecost: 63.40, lr: 0.000030, Train: (LOSS: 0.0638, MAE: 0.0638, RMSE: 0.0996, R2: 0.8445), Valid: (LOSS: 0.0773, MAE: 0.0773, RMSE: 0.1199, R2: 0.7788), PNorm: 162.2490, GNorm: 1.9736
[43/299] timecost: 63.18, lr: 0.000030, Train: (LOSS: 0.0654, MAE: 0.0654, RMSE: 0.1016, R2: 0.8408), Valid: (LOSS: 0.0828, MAE: 0.0828, RMSE: 0.1254, R2: 0.7587), PNorm: 162.0439, GNorm: 2.5368
[44/299] timecost: 65.29, lr: 0.000030, Train: (LOSS: 0.0601, MAE: 0.0601, RMSE: 0.0945, R2: 0.8573), Valid: (LOSS: 0.0751, MAE: 0.0751, RMSE: 0.1153, R2: 0.7971), PNorm: 161.8404, GNorm: 2.6325
[45/299] timecost: 65.55, lr: 0.000030, Train: (LOSS: 0.0594, MAE: 0.0594, RMSE: 0.0942, R2: 0.8646), Valid: (LOSS: 0.0786, MAE: 0.0786, RMSE: 0.1230, R2: 0.7671), PNorm: 161.6382, GNorm: 1.4665
[46/299] timecost: 65.15, lr: 0.000030, Train: (LOSS: 0.0598, MAE: 0.0598, RMSE: 0.0940, R2: 0.8656), Valid: (LOSS: 0.0761, MAE: 0.0761, RMSE: 0.1138, R2: 0.8061), PNorm: 161.4378, GNorm: 1.6472
[47/299] timecost: 65.34, lr: 0.000030, Train: (LOSS: 0.0587, MAE: 0.0587, RMSE: 0.0925, R2: 0.8664), Valid: (LOSS: 0.0740, MAE: 0.0740, RMSE: 0.1158, R2: 0.7930), PNorm: 161.2390, GNorm: 1.5897
[48/299] timecost: 65.53, lr: 0.000030, Train: (LOSS: 0.0560, MAE: 0.0560, RMSE: 0.0876, R2: 0.8778), Valid: (LOSS: 0.0739, MAE: 0.0739, RMSE: 0.1142, R2: 0.7942), PNorm: 161.0413, GNorm: 1.2872
[49/299] timecost: 65.20, lr: 0.000030, Train: (LOSS: 0.0555, MAE: 0.0555, RMSE: 0.0872, R2: 0.8797), Valid: (LOSS: 0.0683, MAE: 0.0683, RMSE: 0.1065, R2: 0.8211), PNorm: 160.8446, GNorm: 1.7837
[50/299] timecost: 65.63, lr: 0.000030, Train: (LOSS: 0.0531, MAE: 0.0531, RMSE: 0.0837, R2: 0.8919), Valid: (LOSS: 0.0689, MAE: 0.0689, RMSE: 0.1014, R2: 0.8364), PNorm: 160.6486, GNorm: 1.6408
[51/299] timecost: 65.67, lr: 0.000030, Train: (LOSS: 0.0518, MAE: 0.0518, RMSE: 0.0822, R2: 0.8935), Valid: (LOSS: 0.0648, MAE: 0.0648, RMSE: 0.1008, R2: 0.8412), PNorm: 160.4539, GNorm: 1.6663
[52/299] timecost: 65.65, lr: 0.000030, Train: (LOSS: 0.0512, MAE: 0.0512, RMSE: 0.0818, R2: 0.8952), Valid: (LOSS: 0.0681, MAE: 0.0681, RMSE: 0.0998, R2: 0.8465), PNorm: 160.2605, GNorm: 1.6933
[53/299] timecost: 65.53, lr: 0.000030, Train: (LOSS: 0.0497, MAE: 0.0497, RMSE: 0.0795, R2: 0.8985), Valid: (LOSS: 0.0739, MAE: 0.0739, RMSE: 0.1103, R2: 0.8111), PNorm: 160.0674, GNorm: 1.8282
[54/299] timecost: 65.36, lr: 0.000030, Train: (LOSS: 0.0491, MAE: 0.0491, RMSE: 0.0758, R2: 0.9067), Valid: (LOSS: 0.0657, MAE: 0.0657, RMSE: 0.0960, R2: 0.8624), PNorm: 159.8754, GNorm: 1.0960
[55/299] timecost: 65.26, lr: 0.000030, Train: (LOSS: 0.0471, MAE: 0.0471, RMSE: 0.0756, R2: 0.9109), Valid: (LOSS: 0.0624, MAE: 0.0624, RMSE: 0.0957, R2: 0.8554), PNorm: 159.6835, GNorm: 0.9145
[56/299] timecost: 65.17, lr: 0.000030, Train: (LOSS: 0.0465, MAE: 0.0465, RMSE: 0.0755, R2: 0.9064), Valid: (LOSS: 0.0610, MAE: 0.0610, RMSE: 0.0959, R2: 0.8525), PNorm: 159.4916, GNorm: 2.3498
[57/299] timecost: 65.23, lr: 0.000030, Train: (LOSS: 0.0466, MAE: 0.0466, RMSE: 0.0765, R2: 0.9018), Valid: (LOSS: 0.0609, MAE: 0.0609, RMSE: 0.0976, R2: 0.8501), PNorm: 159.3007, GNorm: 2.6610
[58/299] timecost: 65.29, lr: 0.000030, Train: (LOSS: 0.0446, MAE: 0.0446, RMSE: 0.0712, R2: 0.9175), Valid: (LOSS: 0.0650, MAE: 0.0650, RMSE: 0.0990, R2: 0.8494), PNorm: 159.1097, GNorm: 0.9145
[59/299] timecost: 65.70, lr: 0.000030, Train: (LOSS: 0.0448, MAE: 0.0448, RMSE: 0.0711, R2: 0.9172), Valid: (LOSS: 0.0593, MAE: 0.0593, RMSE: 0.0904, R2: 0.8713), PNorm: 158.9194, GNorm: 1.2788
[60/299] timecost: 65.22, lr: 0.000030, Train: (LOSS: 0.0441, MAE: 0.0441, RMSE: 0.0708, R2: 0.9167), Valid: (LOSS: 0.0636, MAE: 0.0636, RMSE: 0.0937, R2: 0.8634), PNorm: 158.7294, GNorm: 1.4191
[61/299] timecost: 65.61, lr: 0.000030, Train: (LOSS: 0.0427, MAE: 0.0427, RMSE: 0.0694, R2: 0.9228), Valid: (LOSS: 0.0614, MAE: 0.0614, RMSE: 0.0969, R2: 0.8504), PNorm: 158.5388, GNorm: 1.5754
[62/299] timecost: 63.80, lr: 0.000030, Train: (LOSS: 0.0424, MAE: 0.0424, RMSE: 0.0686, R2: 0.9243), Valid: (LOSS: 0.0607, MAE: 0.0607, RMSE: 0.0945, R2: 0.8594), PNorm: 158.3474, GNorm: 1.1216
[63/299] timecost: 63.20, lr: 0.000030, Train: (LOSS: 0.0414, MAE: 0.0414, RMSE: 0.0671, R2: 0.9255), Valid: (LOSS: 0.0561, MAE: 0.0561, RMSE: 0.0856, R2: 0.8885), PNorm: 158.1566, GNorm: 1.2273
[64/299] timecost: 65.12, lr: 0.000030, Train: (LOSS: 0.0405, MAE: 0.0405, RMSE: 0.0655, R2: 0.9316), Valid: (LOSS: 0.0608, MAE: 0.0608, RMSE: 0.0954, R2: 0.8609), PNorm: 157.9656, GNorm: 0.9335
[65/299] timecost: 64.34, lr: 0.000030, Train: (LOSS: 0.0404, MAE: 0.0404, RMSE: 0.0652, R2: 0.9286), Valid: (LOSS: 0.0649, MAE: 0.0649, RMSE: 0.1023, R2: 0.8368), PNorm: 157.7742, GNorm: 1.1097
[66/299] timecost: 64.61, lr: 0.000030, Train: (LOSS: 0.0417, MAE: 0.0417, RMSE: 0.0673, R2: 0.9268), Valid: (LOSS: 0.0608, MAE: 0.0608, RMSE: 0.0932, R2: 0.8630), PNorm: 157.5837, GNorm: 1.1387
[67/299] timecost: 64.56, lr: 0.000030, Train: (LOSS: 0.0396, MAE: 0.0396, RMSE: 0.0628, R2: 0.9330), Valid: (LOSS: 0.0567, MAE: 0.0567, RMSE: 0.0839, R2: 0.8930), PNorm: 157.3910, GNorm: 1.3482
[68/299] timecost: 63.94, lr: 0.000030, Train: (LOSS: 0.0382, MAE: 0.0382, RMSE: 0.0616, R2: 0.9355), Valid: (LOSS: 0.0620, MAE: 0.0620, RMSE: 0.0900, R2: 0.8747), PNorm: 157.1977, GNorm: 1.2541
[69/299] timecost: 65.20, lr: 0.000030, Train: (LOSS: 0.0381, MAE: 0.0381, RMSE: 0.0617, R2: 0.9369), Valid: (LOSS: 0.0547, MAE: 0.0547, RMSE: 0.0818, R2: 0.8948), PNorm: 157.0038, GNorm: 1.1659
[70/299] timecost: 65.31, lr: 0.000030, Train: (LOSS: 0.0381, MAE: 0.0381, RMSE: 0.0621, R2: 0.9384), Valid: (LOSS: 0.0554, MAE: 0.0554, RMSE: 0.0843, R2: 0.8881), PNorm: 156.8093, GNorm: 1.0612
[71/299] timecost: 65.43, lr: 0.000030, Train: (LOSS: 0.0376, MAE: 0.0376, RMSE: 0.0603, R2: 0.9395), Valid: (LOSS: 0.0554, MAE: 0.0554, RMSE: 0.0813, R2: 0.8988), PNorm: 156.6149, GNorm: 1.7452
[72/299] timecost: 65.38, lr: 0.000030, Train: (LOSS: 0.0363, MAE: 0.0363, RMSE: 0.0590, R2: 0.9421), Valid: (LOSS: 0.0558, MAE: 0.0558, RMSE: 0.0855, R2: 0.8865), PNorm: 156.4193, GNorm: 1.3148
[73/299] timecost: 65.59, lr: 0.000030, Train: (LOSS: 0.0369, MAE: 0.0369, RMSE: 0.0587, R2: 0.9423), Valid: (LOSS: 0.0550, MAE: 0.0550, RMSE: 0.0813, R2: 0.8969), PNorm: 156.2223, GNorm: 1.5527
[74/299] timecost: 65.56, lr: 0.000030, Train: (LOSS: 0.0361, MAE: 0.0361, RMSE: 0.0587, R2: 0.9445), Valid: (LOSS: 0.0509, MAE: 0.0509, RMSE: 0.0764, R2: 0.9075), PNorm: 156.0258, GNorm: 1.3167
[75/299] timecost: 65.40, lr: 0.000030, Train: (LOSS: 0.0355, MAE: 0.0355, RMSE: 0.0571, R2: 0.9444), Valid: (LOSS: 0.0520, MAE: 0.0520, RMSE: 0.0758, R2: 0.9096), PNorm: 155.8278, GNorm: 1.6685
[76/299] timecost: 65.29, lr: 0.000030, Train: (LOSS: 0.0351, MAE: 0.0351, RMSE: 0.0570, R2: 0.9444), Valid: (LOSS: 0.0518, MAE: 0.0518, RMSE: 0.0778, R2: 0.9047), PNorm: 155.6288, GNorm: 1.1206
[77/299] timecost: 65.25, lr: 0.000030, Train: (LOSS: 0.0338, MAE: 0.0338, RMSE: 0.0558, R2: 0.9493), Valid: (LOSS: 0.0492, MAE: 0.0492, RMSE: 0.0729, R2: 0.9155), PNorm: 155.4288, GNorm: 1.0649
[78/299] timecost: 65.32, lr: 0.000030, Train: (LOSS: 0.0333, MAE: 0.0333, RMSE: 0.0539, R2: 0.9492), Valid: (LOSS: 0.0536, MAE: 0.0536, RMSE: 0.0848, R2: 0.8920), PNorm: 155.2281, GNorm: 1.0769
[79/299] timecost: 65.92, lr: 0.000030, Train: (LOSS: 0.0336, MAE: 0.0336, RMSE: 0.0543, R2: 0.9502), Valid: (LOSS: 0.0532, MAE: 0.0532, RMSE: 0.0760, R2: 0.9092), PNorm: 155.0272, GNorm: 1.3633
[80/299] timecost: 65.55, lr: 0.000030, Train: (LOSS: 0.0326, MAE: 0.0326, RMSE: 0.0533, R2: 0.9516), Valid: (LOSS: 0.0494, MAE: 0.0494, RMSE: 0.0750, R2: 0.9117), PNorm: 154.8243, GNorm: 1.3609
[81/299] timecost: 66.38, lr: 0.000030, Train: (LOSS: 0.0322, MAE: 0.0322, RMSE: 0.0526, R2: 0.9508), Valid: (LOSS: 0.0504, MAE: 0.0504, RMSE: 0.0772, R2: 0.9091), PNorm: 154.6212, GNorm: 1.1164
[82/299] timecost: 65.68, lr: 0.000030, Train: (LOSS: 0.0336, MAE: 0.0336, RMSE: 0.0546, R2: 0.9513), Valid: (LOSS: 0.0528, MAE: 0.0528, RMSE: 0.0759, R2: 0.9135), PNorm: 154.4174, GNorm: 1.2926
[83/299] timecost: 65.26, lr: 0.000030, Train: (LOSS: 0.0344, MAE: 0.0344, RMSE: 0.0559, R2: 0.9457), Valid: (LOSS: 0.0534, MAE: 0.0534, RMSE: 0.0814, R2: 0.8996), PNorm: 154.2143, GNorm: 1.5032
[84/299] timecost: 65.11, lr: 0.000030, Train: (LOSS: 0.0394, MAE: 0.0394, RMSE: 0.0635, R2: 0.9255), Valid: (LOSS: 0.0524, MAE: 0.0524, RMSE: 0.0785, R2: 0.9051), PNorm: 154.0114, GNorm: 2.4860
[85/299] timecost: 65.27, lr: 0.000030, Train: (LOSS: 0.0328, MAE: 0.0328, RMSE: 0.0539, R2: 0.9509), Valid: (LOSS: 0.0482, MAE: 0.0482, RMSE: 0.0712, R2: 0.9204), PNorm: 153.8068, GNorm: 1.7240
[86/299] timecost: 64.82, lr: 0.000030, Train: (LOSS: 0.0319, MAE: 0.0319, RMSE: 0.0518, R2: 0.9545), Valid: (LOSS: 0.0517, MAE: 0.0517, RMSE: 0.0788, R2: 0.8995), PNorm: 153.6004, GNorm: 1.1405
[87/299] timecost: 63.03, lr: 0.000030, Train: (LOSS: 0.0300, MAE: 0.0300, RMSE: 0.0498, R2: 0.9570), Valid: (LOSS: 0.0479, MAE: 0.0479, RMSE: 0.0727, R2: 0.9177), PNorm: 153.3926, GNorm: 1.0621
[88/299] timecost: 62.97, lr: 0.000030, Train: (LOSS: 0.0310, MAE: 0.0310, RMSE: 0.0503, R2: 0.9573), Valid: (LOSS: 0.0497, MAE: 0.0497, RMSE: 0.0744, R2: 0.9144), PNorm: 153.1837, GNorm: 1.2893
[89/299] timecost: 62.92, lr: 0.000030, Train: (LOSS: 0.0324, MAE: 0.0324, RMSE: 0.0527, R2: 0.9538), Valid: (LOSS: 0.0497, MAE: 0.0497, RMSE: 0.0712, R2: 0.9206), PNorm: 152.9761, GNorm: 0.9722
[90/299] timecost: 63.17, lr: 0.000030, Train: (LOSS: 0.0295, MAE: 0.0295, RMSE: 0.0484, R2: 0.9603), Valid: (LOSS: 0.0451, MAE: 0.0451, RMSE: 0.0678, R2: 0.9282), PNorm: 152.7657, GNorm: 0.9649
[91/299] timecost: 62.92, lr: 0.000030, Train: (LOSS: 0.0289, MAE: 0.0289, RMSE: 0.0472, R2: 0.9618), Valid: (LOSS: 0.0496, MAE: 0.0496, RMSE: 0.0695, R2: 0.9258), PNorm: 152.5552, GNorm: 0.8122
[92/299] timecost: 62.91, lr: 0.000030, Train: (LOSS: 0.0292, MAE: 0.0292, RMSE: 0.0477, R2: 0.9609), Valid: (LOSS: 0.0494, MAE: 0.0494, RMSE: 0.0762, R2: 0.9105), PNorm: 152.3433, GNorm: 1.5542
[93/299] timecost: 63.11, lr: 0.000030, Train: (LOSS: 0.0295, MAE: 0.0295, RMSE: 0.0481, R2: 0.9591), Valid: (LOSS: 0.0450, MAE: 0.0450, RMSE: 0.0650, R2: 0.9348), PNorm: 152.1313, GNorm: 0.9796
[94/299] timecost: 62.69, lr: 0.000030, Train: (LOSS: 0.0291, MAE: 0.0291, RMSE: 0.0468, R2: 0.9623), Valid: (LOSS: 0.0462, MAE: 0.0462, RMSE: 0.0676, R2: 0.9294), PNorm: 151.9179, GNorm: 1.0066
[95/299] timecost: 62.82, lr: 0.000030, Train: (LOSS: 0.0287, MAE: 0.0287, RMSE: 0.0469, R2: 0.9642), Valid: (LOSS: 0.0490, MAE: 0.0490, RMSE: 0.0742, R2: 0.9164), PNorm: 151.7043, GNorm: 1.0160
[96/299] timecost: 63.17, lr: 0.000030, Train: (LOSS: 0.0303, MAE: 0.0303, RMSE: 0.0481, R2: 0.9612), Valid: (LOSS: 0.0477, MAE: 0.0477, RMSE: 0.0714, R2: 0.9200), PNorm: 151.4909, GNorm: 1.2942
[97/299] timecost: 63.36, lr: 0.000030, Train: (LOSS: 0.0297, MAE: 0.0297, RMSE: 0.0475, R2: 0.9611), Valid: (LOSS: 0.0472, MAE: 0.0472, RMSE: 0.0720, R2: 0.9184), PNorm: 151.2767, GNorm: 1.3038
[98/299] timecost: 63.70, lr: 0.000030, Train: (LOSS: 0.0278, MAE: 0.0278, RMSE: 0.0443, R2: 0.9653), Valid: (LOSS: 0.0456, MAE: 0.0456, RMSE: 0.0686, R2: 0.9270), PNorm: 151.0615, GNorm: 1.1544
[99/299] timecost: 64.07, lr: 0.000030, Train: (LOSS: 0.0277, MAE: 0.0277, RMSE: 0.0447, R2: 0.9653), Valid: (LOSS: 0.0487, MAE: 0.0487, RMSE: 0.0701, R2: 0.9234), PNorm: 150.8463, GNorm: 0.8344
[100/299] timecost: 63.89, lr: 0.000030, Train: (LOSS: 0.0275, MAE: 0.0275, RMSE: 0.0445, R2: 0.9642), Valid: (LOSS: 0.0483, MAE: 0.0483, RMSE: 0.0715, R2: 0.9205), PNorm: 150.6304, GNorm: 1.0816
[101/299] timecost: 63.33, lr: 0.000030, Train: (LOSS: 0.0273, MAE: 0.0273, RMSE: 0.0441, R2: 0.9657), Valid: (LOSS: 0.0481, MAE: 0.0481, RMSE: 0.0715, R2: 0.9209), PNorm: 150.4141, GNorm: 0.8082
[102/299] timecost: 63.60, lr: 0.000030, Train: (LOSS: 0.0257, MAE: 0.0257, RMSE: 0.0418, R2: 0.9700), Valid: (LOSS: 0.0479, MAE: 0.0479, RMSE: 0.0715, R2: 0.9192), PNorm: 150.1973, GNorm: 1.2405
[103/299] timecost: 63.62, lr: 0.000030, Train: (LOSS: 0.0272, MAE: 0.0272, RMSE: 0.0432, R2: 0.9647), Valid: (LOSS: 0.0469, MAE: 0.0469, RMSE: 0.0673, R2: 0.9287), PNorm: 149.9805, GNorm: 0.8220
[104/299] timecost: 63.94, lr: 0.000030, Train: (LOSS: 0.0265, MAE: 0.0265, RMSE: 0.0427, R2: 0.9691), Valid: (LOSS: 0.0457, MAE: 0.0457, RMSE: 0.0663, R2: 0.9307), PNorm: 149.7637, GNorm: 1.3678
[105/299] timecost: 63.27, lr: 0.000030, Train: (LOSS: 0.0273, MAE: 0.0273, RMSE: 0.0435, R2: 0.9666), Valid: (LOSS: 0.0449, MAE: 0.0449, RMSE: 0.0668, R2: 0.9301), PNorm: 149.5474, GNorm: 1.1474
[106/299] timecost: 62.76, lr: 0.000030, Train: (LOSS: 0.0260, MAE: 0.0260, RMSE: 0.0419, R2: 0.9701), Valid: (LOSS: 0.0464, MAE: 0.0464, RMSE: 0.0723, R2: 0.9172), PNorm: 149.3294, GNorm: 1.2166
[107/299] timecost: 63.09, lr: 0.000030, Train: (LOSS: 0.0254, MAE: 0.0254, RMSE: 0.0420, R2: 0.9683), Valid: (LOSS: 0.0520, MAE: 0.0520, RMSE: 0.0825, R2: 0.8934), PNorm: 149.1125, GNorm: 0.9430
[108/299] timecost: 62.87, lr: 0.000030, Train: (LOSS: 0.0260, MAE: 0.0260, RMSE: 0.0412, R2: 0.9698), Valid: (LOSS: 0.0455, MAE: 0.0455, RMSE: 0.0685, R2: 0.9260), PNorm: 148.8958, GNorm: 0.8184
[109/299] timecost: 62.96, lr: 0.000030, Train: (LOSS: 0.0256, MAE: 0.0256, RMSE: 0.0410, R2: 0.9700), Valid: (LOSS: 0.0449, MAE: 0.0449, RMSE: 0.0650, R2: 0.9329), PNorm: 148.6789, GNorm: 0.8871
[110/299] timecost: 62.89, lr: 0.000030, Train: (LOSS: 0.0268, MAE: 0.0268, RMSE: 0.0434, R2: 0.9664), Valid: (LOSS: 0.0494, MAE: 0.0494, RMSE: 0.0764, R2: 0.9077), PNorm: 148.4635, GNorm: 0.7077
Epoch 00112: reducing learning rate of group 0 to 2.7000e-05.
[111/299] timecost: 62.60, lr: 0.000027, Train: (LOSS: 0.0237, MAE: 0.0237, RMSE: 0.0383, R2: 0.9717), Valid: (LOSS: 0.0470, MAE: 0.0470, RMSE: 0.0703, R2: 0.9225), PNorm: 148.2477, GNorm: 1.0637
[112/299] timecost: 62.66, lr: 0.000027, Train: (LOSS: 0.0233, MAE: 0.0233, RMSE: 0.0372, R2: 0.9739), Valid: (LOSS: 0.0455, MAE: 0.0455, RMSE: 0.0665, R2: 0.9299), PNorm: 148.0531, GNorm: 1.2456
[113/299] timecost: 62.80, lr: 0.000027, Train: (LOSS: 0.0232, MAE: 0.0232, RMSE: 0.0377, R2: 0.9737), Valid: (LOSS: 0.0447, MAE: 0.0447, RMSE: 0.0677, R2: 0.9259), PNorm: 147.8575, GNorm: 1.4520
[114/299] timecost: 62.76, lr: 0.000027, Train: (LOSS: 0.0228, MAE: 0.0228, RMSE: 0.0371, R2: 0.9737), Valid: (LOSS: 0.0463, MAE: 0.0463, RMSE: 0.0698, R2: 0.9235), PNorm: 147.6622, GNorm: 1.5034
[115/299] timecost: 62.95, lr: 0.000027, Train: (LOSS: 0.0227, MAE: 0.0227, RMSE: 0.0362, R2: 0.9742), Valid: (LOSS: 0.0449, MAE: 0.0449, RMSE: 0.0681, R2: 0.9260), PNorm: 147.4671, GNorm: 1.2706
[116/299] timecost: 62.85, lr: 0.000027, Train: (LOSS: 0.0225, MAE: 0.0225, RMSE: 0.0363, R2: 0.9753), Valid: (LOSS: 0.0462, MAE: 0.0462, RMSE: 0.0709, R2: 0.9219), PNorm: 147.2713, GNorm: 0.9140
[117/299] timecost: 64.12, lr: 0.000027, Train: (LOSS: 0.0224, MAE: 0.0224, RMSE: 0.0366, R2: 0.9761), Valid: (LOSS: 0.0481, MAE: 0.0481, RMSE: 0.0715, R2: 0.9194), PNorm: 147.0760, GNorm: 1.0813
[118/299] timecost: 64.08, lr: 0.000027, Train: (LOSS: 0.0229, MAE: 0.0229, RMSE: 0.0365, R2: 0.9750), Valid: (LOSS: 0.0434, MAE: 0.0434, RMSE: 0.0661, R2: 0.9303), PNorm: 146.8811, GNorm: 1.1664
[119/299] timecost: 65.95, lr: 0.000027, Train: (LOSS: 0.0212, MAE: 0.0212, RMSE: 0.0358, R2: 0.9764), Valid: (LOSS: 0.0447, MAE: 0.0447, RMSE: 0.0698, R2: 0.9213), PNorm: 146.6856, GNorm: 1.0378
[120/299] timecost: 66.91, lr: 0.000027, Train: (LOSS: 0.0214, MAE: 0.0214, RMSE: 0.0355, R2: 0.9768), Valid: (LOSS: 0.0460, MAE: 0.0460, RMSE: 0.0682, R2: 0.9280), PNorm: 146.4904, GNorm: 1.0537
[121/299] timecost: 66.79, lr: 0.000027, Train: (LOSS: 0.0223, MAE: 0.0223, RMSE: 0.0364, R2: 0.9761), Valid: (LOSS: 0.0459, MAE: 0.0459, RMSE: 0.0693, R2: 0.9254), PNorm: 146.2966, GNorm: 1.0569
[122/299] timecost: 66.28, lr: 0.000027, Train: (LOSS: 0.0219, MAE: 0.0219, RMSE: 0.0351, R2: 0.9766), Valid: (LOSS: 0.0440, MAE: 0.0440, RMSE: 0.0691, R2: 0.9234), PNorm: 146.1023, GNorm: 0.8217
[123/299] timecost: 66.16, lr: 0.000027, Train: (LOSS: 0.0214, MAE: 0.0214, RMSE: 0.0344, R2: 0.9758), Valid: (LOSS: 0.0458, MAE: 0.0458, RMSE: 0.0699, R2: 0.9204), PNorm: 145.9089, GNorm: 1.2708
[124/299] timecost: 66.27, lr: 0.000027, Train: (LOSS: 0.0212, MAE: 0.0212, RMSE: 0.0345, R2: 0.9764), Valid: (LOSS: 0.0453, MAE: 0.0453, RMSE: 0.0695, R2: 0.9247), PNorm: 145.7164, GNorm: 1.4280
[125/299] timecost: 66.00, lr: 0.000027, Train: (LOSS: 0.0215, MAE: 0.0215, RMSE: 0.0355, R2: 0.9773), Valid: (LOSS: 0.0467, MAE: 0.0467, RMSE: 0.0692, R2: 0.9267), PNorm: 145.5241, GNorm: 1.6616
[126/299] timecost: 65.76, lr: 0.000027, Train: (LOSS: 0.0215, MAE: 0.0215, RMSE: 0.0353, R2: 0.9759), Valid: (LOSS: 0.0440, MAE: 0.0440, RMSE: 0.0684, R2: 0.9239), PNorm: 145.3324, GNorm: 1.2139
[127/299] timecost: 66.11, lr: 0.000027, Train: (LOSS: 0.0210, MAE: 0.0210, RMSE: 0.0346, R2: 0.9767), Valid: (LOSS: 0.0424, MAE: 0.0424, RMSE: 0.0639, R2: 0.9350), PNorm: 145.1426, GNorm: 0.8376
[128/299] timecost: 65.94, lr: 0.000027, Train: (LOSS: 0.0206, MAE: 0.0206, RMSE: 0.0334, R2: 0.9795), Valid: (LOSS: 0.0436, MAE: 0.0436, RMSE: 0.0658, R2: 0.9306), PNorm: 144.9524, GNorm: 1.1052
[129/299] timecost: 65.79, lr: 0.000027, Train: (LOSS: 0.0196, MAE: 0.0196, RMSE: 0.0318, R2: 0.9803), Valid: (LOSS: 0.0430, MAE: 0.0430, RMSE: 0.0657, R2: 0.9308), PNorm: 144.7631, GNorm: 0.7601
[130/299] timecost: 65.91, lr: 0.000027, Train: (LOSS: 0.0202, MAE: 0.0202, RMSE: 0.0335, R2: 0.9787), Valid: (LOSS: 0.0424, MAE: 0.0424, RMSE: 0.0626, R2: 0.9380), PNorm: 144.5749, GNorm: 0.9522
[131/299] timecost: 65.77, lr: 0.000027, Train: (LOSS: 0.0202, MAE: 0.0202, RMSE: 0.0327, R2: 0.9792), Valid: (LOSS: 0.0459, MAE: 0.0459, RMSE: 0.0669, R2: 0.9304), PNorm: 144.3878, GNorm: 0.8756
[132/299] timecost: 66.20, lr: 0.000027, Train: (LOSS: 0.0215, MAE: 0.0215, RMSE: 0.0351, R2: 0.9772), Valid: (LOSS: 0.0460, MAE: 0.0460, RMSE: 0.0701, R2: 0.9230), PNorm: 144.2027, GNorm: 1.0065
[133/299] timecost: 66.04, lr: 0.000027, Train: (LOSS: 0.0203, MAE: 0.0203, RMSE: 0.0326, R2: 0.9792), Valid: (LOSS: 0.0451, MAE: 0.0451, RMSE: 0.0668, R2: 0.9300), PNorm: 144.0185, GNorm: 0.8886
[134/299] timecost: 66.31, lr: 0.000027, Train: (LOSS: 0.0195, MAE: 0.0195, RMSE: 0.0322, R2: 0.9800), Valid: (LOSS: 0.0447, MAE: 0.0447, RMSE: 0.0668, R2: 0.9303), PNorm: 143.8352, GNorm: 0.8989
[135/299] timecost: 65.59, lr: 0.000027, Train: (LOSS: 0.0191, MAE: 0.0191, RMSE: 0.0316, R2: 0.9806), Valid: (LOSS: 0.0452, MAE: 0.0452, RMSE: 0.0690, R2: 0.9243), PNorm: 143.6533, GNorm: 1.2115
[136/299] timecost: 63.42, lr: 0.000027, Train: (LOSS: 0.0200, MAE: 0.0200, RMSE: 0.0330, R2: 0.9794), Valid: (LOSS: 0.0418, MAE: 0.0418, RMSE: 0.0628, R2: 0.9382), PNorm: 143.4724, GNorm: 1.1717
[137/299] timecost: 62.98, lr: 0.000027, Train: (LOSS: 0.0196, MAE: 0.0196, RMSE: 0.0320, R2: 0.9813), Valid: (LOSS: 0.0434, MAE: 0.0434, RMSE: 0.0666, R2: 0.9297), PNorm: 143.2937, GNorm: 0.7965
[138/299] timecost: 62.90, lr: 0.000027, Train: (LOSS: 0.0195, MAE: 0.0195, RMSE: 0.0314, R2: 0.9799), Valid: (LOSS: 0.0444, MAE: 0.0444, RMSE: 0.0672, R2: 0.9283), PNorm: 143.1155, GNorm: 0.8320
[139/299] timecost: 62.86, lr: 0.000027, Train: (LOSS: 0.0194, MAE: 0.0194, RMSE: 0.0321, R2: 0.9803), Valid: (LOSS: 0.0430, MAE: 0.0430, RMSE: 0.0678, R2: 0.9252), PNorm: 142.9388, GNorm: 0.9357
[140/299] timecost: 62.68, lr: 0.000027, Train: (LOSS: 0.0191, MAE: 0.0191, RMSE: 0.0315, R2: 0.9819), Valid: (LOSS: 0.0408, MAE: 0.0408, RMSE: 0.0603, R2: 0.9423), PNorm: 142.7639, GNorm: 1.1930
[141/299] timecost: 62.77, lr: 0.000027, Train: (LOSS: 0.0184, MAE: 0.0184, RMSE: 0.0310, R2: 0.9826), Valid: (LOSS: 0.0440, MAE: 0.0440, RMSE: 0.0693, R2: 0.9236), PNorm: 142.5907, GNorm: 1.1024
[142/299] timecost: 62.80, lr: 0.000027, Train: (LOSS: 0.0194, MAE: 0.0194, RMSE: 0.0313, R2: 0.9819), Valid: (LOSS: 0.0438, MAE: 0.0438, RMSE: 0.0662, R2: 0.9317), PNorm: 142.4192, GNorm: 0.8394
[143/299] timecost: 62.91, lr: 0.000027, Train: (LOSS: 0.0188, MAE: 0.0188, RMSE: 0.0313, R2: 0.9812), Valid: (LOSS: 0.0451, MAE: 0.0451, RMSE: 0.0709, R2: 0.9213), PNorm: 142.2492, GNorm: 0.7656
[144/299] timecost: 63.38, lr: 0.000027, Train: (LOSS: 0.0193, MAE: 0.0193, RMSE: 0.0317, R2: 0.9803), Valid: (LOSS: 0.0448, MAE: 0.0448, RMSE: 0.0682, R2: 0.9238), PNorm: 142.0812, GNorm: 1.1093
[145/299] timecost: 63.59, lr: 0.000027, Train: (LOSS: 0.0184, MAE: 0.0184, RMSE: 0.0307, R2: 0.9824), Valid: (LOSS: 0.0438, MAE: 0.0438, RMSE: 0.0677, R2: 0.9252), PNorm: 141.9146, GNorm: 0.8252
[146/299] timecost: 63.09, lr: 0.000027, Train: (LOSS: 0.0177, MAE: 0.0177, RMSE: 0.0293, R2: 0.9831), Valid: (LOSS: 0.0426, MAE: 0.0426, RMSE: 0.0668, R2: 0.9287), PNorm: 141.7496, GNorm: 1.0590
[147/299] timecost: 62.70, lr: 0.000027, Train: (LOSS: 0.0175, MAE: 0.0175, RMSE: 0.0285, R2: 0.9845), Valid: (LOSS: 0.0413, MAE: 0.0413, RMSE: 0.0622, R2: 0.9385), PNorm: 141.5861, GNorm: 0.9599
[148/299] timecost: 62.92, lr: 0.000027, Train: (LOSS: 0.0183, MAE: 0.0183, RMSE: 0.0300, R2: 0.9827), Valid: (LOSS: 0.0434, MAE: 0.0434, RMSE: 0.0656, R2: 0.9319), PNorm: 141.4246, GNorm: 1.2073
[149/299] timecost: 63.10, lr: 0.000027, Train: (LOSS: 0.0187, MAE: 0.0187, RMSE: 0.0309, R2: 0.9811), Valid: (LOSS: 0.0431, MAE: 0.0431, RMSE: 0.0670, R2: 0.9276), PNorm: 141.2652, GNorm: 1.0256
[150/299] timecost: 63.17, lr: 0.000027, Train: (LOSS: 0.0181, MAE: 0.0181, RMSE: 0.0293, R2: 0.9832), Valid: (LOSS: 0.0449, MAE: 0.0449, RMSE: 0.0702, R2: 0.9214), PNorm: 141.1075, GNorm: 0.9609
[151/299] timecost: 64.21, lr: 0.000027, Train: (LOSS: 0.0176, MAE: 0.0176, RMSE: 0.0284, R2: 0.9827), Valid: (LOSS: 0.0432, MAE: 0.0432, RMSE: 0.0680, R2: 0.9244), PNorm: 140.9517, GNorm: 1.0203
[152/299] timecost: 65.80, lr: 0.000027, Train: (LOSS: 0.0176, MAE: 0.0176, RMSE: 0.0288, R2: 0.9831), Valid: (LOSS: 0.0440, MAE: 0.0440, RMSE: 0.0696, R2: 0.9224), PNorm: 140.7978, GNorm: 1.3105
[153/299] timecost: 65.87, lr: 0.000027, Train: (LOSS: 0.0182, MAE: 0.0182, RMSE: 0.0296, R2: 0.9840), Valid: (LOSS: 0.0437, MAE: 0.0437, RMSE: 0.0682, R2: 0.9261), PNorm: 140.6461, GNorm: 1.7535
[154/299] timecost: 66.69, lr: 0.000027, Train: (LOSS: 0.0185, MAE: 0.0185, RMSE: 0.0290, R2: 0.9835), Valid: (LOSS: 0.0420, MAE: 0.0420, RMSE: 0.0628, R2: 0.9388), PNorm: 140.4965, GNorm: 0.9876
[155/299] timecost: 66.67, lr: 0.000027, Train: (LOSS: 0.0173, MAE: 0.0173, RMSE: 0.0285, R2: 0.9845), Valid: (LOSS: 0.0435, MAE: 0.0435, RMSE: 0.0664, R2: 0.9299), PNorm: 140.3482, GNorm: 1.2045
[156/299] timecost: 66.23, lr: 0.000027, Train: (LOSS: 0.0176, MAE: 0.0176, RMSE: 0.0285, R2: 0.9842), Valid: (LOSS: 0.0417, MAE: 0.0417, RMSE: 0.0639, R2: 0.9357), PNorm: 140.2025, GNorm: 0.8990
[157/299] timecost: 66.38, lr: 0.000027, Train: (LOSS: 0.0173, MAE: 0.0173, RMSE: 0.0282, R2: 0.9851), Valid: (LOSS: 0.0429, MAE: 0.0429, RMSE: 0.0657, R2: 0.9330), PNorm: 140.0580, GNorm: 0.6948
[158/299] timecost: 66.45, lr: 0.000027, Train: (LOSS: 0.0166, MAE: 0.0166, RMSE: 0.0272, R2: 0.9854), Valid: (LOSS: 0.0443, MAE: 0.0443, RMSE: 0.0659, R2: 0.9330), PNorm: 139.9153, GNorm: 0.8746
[159/299] timecost: 65.74, lr: 0.000027, Train: (LOSS: 0.0170, MAE: 0.0170, RMSE: 0.0277, R2: 0.9848), Valid: (LOSS: 0.0412, MAE: 0.0412, RMSE: 0.0614, R2: 0.9407), PNorm: 139.7753, GNorm: 0.9599
[160/299] timecost: 65.98, lr: 0.000027, Train: (LOSS: 0.0166, MAE: 0.0166, RMSE: 0.0273, R2: 0.9857), Valid: (LOSS: 0.0410, MAE: 0.0410, RMSE: 0.0605, R2: 0.9410), PNorm: 139.6365, GNorm: 0.7984
Epoch 00162: reducing learning rate of group 0 to 2.4300e-05.
[161/299] timecost: 65.65, lr: 0.000024, Train: (LOSS: 0.0167, MAE: 0.0167, RMSE: 0.0273, R2: 0.9847), Valid: (LOSS: 0.0415, MAE: 0.0415, RMSE: 0.0627, R2: 0.9377), PNorm: 139.4998, GNorm: 0.8245
[162/299] timecost: 66.25, lr: 0.000024, Train: (LOSS: 0.0158, MAE: 0.0158, RMSE: 0.0257, R2: 0.9869), Valid: (LOSS: 0.0400, MAE: 0.0400, RMSE: 0.0607, R2: 0.9420), PNorm: 139.3777, GNorm: 0.7955
[163/299] timecost: 66.62, lr: 0.000024, Train: (LOSS: 0.0160, MAE: 0.0160, RMSE: 0.0266, R2: 0.9872), Valid: (LOSS: 0.0422, MAE: 0.0422, RMSE: 0.0671, R2: 0.9272), PNorm: 139.2569, GNorm: 1.0775
[164/299] timecost: 66.91, lr: 0.000024, Train: (LOSS: 0.0163, MAE: 0.0163, RMSE: 0.0263, R2: 0.9872), Valid: (LOSS: 0.0401, MAE: 0.0401, RMSE: 0.0591, R2: 0.9456), PNorm: 139.1387, GNorm: 0.8212
[165/299] timecost: 65.93, lr: 0.000024, Train: (LOSS: 0.0167, MAE: 0.0167, RMSE: 0.0267, R2: 0.9866), Valid: (LOSS: 0.0423, MAE: 0.0423, RMSE: 0.0629, R2: 0.9369), PNorm: 139.0209, GNorm: 0.7850
[166/299] timecost: 66.41, lr: 0.000024, Train: (LOSS: 0.0159, MAE: 0.0159, RMSE: 0.0257, R2: 0.9864), Valid: (LOSS: 0.0400, MAE: 0.0400, RMSE: 0.0599, R2: 0.9432), PNorm: 138.9043, GNorm: 0.7845
[167/299] timecost: 65.84, lr: 0.000024, Train: (LOSS: 0.0159, MAE: 0.0159, RMSE: 0.0263, R2: 0.9866), Valid: (LOSS: 0.0437, MAE: 0.0437, RMSE: 0.0677, R2: 0.9269), PNorm: 138.7896, GNorm: 0.9176
[168/299] timecost: 66.13, lr: 0.000024, Train: (LOSS: 0.0154, MAE: 0.0154, RMSE: 0.0254, R2: 0.9873), Valid: (LOSS: 0.0418, MAE: 0.0418, RMSE: 0.0625, R2: 0.9375), PNorm: 138.6753, GNorm: 1.0168
[169/299] timecost: 66.39, lr: 0.000024, Train: (LOSS: 0.0156, MAE: 0.0156, RMSE: 0.0255, R2: 0.9877), Valid: (LOSS: 0.0417, MAE: 0.0417, RMSE: 0.0623, R2: 0.9386), PNorm: 138.5630, GNorm: 0.8930
[170/299] timecost: 66.03, lr: 0.000024, Train: (LOSS: 0.0147, MAE: 0.0147, RMSE: 0.0239, R2: 0.9887), Valid: (LOSS: 0.0413, MAE: 0.0413, RMSE: 0.0622, R2: 0.9379), PNorm: 138.4519, GNorm: 0.6997
[171/299] timecost: 66.23, lr: 0.000024, Train: (LOSS: 0.0144, MAE: 0.0144, RMSE: 0.0232, R2: 0.9890), Valid: (LOSS: 0.0409, MAE: 0.0409, RMSE: 0.0609, R2: 0.9409), PNorm: 138.3408, GNorm: 0.8130
[172/299] timecost: 66.62, lr: 0.000024, Train: (LOSS: 0.0145, MAE: 0.0145, RMSE: 0.0244, R2: 0.9883), Valid: (LOSS: 0.0408, MAE: 0.0408, RMSE: 0.0615, R2: 0.9397), PNorm: 138.2318, GNorm: 1.0185
[173/299] timecost: 65.35, lr: 0.000024, Train: (LOSS: 0.0151, MAE: 0.0151, RMSE: 0.0246, R2: 0.9879), Valid: (LOSS: 0.0413, MAE: 0.0413, RMSE: 0.0619, R2: 0.9394), PNorm: 138.1235, GNorm: 0.9891
[174/299] timecost: 65.02, lr: 0.000024, Train: (LOSS: 0.0144, MAE: 0.0144, RMSE: 0.0232, R2: 0.9896), Valid: (LOSS: 0.0409, MAE: 0.0409, RMSE: 0.0632, R2: 0.9357), PNorm: 138.0168, GNorm: 0.9917
[175/299] timecost: 64.83, lr: 0.000024, Train: (LOSS: 0.0153, MAE: 0.0153, RMSE: 0.0248, R2: 0.9885), Valid: (LOSS: 0.0425, MAE: 0.0425, RMSE: 0.0640, R2: 0.9342), PNorm: 137.9118, GNorm: 0.6961
[176/299] timecost: 64.67, lr: 0.000024, Train: (LOSS: 0.0143, MAE: 0.0143, RMSE: 0.0233, R2: 0.9892), Valid: (LOSS: 0.0414, MAE: 0.0414, RMSE: 0.0625, R2: 0.9376), PNorm: 137.8080, GNorm: 0.9491
[177/299] timecost: 65.14, lr: 0.000024, Train: (LOSS: 0.0150, MAE: 0.0150, RMSE: 0.0238, R2: 0.9894), Valid: (LOSS: 0.0408, MAE: 0.0408, RMSE: 0.0620, R2: 0.9394), PNorm: 137.7050, GNorm: 1.0116
[178/299] timecost: 64.81, lr: 0.000024, Train: (LOSS: 0.0149, MAE: 0.0149, RMSE: 0.0240, R2: 0.9891), Valid: (LOSS: 0.0436, MAE: 0.0436, RMSE: 0.0674, R2: 0.9291), PNorm: 137.6031, GNorm: 1.3403
[179/299] timecost: 64.95, lr: 0.000024, Train: (LOSS: 0.0149, MAE: 0.0149, RMSE: 0.0240, R2: 0.9894), Valid: (LOSS: 0.0439, MAE: 0.0439, RMSE: 0.0645, R2: 0.9337), PNorm: 137.5028, GNorm: 1.5666
[180/299] timecost: 66.14, lr: 0.000024, Train: (LOSS: 0.0139, MAE: 0.0139, RMSE: 0.0225, R2: 0.9895), Valid: (LOSS: 0.0409, MAE: 0.0409, RMSE: 0.0606, R2: 0.9425), PNorm: 137.4032, GNorm: 0.9451
[181/299] timecost: 66.01, lr: 0.000024, Train: (LOSS: 0.0140, MAE: 0.0140, RMSE: 0.0227, R2: 0.9894), Valid: (LOSS: 0.0411, MAE: 0.0411, RMSE: 0.0602, R2: 0.9425), PNorm: 137.3048, GNorm: 0.7958
[182/299] timecost: 65.97, lr: 0.000024, Train: (LOSS: 0.0144, MAE: 0.0144, RMSE: 0.0230, R2: 0.9908), Valid: (LOSS: 0.0423, MAE: 0.0423, RMSE: 0.0652, R2: 0.9324), PNorm: 137.2067, GNorm: 0.8633
Epoch 00184: reducing learning rate of group 0 to 2.1870e-05.
[183/299] timecost: 66.13, lr: 0.000022, Train: (LOSS: 0.0139, MAE: 0.0139, RMSE: 0.0222, R2: 0.9912), Valid: (LOSS: 0.0397, MAE: 0.0397, RMSE: 0.0578, R2: 0.9466), PNorm: 137.1095, GNorm: 0.9296
[184/299] timecost: 66.41, lr: 0.000022, Train: (LOSS: 0.0134, MAE: 0.0134, RMSE: 0.0219, R2: 0.9914), Valid: (LOSS: 0.0403, MAE: 0.0403, RMSE: 0.0604, R2: 0.9432), PNorm: 137.0233, GNorm: 0.9355
[185/299] timecost: 66.03, lr: 0.000022, Train: (LOSS: 0.0138, MAE: 0.0138, RMSE: 0.0217, R2: 0.9913), Valid: (LOSS: 0.0413, MAE: 0.0413, RMSE: 0.0615, R2: 0.9405), PNorm: 136.9385, GNorm: 0.6948
[186/299] timecost: 65.19, lr: 0.000022, Train: (LOSS: 0.0128, MAE: 0.0128, RMSE: 0.0203, R2: 0.9916), Valid: (LOSS: 0.0424, MAE: 0.0424, RMSE: 0.0630, R2: 0.9391), PNorm: 136.8531, GNorm: 0.9649
[187/299] timecost: 64.11, lr: 0.000022, Train: (LOSS: 0.0137, MAE: 0.0137, RMSE: 0.0217, R2: 0.9916), Valid: (LOSS: 0.0405, MAE: 0.0405, RMSE: 0.0624, R2: 0.9395), PNorm: 136.7690, GNorm: 1.0709
[188/299] timecost: 62.68, lr: 0.000022, Train: (LOSS: 0.0134, MAE: 0.0134, RMSE: 0.0214, R2: 0.9917), Valid: (LOSS: 0.0410, MAE: 0.0410, RMSE: 0.0612, R2: 0.9423), PNorm: 136.6857, GNorm: 1.3947
[189/299] timecost: 62.95, lr: 0.000022, Train: (LOSS: 0.0134, MAE: 0.0134, RMSE: 0.0214, R2: 0.9914), Valid: (LOSS: 0.0400, MAE: 0.0400, RMSE: 0.0589, R2: 0.9463), PNorm: 136.6032, GNorm: 0.8998
[190/299] timecost: 63.00, lr: 0.000022, Train: (LOSS: 0.0131, MAE: 0.0131, RMSE: 0.0207, R2: 0.9921), Valid: (LOSS: 0.0423, MAE: 0.0423, RMSE: 0.0619, R2: 0.9403), PNorm: 136.5215, GNorm: 0.8753
[191/299] timecost: 63.03, lr: 0.000022, Train: (LOSS: 0.0133, MAE: 0.0133, RMSE: 0.0211, R2: 0.9922), Valid: (LOSS: 0.0418, MAE: 0.0418, RMSE: 0.0649, R2: 0.9340), PNorm: 136.4402, GNorm: 1.4139
[192/299] timecost: 62.99, lr: 0.000022, Train: (LOSS: 0.0125, MAE: 0.0125, RMSE: 0.0199, R2: 0.9921), Valid: (LOSS: 0.0417, MAE: 0.0417, RMSE: 0.0641, R2: 0.9348), PNorm: 136.3597, GNorm: 0.8468
[193/299] timecost: 62.96, lr: 0.000022, Train: (LOSS: 0.0135, MAE: 0.0135, RMSE: 0.0210, R2: 0.9921), Valid: (LOSS: 0.0400, MAE: 0.0400, RMSE: 0.0598, R2: 0.9439), PNorm: 136.2793, GNorm: 1.1564
[194/299] timecost: 62.67, lr: 0.000022, Train: (LOSS: 0.0125, MAE: 0.0125, RMSE: 0.0203, R2: 0.9924), Valid: (LOSS: 0.0398, MAE: 0.0398, RMSE: 0.0598, R2: 0.9435), PNorm: 136.1999, GNorm: 0.7643
[195/299] timecost: 62.78, lr: 0.000022, Train: (LOSS: 0.0133, MAE: 0.0133, RMSE: 0.0208, R2: 0.9924), Valid: (LOSS: 0.0410, MAE: 0.0410, RMSE: 0.0606, R2: 0.9420), PNorm: 136.1207, GNorm: 1.2252
[196/299] timecost: 62.86, lr: 0.000022, Train: (LOSS: 0.0133, MAE: 0.0133, RMSE: 0.0205, R2: 0.9925), Valid: (LOSS: 0.0403, MAE: 0.0403, RMSE: 0.0598, R2: 0.9453), PNorm: 136.0429, GNorm: 0.9393
[197/299] timecost: 62.74, lr: 0.000022, Train: (LOSS: 0.0128, MAE: 0.0128, RMSE: 0.0205, R2: 0.9927), Valid: (LOSS: 0.0397, MAE: 0.0397, RMSE: 0.0592, R2: 0.9453), PNorm: 135.9654, GNorm: 1.1702
[198/299] timecost: 62.98, lr: 0.000022, Train: (LOSS: 0.0122, MAE: 0.0122, RMSE: 0.0193, R2: 0.9936), Valid: (LOSS: 0.0417, MAE: 0.0417, RMSE: 0.0618, R2: 0.9401), PNorm: 135.8882, GNorm: 0.8706
[199/299] timecost: 62.72, lr: 0.000022, Train: (LOSS: 0.0126, MAE: 0.0126, RMSE: 0.0194, R2: 0.9936), Valid: (LOSS: 0.0401, MAE: 0.0401, RMSE: 0.0599, R2: 0.9435), PNorm: 135.8114, GNorm: 1.1313
[200/299] timecost: 62.73, lr: 0.000022, Train: (LOSS: 0.0131, MAE: 0.0131, RMSE: 0.0203, R2: 0.9926), Valid: (LOSS: 0.0400, MAE: 0.0400, RMSE: 0.0583, R2: 0.9467), PNorm: 135.7355, GNorm: 0.7145
[201/299] timecost: 64.37, lr: 0.000022, Train: (LOSS: 0.0122, MAE: 0.0122, RMSE: 0.0190, R2: 0.9934), Valid: (LOSS: 0.0420, MAE: 0.0420, RMSE: 0.0616, R2: 0.9401), PNorm: 135.6594, GNorm: 0.7470
[202/299] timecost: 65.56, lr: 0.000022, Train: (LOSS: 0.0124, MAE: 0.0124, RMSE: 0.0190, R2: 0.9940), Valid: (LOSS: 0.0388, MAE: 0.0388, RMSE: 0.0582, R2: 0.9458), PNorm: 135.5841, GNorm: 0.8326
[203/299] timecost: 66.57, lr: 0.000022, Train: (LOSS: 0.0120, MAE: 0.0120, RMSE: 0.0184, R2: 0.9941), Valid: (LOSS: 0.0414, MAE: 0.0414, RMSE: 0.0616, R2: 0.9399), PNorm: 135.5092, GNorm: 0.7195
[204/299] timecost: 66.52, lr: 0.000022, Train: (LOSS: 0.0120, MAE: 0.0120, RMSE: 0.0183, R2: 0.9946), Valid: (LOSS: 0.0400, MAE: 0.0400, RMSE: 0.0581, R2: 0.9482), PNorm: 135.4343, GNorm: 0.9089
[205/299] timecost: 66.40, lr: 0.000022, Train: (LOSS: 0.0121, MAE: 0.0121, RMSE: 0.0181, R2: 0.9942), Valid: (LOSS: 0.0401, MAE: 0.0401, RMSE: 0.0591, R2: 0.9454), PNorm: 135.3603, GNorm: 0.8073
[206/299] timecost: 66.19, lr: 0.000022, Train: (LOSS: 0.0120, MAE: 0.0120, RMSE: 0.0186, R2: 0.9940), Valid: (LOSS: 0.0404, MAE: 0.0404, RMSE: 0.0593, R2: 0.9451), PNorm: 135.2869, GNorm: 0.8690
[207/299] timecost: 66.81, lr: 0.000022, Train: (LOSS: 0.0119, MAE: 0.0119, RMSE: 0.0183, R2: 0.9940), Valid: (LOSS: 0.0404, MAE: 0.0404, RMSE: 0.0603, R2: 0.9411), PNorm: 135.2137, GNorm: 1.4022
[208/299] timecost: 65.74, lr: 0.000022, Train: (LOSS: 0.0125, MAE: 0.0125, RMSE: 0.0186, R2: 0.9942), Valid: (LOSS: 0.0401, MAE: 0.0401, RMSE: 0.0591, R2: 0.9462), PNorm: 135.1409, GNorm: 0.8283
[209/299] timecost: 64.94, lr: 0.000022, Train: (LOSS: 0.0118, MAE: 0.0118, RMSE: 0.0178, R2: 0.9947), Valid: (LOSS: 0.0403, MAE: 0.0403, RMSE: 0.0586, R2: 0.9469), PNorm: 135.0679, GNorm: 0.8178
[210/299] timecost: 64.57, lr: 0.000022, Train: (LOSS: 0.0111, MAE: 0.0111, RMSE: 0.0170, R2: 0.9951), Valid: (LOSS: 0.0403, MAE: 0.0403, RMSE: 0.0588, R2: 0.9460), PNorm: 134.9958, GNorm: 1.2165
[211/299] timecost: 64.73, lr: 0.000022, Train: (LOSS: 0.0114, MAE: 0.0114, RMSE: 0.0173, R2: 0.9948), Valid: (LOSS: 0.0403, MAE: 0.0403, RMSE: 0.0598, R2: 0.9444), PNorm: 134.9236, GNorm: 0.9243
[212/299] timecost: 65.40, lr: 0.000022, Train: (LOSS: 0.0124, MAE: 0.0124, RMSE: 0.0184, R2: 0.9944), Valid: (LOSS: 0.0407, MAE: 0.0407, RMSE: 0.0603, R2: 0.9432), PNorm: 134.8525, GNorm: 0.7248
[213/299] timecost: 66.00, lr: 0.000022, Train: (LOSS: 0.0109, MAE: 0.0109, RMSE: 0.0166, R2: 0.9956), Valid: (LOSS: 0.0405, MAE: 0.0405, RMSE: 0.0591, R2: 0.9456), PNorm: 134.7809, GNorm: 0.9800
[214/299] timecost: 66.31, lr: 0.000022, Train: (LOSS: 0.0117, MAE: 0.0117, RMSE: 0.0176, R2: 0.9949), Valid: (LOSS: 0.0409, MAE: 0.0409, RMSE: 0.0596, R2: 0.9444), PNorm: 134.7105, GNorm: 1.0437
[215/299] timecost: 66.36, lr: 0.000022, Train: (LOSS: 0.0115, MAE: 0.0115, RMSE: 0.0174, R2: 0.9949), Valid: (LOSS: 0.0397, MAE: 0.0397, RMSE: 0.0585, R2: 0.9461), PNorm: 134.6396, GNorm: 0.7551
[216/299] timecost: 66.22, lr: 0.000022, Train: (LOSS: 0.0113, MAE: 0.0113, RMSE: 0.0170, R2: 0.9954), Valid: (LOSS: 0.0398, MAE: 0.0398, RMSE: 0.0585, R2: 0.9455), PNorm: 134.5690, GNorm: 0.7983
[217/299] timecost: 65.45, lr: 0.000022, Train: (LOSS: 0.0112, MAE: 0.0112, RMSE: 0.0168, R2: 0.9951), Valid: (LOSS: 0.0408, MAE: 0.0408, RMSE: 0.0602, R2: 0.9437), PNorm: 134.4990, GNorm: 0.7923
[218/299] timecost: 63.40, lr: 0.000022, Train: (LOSS: 0.0118, MAE: 0.0118, RMSE: 0.0174, R2: 0.9952), Valid: (LOSS: 0.0401, MAE: 0.0401, RMSE: 0.0587, R2: 0.9464), PNorm: 134.4300, GNorm: 0.7361
[219/299] timecost: 63.97, lr: 0.000022, Train: (LOSS: 0.0116, MAE: 0.0116, RMSE: 0.0174, R2: 0.9948), Valid: (LOSS: 0.0400, MAE: 0.0400, RMSE: 0.0595, R2: 0.9451), PNorm: 134.3609, GNorm: 0.8228
[220/299] timecost: 63.69, lr: 0.000022, Train: (LOSS: 0.0117, MAE: 0.0117, RMSE: 0.0175, R2: 0.9949), Valid: (LOSS: 0.0406, MAE: 0.0406, RMSE: 0.0581, R2: 0.9472), PNorm: 134.2917, GNorm: 1.0652
[221/299] timecost: 63.03, lr: 0.000022, Train: (LOSS: 0.0110, MAE: 0.0110, RMSE: 0.0167, R2: 0.9951), Valid: (LOSS: 0.0403, MAE: 0.0403, RMSE: 0.0593, R2: 0.9441), PNorm: 134.2222, GNorm: 1.1582
[222/299] timecost: 62.99, lr: 0.000022, Train: (LOSS: 0.0112, MAE: 0.0112, RMSE: 0.0166, R2: 0.9954), Valid: (LOSS: 0.0409, MAE: 0.0409, RMSE: 0.0611, R2: 0.9409), PNorm: 134.1535, GNorm: 1.0143
Epoch 00224: reducing learning rate of group 0 to 1.9683e-05.
[223/299] timecost: 62.99, lr: 0.000020, Train: (LOSS: 0.0112, MAE: 0.0112, RMSE: 0.0169, R2: 0.9954), Valid: (LOSS: 0.0396, MAE: 0.0396, RMSE: 0.0580, R2: 0.9475), PNorm: 134.0858, GNorm: 1.0175
[224/299] timecost: 62.92, lr: 0.000020, Train: (LOSS: 0.0105, MAE: 0.0105, RMSE: 0.0157, R2: 0.9960), Valid: (LOSS: 0.0402, MAE: 0.0402, RMSE: 0.0585, R2: 0.9460), PNorm: 134.0234, GNorm: 0.8764
[225/299] timecost: 62.72, lr: 0.000020, Train: (LOSS: 0.0101, MAE: 0.0101, RMSE: 0.0155, R2: 0.9959), Valid: (LOSS: 0.0405, MAE: 0.0405, RMSE: 0.0590, R2: 0.9450), PNorm: 133.9617, GNorm: 0.8798
[226/299] timecost: 62.77, lr: 0.000020, Train: (LOSS: 0.0101, MAE: 0.0101, RMSE: 0.0151, R2: 0.9964), Valid: (LOSS: 0.0408, MAE: 0.0408, RMSE: 0.0591, R2: 0.9452), PNorm: 133.8999, GNorm: 0.8006
[227/299] timecost: 63.64, lr: 0.000020, Train: (LOSS: 0.0097, MAE: 0.0097, RMSE: 0.0148, R2: 0.9963), Valid: (LOSS: 0.0395, MAE: 0.0395, RMSE: 0.0599, R2: 0.9447), PNorm: 133.8379, GNorm: 0.9316
[228/299] timecost: 65.25, lr: 0.000020, Train: (LOSS: 0.0099, MAE: 0.0099, RMSE: 0.0150, R2: 0.9964), Valid: (LOSS: 0.0390, MAE: 0.0390, RMSE: 0.0580, R2: 0.9473), PNorm: 133.7758, GNorm: 0.8208
[229/299] timecost: 65.44, lr: 0.000020, Train: (LOSS: 0.0104, MAE: 0.0104, RMSE: 0.0155, R2: 0.9961), Valid: (LOSS: 0.0399, MAE: 0.0399, RMSE: 0.0591, R2: 0.9444), PNorm: 133.7140, GNorm: 0.9119
[230/299] timecost: 64.65, lr: 0.000020, Train: (LOSS: 0.0104, MAE: 0.0104, RMSE: 0.0155, R2: 0.9961), Valid: (LOSS: 0.0384, MAE: 0.0384, RMSE: 0.0563, R2: 0.9513), PNorm: 133.6526, GNorm: 0.7335
[231/299] timecost: 65.08, lr: 0.000020, Train: (LOSS: 0.0099, MAE: 0.0099, RMSE: 0.0151, R2: 0.9962), Valid: (LOSS: 0.0383, MAE: 0.0383, RMSE: 0.0572, R2: 0.9495), PNorm: 133.5913, GNorm: 0.8410
[232/299] timecost: 65.06, lr: 0.000020, Train: (LOSS: 0.0101, MAE: 0.0101, RMSE: 0.0153, R2: 0.9960), Valid: (LOSS: 0.0389, MAE: 0.0389, RMSE: 0.0580, R2: 0.9471), PNorm: 133.5298, GNorm: 1.2857
[233/299] timecost: 65.05, lr: 0.000020, Train: (LOSS: 0.0100, MAE: 0.0100, RMSE: 0.0152, R2: 0.9961), Valid: (LOSS: 0.0383, MAE: 0.0383, RMSE: 0.0578, R2: 0.9481), PNorm: 133.4692, GNorm: 0.8263
[234/299] timecost: 64.99, lr: 0.000020, Train: (LOSS: 0.0104, MAE: 0.0104, RMSE: 0.0155, R2: 0.9959), Valid: (LOSS: 0.0399, MAE: 0.0399, RMSE: 0.0598, R2: 0.9437), PNorm: 133.4084, GNorm: 0.8862
[235/299] timecost: 65.39, lr: 0.000020, Train: (LOSS: 0.0097, MAE: 0.0097, RMSE: 0.0147, R2: 0.9964), Valid: (LOSS: 0.0379, MAE: 0.0379, RMSE: 0.0558, R2: 0.9514), PNorm: 133.3473, GNorm: 0.7144
[236/299] timecost: 64.74, lr: 0.000020, Train: (LOSS: 0.0100, MAE: 0.0100, RMSE: 0.0151, R2: 0.9963), Valid: (LOSS: 0.0394, MAE: 0.0394, RMSE: 0.0573, R2: 0.9479), PNorm: 133.2863, GNorm: 0.7836
[237/299] timecost: 64.96, lr: 0.000020, Train: (LOSS: 0.0091, MAE: 0.0091, RMSE: 0.0141, R2: 0.9966), Valid: (LOSS: 0.0387, MAE: 0.0387, RMSE: 0.0569, R2: 0.9502), PNorm: 133.2247, GNorm: 1.0357
[238/299] timecost: 65.26, lr: 0.000020, Train: (LOSS: 0.0097, MAE: 0.0097, RMSE: 0.0144, R2: 0.9965), Valid: (LOSS: 0.0389, MAE: 0.0389, RMSE: 0.0567, R2: 0.9505), PNorm: 133.1640, GNorm: 0.9633
[239/299] timecost: 66.40, lr: 0.000020, Train: (LOSS: 0.0100, MAE: 0.0100, RMSE: 0.0150, R2: 0.9962), Valid: (LOSS: 0.0399, MAE: 0.0399, RMSE: 0.0597, R2: 0.9448), PNorm: 133.1029, GNorm: 0.9697
[240/299] timecost: 66.31, lr: 0.000020, Train: (LOSS: 0.0097, MAE: 0.0097, RMSE: 0.0147, R2: 0.9964), Valid: (LOSS: 0.0390, MAE: 0.0390, RMSE: 0.0569, R2: 0.9508), PNorm: 133.0426, GNorm: 0.7994
[241/299] timecost: 65.88, lr: 0.000020, Train: (LOSS: 0.0094, MAE: 0.0094, RMSE: 0.0142, R2: 0.9967), Valid: (LOSS: 0.0385, MAE: 0.0385, RMSE: 0.0573, R2: 0.9486), PNorm: 132.9823, GNorm: 0.9625
[242/299] timecost: 66.42, lr: 0.000020, Train: (LOSS: 0.0095, MAE: 0.0095, RMSE: 0.0141, R2: 0.9968), Valid: (LOSS: 0.0388, MAE: 0.0388, RMSE: 0.0568, R2: 0.9503), PNorm: 132.9213, GNorm: 1.0093
[243/299] timecost: 66.46, lr: 0.000020, Train: (LOSS: 0.0097, MAE: 0.0097, RMSE: 0.0144, R2: 0.9965), Valid: (LOSS: 0.0380, MAE: 0.0380, RMSE: 0.0557, R2: 0.9512), PNorm: 132.8609, GNorm: 0.9420
[244/299] timecost: 66.19, lr: 0.000020, Train: (LOSS: 0.0094, MAE: 0.0094, RMSE: 0.0141, R2: 0.9967), Valid: (LOSS: 0.0398, MAE: 0.0398, RMSE: 0.0580, R2: 0.9474), PNorm: 132.8006, GNorm: 0.6669
[245/299] timecost: 66.08, lr: 0.000020, Train: (LOSS: 0.0098, MAE: 0.0098, RMSE: 0.0146, R2: 0.9964), Valid: (LOSS: 0.0393, MAE: 0.0393, RMSE: 0.0572, R2: 0.9495), PNorm: 132.7402, GNorm: 0.8446
[246/299] timecost: 66.11, lr: 0.000020, Train: (LOSS: 0.0098, MAE: 0.0098, RMSE: 0.0146, R2: 0.9966), Valid: (LOSS: 0.0388, MAE: 0.0388, RMSE: 0.0568, R2: 0.9493), PNorm: 132.6801, GNorm: 1.0337
[247/299] timecost: 66.48, lr: 0.000020, Train: (LOSS: 0.0091, MAE: 0.0091, RMSE: 0.0136, R2: 0.9970), Valid: (LOSS: 0.0409, MAE: 0.0409, RMSE: 0.0601, R2: 0.9441), PNorm: 132.6196, GNorm: 1.0802
[248/299] timecost: 65.48, lr: 0.000020, Train: (LOSS: 0.0094, MAE: 0.0094, RMSE: 0.0138, R2: 0.9969), Valid: (LOSS: 0.0397, MAE: 0.0397, RMSE: 0.0584, R2: 0.9467), PNorm: 132.5594, GNorm: 1.0068
[249/299] timecost: 65.04, lr: 0.000020, Train: (LOSS: 0.0095, MAE: 0.0095, RMSE: 0.0142, R2: 0.9967), Valid: (LOSS: 0.0389, MAE: 0.0389, RMSE: 0.0566, R2: 0.9508), PNorm: 132.4998, GNorm: 0.8090
[250/299] timecost: 64.92, lr: 0.000020, Train: (LOSS: 0.0095, MAE: 0.0095, RMSE: 0.0142, R2: 0.9969), Valid: (LOSS: 0.0392, MAE: 0.0392, RMSE: 0.0562, R2: 0.9508), PNorm: 132.4402, GNorm: 0.8070
[251/299] timecost: 64.99, lr: 0.000020, Train: (LOSS: 0.0089, MAE: 0.0089, RMSE: 0.0136, R2: 0.9969), Valid: (LOSS: 0.0389, MAE: 0.0389, RMSE: 0.0565, R2: 0.9498), PNorm: 132.3804, GNorm: 0.9728
[252/299] timecost: 65.11, lr: 0.000020, Train: (LOSS: 0.0092, MAE: 0.0092, RMSE: 0.0138, R2: 0.9969), Valid: (LOSS: 0.0390, MAE: 0.0390, RMSE: 0.0576, R2: 0.9477), PNorm: 132.3209, GNorm: 0.7628
[253/299] timecost: 64.86, lr: 0.000020, Train: (LOSS: 0.0092, MAE: 0.0092, RMSE: 0.0139, R2: 0.9969), Valid: (LOSS: 0.0383, MAE: 0.0383, RMSE: 0.0565, R2: 0.9501), PNorm: 132.2618, GNorm: 1.0226
[254/299] timecost: 64.92, lr: 0.000020, Train: (LOSS: 0.0089, MAE: 0.0089, RMSE: 0.0134, R2: 0.9970), Valid: (LOSS: 0.0387, MAE: 0.0387, RMSE: 0.0564, R2: 0.9501), PNorm: 132.2019, GNorm: 0.8505
[255/299] timecost: 64.93, lr: 0.000020, Train: (LOSS: 0.0091, MAE: 0.0091, RMSE: 0.0136, R2: 0.9970), Valid: (LOSS: 0.0395, MAE: 0.0395, RMSE: 0.0582, R2: 0.9479), PNorm: 132.1420, GNorm: 0.8176
Epoch 00257: reducing learning rate of group 0 to 1.7715e-05.
[256/299] timecost: 65.11, lr: 0.000018, Train: (LOSS: 0.0089, MAE: 0.0089, RMSE: 0.0133, R2: 0.9972), Valid: (LOSS: 0.0378, MAE: 0.0378, RMSE: 0.0553, R2: 0.9514), PNorm: 132.0828, GNorm: 0.6311
[257/299] timecost: 65.53, lr: 0.000018, Train: (LOSS: 0.0086, MAE: 0.0086, RMSE: 0.0131, R2: 0.9971), Valid: (LOSS: 0.0384, MAE: 0.0384, RMSE: 0.0567, R2: 0.9506), PNorm: 132.0287, GNorm: 0.7833
[258/299] timecost: 66.59, lr: 0.000018, Train: (LOSS: 0.0086, MAE: 0.0086, RMSE: 0.0129, R2: 0.9973), Valid: (LOSS: 0.0389, MAE: 0.0389, RMSE: 0.0576, R2: 0.9471), PNorm: 131.9747, GNorm: 0.7222
[259/299] timecost: 66.63, lr: 0.000018, Train: (LOSS: 0.0088, MAE: 0.0088, RMSE: 0.0132, R2: 0.9972), Valid: (LOSS: 0.0384, MAE: 0.0384, RMSE: 0.0571, R2: 0.9488), PNorm: 131.9215, GNorm: 0.7898
[260/299] timecost: 65.93, lr: 0.000018, Train: (LOSS: 0.0085, MAE: 0.0085, RMSE: 0.0130, R2: 0.9971), Valid: (LOSS: 0.0381, MAE: 0.0381, RMSE: 0.0567, R2: 0.9513), PNorm: 131.8682, GNorm: 0.8591
[261/299] timecost: 65.64, lr: 0.000018, Train: (LOSS: 0.0083, MAE: 0.0083, RMSE: 0.0127, R2: 0.9974), Valid: (LOSS: 0.0386, MAE: 0.0386, RMSE: 0.0567, R2: 0.9499), PNorm: 131.8145, GNorm: 0.8169
[262/299] timecost: 66.88, lr: 0.000018, Train: (LOSS: 0.0084, MAE: 0.0084, RMSE: 0.0126, R2: 0.9974), Valid: (LOSS: 0.0388, MAE: 0.0388, RMSE: 0.0565, R2: 0.9498), PNorm: 131.7613, GNorm: 0.6961
[263/299] timecost: 66.54, lr: 0.000018, Train: (LOSS: 0.0085, MAE: 0.0085, RMSE: 0.0128, R2: 0.9973), Valid: (LOSS: 0.0400, MAE: 0.0400, RMSE: 0.0576, R2: 0.9482), PNorm: 131.7082, GNorm: 0.7946
[264/299] timecost: 66.80, lr: 0.000018, Train: (LOSS: 0.0084, MAE: 0.0084, RMSE: 0.0126, R2: 0.9974), Valid: (LOSS: 0.0410, MAE: 0.0410, RMSE: 0.0594, R2: 0.9447), PNorm: 131.6549, GNorm: 0.9192
[265/299] timecost: 66.59, lr: 0.000018, Train: (LOSS: 0.0089, MAE: 0.0089, RMSE: 0.0133, R2: 0.9972), Valid: (LOSS: 0.0379, MAE: 0.0379, RMSE: 0.0552, R2: 0.9529), PNorm: 131.6017, GNorm: 0.7060
[266/299] timecost: 66.31, lr: 0.000018, Train: (LOSS: 0.0082, MAE: 0.0082, RMSE: 0.0125, R2: 0.9973), Valid: (LOSS: 0.0381, MAE: 0.0381, RMSE: 0.0562, R2: 0.9502), PNorm: 131.5481, GNorm: 0.6173
[267/299] timecost: 65.90, lr: 0.000018, Train: (LOSS: 0.0082, MAE: 0.0082, RMSE: 0.0122, R2: 0.9975), Valid: (LOSS: 0.0391, MAE: 0.0391, RMSE: 0.0573, R2: 0.9490), PNorm: 131.4954, GNorm: 0.7411
[268/299] timecost: 65.93, lr: 0.000018, Train: (LOSS: 0.0081, MAE: 0.0081, RMSE: 0.0120, R2: 0.9976), Valid: (LOSS: 0.0392, MAE: 0.0392, RMSE: 0.0566, R2: 0.9494), PNorm: 131.4419, GNorm: 0.6494
[269/299] timecost: 66.25, lr: 0.000018, Train: (LOSS: 0.0081, MAE: 0.0081, RMSE: 0.0123, R2: 0.9975), Valid: (LOSS: 0.0396, MAE: 0.0396, RMSE: 0.0572, R2: 0.9491), PNorm: 131.3888, GNorm: 0.8325
[270/299] timecost: 65.94, lr: 0.000018, Train: (LOSS: 0.0084, MAE: 0.0084, RMSE: 0.0125, R2: 0.9974), Valid: (LOSS: 0.0383, MAE: 0.0383, RMSE: 0.0564, R2: 0.9501), PNorm: 131.3356, GNorm: 0.8790
[271/299] timecost: 66.04, lr: 0.000018, Train: (LOSS: 0.0080, MAE: 0.0080, RMSE: 0.0121, R2: 0.9977), Valid: (LOSS: 0.0390, MAE: 0.0390, RMSE: 0.0570, R2: 0.9487), PNorm: 131.2823, GNorm: 0.7735
[272/299] timecost: 66.06, lr: 0.000018, Train: (LOSS: 0.0082, MAE: 0.0082, RMSE: 0.0124, R2: 0.9976), Valid: (LOSS: 0.0386, MAE: 0.0386, RMSE: 0.0584, R2: 0.9460), PNorm: 131.2291, GNorm: 0.9485
[273/299] timecost: 66.77, lr: 0.000018, Train: (LOSS: 0.0083, MAE: 0.0083, RMSE: 0.0126, R2: 0.9974), Valid: (LOSS: 0.0383, MAE: 0.0383, RMSE: 0.0570, R2: 0.9486), PNorm: 131.1764, GNorm: 0.9189
[274/299] timecost: 65.39, lr: 0.000018, Train: (LOSS: 0.0081, MAE: 0.0081, RMSE: 0.0122, R2: 0.9975), Valid: (LOSS: 0.0379, MAE: 0.0379, RMSE: 0.0566, R2: 0.9490), PNorm: 131.1238, GNorm: 0.9955
[275/299] timecost: 64.04, lr: 0.000018, Train: (LOSS: 0.0085, MAE: 0.0085, RMSE: 0.0127, R2: 0.9974), Valid: (LOSS: 0.0403, MAE: 0.0403, RMSE: 0.0593, R2: 0.9447), PNorm: 131.0709, GNorm: 0.9632
[276/299] timecost: 63.76, lr: 0.000018, Train: (LOSS: 0.0092, MAE: 0.0092, RMSE: 0.0136, R2: 0.9971), Valid: (LOSS: 0.0393, MAE: 0.0393, RMSE: 0.0564, R2: 0.9508), PNorm: 131.0191, GNorm: 0.8364
Epoch 00278: reducing learning rate of group 0 to 1.5943e-05.
[277/299] timecost: 63.88, lr: 0.000016, Train: (LOSS: 0.0081, MAE: 0.0081, RMSE: 0.0120, R2: 0.9976), Valid: (LOSS: 0.0392, MAE: 0.0392, RMSE: 0.0571, R2: 0.9485), PNorm: 130.9664, GNorm: 0.9105
[278/299] timecost: 65.45, lr: 0.000016, Train: (LOSS: 0.0076, MAE: 0.0076, RMSE: 0.0115, R2: 0.9979), Valid: (LOSS: 0.0383, MAE: 0.0383, RMSE: 0.0558, R2: 0.9507), PNorm: 130.9189, GNorm: 1.0699
[279/299] timecost: 66.00, lr: 0.000016, Train: (LOSS: 0.0074, MAE: 0.0074, RMSE: 0.0112, R2: 0.9978), Valid: (LOSS: 0.0389, MAE: 0.0389, RMSE: 0.0576, R2: 0.9475), PNorm: 130.8717, GNorm: 0.8728
[280/299] timecost: 66.00, lr: 0.000016, Train: (LOSS: 0.0075, MAE: 0.0075, RMSE: 0.0113, R2: 0.9979), Valid: (LOSS: 0.0380, MAE: 0.0380, RMSE: 0.0561, R2: 0.9509), PNorm: 130.8249, GNorm: 0.8632
[281/299] timecost: 66.11, lr: 0.000016, Train: (LOSS: 0.0074, MAE: 0.0074, RMSE: 0.0111, R2: 0.9979), Valid: (LOSS: 0.0383, MAE: 0.0383, RMSE: 0.0566, R2: 0.9498), PNorm: 130.7776, GNorm: 1.2662
[282/299] timecost: 66.10, lr: 0.000016, Train: (LOSS: 0.0076, MAE: 0.0076, RMSE: 0.0114, R2: 0.9977), Valid: (LOSS: 0.0383, MAE: 0.0383, RMSE: 0.0565, R2: 0.9504), PNorm: 130.7304, GNorm: 0.9643
[283/299] timecost: 66.11, lr: 0.000016, Train: (LOSS: 0.0071, MAE: 0.0071, RMSE: 0.0107, R2: 0.9981), Valid: (LOSS: 0.0379, MAE: 0.0379, RMSE: 0.0560, R2: 0.9505), PNorm: 130.6829, GNorm: 0.6646
[284/299] timecost: 66.02, lr: 0.000016, Train: (LOSS: 0.0073, MAE: 0.0073, RMSE: 0.0109, R2: 0.9981), Valid: (LOSS: 0.0380, MAE: 0.0380, RMSE: 0.0566, R2: 0.9481), PNorm: 130.6357, GNorm: 0.8936
[285/299] timecost: 66.41, lr: 0.000016, Train: (LOSS: 0.0079, MAE: 0.0079, RMSE: 0.0118, R2: 0.9976), Valid: (LOSS: 0.0377, MAE: 0.0377, RMSE: 0.0554, R2: 0.9516), PNorm: 130.5887, GNorm: 0.7868
[286/299] timecost: 66.73, lr: 0.000016, Train: (LOSS: 0.0074, MAE: 0.0074, RMSE: 0.0113, R2: 0.9979), Valid: (LOSS: 0.0375, MAE: 0.0375, RMSE: 0.0553, R2: 0.9520), PNorm: 130.5422, GNorm: 0.7264
[287/299] timecost: 66.70, lr: 0.000016, Train: (LOSS: 0.0075, MAE: 0.0075, RMSE: 0.0112, R2: 0.9980), Valid: (LOSS: 0.0395, MAE: 0.0395, RMSE: 0.0578, R2: 0.9476), PNorm: 130.4950, GNorm: 0.5207
[288/299] timecost: 66.41, lr: 0.000016, Train: (LOSS: 0.0074, MAE: 0.0074, RMSE: 0.0112, R2: 0.9979), Valid: (LOSS: 0.0382, MAE: 0.0382, RMSE: 0.0574, R2: 0.9467), PNorm: 130.4480, GNorm: 0.9663
[289/299] timecost: 66.17, lr: 0.000016, Train: (LOSS: 0.0077, MAE: 0.0077, RMSE: 0.0114, R2: 0.9979), Valid: (LOSS: 0.0394, MAE: 0.0394, RMSE: 0.0573, R2: 0.9481), PNorm: 130.4009, GNorm: 0.7456
[290/299] timecost: 66.72, lr: 0.000016, Train: (LOSS: 0.0073, MAE: 0.0073, RMSE: 0.0109, R2: 0.9981), Valid: (LOSS: 0.0386, MAE: 0.0386, RMSE: 0.0568, R2: 0.9493), PNorm: 130.3545, GNorm: 0.7228
[291/299] timecost: 66.24, lr: 0.000016, Train: (LOSS: 0.0072, MAE: 0.0072, RMSE: 0.0110, R2: 0.9980), Valid: (LOSS: 0.0402, MAE: 0.0402, RMSE: 0.0589, R2: 0.9461), PNorm: 130.3079, GNorm: 0.6749
[292/299] timecost: 66.27, lr: 0.000016, Train: (LOSS: 0.0071, MAE: 0.0071, RMSE: 0.0108, R2: 0.9980), Valid: (LOSS: 0.0400, MAE: 0.0400, RMSE: 0.0588, R2: 0.9461), PNorm: 130.2611, GNorm: 0.8078
[293/299] timecost: 66.06, lr: 0.000016, Train: (LOSS: 0.0069, MAE: 0.0069, RMSE: 0.0107, R2: 0.9981), Valid: (LOSS: 0.0395, MAE: 0.0395, RMSE: 0.0574, R2: 0.9477), PNorm: 130.2142, GNorm: 0.9591
[294/299] timecost: 65.83, lr: 0.000016, Train: (LOSS: 0.0069, MAE: 0.0069, RMSE: 0.0106, R2: 0.9982), Valid: (LOSS: 0.0392, MAE: 0.0392, RMSE: 0.0577, R2: 0.9483), PNorm: 130.1674, GNorm: 0.8052
[295/299] timecost: 65.98, lr: 0.000016, Train: (LOSS: 0.0076, MAE: 0.0076, RMSE: 0.0111, R2: 0.9979), Valid: (LOSS: 0.0384, MAE: 0.0384, RMSE: 0.0566, R2: 0.9490), PNorm: 130.1210, GNorm: 0.7226
[296/299] timecost: 66.08, lr: 0.000016, Train: (LOSS: 0.0071, MAE: 0.0071, RMSE: 0.0108, R2: 0.9981), Valid: (LOSS: 0.0386, MAE: 0.0386, RMSE: 0.0566, R2: 0.9492), PNorm: 130.0741, GNorm: 0.8725
[297/299] timecost: 66.24, lr: 0.000016, Train: (LOSS: 0.0071, MAE: 0.0071, RMSE: 0.0108, R2: 0.9981), Valid: (LOSS: 0.0385, MAE: 0.0385, RMSE: 0.0565, R2: 0.9507), PNorm: 130.0275, GNorm: 0.8613
Epoch 00299: reducing learning rate of group 0 to 1.4349e-05.
[298/299] timecost: 65.97, lr: 0.000014, Train: (LOSS: 0.0071, MAE: 0.0071, RMSE: 0.0106, R2: 0.9981), Valid: (LOSS: 0.0391, MAE: 0.0391, RMSE: 0.0572, R2: 0.9487), PNorm: 129.9809, GNorm: 0.7970
[299/299] timecost: 66.65, lr: 0.000014, Train: (LOSS: 0.0066, MAE: 0.0066, RMSE: 0.0100, R2: 0.9983), Valid: (LOSS: 0.0387, MAE: 0.0387, RMSE: 0.0569, R2: 0.9480), PNorm: 129.9393, GNorm: 0.7961
==========Training End==========
==========Test Best Model==========
================Final Results=======================
mse: 0.0338 +- 0.0000:
rmse: 0.0507 +- 0.0000:
mae: 0.0338 +- 0.0000:
r2: 0.9595 +- 0.0000:
tensor([[0.1019, 0.1066],
        [0.0000, 0.0000],
        [0.0000, 0.0000],
        ...,
        [0.0161, 0.0144],
        [0.5719, 0.4354],
        [0.0000, 0.0000]], device='cuda:0')
