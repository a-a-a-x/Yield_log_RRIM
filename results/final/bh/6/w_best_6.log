cuda available with GPU: Tesla V100-PCIE-16GB
==========Load Seed==========
set_random_seed
0
==========Training Start==========
Training Graphs:  2491
Valid Graphs:  277
Test Graphs:  1187
============Loading pretrained weights to generate initialization============
============Creating new layers============
============Creating Model============
Training Graphs Batches:  78
Valid Graphs Batches:  9
Test Graphs Batches:  37
[0/299] timecost: 65.49, lr: 0.000030, Train: (LOSS: 0.2267, MAE: 0.2267, RMSE: 0.2693, R2: -0.0499), Valid: (LOSS: 0.2234, MAE: 0.2234, RMSE: 0.2691, R2: 0.0029), PNorm: 174.3893, GNorm: 0.3546
[1/299] timecost: 64.27, lr: 0.000030, Train: (LOSS: 0.2186, MAE: 0.2186, RMSE: 0.2631, R2: 0.0101), Valid: (LOSS: 0.2159, MAE: 0.2159, RMSE: 0.2575, R2: 0.0851), PNorm: 173.7017, GNorm: 1.0242
[2/299] timecost: 64.57, lr: 0.000030, Train: (LOSS: 0.2000, MAE: 0.2000, RMSE: 0.2437, R2: 0.1291), Valid: (LOSS: 0.1947, MAE: 0.1947, RMSE: 0.2366, R2: 0.2128), PNorm: 173.1684, GNorm: 0.9658
[3/299] timecost: 64.54, lr: 0.000030, Train: (LOSS: 0.1781, MAE: 0.1781, RMSE: 0.2236, R2: 0.2668), Valid: (LOSS: 0.1653, MAE: 0.1653, RMSE: 0.2126, R2: 0.3494), PNorm: 172.7721, GNorm: 12.2285
[4/299] timecost: 64.22, lr: 0.000030, Train: (LOSS: 0.1649, MAE: 0.1649, RMSE: 0.2116, R2: 0.3316), Valid: (LOSS: 0.1637, MAE: 0.1637, RMSE: 0.2116, R2: 0.3763), PNorm: 172.4578, GNorm: 3.5468
[5/299] timecost: 64.36, lr: 0.000030, Train: (LOSS: 0.1430, MAE: 0.1430, RMSE: 0.1895, R2: 0.4682), Valid: (LOSS: 0.1576, MAE: 0.1576, RMSE: 0.2032, R2: 0.4134), PNorm: 172.1618, GNorm: 3.0412
[6/299] timecost: 64.10, lr: 0.000030, Train: (LOSS: 0.1395, MAE: 0.1395, RMSE: 0.1886, R2: 0.4756), Valid: (LOSS: 0.1392, MAE: 0.1392, RMSE: 0.1847, R2: 0.5158), PNorm: 171.8824, GNorm: 2.7128
[7/299] timecost: 64.39, lr: 0.000030, Train: (LOSS: 0.1344, MAE: 0.1344, RMSE: 0.1829, R2: 0.4915), Valid: (LOSS: 0.1337, MAE: 0.1337, RMSE: 0.1790, R2: 0.5466), PNorm: 171.6151, GNorm: 1.6754
[8/299] timecost: 64.46, lr: 0.000030, Train: (LOSS: 0.1420, MAE: 0.1420, RMSE: 0.1896, R2: 0.4607), Valid: (LOSS: 0.1313, MAE: 0.1313, RMSE: 0.1774, R2: 0.5552), PNorm: 171.3669, GNorm: 1.9384
[9/299] timecost: 64.10, lr: 0.000030, Train: (LOSS: 0.1304, MAE: 0.1304, RMSE: 0.1776, R2: 0.5225), Valid: (LOSS: 0.1288, MAE: 0.1288, RMSE: 0.1762, R2: 0.5575), PNorm: 171.1307, GNorm: 3.7584
[10/299] timecost: 64.15, lr: 0.000030, Train: (LOSS: 0.1208, MAE: 0.1208, RMSE: 0.1681, R2: 0.5850), Valid: (LOSS: 0.1380, MAE: 0.1380, RMSE: 0.1893, R2: 0.4949), PNorm: 170.9015, GNorm: 8.5053
[11/299] timecost: 63.31, lr: 0.000030, Train: (LOSS: 0.1228, MAE: 0.1228, RMSE: 0.1687, R2: 0.5724), Valid: (LOSS: 0.1236, MAE: 0.1236, RMSE: 0.1672, R2: 0.6155), PNorm: 170.6826, GNorm: 2.3479
[12/299] timecost: 63.42, lr: 0.000030, Train: (LOSS: 0.1190, MAE: 0.1190, RMSE: 0.1647, R2: 0.5970), Valid: (LOSS: 0.1261, MAE: 0.1261, RMSE: 0.1707, R2: 0.5886), PNorm: 170.4677, GNorm: 1.2717
[13/299] timecost: 64.27, lr: 0.000030, Train: (LOSS: 0.1170, MAE: 0.1170, RMSE: 0.1632, R2: 0.5986), Valid: (LOSS: 0.1248, MAE: 0.1248, RMSE: 0.1689, R2: 0.6096), PNorm: 170.2607, GNorm: 1.9045
[14/299] timecost: 64.99, lr: 0.000030, Train: (LOSS: 0.1123, MAE: 0.1123, RMSE: 0.1596, R2: 0.6160), Valid: (LOSS: 0.1258, MAE: 0.1258, RMSE: 0.1716, R2: 0.5790), PNorm: 170.0575, GNorm: 2.4634
[15/299] timecost: 64.91, lr: 0.000030, Train: (LOSS: 0.1136, MAE: 0.1136, RMSE: 0.1608, R2: 0.6154), Valid: (LOSS: 0.1116, MAE: 0.1116, RMSE: 0.1506, R2: 0.6867), PNorm: 169.8617, GNorm: 1.2279
[16/299] timecost: 65.31, lr: 0.000030, Train: (LOSS: 0.1060, MAE: 0.1060, RMSE: 0.1510, R2: 0.6533), Valid: (LOSS: 0.1000, MAE: 0.1000, RMSE: 0.1381, R2: 0.7325), PNorm: 169.6712, GNorm: 3.0822
[17/299] timecost: 64.12, lr: 0.000030, Train: (LOSS: 0.1035, MAE: 0.1035, RMSE: 0.1496, R2: 0.6612), Valid: (LOSS: 0.1214, MAE: 0.1214, RMSE: 0.1662, R2: 0.5952), PNorm: 169.4874, GNorm: 2.2364
[18/299] timecost: 63.79, lr: 0.000030, Train: (LOSS: 0.1023, MAE: 0.1023, RMSE: 0.1470, R2: 0.6723), Valid: (LOSS: 0.0976, MAE: 0.0976, RMSE: 0.1408, R2: 0.7250), PNorm: 169.3099, GNorm: 1.2522
[19/299] timecost: 63.41, lr: 0.000030, Train: (LOSS: 0.1004, MAE: 0.1004, RMSE: 0.1462, R2: 0.6695), Valid: (LOSS: 0.0995, MAE: 0.0995, RMSE: 0.1406, R2: 0.7263), PNorm: 169.1342, GNorm: 1.0494
[20/299] timecost: 63.40, lr: 0.000030, Train: (LOSS: 0.0979, MAE: 0.0979, RMSE: 0.1445, R2: 0.6851), Valid: (LOSS: 0.0938, MAE: 0.0938, RMSE: 0.1327, R2: 0.7500), PNorm: 168.9636, GNorm: 3.9879
[21/299] timecost: 63.43, lr: 0.000030, Train: (LOSS: 0.0959, MAE: 0.0959, RMSE: 0.1399, R2: 0.6994), Valid: (LOSS: 0.0917, MAE: 0.0917, RMSE: 0.1302, R2: 0.7607), PNorm: 168.7958, GNorm: 1.1283
[22/299] timecost: 63.72, lr: 0.000030, Train: (LOSS: 0.0919, MAE: 0.0919, RMSE: 0.1362, R2: 0.7082), Valid: (LOSS: 0.1020, MAE: 0.1020, RMSE: 0.1447, R2: 0.7014), PNorm: 168.6293, GNorm: 3.5379
[23/299] timecost: 63.95, lr: 0.000030, Train: (LOSS: 0.0915, MAE: 0.0915, RMSE: 0.1356, R2: 0.7137), Valid: (LOSS: 0.0990, MAE: 0.0990, RMSE: 0.1364, R2: 0.7311), PNorm: 168.4664, GNorm: 1.6492
[24/299] timecost: 63.93, lr: 0.000030, Train: (LOSS: 0.0920, MAE: 0.0920, RMSE: 0.1349, R2: 0.7219), Valid: (LOSS: 0.0971, MAE: 0.0971, RMSE: 0.1435, R2: 0.6997), PNorm: 168.3070, GNorm: 3.0097
[25/299] timecost: 63.61, lr: 0.000030, Train: (LOSS: 0.0922, MAE: 0.0922, RMSE: 0.1358, R2: 0.7195), Valid: (LOSS: 0.0962, MAE: 0.0962, RMSE: 0.1365, R2: 0.7258), PNorm: 168.1510, GNorm: 1.2876
[26/299] timecost: 64.23, lr: 0.000030, Train: (LOSS: 0.0897, MAE: 0.0897, RMSE: 0.1330, R2: 0.7273), Valid: (LOSS: 0.0961, MAE: 0.0961, RMSE: 0.1377, R2: 0.7198), PNorm: 167.9963, GNorm: 4.5005
[27/299] timecost: 64.05, lr: 0.000030, Train: (LOSS: 0.0852, MAE: 0.0852, RMSE: 0.1275, R2: 0.7478), Valid: (LOSS: 0.0883, MAE: 0.0883, RMSE: 0.1261, R2: 0.7713), PNorm: 167.8423, GNorm: 3.8362
[28/299] timecost: 64.12, lr: 0.000030, Train: (LOSS: 0.0871, MAE: 0.0871, RMSE: 0.1310, R2: 0.7442), Valid: (LOSS: 0.0912, MAE: 0.0912, RMSE: 0.1267, R2: 0.7601), PNorm: 167.6934, GNorm: 2.4077
[29/299] timecost: 62.97, lr: 0.000030, Train: (LOSS: 0.0813, MAE: 0.0813, RMSE: 0.1238, R2: 0.7650), Valid: (LOSS: 0.0776, MAE: 0.0776, RMSE: 0.1129, R2: 0.8155), PNorm: 167.5466, GNorm: 3.7560
[30/299] timecost: 63.43, lr: 0.000030, Train: (LOSS: 0.0810, MAE: 0.0810, RMSE: 0.1218, R2: 0.7756), Valid: (LOSS: 0.0837, MAE: 0.0837, RMSE: 0.1276, R2: 0.7594), PNorm: 167.4024, GNorm: 2.9935
[31/299] timecost: 64.26, lr: 0.000030, Train: (LOSS: 0.0813, MAE: 0.0813, RMSE: 0.1240, R2: 0.7546), Valid: (LOSS: 0.0838, MAE: 0.0838, RMSE: 0.1215, R2: 0.7898), PNorm: 167.2614, GNorm: 1.2254
[32/299] timecost: 64.37, lr: 0.000030, Train: (LOSS: 0.0801, MAE: 0.0801, RMSE: 0.1197, R2: 0.7741), Valid: (LOSS: 0.0864, MAE: 0.0864, RMSE: 0.1241, R2: 0.7661), PNorm: 167.1234, GNorm: 2.4809
[33/299] timecost: 63.87, lr: 0.000030, Train: (LOSS: 0.0779, MAE: 0.0779, RMSE: 0.1190, R2: 0.7783), Valid: (LOSS: 0.0759, MAE: 0.0759, RMSE: 0.1130, R2: 0.8091), PNorm: 166.9883, GNorm: 2.5392
[34/299] timecost: 63.88, lr: 0.000030, Train: (LOSS: 0.0761, MAE: 0.0761, RMSE: 0.1168, R2: 0.7892), Valid: (LOSS: 0.0842, MAE: 0.0842, RMSE: 0.1199, R2: 0.7910), PNorm: 166.8527, GNorm: 1.8238
[35/299] timecost: 64.08, lr: 0.000030, Train: (LOSS: 0.0755, MAE: 0.0755, RMSE: 0.1164, R2: 0.7873), Valid: (LOSS: 0.0787, MAE: 0.0787, RMSE: 0.1194, R2: 0.7852), PNorm: 166.7207, GNorm: 1.1868
[36/299] timecost: 63.78, lr: 0.000030, Train: (LOSS: 0.0743, MAE: 0.0743, RMSE: 0.1157, R2: 0.7940), Valid: (LOSS: 0.0741, MAE: 0.0741, RMSE: 0.1092, R2: 0.8155), PNorm: 166.5887, GNorm: 1.5147
[37/299] timecost: 64.00, lr: 0.000030, Train: (LOSS: 0.0715, MAE: 0.0715, RMSE: 0.1117, R2: 0.8052), Valid: (LOSS: 0.0768, MAE: 0.0768, RMSE: 0.1192, R2: 0.7942), PNorm: 166.4568, GNorm: 0.8938
[38/299] timecost: 62.89, lr: 0.000030, Train: (LOSS: 0.0780, MAE: 0.0780, RMSE: 0.1210, R2: 0.7726), Valid: (LOSS: 0.0761, MAE: 0.0761, RMSE: 0.1139, R2: 0.7995), PNorm: 166.3299, GNorm: 3.1848
[39/299] timecost: 63.34, lr: 0.000030, Train: (LOSS: 0.0702, MAE: 0.0702, RMSE: 0.1109, R2: 0.8092), Valid: (LOSS: 0.0677, MAE: 0.0677, RMSE: 0.1040, R2: 0.8314), PNorm: 166.2013, GNorm: 1.7938
[40/299] timecost: 63.55, lr: 0.000030, Train: (LOSS: 0.0690, MAE: 0.0690, RMSE: 0.1089, R2: 0.8177), Valid: (LOSS: 0.0716, MAE: 0.0716, RMSE: 0.1108, R2: 0.8029), PNorm: 166.0738, GNorm: 1.0989
[41/299] timecost: 63.49, lr: 0.000030, Train: (LOSS: 0.0687, MAE: 0.0687, RMSE: 0.1091, R2: 0.8185), Valid: (LOSS: 0.0682, MAE: 0.0682, RMSE: 0.1102, R2: 0.8100), PNorm: 165.9473, GNorm: 1.2617
[42/299] timecost: 62.46, lr: 0.000030, Train: (LOSS: 0.0690, MAE: 0.0690, RMSE: 0.1088, R2: 0.8126), Valid: (LOSS: 0.0807, MAE: 0.0807, RMSE: 0.1134, R2: 0.8064), PNorm: 165.8224, GNorm: 2.5514
[43/299] timecost: 62.33, lr: 0.000030, Train: (LOSS: 0.0695, MAE: 0.0695, RMSE: 0.1098, R2: 0.8103), Valid: (LOSS: 0.0720, MAE: 0.0720, RMSE: 0.1052, R2: 0.8271), PNorm: 165.7000, GNorm: 2.0242
[44/299] timecost: 63.17, lr: 0.000030, Train: (LOSS: 0.0680, MAE: 0.0680, RMSE: 0.1062, R2: 0.8258), Valid: (LOSS: 0.0701, MAE: 0.0701, RMSE: 0.1009, R2: 0.8373), PNorm: 165.5789, GNorm: 1.3205
[45/299] timecost: 64.20, lr: 0.000030, Train: (LOSS: 0.0661, MAE: 0.0661, RMSE: 0.1037, R2: 0.8295), Valid: (LOSS: 0.0604, MAE: 0.0604, RMSE: 0.0959, R2: 0.8513), PNorm: 165.4576, GNorm: 1.2984
[46/299] timecost: 63.62, lr: 0.000030, Train: (LOSS: 0.0649, MAE: 0.0649, RMSE: 0.1020, R2: 0.8360), Valid: (LOSS: 0.0608, MAE: 0.0608, RMSE: 0.0884, R2: 0.8772), PNorm: 165.3379, GNorm: 1.1033
[47/299] timecost: 63.76, lr: 0.000030, Train: (LOSS: 0.0615, MAE: 0.0615, RMSE: 0.0971, R2: 0.8492), Valid: (LOSS: 0.0756, MAE: 0.0756, RMSE: 0.1075, R2: 0.8205), PNorm: 165.2171, GNorm: 1.1443
[48/299] timecost: 63.68, lr: 0.000030, Train: (LOSS: 0.0623, MAE: 0.0623, RMSE: 0.0998, R2: 0.8422), Valid: (LOSS: 0.0586, MAE: 0.0586, RMSE: 0.0844, R2: 0.8883), PNorm: 165.0987, GNorm: 1.8070
[49/299] timecost: 63.81, lr: 0.000030, Train: (LOSS: 0.0610, MAE: 0.0610, RMSE: 0.0963, R2: 0.8562), Valid: (LOSS: 0.0606, MAE: 0.0606, RMSE: 0.0858, R2: 0.8884), PNorm: 164.9802, GNorm: 3.5425
[50/299] timecost: 64.23, lr: 0.000030, Train: (LOSS: 0.0593, MAE: 0.0593, RMSE: 0.0955, R2: 0.8549), Valid: (LOSS: 0.0755, MAE: 0.0755, RMSE: 0.1113, R2: 0.8256), PNorm: 164.8639, GNorm: 1.1488
[51/299] timecost: 63.93, lr: 0.000030, Train: (LOSS: 0.0592, MAE: 0.0592, RMSE: 0.0944, R2: 0.8575), Valid: (LOSS: 0.0558, MAE: 0.0558, RMSE: 0.0788, R2: 0.9029), PNorm: 164.7488, GNorm: 1.3167
[52/299] timecost: 63.78, lr: 0.000030, Train: (LOSS: 0.0561, MAE: 0.0561, RMSE: 0.0895, R2: 0.8668), Valid: (LOSS: 0.0534, MAE: 0.0534, RMSE: 0.0777, R2: 0.9044), PNorm: 164.6343, GNorm: 1.8242
[53/299] timecost: 63.85, lr: 0.000030, Train: (LOSS: 0.0533, MAE: 0.0533, RMSE: 0.0867, R2: 0.8787), Valid: (LOSS: 0.0519, MAE: 0.0519, RMSE: 0.0752, R2: 0.9106), PNorm: 164.5200, GNorm: 1.0849
[54/299] timecost: 63.94, lr: 0.000030, Train: (LOSS: 0.0565, MAE: 0.0565, RMSE: 0.0902, R2: 0.8743), Valid: (LOSS: 0.0593, MAE: 0.0593, RMSE: 0.0828, R2: 0.8983), PNorm: 164.4069, GNorm: 2.3754
[55/299] timecost: 63.72, lr: 0.000030, Train: (LOSS: 0.0550, MAE: 0.0550, RMSE: 0.0877, R2: 0.8755), Valid: (LOSS: 0.0566, MAE: 0.0566, RMSE: 0.0784, R2: 0.9100), PNorm: 164.2946, GNorm: 1.6518
[56/299] timecost: 64.01, lr: 0.000030, Train: (LOSS: 0.0524, MAE: 0.0524, RMSE: 0.0842, R2: 0.8870), Valid: (LOSS: 0.0524, MAE: 0.0524, RMSE: 0.0745, R2: 0.9120), PNorm: 164.1831, GNorm: 1.5086
[57/299] timecost: 63.53, lr: 0.000030, Train: (LOSS: 0.0529, MAE: 0.0529, RMSE: 0.0837, R2: 0.8835), Valid: (LOSS: 0.0542, MAE: 0.0542, RMSE: 0.0782, R2: 0.9021), PNorm: 164.0717, GNorm: 1.6527
[58/299] timecost: 63.80, lr: 0.000030, Train: (LOSS: 0.0506, MAE: 0.0506, RMSE: 0.0827, R2: 0.8877), Valid: (LOSS: 0.0498, MAE: 0.0498, RMSE: 0.0759, R2: 0.9093), PNorm: 163.9602, GNorm: 1.4333
[59/299] timecost: 63.69, lr: 0.000030, Train: (LOSS: 0.0485, MAE: 0.0485, RMSE: 0.0789, R2: 0.8939), Valid: (LOSS: 0.0543, MAE: 0.0543, RMSE: 0.0770, R2: 0.9073), PNorm: 163.8493, GNorm: 0.9880
[60/299] timecost: 64.12, lr: 0.000030, Train: (LOSS: 0.0505, MAE: 0.0505, RMSE: 0.0818, R2: 0.8881), Valid: (LOSS: 0.0594, MAE: 0.0594, RMSE: 0.0950, R2: 0.8660), PNorm: 163.7377, GNorm: 1.6956
[61/299] timecost: 63.90, lr: 0.000030, Train: (LOSS: 0.0491, MAE: 0.0491, RMSE: 0.0796, R2: 0.8963), Valid: (LOSS: 0.0501, MAE: 0.0501, RMSE: 0.0719, R2: 0.9227), PNorm: 163.6277, GNorm: 1.2839
[62/299] timecost: 62.60, lr: 0.000030, Train: (LOSS: 0.0495, MAE: 0.0495, RMSE: 0.0799, R2: 0.8974), Valid: (LOSS: 0.0521, MAE: 0.0521, RMSE: 0.0729, R2: 0.9187), PNorm: 163.5183, GNorm: 1.5595
[63/299] timecost: 62.56, lr: 0.000030, Train: (LOSS: 0.0467, MAE: 0.0467, RMSE: 0.0781, R2: 0.9021), Valid: (LOSS: 0.0473, MAE: 0.0473, RMSE: 0.0688, R2: 0.9269), PNorm: 163.4072, GNorm: 1.6503
[64/299] timecost: 62.56, lr: 0.000030, Train: (LOSS: 0.0479, MAE: 0.0479, RMSE: 0.0792, R2: 0.8967), Valid: (LOSS: 0.0534, MAE: 0.0534, RMSE: 0.0776, R2: 0.9130), PNorm: 163.2965, GNorm: 1.3868
[65/299] timecost: 62.52, lr: 0.000030, Train: (LOSS: 0.0477, MAE: 0.0477, RMSE: 0.0781, R2: 0.9006), Valid: (LOSS: 0.0489, MAE: 0.0489, RMSE: 0.0684, R2: 0.9270), PNorm: 163.1887, GNorm: 1.1927
[66/299] timecost: 62.37, lr: 0.000030, Train: (LOSS: 0.0455, MAE: 0.0455, RMSE: 0.0752, R2: 0.9079), Valid: (LOSS: 0.0470, MAE: 0.0470, RMSE: 0.0686, R2: 0.9287), PNorm: 163.0787, GNorm: 1.3552
[67/299] timecost: 64.32, lr: 0.000030, Train: (LOSS: 0.0451, MAE: 0.0451, RMSE: 0.0732, R2: 0.9071), Valid: (LOSS: 0.0463, MAE: 0.0463, RMSE: 0.0668, R2: 0.9333), PNorm: 162.9686, GNorm: 2.2141
[68/299] timecost: 63.06, lr: 0.000030, Train: (LOSS: 0.0454, MAE: 0.0454, RMSE: 0.0741, R2: 0.9106), Valid: (LOSS: 0.0496, MAE: 0.0496, RMSE: 0.0710, R2: 0.9261), PNorm: 162.8602, GNorm: 1.7399
[69/299] timecost: 63.50, lr: 0.000030, Train: (LOSS: 0.0455, MAE: 0.0455, RMSE: 0.0749, R2: 0.9092), Valid: (LOSS: 0.0451, MAE: 0.0451, RMSE: 0.0645, R2: 0.9385), PNorm: 162.7508, GNorm: 1.6219
[70/299] timecost: 63.66, lr: 0.000030, Train: (LOSS: 0.0438, MAE: 0.0438, RMSE: 0.0707, R2: 0.9179), Valid: (LOSS: 0.0462, MAE: 0.0462, RMSE: 0.0652, R2: 0.9401), PNorm: 162.6423, GNorm: 1.5937
[71/299] timecost: 63.53, lr: 0.000030, Train: (LOSS: 0.0427, MAE: 0.0427, RMSE: 0.0698, R2: 0.9230), Valid: (LOSS: 0.0450, MAE: 0.0450, RMSE: 0.0641, R2: 0.9379), PNorm: 162.5334, GNorm: 1.1396
[72/299] timecost: 63.84, lr: 0.000030, Train: (LOSS: 0.0416, MAE: 0.0416, RMSE: 0.0681, R2: 0.9221), Valid: (LOSS: 0.0501, MAE: 0.0501, RMSE: 0.0683, R2: 0.9320), PNorm: 162.4243, GNorm: 2.2840
[73/299] timecost: 63.73, lr: 0.000030, Train: (LOSS: 0.0418, MAE: 0.0418, RMSE: 0.0682, R2: 0.9239), Valid: (LOSS: 0.0524, MAE: 0.0524, RMSE: 0.0745, R2: 0.9180), PNorm: 162.3157, GNorm: 1.6015
[74/299] timecost: 63.53, lr: 0.000030, Train: (LOSS: 0.0413, MAE: 0.0413, RMSE: 0.0671, R2: 0.9272), Valid: (LOSS: 0.0459, MAE: 0.0459, RMSE: 0.0646, R2: 0.9373), PNorm: 162.2071, GNorm: 1.0224
[75/299] timecost: 62.91, lr: 0.000030, Train: (LOSS: 0.0423, MAE: 0.0423, RMSE: 0.0686, R2: 0.9204), Valid: (LOSS: 0.0498, MAE: 0.0498, RMSE: 0.0699, R2: 0.9322), PNorm: 162.0981, GNorm: 1.4512
[76/299] timecost: 63.06, lr: 0.000030, Train: (LOSS: 0.0406, MAE: 0.0406, RMSE: 0.0661, R2: 0.9271), Valid: (LOSS: 0.0450, MAE: 0.0450, RMSE: 0.0630, R2: 0.9417), PNorm: 161.9893, GNorm: 1.0168
[77/299] timecost: 63.52, lr: 0.000030, Train: (LOSS: 0.0401, MAE: 0.0401, RMSE: 0.0662, R2: 0.9254), Valid: (LOSS: 0.0470, MAE: 0.0470, RMSE: 0.0656, R2: 0.9370), PNorm: 161.8811, GNorm: 1.8626
[78/299] timecost: 64.49, lr: 0.000030, Train: (LOSS: 0.0395, MAE: 0.0395, RMSE: 0.0639, R2: 0.9342), Valid: (LOSS: 0.0442, MAE: 0.0442, RMSE: 0.0648, R2: 0.9394), PNorm: 161.7722, GNorm: 1.0306
[79/299] timecost: 64.52, lr: 0.000030, Train: (LOSS: 0.0388, MAE: 0.0388, RMSE: 0.0632, R2: 0.9337), Valid: (LOSS: 0.0534, MAE: 0.0534, RMSE: 0.0742, R2: 0.9231), PNorm: 161.6637, GNorm: 2.5470
[80/299] timecost: 64.20, lr: 0.000030, Train: (LOSS: 0.0377, MAE: 0.0377, RMSE: 0.0625, R2: 0.9358), Valid: (LOSS: 0.0458, MAE: 0.0458, RMSE: 0.0651, R2: 0.9369), PNorm: 161.5556, GNorm: 1.0992
[81/299] timecost: 63.84, lr: 0.000030, Train: (LOSS: 0.0386, MAE: 0.0386, RMSE: 0.0646, R2: 0.9318), Valid: (LOSS: 0.0443, MAE: 0.0443, RMSE: 0.0637, R2: 0.9402), PNorm: 161.4458, GNorm: 1.6248
[82/299] timecost: 63.90, lr: 0.000030, Train: (LOSS: 0.0388, MAE: 0.0388, RMSE: 0.0626, R2: 0.9328), Valid: (LOSS: 0.0473, MAE: 0.0473, RMSE: 0.0665, R2: 0.9369), PNorm: 161.3374, GNorm: 1.1080
[83/299] timecost: 63.93, lr: 0.000030, Train: (LOSS: 0.0377, MAE: 0.0377, RMSE: 0.0614, R2: 0.9365), Valid: (LOSS: 0.0504, MAE: 0.0504, RMSE: 0.0728, R2: 0.9249), PNorm: 161.2289, GNorm: 1.9530
[84/299] timecost: 63.84, lr: 0.000030, Train: (LOSS: 0.0377, MAE: 0.0377, RMSE: 0.0602, R2: 0.9368), Valid: (LOSS: 0.0446, MAE: 0.0446, RMSE: 0.0645, R2: 0.9404), PNorm: 161.1188, GNorm: 1.1317
[85/299] timecost: 63.83, lr: 0.000030, Train: (LOSS: 0.0370, MAE: 0.0370, RMSE: 0.0599, R2: 0.9406), Valid: (LOSS: 0.0416, MAE: 0.0416, RMSE: 0.0588, R2: 0.9492), PNorm: 161.0101, GNorm: 2.5274
[86/299] timecost: 63.69, lr: 0.000030, Train: (LOSS: 0.0394, MAE: 0.0394, RMSE: 0.0633, R2: 0.9302), Valid: (LOSS: 0.0439, MAE: 0.0439, RMSE: 0.0614, R2: 0.9439), PNorm: 160.9040, GNorm: 1.1031
[87/299] timecost: 64.55, lr: 0.000030, Train: (LOSS: 0.0377, MAE: 0.0377, RMSE: 0.0608, R2: 0.9370), Valid: (LOSS: 0.0440, MAE: 0.0440, RMSE: 0.0646, R2: 0.9386), PNorm: 160.7965, GNorm: 1.5876
[88/299] timecost: 64.97, lr: 0.000030, Train: (LOSS: 0.0366, MAE: 0.0366, RMSE: 0.0590, R2: 0.9408), Valid: (LOSS: 0.0411, MAE: 0.0411, RMSE: 0.0592, R2: 0.9497), PNorm: 160.6881, GNorm: 0.9365
[89/299] timecost: 64.28, lr: 0.000030, Train: (LOSS: 0.0349, MAE: 0.0349, RMSE: 0.0572, R2: 0.9403), Valid: (LOSS: 0.0411, MAE: 0.0411, RMSE: 0.0595, R2: 0.9485), PNorm: 160.5793, GNorm: 0.7960
[90/299] timecost: 64.61, lr: 0.000030, Train: (LOSS: 0.0368, MAE: 0.0368, RMSE: 0.0590, R2: 0.9420), Valid: (LOSS: 0.0417, MAE: 0.0417, RMSE: 0.0591, R2: 0.9492), PNorm: 160.4716, GNorm: 1.0044
[91/299] timecost: 64.60, lr: 0.000030, Train: (LOSS: 0.0350, MAE: 0.0350, RMSE: 0.0578, R2: 0.9452), Valid: (LOSS: 0.0419, MAE: 0.0419, RMSE: 0.0603, R2: 0.9482), PNorm: 160.3642, GNorm: 1.7005
[92/299] timecost: 64.33, lr: 0.000030, Train: (LOSS: 0.0340, MAE: 0.0340, RMSE: 0.0565, R2: 0.9463), Valid: (LOSS: 0.0421, MAE: 0.0421, RMSE: 0.0610, R2: 0.9472), PNorm: 160.2551, GNorm: 1.1179
[93/299] timecost: 64.11, lr: 0.000030, Train: (LOSS: 0.0345, MAE: 0.0345, RMSE: 0.0566, R2: 0.9474), Valid: (LOSS: 0.0467, MAE: 0.0467, RMSE: 0.0677, R2: 0.9346), PNorm: 160.1470, GNorm: 1.3549
[94/299] timecost: 63.32, lr: 0.000030, Train: (LOSS: 0.0340, MAE: 0.0340, RMSE: 0.0544, R2: 0.9487), Valid: (LOSS: 0.0414, MAE: 0.0414, RMSE: 0.0606, R2: 0.9474), PNorm: 160.0382, GNorm: 1.5919
[95/299] timecost: 63.31, lr: 0.000030, Train: (LOSS: 0.0333, MAE: 0.0333, RMSE: 0.0538, R2: 0.9500), Valid: (LOSS: 0.0420, MAE: 0.0420, RMSE: 0.0618, R2: 0.9444), PNorm: 159.9292, GNorm: 1.4326
[96/299] timecost: 63.57, lr: 0.000030, Train: (LOSS: 0.0362, MAE: 0.0362, RMSE: 0.0577, R2: 0.9448), Valid: (LOSS: 0.0409, MAE: 0.0409, RMSE: 0.0590, R2: 0.9499), PNorm: 159.8219, GNorm: 1.4172
[97/299] timecost: 62.89, lr: 0.000030, Train: (LOSS: 0.0335, MAE: 0.0335, RMSE: 0.0537, R2: 0.9494), Valid: (LOSS: 0.0390, MAE: 0.0390, RMSE: 0.0572, R2: 0.9532), PNorm: 159.7137, GNorm: 0.9458
[98/299] timecost: 63.69, lr: 0.000030, Train: (LOSS: 0.0329, MAE: 0.0329, RMSE: 0.0527, R2: 0.9543), Valid: (LOSS: 0.0446, MAE: 0.0446, RMSE: 0.0644, R2: 0.9418), PNorm: 159.6059, GNorm: 1.9243
[99/299] timecost: 63.42, lr: 0.000030, Train: (LOSS: 0.0336, MAE: 0.0336, RMSE: 0.0543, R2: 0.9503), Valid: (LOSS: 0.0408, MAE: 0.0408, RMSE: 0.0603, R2: 0.9481), PNorm: 159.4981, GNorm: 1.6997
[100/299] timecost: 64.04, lr: 0.000030, Train: (LOSS: 0.0316, MAE: 0.0316, RMSE: 0.0504, R2: 0.9566), Valid: (LOSS: 0.0419, MAE: 0.0419, RMSE: 0.0624, R2: 0.9457), PNorm: 159.3891, GNorm: 0.9846
[101/299] timecost: 65.19, lr: 0.000030, Train: (LOSS: 0.0330, MAE: 0.0330, RMSE: 0.0527, R2: 0.9547), Valid: (LOSS: 0.0417, MAE: 0.0417, RMSE: 0.0612, R2: 0.9450), PNorm: 159.2797, GNorm: 1.8714
[102/299] timecost: 64.97, lr: 0.000030, Train: (LOSS: 0.0335, MAE: 0.0335, RMSE: 0.0535, R2: 0.9537), Valid: (LOSS: 0.0420, MAE: 0.0420, RMSE: 0.0595, R2: 0.9509), PNorm: 159.1731, GNorm: 1.2031
[103/299] timecost: 64.81, lr: 0.000030, Train: (LOSS: 0.0334, MAE: 0.0334, RMSE: 0.0533, R2: 0.9530), Valid: (LOSS: 0.0460, MAE: 0.0460, RMSE: 0.0666, R2: 0.9387), PNorm: 159.0661, GNorm: 1.0454
[104/299] timecost: 63.70, lr: 0.000030, Train: (LOSS: 0.0308, MAE: 0.0308, RMSE: 0.0497, R2: 0.9562), Valid: (LOSS: 0.0395, MAE: 0.0395, RMSE: 0.0594, R2: 0.9496), PNorm: 158.9598, GNorm: 0.8496
[105/299] timecost: 63.83, lr: 0.000030, Train: (LOSS: 0.0310, MAE: 0.0310, RMSE: 0.0501, R2: 0.9562), Valid: (LOSS: 0.0421, MAE: 0.0421, RMSE: 0.0607, R2: 0.9458), PNorm: 158.8532, GNorm: 1.6699
[106/299] timecost: 64.69, lr: 0.000030, Train: (LOSS: 0.0308, MAE: 0.0308, RMSE: 0.0497, R2: 0.9588), Valid: (LOSS: 0.0410, MAE: 0.0410, RMSE: 0.0590, R2: 0.9497), PNorm: 158.7477, GNorm: 1.0178
[107/299] timecost: 65.03, lr: 0.000030, Train: (LOSS: 0.0313, MAE: 0.0313, RMSE: 0.0497, R2: 0.9588), Valid: (LOSS: 0.0416, MAE: 0.0416, RMSE: 0.0607, R2: 0.9470), PNorm: 158.6401, GNorm: 1.3950
[108/299] timecost: 64.40, lr: 0.000030, Train: (LOSS: 0.0297, MAE: 0.0297, RMSE: 0.0473, R2: 0.9605), Valid: (LOSS: 0.0396, MAE: 0.0396, RMSE: 0.0590, R2: 0.9499), PNorm: 158.5339, GNorm: 1.4269
[109/299] timecost: 64.91, lr: 0.000030, Train: (LOSS: 0.0312, MAE: 0.0312, RMSE: 0.0493, R2: 0.9589), Valid: (LOSS: 0.0456, MAE: 0.0456, RMSE: 0.0652, R2: 0.9408), PNorm: 158.4273, GNorm: 1.0899
[110/299] timecost: 64.56, lr: 0.000030, Train: (LOSS: 0.0308, MAE: 0.0308, RMSE: 0.0488, R2: 0.9612), Valid: (LOSS: 0.0433, MAE: 0.0433, RMSE: 0.0648, R2: 0.9418), PNorm: 158.3219, GNorm: 1.9504
[111/299] timecost: 64.69, lr: 0.000030, Train: (LOSS: 0.0298, MAE: 0.0298, RMSE: 0.0472, R2: 0.9636), Valid: (LOSS: 0.0425, MAE: 0.0425, RMSE: 0.0615, R2: 0.9459), PNorm: 158.2157, GNorm: 0.8635
[112/299] timecost: 64.89, lr: 0.000030, Train: (LOSS: 0.0297, MAE: 0.0297, RMSE: 0.0473, R2: 0.9631), Valid: (LOSS: 0.0404, MAE: 0.0404, RMSE: 0.0583, R2: 0.9517), PNorm: 158.1091, GNorm: 0.9664
[113/299] timecost: 64.85, lr: 0.000030, Train: (LOSS: 0.0278, MAE: 0.0278, RMSE: 0.0447, R2: 0.9659), Valid: (LOSS: 0.0408, MAE: 0.0408, RMSE: 0.0593, R2: 0.9501), PNorm: 158.0013, GNorm: 1.2242
[114/299] timecost: 64.98, lr: 0.000030, Train: (LOSS: 0.0293, MAE: 0.0293, RMSE: 0.0461, R2: 0.9632), Valid: (LOSS: 0.0431, MAE: 0.0431, RMSE: 0.0622, R2: 0.9425), PNorm: 157.8948, GNorm: 2.0664
[115/299] timecost: 63.61, lr: 0.000030, Train: (LOSS: 0.0289, MAE: 0.0289, RMSE: 0.0459, R2: 0.9634), Valid: (LOSS: 0.0443, MAE: 0.0443, RMSE: 0.0641, R2: 0.9413), PNorm: 157.7863, GNorm: 1.9521
[116/299] timecost: 63.24, lr: 0.000030, Train: (LOSS: 0.0293, MAE: 0.0293, RMSE: 0.0457, R2: 0.9642), Valid: (LOSS: 0.0417, MAE: 0.0417, RMSE: 0.0616, R2: 0.9461), PNorm: 157.6791, GNorm: 0.9301
[117/299] timecost: 62.89, lr: 0.000030, Train: (LOSS: 0.0281, MAE: 0.0281, RMSE: 0.0452, R2: 0.9641), Valid: (LOSS: 0.0417, MAE: 0.0417, RMSE: 0.0602, R2: 0.9492), PNorm: 157.5724, GNorm: 1.2026
Epoch 00119: reducing learning rate of group 0 to 2.7000e-05.
[118/299] timecost: 63.19, lr: 0.000027, Train: (LOSS: 0.0269, MAE: 0.0269, RMSE: 0.0423, R2: 0.9704), Valid: (LOSS: 0.0414, MAE: 0.0414, RMSE: 0.0599, R2: 0.9486), PNorm: 157.4655, GNorm: 1.1895
[119/299] timecost: 62.77, lr: 0.000027, Train: (LOSS: 0.0278, MAE: 0.0278, RMSE: 0.0431, R2: 0.9691), Valid: (LOSS: 0.0439, MAE: 0.0439, RMSE: 0.0627, R2: 0.9443), PNorm: 157.3686, GNorm: 2.1203
[120/299] timecost: 63.09, lr: 0.000027, Train: (LOSS: 0.0268, MAE: 0.0268, RMSE: 0.0431, R2: 0.9699), Valid: (LOSS: 0.0392, MAE: 0.0392, RMSE: 0.0571, R2: 0.9524), PNorm: 157.2714, GNorm: 1.2435
[121/299] timecost: 62.68, lr: 0.000027, Train: (LOSS: 0.0263, MAE: 0.0263, RMSE: 0.0412, R2: 0.9712), Valid: (LOSS: 0.0396, MAE: 0.0396, RMSE: 0.0586, R2: 0.9521), PNorm: 157.1738, GNorm: 1.3135
[122/299] timecost: 62.56, lr: 0.000027, Train: (LOSS: 0.0263, MAE: 0.0263, RMSE: 0.0410, R2: 0.9708), Valid: (LOSS: 0.0394, MAE: 0.0394, RMSE: 0.0612, R2: 0.9471), PNorm: 157.0766, GNorm: 1.1231
[123/299] timecost: 62.63, lr: 0.000027, Train: (LOSS: 0.0271, MAE: 0.0271, RMSE: 0.0418, R2: 0.9706), Valid: (LOSS: 0.0413, MAE: 0.0413, RMSE: 0.0601, R2: 0.9477), PNorm: 156.9799, GNorm: 1.0562
[124/299] timecost: 62.61, lr: 0.000027, Train: (LOSS: 0.0258, MAE: 0.0258, RMSE: 0.0406, R2: 0.9732), Valid: (LOSS: 0.0435, MAE: 0.0435, RMSE: 0.0621, R2: 0.9459), PNorm: 156.8824, GNorm: 2.0877
[125/299] timecost: 62.68, lr: 0.000027, Train: (LOSS: 0.0265, MAE: 0.0265, RMSE: 0.0412, R2: 0.9718), Valid: (LOSS: 0.0416, MAE: 0.0416, RMSE: 0.0624, R2: 0.9432), PNorm: 156.7851, GNorm: 1.3449
[126/299] timecost: 62.66, lr: 0.000027, Train: (LOSS: 0.0265, MAE: 0.0265, RMSE: 0.0410, R2: 0.9724), Valid: (LOSS: 0.0401, MAE: 0.0401, RMSE: 0.0589, R2: 0.9514), PNorm: 156.6876, GNorm: 1.3493
[127/299] timecost: 62.61, lr: 0.000027, Train: (LOSS: 0.0256, MAE: 0.0256, RMSE: 0.0397, R2: 0.9740), Valid: (LOSS: 0.0405, MAE: 0.0405, RMSE: 0.0606, R2: 0.9486), PNorm: 156.5907, GNorm: 0.9888
[128/299] timecost: 63.34, lr: 0.000027, Train: (LOSS: 0.0255, MAE: 0.0255, RMSE: 0.0398, R2: 0.9749), Valid: (LOSS: 0.0410, MAE: 0.0410, RMSE: 0.0604, R2: 0.9490), PNorm: 156.4947, GNorm: 1.2739
[129/299] timecost: 63.23, lr: 0.000027, Train: (LOSS: 0.0259, MAE: 0.0259, RMSE: 0.0393, R2: 0.9754), Valid: (LOSS: 0.0391, MAE: 0.0391, RMSE: 0.0586, R2: 0.9503), PNorm: 156.3972, GNorm: 1.6209
[130/299] timecost: 62.50, lr: 0.000027, Train: (LOSS: 0.0258, MAE: 0.0258, RMSE: 0.0397, R2: 0.9745), Valid: (LOSS: 0.0396, MAE: 0.0396, RMSE: 0.0572, R2: 0.9541), PNorm: 156.3007, GNorm: 0.9078
[131/299] timecost: 62.54, lr: 0.000027, Train: (LOSS: 0.0256, MAE: 0.0256, RMSE: 0.0395, R2: 0.9735), Valid: (LOSS: 0.0387, MAE: 0.0387, RMSE: 0.0564, R2: 0.9558), PNorm: 156.2042, GNorm: 0.9513
[132/299] timecost: 62.22, lr: 0.000027, Train: (LOSS: 0.0243, MAE: 0.0243, RMSE: 0.0376, R2: 0.9771), Valid: (LOSS: 0.0416, MAE: 0.0416, RMSE: 0.0623, R2: 0.9443), PNorm: 156.1071, GNorm: 2.0703
[133/299] timecost: 62.35, lr: 0.000027, Train: (LOSS: 0.0242, MAE: 0.0242, RMSE: 0.0367, R2: 0.9788), Valid: (LOSS: 0.0392, MAE: 0.0392, RMSE: 0.0558, R2: 0.9565), PNorm: 156.0107, GNorm: 0.8255
[134/299] timecost: 62.23, lr: 0.000027, Train: (LOSS: 0.0251, MAE: 0.0251, RMSE: 0.0385, R2: 0.9763), Valid: (LOSS: 0.0396, MAE: 0.0396, RMSE: 0.0567, R2: 0.9538), PNorm: 155.9144, GNorm: 1.2046
[135/299] timecost: 62.94, lr: 0.000027, Train: (LOSS: 0.0235, MAE: 0.0235, RMSE: 0.0362, R2: 0.9784), Valid: (LOSS: 0.0412, MAE: 0.0412, RMSE: 0.0607, R2: 0.9474), PNorm: 155.8188, GNorm: 2.0048
[136/299] timecost: 62.79, lr: 0.000027, Train: (LOSS: 0.0231, MAE: 0.0231, RMSE: 0.0358, R2: 0.9789), Valid: (LOSS: 0.0414, MAE: 0.0414, RMSE: 0.0607, R2: 0.9473), PNorm: 155.7209, GNorm: 0.9847
[137/299] timecost: 63.61, lr: 0.000027, Train: (LOSS: 0.0241, MAE: 0.0241, RMSE: 0.0365, R2: 0.9779), Valid: (LOSS: 0.0363, MAE: 0.0363, RMSE: 0.0542, R2: 0.9585), PNorm: 155.6250, GNorm: 1.0802
[138/299] timecost: 63.54, lr: 0.000027, Train: (LOSS: 0.0233, MAE: 0.0233, RMSE: 0.0351, R2: 0.9799), Valid: (LOSS: 0.0380, MAE: 0.0380, RMSE: 0.0580, R2: 0.9517), PNorm: 155.5290, GNorm: 1.2752
[139/299] timecost: 63.31, lr: 0.000027, Train: (LOSS: 0.0231, MAE: 0.0231, RMSE: 0.0361, R2: 0.9786), Valid: (LOSS: 0.0398, MAE: 0.0398, RMSE: 0.0595, R2: 0.9513), PNorm: 155.4313, GNorm: 1.8774
[140/299] timecost: 63.03, lr: 0.000027, Train: (LOSS: 0.0227, MAE: 0.0227, RMSE: 0.0334, R2: 0.9808), Valid: (LOSS: 0.0400, MAE: 0.0400, RMSE: 0.0572, R2: 0.9546), PNorm: 155.3348, GNorm: 0.7717
[141/299] timecost: 62.84, lr: 0.000027, Train: (LOSS: 0.0235, MAE: 0.0235, RMSE: 0.0357, R2: 0.9791), Valid: (LOSS: 0.0393, MAE: 0.0393, RMSE: 0.0580, R2: 0.9520), PNorm: 155.2392, GNorm: 1.1685
[142/299] timecost: 62.85, lr: 0.000027, Train: (LOSS: 0.0232, MAE: 0.0232, RMSE: 0.0354, R2: 0.9792), Valid: (LOSS: 0.0430, MAE: 0.0430, RMSE: 0.0625, R2: 0.9429), PNorm: 155.1431, GNorm: 1.9161
[143/299] timecost: 62.91, lr: 0.000027, Train: (LOSS: 0.0225, MAE: 0.0225, RMSE: 0.0340, R2: 0.9806), Valid: (LOSS: 0.0368, MAE: 0.0368, RMSE: 0.0549, R2: 0.9572), PNorm: 155.0474, GNorm: 1.1818
[144/299] timecost: 62.81, lr: 0.000027, Train: (LOSS: 0.0219, MAE: 0.0219, RMSE: 0.0331, R2: 0.9825), Valid: (LOSS: 0.0365, MAE: 0.0365, RMSE: 0.0532, R2: 0.9596), PNorm: 154.9505, GNorm: 0.9968
[145/299] timecost: 63.39, lr: 0.000027, Train: (LOSS: 0.0219, MAE: 0.0219, RMSE: 0.0324, R2: 0.9826), Valid: (LOSS: 0.0383, MAE: 0.0383, RMSE: 0.0556, R2: 0.9573), PNorm: 154.8548, GNorm: 0.8720
[146/299] timecost: 62.91, lr: 0.000027, Train: (LOSS: 0.0221, MAE: 0.0221, RMSE: 0.0335, R2: 0.9817), Valid: (LOSS: 0.0388, MAE: 0.0388, RMSE: 0.0571, R2: 0.9547), PNorm: 154.7591, GNorm: 1.4372
[147/299] timecost: 63.66, lr: 0.000027, Train: (LOSS: 0.0220, MAE: 0.0220, RMSE: 0.0325, R2: 0.9828), Valid: (LOSS: 0.0375, MAE: 0.0375, RMSE: 0.0552, R2: 0.9565), PNorm: 154.6640, GNorm: 1.2666
[148/299] timecost: 63.76, lr: 0.000027, Train: (LOSS: 0.0217, MAE: 0.0217, RMSE: 0.0324, R2: 0.9833), Valid: (LOSS: 0.0374, MAE: 0.0374, RMSE: 0.0546, R2: 0.9577), PNorm: 154.5684, GNorm: 0.9735
[149/299] timecost: 62.97, lr: 0.000027, Train: (LOSS: 0.0210, MAE: 0.0210, RMSE: 0.0316, R2: 0.9838), Valid: (LOSS: 0.0379, MAE: 0.0379, RMSE: 0.0567, R2: 0.9546), PNorm: 154.4725, GNorm: 0.7871
[150/299] timecost: 63.39, lr: 0.000027, Train: (LOSS: 0.0207, MAE: 0.0207, RMSE: 0.0308, R2: 0.9843), Valid: (LOSS: 0.0370, MAE: 0.0370, RMSE: 0.0546, R2: 0.9577), PNorm: 154.3759, GNorm: 1.3071
[151/299] timecost: 64.98, lr: 0.000027, Train: (LOSS: 0.0197, MAE: 0.0197, RMSE: 0.0300, R2: 0.9856), Valid: (LOSS: 0.0379, MAE: 0.0379, RMSE: 0.0560, R2: 0.9560), PNorm: 154.2802, GNorm: 0.7349
[152/299] timecost: 65.28, lr: 0.000027, Train: (LOSS: 0.0206, MAE: 0.0206, RMSE: 0.0312, R2: 0.9849), Valid: (LOSS: 0.0402, MAE: 0.0402, RMSE: 0.0583, R2: 0.9531), PNorm: 154.1847, GNorm: 0.8851
[153/299] timecost: 64.79, lr: 0.000027, Train: (LOSS: 0.0209, MAE: 0.0209, RMSE: 0.0311, R2: 0.9844), Valid: (LOSS: 0.0377, MAE: 0.0377, RMSE: 0.0553, R2: 0.9577), PNorm: 154.0885, GNorm: 1.5068
[154/299] timecost: 64.76, lr: 0.000027, Train: (LOSS: 0.0198, MAE: 0.0198, RMSE: 0.0291, R2: 0.9867), Valid: (LOSS: 0.0370, MAE: 0.0370, RMSE: 0.0563, R2: 0.9547), PNorm: 153.9933, GNorm: 0.9303
[155/299] timecost: 64.99, lr: 0.000027, Train: (LOSS: 0.0213, MAE: 0.0213, RMSE: 0.0314, R2: 0.9842), Valid: (LOSS: 0.0392, MAE: 0.0392, RMSE: 0.0574, R2: 0.9530), PNorm: 153.8978, GNorm: 1.5340
[156/299] timecost: 64.71, lr: 0.000027, Train: (LOSS: 0.0203, MAE: 0.0203, RMSE: 0.0303, R2: 0.9852), Valid: (LOSS: 0.0353, MAE: 0.0353, RMSE: 0.0528, R2: 0.9606), PNorm: 153.8033, GNorm: 1.2036
[157/299] timecost: 64.13, lr: 0.000027, Train: (LOSS: 0.0199, MAE: 0.0199, RMSE: 0.0296, R2: 0.9858), Valid: (LOSS: 0.0371, MAE: 0.0371, RMSE: 0.0528, R2: 0.9603), PNorm: 153.7079, GNorm: 0.8880
[158/299] timecost: 63.33, lr: 0.000027, Train: (LOSS: 0.0191, MAE: 0.0191, RMSE: 0.0281, R2: 0.9877), Valid: (LOSS: 0.0365, MAE: 0.0365, RMSE: 0.0539, R2: 0.9589), PNorm: 153.6133, GNorm: 0.9728
[159/299] timecost: 62.89, lr: 0.000027, Train: (LOSS: 0.0197, MAE: 0.0197, RMSE: 0.0291, R2: 0.9868), Valid: (LOSS: 0.0373, MAE: 0.0373, RMSE: 0.0529, R2: 0.9584), PNorm: 153.5175, GNorm: 1.0837
[160/299] timecost: 63.30, lr: 0.000027, Train: (LOSS: 0.0204, MAE: 0.0204, RMSE: 0.0301, R2: 0.9863), Valid: (LOSS: 0.0406, MAE: 0.0406, RMSE: 0.0581, R2: 0.9531), PNorm: 153.4233, GNorm: 1.1172
[161/299] timecost: 63.03, lr: 0.000027, Train: (LOSS: 0.0200, MAE: 0.0200, RMSE: 0.0291, R2: 0.9866), Valid: (LOSS: 0.0392, MAE: 0.0392, RMSE: 0.0571, R2: 0.9536), PNorm: 153.3290, GNorm: 1.3232
[162/299] timecost: 62.91, lr: 0.000027, Train: (LOSS: 0.0190, MAE: 0.0190, RMSE: 0.0288, R2: 0.9863), Valid: (LOSS: 0.0390, MAE: 0.0390, RMSE: 0.0579, R2: 0.9534), PNorm: 153.2352, GNorm: 1.0122
[163/299] timecost: 63.06, lr: 0.000027, Train: (LOSS: 0.0199, MAE: 0.0199, RMSE: 0.0291, R2: 0.9865), Valid: (LOSS: 0.0366, MAE: 0.0366, RMSE: 0.0534, R2: 0.9588), PNorm: 153.1402, GNorm: 1.0650
[164/299] timecost: 63.57, lr: 0.000027, Train: (LOSS: 0.0190, MAE: 0.0190, RMSE: 0.0282, R2: 0.9869), Valid: (LOSS: 0.0379, MAE: 0.0379, RMSE: 0.0553, R2: 0.9573), PNorm: 153.0459, GNorm: 2.0399
[165/299] timecost: 63.18, lr: 0.000027, Train: (LOSS: 0.0179, MAE: 0.0179, RMSE: 0.0269, R2: 0.9877), Valid: (LOSS: 0.0370, MAE: 0.0370, RMSE: 0.0564, R2: 0.9550), PNorm: 152.9519, GNorm: 1.2614
[166/299] timecost: 64.91, lr: 0.000027, Train: (LOSS: 0.0190, MAE: 0.0190, RMSE: 0.0276, R2: 0.9880), Valid: (LOSS: 0.0359, MAE: 0.0359, RMSE: 0.0534, R2: 0.9604), PNorm: 152.8581, GNorm: 1.0039
[167/299] timecost: 63.96, lr: 0.000027, Train: (LOSS: 0.0179, MAE: 0.0179, RMSE: 0.0260, R2: 0.9894), Valid: (LOSS: 0.0372, MAE: 0.0372, RMSE: 0.0563, R2: 0.9551), PNorm: 152.7637, GNorm: 1.9730
[168/299] timecost: 63.10, lr: 0.000027, Train: (LOSS: 0.0182, MAE: 0.0182, RMSE: 0.0267, R2: 0.9881), Valid: (LOSS: 0.0362, MAE: 0.0362, RMSE: 0.0559, R2: 0.9549), PNorm: 152.6686, GNorm: 1.1881
[169/299] timecost: 64.19, lr: 0.000027, Train: (LOSS: 0.0181, MAE: 0.0181, RMSE: 0.0271, R2: 0.9882), Valid: (LOSS: 0.0361, MAE: 0.0361, RMSE: 0.0540, R2: 0.9568), PNorm: 152.5754, GNorm: 0.8057
[170/299] timecost: 64.98, lr: 0.000027, Train: (LOSS: 0.0189, MAE: 0.0189, RMSE: 0.0272, R2: 0.9875), Valid: (LOSS: 0.0353, MAE: 0.0353, RMSE: 0.0528, R2: 0.9592), PNorm: 152.4813, GNorm: 1.0085
[171/299] timecost: 65.10, lr: 0.000027, Train: (LOSS: 0.0172, MAE: 0.0172, RMSE: 0.0256, R2: 0.9894), Valid: (LOSS: 0.0372, MAE: 0.0372, RMSE: 0.0546, R2: 0.9577), PNorm: 152.3863, GNorm: 1.1768
[172/299] timecost: 64.73, lr: 0.000027, Train: (LOSS: 0.0172, MAE: 0.0172, RMSE: 0.0256, R2: 0.9895), Valid: (LOSS: 0.0362, MAE: 0.0362, RMSE: 0.0531, R2: 0.9593), PNorm: 152.2917, GNorm: 1.2168
[173/299] timecost: 64.21, lr: 0.000027, Train: (LOSS: 0.0175, MAE: 0.0175, RMSE: 0.0256, R2: 0.9896), Valid: (LOSS: 0.0361, MAE: 0.0361, RMSE: 0.0530, R2: 0.9597), PNorm: 152.1976, GNorm: 1.1572
[174/299] timecost: 62.85, lr: 0.000027, Train: (LOSS: 0.0178, MAE: 0.0178, RMSE: 0.0262, R2: 0.9889), Valid: (LOSS: 0.0361, MAE: 0.0361, RMSE: 0.0531, R2: 0.9594), PNorm: 152.1039, GNorm: 1.0122
[175/299] timecost: 62.96, lr: 0.000027, Train: (LOSS: 0.0185, MAE: 0.0185, RMSE: 0.0269, R2: 0.9883), Valid: (LOSS: 0.0374, MAE: 0.0374, RMSE: 0.0555, R2: 0.9572), PNorm: 152.0110, GNorm: 2.1825
[176/299] timecost: 62.84, lr: 0.000027, Train: (LOSS: 0.0180, MAE: 0.0180, RMSE: 0.0261, R2: 0.9891), Valid: (LOSS: 0.0358, MAE: 0.0358, RMSE: 0.0529, R2: 0.9591), PNorm: 151.9178, GNorm: 0.8968
Epoch 00178: reducing learning rate of group 0 to 2.4300e-05.
[177/299] timecost: 62.76, lr: 0.000024, Train: (LOSS: 0.0188, MAE: 0.0188, RMSE: 0.0272, R2: 0.9883), Valid: (LOSS: 0.0363, MAE: 0.0363, RMSE: 0.0533, R2: 0.9587), PNorm: 151.8248, GNorm: 1.1324
[178/299] timecost: 62.91, lr: 0.000024, Train: (LOSS: 0.0167, MAE: 0.0167, RMSE: 0.0246, R2: 0.9903), Valid: (LOSS: 0.0353, MAE: 0.0353, RMSE: 0.0529, R2: 0.9594), PNorm: 151.7411, GNorm: 1.6671
[179/299] timecost: 63.10, lr: 0.000024, Train: (LOSS: 0.0163, MAE: 0.0163, RMSE: 0.0239, R2: 0.9908), Valid: (LOSS: 0.0347, MAE: 0.0347, RMSE: 0.0513, R2: 0.9621), PNorm: 151.6569, GNorm: 1.6735
[180/299] timecost: 63.57, lr: 0.000024, Train: (LOSS: 0.0155, MAE: 0.0155, RMSE: 0.0228, R2: 0.9919), Valid: (LOSS: 0.0364, MAE: 0.0364, RMSE: 0.0521, R2: 0.9624), PNorm: 151.5733, GNorm: 0.8686
[181/299] timecost: 63.22, lr: 0.000024, Train: (LOSS: 0.0163, MAE: 0.0163, RMSE: 0.0236, R2: 0.9913), Valid: (LOSS: 0.0350, MAE: 0.0350, RMSE: 0.0514, R2: 0.9615), PNorm: 151.4891, GNorm: 1.1199
[182/299] timecost: 62.93, lr: 0.000024, Train: (LOSS: 0.0158, MAE: 0.0158, RMSE: 0.0234, R2: 0.9911), Valid: (LOSS: 0.0344, MAE: 0.0344, RMSE: 0.0498, R2: 0.9646), PNorm: 151.4047, GNorm: 1.0696
[183/299] timecost: 63.20, lr: 0.000024, Train: (LOSS: 0.0162, MAE: 0.0162, RMSE: 0.0234, R2: 0.9913), Valid: (LOSS: 0.0353, MAE: 0.0353, RMSE: 0.0523, R2: 0.9615), PNorm: 151.3207, GNorm: 0.9875
[184/299] timecost: 63.07, lr: 0.000024, Train: (LOSS: 0.0157, MAE: 0.0157, RMSE: 0.0232, R2: 0.9912), Valid: (LOSS: 0.0370, MAE: 0.0370, RMSE: 0.0543, R2: 0.9591), PNorm: 151.2365, GNorm: 1.1279
[185/299] timecost: 63.03, lr: 0.000024, Train: (LOSS: 0.0152, MAE: 0.0152, RMSE: 0.0223, R2: 0.9919), Valid: (LOSS: 0.0347, MAE: 0.0347, RMSE: 0.0529, R2: 0.9603), PNorm: 151.1526, GNorm: 1.2683
[186/299] timecost: 63.26, lr: 0.000024, Train: (LOSS: 0.0155, MAE: 0.0155, RMSE: 0.0225, R2: 0.9920), Valid: (LOSS: 0.0347, MAE: 0.0347, RMSE: 0.0520, R2: 0.9613), PNorm: 151.0684, GNorm: 0.7943
[187/299] timecost: 63.17, lr: 0.000024, Train: (LOSS: 0.0162, MAE: 0.0162, RMSE: 0.0235, R2: 0.9912), Valid: (LOSS: 0.0353, MAE: 0.0353, RMSE: 0.0542, R2: 0.9586), PNorm: 150.9852, GNorm: 1.2691
[188/299] timecost: 63.29, lr: 0.000024, Train: (LOSS: 0.0155, MAE: 0.0155, RMSE: 0.0226, R2: 0.9919), Valid: (LOSS: 0.0369, MAE: 0.0369, RMSE: 0.0541, R2: 0.9587), PNorm: 150.9016, GNorm: 1.0205
[189/299] timecost: 62.89, lr: 0.000024, Train: (LOSS: 0.0165, MAE: 0.0165, RMSE: 0.0239, R2: 0.9910), Valid: (LOSS: 0.0366, MAE: 0.0366, RMSE: 0.0545, R2: 0.9584), PNorm: 150.8179, GNorm: 0.8308
[190/299] timecost: 63.00, lr: 0.000024, Train: (LOSS: 0.0162, MAE: 0.0162, RMSE: 0.0236, R2: 0.9911), Valid: (LOSS: 0.0361, MAE: 0.0361, RMSE: 0.0536, R2: 0.9579), PNorm: 150.7354, GNorm: 0.8809
[191/299] timecost: 63.20, lr: 0.000024, Train: (LOSS: 0.0151, MAE: 0.0151, RMSE: 0.0220, R2: 0.9922), Valid: (LOSS: 0.0369, MAE: 0.0369, RMSE: 0.0542, R2: 0.9591), PNorm: 150.6527, GNorm: 1.3763
[192/299] timecost: 64.71, lr: 0.000024, Train: (LOSS: 0.0156, MAE: 0.0156, RMSE: 0.0225, R2: 0.9918), Valid: (LOSS: 0.0370, MAE: 0.0370, RMSE: 0.0564, R2: 0.9551), PNorm: 150.5691, GNorm: 1.3698
[193/299] timecost: 64.58, lr: 0.000024, Train: (LOSS: 0.0154, MAE: 0.0154, RMSE: 0.0228, R2: 0.9917), Valid: (LOSS: 0.0374, MAE: 0.0374, RMSE: 0.0541, R2: 0.9578), PNorm: 150.4859, GNorm: 1.3290
[194/299] timecost: 64.66, lr: 0.000024, Train: (LOSS: 0.0143, MAE: 0.0143, RMSE: 0.0210, R2: 0.9926), Valid: (LOSS: 0.0377, MAE: 0.0377, RMSE: 0.0550, R2: 0.9578), PNorm: 150.4025, GNorm: 1.0369
[195/299] timecost: 64.68, lr: 0.000024, Train: (LOSS: 0.0152, MAE: 0.0152, RMSE: 0.0220, R2: 0.9920), Valid: (LOSS: 0.0369, MAE: 0.0369, RMSE: 0.0541, R2: 0.9590), PNorm: 150.3188, GNorm: 0.7421
[196/299] timecost: 65.10, lr: 0.000024, Train: (LOSS: 0.0146, MAE: 0.0146, RMSE: 0.0213, R2: 0.9929), Valid: (LOSS: 0.0356, MAE: 0.0356, RMSE: 0.0519, R2: 0.9621), PNorm: 150.2352, GNorm: 0.7437
[197/299] timecost: 65.02, lr: 0.000024, Train: (LOSS: 0.0158, MAE: 0.0158, RMSE: 0.0228, R2: 0.9918), Valid: (LOSS: 0.0336, MAE: 0.0336, RMSE: 0.0502, R2: 0.9641), PNorm: 150.1520, GNorm: 1.6737
[198/299] timecost: 65.19, lr: 0.000024, Train: (LOSS: 0.0142, MAE: 0.0142, RMSE: 0.0209, R2: 0.9931), Valid: (LOSS: 0.0351, MAE: 0.0351, RMSE: 0.0530, R2: 0.9591), PNorm: 150.0688, GNorm: 1.4585
[199/299] timecost: 64.96, lr: 0.000024, Train: (LOSS: 0.0145, MAE: 0.0145, RMSE: 0.0212, R2: 0.9926), Valid: (LOSS: 0.0352, MAE: 0.0352, RMSE: 0.0525, R2: 0.9609), PNorm: 149.9850, GNorm: 0.8383
[200/299] timecost: 64.89, lr: 0.000024, Train: (LOSS: 0.0145, MAE: 0.0145, RMSE: 0.0210, R2: 0.9926), Valid: (LOSS: 0.0364, MAE: 0.0364, RMSE: 0.0525, R2: 0.9604), PNorm: 149.9022, GNorm: 1.7102
[201/299] timecost: 64.54, lr: 0.000024, Train: (LOSS: 0.0156, MAE: 0.0156, RMSE: 0.0226, R2: 0.9917), Valid: (LOSS: 0.0356, MAE: 0.0356, RMSE: 0.0541, R2: 0.9585), PNorm: 149.8195, GNorm: 1.8194
[202/299] timecost: 63.48, lr: 0.000024, Train: (LOSS: 0.0146, MAE: 0.0146, RMSE: 0.0213, R2: 0.9929), Valid: (LOSS: 0.0343, MAE: 0.0343, RMSE: 0.0511, R2: 0.9618), PNorm: 149.7373, GNorm: 0.9752
[203/299] timecost: 63.68, lr: 0.000024, Train: (LOSS: 0.0141, MAE: 0.0141, RMSE: 0.0207, R2: 0.9929), Valid: (LOSS: 0.0340, MAE: 0.0340, RMSE: 0.0498, R2: 0.9649), PNorm: 149.6548, GNorm: 2.4388
[204/299] timecost: 63.54, lr: 0.000024, Train: (LOSS: 0.0145, MAE: 0.0145, RMSE: 0.0214, R2: 0.9927), Valid: (LOSS: 0.0366, MAE: 0.0366, RMSE: 0.0544, R2: 0.9586), PNorm: 149.5725, GNorm: 1.1890
[205/299] timecost: 63.56, lr: 0.000024, Train: (LOSS: 0.0145, MAE: 0.0145, RMSE: 0.0210, R2: 0.9928), Valid: (LOSS: 0.0351, MAE: 0.0351, RMSE: 0.0521, R2: 0.9600), PNorm: 149.4906, GNorm: 1.4114
[206/299] timecost: 64.57, lr: 0.000024, Train: (LOSS: 0.0143, MAE: 0.0143, RMSE: 0.0210, R2: 0.9931), Valid: (LOSS: 0.0350, MAE: 0.0350, RMSE: 0.0524, R2: 0.9610), PNorm: 149.4073, GNorm: 0.9138
[207/299] timecost: 65.02, lr: 0.000024, Train: (LOSS: 0.0143, MAE: 0.0143, RMSE: 0.0209, R2: 0.9923), Valid: (LOSS: 0.0366, MAE: 0.0366, RMSE: 0.0543, R2: 0.9579), PNorm: 149.3258, GNorm: 0.9827
[208/299] timecost: 64.49, lr: 0.000024, Train: (LOSS: 0.0141, MAE: 0.0141, RMSE: 0.0202, R2: 0.9933), Valid: (LOSS: 0.0356, MAE: 0.0356, RMSE: 0.0526, R2: 0.9608), PNorm: 149.2437, GNorm: 0.8663
[209/299] timecost: 64.98, lr: 0.000024, Train: (LOSS: 0.0144, MAE: 0.0144, RMSE: 0.0207, R2: 0.9930), Valid: (LOSS: 0.0348, MAE: 0.0348, RMSE: 0.0516, R2: 0.9614), PNorm: 149.1617, GNorm: 0.8806
[210/299] timecost: 65.11, lr: 0.000024, Train: (LOSS: 0.0141, MAE: 0.0141, RMSE: 0.0205, R2: 0.9927), Valid: (LOSS: 0.0356, MAE: 0.0356, RMSE: 0.0534, R2: 0.9593), PNorm: 149.0795, GNorm: 0.8499
[211/299] timecost: 64.59, lr: 0.000024, Train: (LOSS: 0.0135, MAE: 0.0135, RMSE: 0.0196, R2: 0.9940), Valid: (LOSS: 0.0349, MAE: 0.0349, RMSE: 0.0521, R2: 0.9608), PNorm: 148.9980, GNorm: 0.7440
[212/299] timecost: 64.79, lr: 0.000024, Train: (LOSS: 0.0138, MAE: 0.0138, RMSE: 0.0200, R2: 0.9935), Valid: (LOSS: 0.0349, MAE: 0.0349, RMSE: 0.0523, R2: 0.9613), PNorm: 148.9164, GNorm: 0.8445
[213/299] timecost: 64.86, lr: 0.000024, Train: (LOSS: 0.0138, MAE: 0.0138, RMSE: 0.0199, R2: 0.9936), Valid: (LOSS: 0.0348, MAE: 0.0348, RMSE: 0.0522, R2: 0.9620), PNorm: 148.8350, GNorm: 0.9912
[214/299] timecost: 64.06, lr: 0.000024, Train: (LOSS: 0.0138, MAE: 0.0138, RMSE: 0.0201, R2: 0.9933), Valid: (LOSS: 0.0355, MAE: 0.0355, RMSE: 0.0520, R2: 0.9617), PNorm: 148.7536, GNorm: 0.8847
[215/299] timecost: 64.06, lr: 0.000024, Train: (LOSS: 0.0141, MAE: 0.0141, RMSE: 0.0202, R2: 0.9933), Valid: (LOSS: 0.0362, MAE: 0.0362, RMSE: 0.0528, R2: 0.9598), PNorm: 148.6727, GNorm: 2.1084
[216/299] timecost: 62.80, lr: 0.000024, Train: (LOSS: 0.0140, MAE: 0.0140, RMSE: 0.0202, R2: 0.9933), Valid: (LOSS: 0.0346, MAE: 0.0346, RMSE: 0.0505, R2: 0.9637), PNorm: 148.5911, GNorm: 1.5104
[217/299] timecost: 62.53, lr: 0.000024, Train: (LOSS: 0.0139, MAE: 0.0139, RMSE: 0.0199, R2: 0.9939), Valid: (LOSS: 0.0349, MAE: 0.0349, RMSE: 0.0515, R2: 0.9625), PNorm: 148.5104, GNorm: 1.3769
Epoch 00219: reducing learning rate of group 0 to 2.1870e-05.
[218/299] timecost: 62.72, lr: 0.000022, Train: (LOSS: 0.0131, MAE: 0.0131, RMSE: 0.0191, R2: 0.9943), Valid: (LOSS: 0.0353, MAE: 0.0353, RMSE: 0.0523, R2: 0.9612), PNorm: 148.4295, GNorm: 0.8935
[219/299] timecost: 62.55, lr: 0.000022, Train: (LOSS: 0.0125, MAE: 0.0125, RMSE: 0.0184, R2: 0.9945), Valid: (LOSS: 0.0351, MAE: 0.0351, RMSE: 0.0509, R2: 0.9637), PNorm: 148.3560, GNorm: 1.2285
[220/299] timecost: 63.10, lr: 0.000022, Train: (LOSS: 0.0127, MAE: 0.0127, RMSE: 0.0183, R2: 0.9945), Valid: (LOSS: 0.0355, MAE: 0.0355, RMSE: 0.0536, R2: 0.9592), PNorm: 148.2821, GNorm: 1.2062
[221/299] timecost: 63.81, lr: 0.000022, Train: (LOSS: 0.0125, MAE: 0.0125, RMSE: 0.0186, R2: 0.9946), Valid: (LOSS: 0.0354, MAE: 0.0354, RMSE: 0.0515, R2: 0.9625), PNorm: 148.2095, GNorm: 1.1712
[222/299] timecost: 63.94, lr: 0.000022, Train: (LOSS: 0.0128, MAE: 0.0128, RMSE: 0.0186, R2: 0.9943), Valid: (LOSS: 0.0356, MAE: 0.0356, RMSE: 0.0523, R2: 0.9616), PNorm: 148.1370, GNorm: 1.3819
[223/299] timecost: 63.60, lr: 0.000022, Train: (LOSS: 0.0124, MAE: 0.0124, RMSE: 0.0182, R2: 0.9947), Valid: (LOSS: 0.0352, MAE: 0.0352, RMSE: 0.0521, R2: 0.9620), PNorm: 148.0638, GNorm: 0.7938
[224/299] timecost: 63.27, lr: 0.000022, Train: (LOSS: 0.0123, MAE: 0.0123, RMSE: 0.0177, R2: 0.9948), Valid: (LOSS: 0.0334, MAE: 0.0334, RMSE: 0.0498, R2: 0.9651), PNorm: 147.9904, GNorm: 0.7722
[225/299] timecost: 63.46, lr: 0.000022, Train: (LOSS: 0.0115, MAE: 0.0115, RMSE: 0.0169, R2: 0.9955), Valid: (LOSS: 0.0340, MAE: 0.0340, RMSE: 0.0503, R2: 0.9637), PNorm: 147.9168, GNorm: 1.1351
[226/299] timecost: 63.42, lr: 0.000022, Train: (LOSS: 0.0125, MAE: 0.0125, RMSE: 0.0180, R2: 0.9948), Valid: (LOSS: 0.0355, MAE: 0.0355, RMSE: 0.0534, R2: 0.9593), PNorm: 147.8435, GNorm: 1.0715
[227/299] timecost: 63.47, lr: 0.000022, Train: (LOSS: 0.0128, MAE: 0.0128, RMSE: 0.0185, R2: 0.9945), Valid: (LOSS: 0.0344, MAE: 0.0344, RMSE: 0.0511, R2: 0.9613), PNorm: 147.7714, GNorm: 1.3824
[228/299] timecost: 63.74, lr: 0.000022, Train: (LOSS: 0.0135, MAE: 0.0135, RMSE: 0.0194, R2: 0.9939), Valid: (LOSS: 0.0347, MAE: 0.0347, RMSE: 0.0526, R2: 0.9597), PNorm: 147.6991, GNorm: 0.6461
[229/299] timecost: 63.64, lr: 0.000022, Train: (LOSS: 0.0133, MAE: 0.0133, RMSE: 0.0193, R2: 0.9941), Valid: (LOSS: 0.0347, MAE: 0.0347, RMSE: 0.0516, R2: 0.9617), PNorm: 147.6271, GNorm: 1.5891
[230/299] timecost: 63.26, lr: 0.000022, Train: (LOSS: 0.0122, MAE: 0.0122, RMSE: 0.0175, R2: 0.9951), Valid: (LOSS: 0.0366, MAE: 0.0366, RMSE: 0.0534, R2: 0.9587), PNorm: 147.5544, GNorm: 1.1861
[231/299] timecost: 63.60, lr: 0.000022, Train: (LOSS: 0.0119, MAE: 0.0119, RMSE: 0.0173, R2: 0.9952), Valid: (LOSS: 0.0343, MAE: 0.0343, RMSE: 0.0524, R2: 0.9603), PNorm: 147.4820, GNorm: 0.7983
[232/299] timecost: 63.72, lr: 0.000022, Train: (LOSS: 0.0122, MAE: 0.0122, RMSE: 0.0177, R2: 0.9949), Valid: (LOSS: 0.0342, MAE: 0.0342, RMSE: 0.0505, R2: 0.9639), PNorm: 147.4094, GNorm: 0.7095
[233/299] timecost: 63.80, lr: 0.000022, Train: (LOSS: 0.0117, MAE: 0.0117, RMSE: 0.0169, R2: 0.9954), Valid: (LOSS: 0.0356, MAE: 0.0356, RMSE: 0.0525, R2: 0.9606), PNorm: 147.3364, GNorm: 0.8169
[234/299] timecost: 62.92, lr: 0.000022, Train: (LOSS: 0.0117, MAE: 0.0117, RMSE: 0.0169, R2: 0.9951), Valid: (LOSS: 0.0342, MAE: 0.0342, RMSE: 0.0526, R2: 0.9601), PNorm: 147.2643, GNorm: 0.7414
[235/299] timecost: 63.18, lr: 0.000022, Train: (LOSS: 0.0117, MAE: 0.0117, RMSE: 0.0171, R2: 0.9950), Valid: (LOSS: 0.0340, MAE: 0.0340, RMSE: 0.0515, R2: 0.9619), PNorm: 147.1910, GNorm: 1.5352
[236/299] timecost: 63.27, lr: 0.000022, Train: (LOSS: 0.0121, MAE: 0.0121, RMSE: 0.0173, R2: 0.9951), Valid: (LOSS: 0.0349, MAE: 0.0349, RMSE: 0.0518, R2: 0.9616), PNorm: 147.1186, GNorm: 1.1092
[237/299] timecost: 63.61, lr: 0.000022, Train: (LOSS: 0.0121, MAE: 0.0121, RMSE: 0.0175, R2: 0.9951), Valid: (LOSS: 0.0335, MAE: 0.0335, RMSE: 0.0502, R2: 0.9641), PNorm: 147.0476, GNorm: 1.2625
[238/299] timecost: 63.82, lr: 0.000022, Train: (LOSS: 0.0115, MAE: 0.0115, RMSE: 0.0168, R2: 0.9955), Valid: (LOSS: 0.0341, MAE: 0.0341, RMSE: 0.0509, R2: 0.9631), PNorm: 146.9752, GNorm: 1.0872
Epoch 00240: reducing learning rate of group 0 to 1.9683e-05.
[239/299] timecost: 63.96, lr: 0.000020, Train: (LOSS: 0.0118, MAE: 0.0118, RMSE: 0.0166, R2: 0.9956), Valid: (LOSS: 0.0357, MAE: 0.0357, RMSE: 0.0532, R2: 0.9594), PNorm: 146.9024, GNorm: 1.4702
[240/299] timecost: 63.77, lr: 0.000020, Train: (LOSS: 0.0116, MAE: 0.0116, RMSE: 0.0167, R2: 0.9953), Valid: (LOSS: 0.0340, MAE: 0.0340, RMSE: 0.0510, R2: 0.9626), PNorm: 146.8381, GNorm: 1.3583
[241/299] timecost: 63.60, lr: 0.000020, Train: (LOSS: 0.0114, MAE: 0.0114, RMSE: 0.0165, R2: 0.9955), Valid: (LOSS: 0.0335, MAE: 0.0335, RMSE: 0.0497, R2: 0.9644), PNorm: 146.7730, GNorm: 0.8252
[242/299] timecost: 63.67, lr: 0.000020, Train: (LOSS: 0.0111, MAE: 0.0111, RMSE: 0.0159, R2: 0.9958), Valid: (LOSS: 0.0351, MAE: 0.0351, RMSE: 0.0527, R2: 0.9604), PNorm: 146.7083, GNorm: 0.8200
[243/299] timecost: 63.96, lr: 0.000020, Train: (LOSS: 0.0108, MAE: 0.0108, RMSE: 0.0158, R2: 0.9961), Valid: (LOSS: 0.0347, MAE: 0.0347, RMSE: 0.0506, R2: 0.9620), PNorm: 146.6436, GNorm: 0.6837
[244/299] timecost: 63.90, lr: 0.000020, Train: (LOSS: 0.0122, MAE: 0.0122, RMSE: 0.0175, R2: 0.9950), Valid: (LOSS: 0.0337, MAE: 0.0337, RMSE: 0.0495, R2: 0.9642), PNorm: 146.5790, GNorm: 0.8006
[245/299] timecost: 63.55, lr: 0.000020, Train: (LOSS: 0.0117, MAE: 0.0117, RMSE: 0.0167, R2: 0.9954), Valid: (LOSS: 0.0357, MAE: 0.0357, RMSE: 0.0532, R2: 0.9602), PNorm: 146.5149, GNorm: 1.0188
[246/299] timecost: 63.94, lr: 0.000020, Train: (LOSS: 0.0117, MAE: 0.0117, RMSE: 0.0165, R2: 0.9955), Valid: (LOSS: 0.0337, MAE: 0.0337, RMSE: 0.0519, R2: 0.9616), PNorm: 146.4506, GNorm: 0.8906
[247/299] timecost: 63.73, lr: 0.000020, Train: (LOSS: 0.0111, MAE: 0.0111, RMSE: 0.0162, R2: 0.9958), Valid: (LOSS: 0.0342, MAE: 0.0342, RMSE: 0.0508, R2: 0.9625), PNorm: 146.3864, GNorm: 0.9180
[248/299] timecost: 63.72, lr: 0.000020, Train: (LOSS: 0.0104, MAE: 0.0104, RMSE: 0.0150, R2: 0.9962), Valid: (LOSS: 0.0339, MAE: 0.0339, RMSE: 0.0509, R2: 0.9625), PNorm: 146.3216, GNorm: 0.9710
[249/299] timecost: 63.86, lr: 0.000020, Train: (LOSS: 0.0100, MAE: 0.0100, RMSE: 0.0147, R2: 0.9965), Valid: (LOSS: 0.0339, MAE: 0.0339, RMSE: 0.0502, R2: 0.9628), PNorm: 146.2573, GNorm: 1.4138
[250/299] timecost: 63.92, lr: 0.000020, Train: (LOSS: 0.0110, MAE: 0.0110, RMSE: 0.0157, R2: 0.9960), Valid: (LOSS: 0.0351, MAE: 0.0351, RMSE: 0.0513, R2: 0.9618), PNorm: 146.1930, GNorm: 0.6691
[251/299] timecost: 63.53, lr: 0.000020, Train: (LOSS: 0.0103, MAE: 0.0103, RMSE: 0.0151, R2: 0.9960), Valid: (LOSS: 0.0341, MAE: 0.0341, RMSE: 0.0516, R2: 0.9618), PNorm: 146.1275, GNorm: 1.2390
[252/299] timecost: 63.68, lr: 0.000020, Train: (LOSS: 0.0097, MAE: 0.0097, RMSE: 0.0143, R2: 0.9967), Valid: (LOSS: 0.0340, MAE: 0.0340, RMSE: 0.0496, R2: 0.9639), PNorm: 146.0631, GNorm: 0.9436
[253/299] timecost: 63.60, lr: 0.000020, Train: (LOSS: 0.0099, MAE: 0.0099, RMSE: 0.0145, R2: 0.9965), Valid: (LOSS: 0.0336, MAE: 0.0336, RMSE: 0.0492, R2: 0.9654), PNorm: 145.9979, GNorm: 1.1035
[254/299] timecost: 63.71, lr: 0.000020, Train: (LOSS: 0.0103, MAE: 0.0103, RMSE: 0.0151, R2: 0.9962), Valid: (LOSS: 0.0336, MAE: 0.0336, RMSE: 0.0514, R2: 0.9620), PNorm: 145.9329, GNorm: 0.8851
[255/299] timecost: 62.64, lr: 0.000020, Train: (LOSS: 0.0104, MAE: 0.0104, RMSE: 0.0153, R2: 0.9960), Valid: (LOSS: 0.0350, MAE: 0.0350, RMSE: 0.0519, R2: 0.9609), PNorm: 145.8677, GNorm: 0.7826
[256/299] timecost: 63.50, lr: 0.000020, Train: (LOSS: 0.0104, MAE: 0.0104, RMSE: 0.0153, R2: 0.9963), Valid: (LOSS: 0.0346, MAE: 0.0346, RMSE: 0.0509, R2: 0.9627), PNorm: 145.8033, GNorm: 0.8796
[257/299] timecost: 62.70, lr: 0.000020, Train: (LOSS: 0.0105, MAE: 0.0105, RMSE: 0.0151, R2: 0.9964), Valid: (LOSS: 0.0346, MAE: 0.0346, RMSE: 0.0520, R2: 0.9613), PNorm: 145.7386, GNorm: 1.4328
[258/299] timecost: 63.64, lr: 0.000020, Train: (LOSS: 0.0104, MAE: 0.0104, RMSE: 0.0152, R2: 0.9963), Valid: (LOSS: 0.0343, MAE: 0.0343, RMSE: 0.0507, R2: 0.9624), PNorm: 145.6741, GNorm: 1.8710
[259/299] timecost: 63.52, lr: 0.000020, Train: (LOSS: 0.0104, MAE: 0.0104, RMSE: 0.0151, R2: 0.9961), Valid: (LOSS: 0.0341, MAE: 0.0341, RMSE: 0.0510, R2: 0.9624), PNorm: 145.6102, GNorm: 1.0708
Epoch 00261: reducing learning rate of group 0 to 1.7715e-05.
[260/299] timecost: 63.52, lr: 0.000018, Train: (LOSS: 0.0096, MAE: 0.0096, RMSE: 0.0140, R2: 0.9969), Valid: (LOSS: 0.0334, MAE: 0.0334, RMSE: 0.0508, R2: 0.9627), PNorm: 145.5457, GNorm: 0.8696
[261/299] timecost: 63.95, lr: 0.000018, Train: (LOSS: 0.0091, MAE: 0.0091, RMSE: 0.0135, R2: 0.9970), Valid: (LOSS: 0.0343, MAE: 0.0343, RMSE: 0.0512, R2: 0.9624), PNorm: 145.4873, GNorm: 0.8002
[262/299] timecost: 63.61, lr: 0.000018, Train: (LOSS: 0.0094, MAE: 0.0094, RMSE: 0.0137, R2: 0.9967), Valid: (LOSS: 0.0340, MAE: 0.0340, RMSE: 0.0510, R2: 0.9629), PNorm: 145.4289, GNorm: 1.1069
[263/299] timecost: 63.12, lr: 0.000018, Train: (LOSS: 0.0094, MAE: 0.0094, RMSE: 0.0140, R2: 0.9967), Valid: (LOSS: 0.0355, MAE: 0.0355, RMSE: 0.0519, R2: 0.9620), PNorm: 145.3705, GNorm: 1.0416
[264/299] timecost: 63.04, lr: 0.000018, Train: (LOSS: 0.0104, MAE: 0.0104, RMSE: 0.0152, R2: 0.9963), Valid: (LOSS: 0.0341, MAE: 0.0341, RMSE: 0.0516, R2: 0.9617), PNorm: 145.3129, GNorm: 1.0822
[265/299] timecost: 63.58, lr: 0.000018, Train: (LOSS: 0.0094, MAE: 0.0094, RMSE: 0.0138, R2: 0.9968), Valid: (LOSS: 0.0344, MAE: 0.0344, RMSE: 0.0514, R2: 0.9619), PNorm: 145.2547, GNorm: 2.2174
[266/299] timecost: 63.76, lr: 0.000018, Train: (LOSS: 0.0101, MAE: 0.0101, RMSE: 0.0147, R2: 0.9960), Valid: (LOSS: 0.0338, MAE: 0.0338, RMSE: 0.0502, R2: 0.9635), PNorm: 145.1969, GNorm: 1.5305
[267/299] timecost: 64.14, lr: 0.000018, Train: (LOSS: 0.0099, MAE: 0.0099, RMSE: 0.0145, R2: 0.9966), Valid: (LOSS: 0.0349, MAE: 0.0349, RMSE: 0.0526, R2: 0.9598), PNorm: 145.1386, GNorm: 1.2082
[268/299] timecost: 64.12, lr: 0.000018, Train: (LOSS: 0.0099, MAE: 0.0099, RMSE: 0.0142, R2: 0.9967), Valid: (LOSS: 0.0348, MAE: 0.0348, RMSE: 0.0514, R2: 0.9621), PNorm: 145.0810, GNorm: 1.2354
[269/299] timecost: 63.64, lr: 0.000018, Train: (LOSS: 0.0089, MAE: 0.0089, RMSE: 0.0133, R2: 0.9970), Valid: (LOSS: 0.0339, MAE: 0.0339, RMSE: 0.0516, R2: 0.9622), PNorm: 145.0237, GNorm: 0.8231
[270/299] timecost: 63.79, lr: 0.000018, Train: (LOSS: 0.0094, MAE: 0.0094, RMSE: 0.0139, R2: 0.9967), Valid: (LOSS: 0.0326, MAE: 0.0326, RMSE: 0.0488, R2: 0.9658), PNorm: 144.9665, GNorm: 0.9156
[271/299] timecost: 63.93, lr: 0.000018, Train: (LOSS: 0.0094, MAE: 0.0094, RMSE: 0.0135, R2: 0.9969), Valid: (LOSS: 0.0343, MAE: 0.0343, RMSE: 0.0511, R2: 0.9621), PNorm: 144.9077, GNorm: 1.1060
[272/299] timecost: 64.46, lr: 0.000018, Train: (LOSS: 0.0094, MAE: 0.0094, RMSE: 0.0135, R2: 0.9970), Valid: (LOSS: 0.0342, MAE: 0.0342, RMSE: 0.0511, R2: 0.9622), PNorm: 144.8505, GNorm: 0.7067
[273/299] timecost: 63.67, lr: 0.000018, Train: (LOSS: 0.0089, MAE: 0.0089, RMSE: 0.0131, R2: 0.9973), Valid: (LOSS: 0.0342, MAE: 0.0342, RMSE: 0.0511, R2: 0.9626), PNorm: 144.7925, GNorm: 0.6912
[274/299] timecost: 62.58, lr: 0.000018, Train: (LOSS: 0.0084, MAE: 0.0084, RMSE: 0.0125, R2: 0.9975), Valid: (LOSS: 0.0337, MAE: 0.0337, RMSE: 0.0509, R2: 0.9624), PNorm: 144.7347, GNorm: 0.6105
[275/299] timecost: 62.74, lr: 0.000018, Train: (LOSS: 0.0089, MAE: 0.0089, RMSE: 0.0128, R2: 0.9972), Valid: (LOSS: 0.0347, MAE: 0.0347, RMSE: 0.0516, R2: 0.9608), PNorm: 144.6765, GNorm: 0.9328
[276/299] timecost: 63.50, lr: 0.000018, Train: (LOSS: 0.0089, MAE: 0.0089, RMSE: 0.0131, R2: 0.9971), Valid: (LOSS: 0.0345, MAE: 0.0345, RMSE: 0.0516, R2: 0.9613), PNorm: 144.6188, GNorm: 1.1133
[277/299] timecost: 63.86, lr: 0.000018, Train: (LOSS: 0.0088, MAE: 0.0088, RMSE: 0.0129, R2: 0.9972), Valid: (LOSS: 0.0349, MAE: 0.0349, RMSE: 0.0514, R2: 0.9612), PNorm: 144.5602, GNorm: 1.9779
[278/299] timecost: 63.52, lr: 0.000018, Train: (LOSS: 0.0094, MAE: 0.0094, RMSE: 0.0137, R2: 0.9968), Valid: (LOSS: 0.0340, MAE: 0.0340, RMSE: 0.0505, R2: 0.9625), PNorm: 144.5026, GNorm: 1.5065
[279/299] timecost: 63.42, lr: 0.000018, Train: (LOSS: 0.0091, MAE: 0.0091, RMSE: 0.0132, R2: 0.9972), Valid: (LOSS: 0.0330, MAE: 0.0330, RMSE: 0.0489, R2: 0.9656), PNorm: 144.4456, GNorm: 0.7974
[280/299] timecost: 63.63, lr: 0.000018, Train: (LOSS: 0.0086, MAE: 0.0086, RMSE: 0.0127, R2: 0.9973), Valid: (LOSS: 0.0340, MAE: 0.0340, RMSE: 0.0514, R2: 0.9614), PNorm: 144.3878, GNorm: 1.8936
[281/299] timecost: 63.39, lr: 0.000018, Train: (LOSS: 0.0092, MAE: 0.0092, RMSE: 0.0132, R2: 0.9969), Valid: (LOSS: 0.0343, MAE: 0.0343, RMSE: 0.0515, R2: 0.9613), PNorm: 144.3304, GNorm: 0.8484
[282/299] timecost: 63.44, lr: 0.000018, Train: (LOSS: 0.0088, MAE: 0.0088, RMSE: 0.0128, R2: 0.9972), Valid: (LOSS: 0.0340, MAE: 0.0340, RMSE: 0.0511, R2: 0.9627), PNorm: 144.2730, GNorm: 0.8937
[283/299] timecost: 63.15, lr: 0.000018, Train: (LOSS: 0.0089, MAE: 0.0089, RMSE: 0.0130, R2: 0.9971), Valid: (LOSS: 0.0332, MAE: 0.0332, RMSE: 0.0492, R2: 0.9650), PNorm: 144.2154, GNorm: 0.9655
[284/299] timecost: 63.58, lr: 0.000018, Train: (LOSS: 0.0087, MAE: 0.0087, RMSE: 0.0128, R2: 0.9972), Valid: (LOSS: 0.0347, MAE: 0.0347, RMSE: 0.0514, R2: 0.9626), PNorm: 144.1578, GNorm: 0.8064
[285/299] timecost: 63.62, lr: 0.000018, Train: (LOSS: 0.0093, MAE: 0.0093, RMSE: 0.0135, R2: 0.9970), Valid: (LOSS: 0.0343, MAE: 0.0343, RMSE: 0.0511, R2: 0.9618), PNorm: 144.1005, GNorm: 0.8056
[286/299] timecost: 63.53, lr: 0.000018, Train: (LOSS: 0.0084, MAE: 0.0084, RMSE: 0.0123, R2: 0.9974), Valid: (LOSS: 0.0340, MAE: 0.0340, RMSE: 0.0512, R2: 0.9616), PNorm: 144.0433, GNorm: 1.5685
[287/299] timecost: 63.38, lr: 0.000018, Train: (LOSS: 0.0092, MAE: 0.0092, RMSE: 0.0135, R2: 0.9971), Valid: (LOSS: 0.0342, MAE: 0.0342, RMSE: 0.0512, R2: 0.9622), PNorm: 143.9859, GNorm: 0.7199
[288/299] timecost: 63.34, lr: 0.000018, Train: (LOSS: 0.0091, MAE: 0.0091, RMSE: 0.0131, R2: 0.9972), Valid: (LOSS: 0.0344, MAE: 0.0344, RMSE: 0.0513, R2: 0.9623), PNorm: 143.9296, GNorm: 1.0414
[289/299] timecost: 63.34, lr: 0.000018, Train: (LOSS: 0.0087, MAE: 0.0087, RMSE: 0.0128, R2: 0.9974), Valid: (LOSS: 0.0349, MAE: 0.0349, RMSE: 0.0519, R2: 0.9599), PNorm: 143.8728, GNorm: 1.1377
[290/299] timecost: 63.68, lr: 0.000018, Train: (LOSS: 0.0089, MAE: 0.0089, RMSE: 0.0129, R2: 0.9971), Valid: (LOSS: 0.0351, MAE: 0.0351, RMSE: 0.0520, R2: 0.9615), PNorm: 143.8163, GNorm: 0.9064
Epoch 00292: reducing learning rate of group 0 to 1.5943e-05.
[291/299] timecost: 63.48, lr: 0.000016, Train: (LOSS: 0.0096, MAE: 0.0096, RMSE: 0.0137, R2: 0.9970), Valid: (LOSS: 0.0335, MAE: 0.0335, RMSE: 0.0504, R2: 0.9631), PNorm: 143.7604, GNorm: 1.8900
[292/299] timecost: 63.27, lr: 0.000016, Train: (LOSS: 0.0080, MAE: 0.0080, RMSE: 0.0120, R2: 0.9977), Valid: (LOSS: 0.0346, MAE: 0.0346, RMSE: 0.0514, R2: 0.9617), PNorm: 143.7095, GNorm: 1.0038
[293/299] timecost: 63.59, lr: 0.000016, Train: (LOSS: 0.0081, MAE: 0.0081, RMSE: 0.0120, R2: 0.9975), Valid: (LOSS: 0.0340, MAE: 0.0340, RMSE: 0.0510, R2: 0.9633), PNorm: 143.6585, GNorm: 1.1081
[294/299] timecost: 63.62, lr: 0.000016, Train: (LOSS: 0.0087, MAE: 0.0087, RMSE: 0.0127, R2: 0.9973), Valid: (LOSS: 0.0342, MAE: 0.0342, RMSE: 0.0512, R2: 0.9622), PNorm: 143.6079, GNorm: 1.0685
[295/299] timecost: 63.77, lr: 0.000016, Train: (LOSS: 0.0080, MAE: 0.0080, RMSE: 0.0117, R2: 0.9977), Valid: (LOSS: 0.0340, MAE: 0.0340, RMSE: 0.0510, R2: 0.9620), PNorm: 143.5571, GNorm: 1.0111
[296/299] timecost: 63.65, lr: 0.000016, Train: (LOSS: 0.0078, MAE: 0.0078, RMSE: 0.0114, R2: 0.9978), Valid: (LOSS: 0.0331, MAE: 0.0331, RMSE: 0.0497, R2: 0.9645), PNorm: 143.5072, GNorm: 1.3799
[297/299] timecost: 63.49, lr: 0.000016, Train: (LOSS: 0.0085, MAE: 0.0085, RMSE: 0.0123, R2: 0.9975), Valid: (LOSS: 0.0335, MAE: 0.0335, RMSE: 0.0495, R2: 0.9648), PNorm: 143.4562, GNorm: 1.3741
[298/299] timecost: 62.64, lr: 0.000016, Train: (LOSS: 0.0082, MAE: 0.0082, RMSE: 0.0118, R2: 0.9977), Valid: (LOSS: 0.0342, MAE: 0.0342, RMSE: 0.0515, R2: 0.9617), PNorm: 143.4059, GNorm: 0.7674
[299/299] timecost: 62.80, lr: 0.000016, Train: (LOSS: 0.0079, MAE: 0.0079, RMSE: 0.0116, R2: 0.9976), Valid: (LOSS: 0.0352, MAE: 0.0352, RMSE: 0.0532, R2: 0.9596), PNorm: 143.3550, GNorm: 1.1686
==========Training End==========
==========Test Best Model==========
================Final Results=======================
mse: 0.0340 +- 0.0000:
rmse: 0.0503 +- 0.0000:
mae: 0.0340 +- 0.0000:
r2: 0.9629 +- 0.0000:
tensor([[0.0000, 0.0000],
        [0.1336, 0.1475],
        [0.1458, 0.1828],
        ...,
        [0.0000, 0.0000],
        [0.4818, 0.4354],
        [0.0000, 0.0000]], device='cuda:0')
