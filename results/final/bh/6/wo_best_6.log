cuda available with GPU: Tesla V100-PCIE-16GB
==========Load Seed==========
set_random_seed
0
==========Training Start==========
Training Graphs:  2491
Valid Graphs:  277
Test Graphs:  1187
============Not pretrained weights used============
============Creating new layers============
============Creating Model============
Training Graphs Batches:  78
Valid Graphs Batches:  9
Test Graphs Batches:  37
[0/299] timecost: 61.94, lr: 0.000100, Train: (LOSS: 0.2251, MAE: 0.2251, RMSE: 0.2737, R2: -0.1099), Valid: (LOSS: 0.1888, MAE: 0.1888, RMSE: 0.2409, R2: 0.1943), PNorm: 187.0383, GNorm: 0.5000
[1/299] timecost: 59.99, lr: 0.000100, Train: (LOSS: 0.1736, MAE: 0.1736, RMSE: 0.2251, R2: 0.2669), Valid: (LOSS: 0.1678, MAE: 0.1678, RMSE: 0.2136, R2: 0.3718), PNorm: 187.0532, GNorm: 0.2844
[2/299] timecost: 60.27, lr: 0.000100, Train: (LOSS: 0.1671, MAE: 0.1671, RMSE: 0.2175, R2: 0.3141), Valid: (LOSS: 0.2233, MAE: 0.2233, RMSE: 0.2969, R2: -0.2112), PNorm: 187.0937, GNorm: 0.5000
[3/299] timecost: 60.61, lr: 0.000100, Train: (LOSS: 0.1530, MAE: 0.1530, RMSE: 0.2046, R2: 0.3767), Valid: (LOSS: 0.1497, MAE: 0.1497, RMSE: 0.1903, R2: 0.4993), PNorm: 187.1356, GNorm: 0.5000
[4/299] timecost: 60.78, lr: 0.000100, Train: (LOSS: 0.1410, MAE: 0.1410, RMSE: 0.1881, R2: 0.4775), Valid: (LOSS: 0.1401, MAE: 0.1401, RMSE: 0.1919, R2: 0.4666), PNorm: 187.1632, GNorm: 0.5000
[5/299] timecost: 59.45, lr: 0.000100, Train: (LOSS: 0.1339, MAE: 0.1339, RMSE: 0.1807, R2: 0.5156), Valid: (LOSS: 0.1557, MAE: 0.1557, RMSE: 0.2048, R2: 0.3994), PNorm: 187.1871, GNorm: 0.5000
[6/299] timecost: 59.67, lr: 0.000100, Train: (LOSS: 0.1309, MAE: 0.1309, RMSE: 0.1782, R2: 0.5274), Valid: (LOSS: 0.1280, MAE: 0.1280, RMSE: 0.1798, R2: 0.5327), PNorm: 187.2128, GNorm: 0.3774
[7/299] timecost: 59.42, lr: 0.000100, Train: (LOSS: 0.1254, MAE: 0.1254, RMSE: 0.1713, R2: 0.5564), Valid: (LOSS: 0.1234, MAE: 0.1234, RMSE: 0.1667, R2: 0.6067), PNorm: 187.2403, GNorm: 0.5000
[8/299] timecost: 59.80, lr: 0.000100, Train: (LOSS: 0.1202, MAE: 0.1202, RMSE: 0.1665, R2: 0.5923), Valid: (LOSS: 0.1235, MAE: 0.1235, RMSE: 0.1664, R2: 0.6049), PNorm: 187.2643, GNorm: 0.5000
[9/299] timecost: 59.77, lr: 0.000100, Train: (LOSS: 0.1211, MAE: 0.1211, RMSE: 0.1674, R2: 0.5768), Valid: (LOSS: 0.1335, MAE: 0.1335, RMSE: 0.1873, R2: 0.4850), PNorm: 187.2907, GNorm: 0.5000
[10/299] timecost: 60.65, lr: 0.000100, Train: (LOSS: 0.1188, MAE: 0.1188, RMSE: 0.1669, R2: 0.5840), Valid: (LOSS: 0.1279, MAE: 0.1279, RMSE: 0.1736, R2: 0.5821), PNorm: 187.3144, GNorm: 0.5000
[11/299] timecost: 60.16, lr: 0.000100, Train: (LOSS: 0.1170, MAE: 0.1170, RMSE: 0.1626, R2: 0.5953), Valid: (LOSS: 0.1266, MAE: 0.1266, RMSE: 0.1691, R2: 0.5936), PNorm: 187.3429, GNorm: 0.4648
[12/299] timecost: 59.45, lr: 0.000100, Train: (LOSS: 0.1103, MAE: 0.1103, RMSE: 0.1558, R2: 0.6258), Valid: (LOSS: 0.1085, MAE: 0.1085, RMSE: 0.1526, R2: 0.6789), PNorm: 187.3751, GNorm: 0.5000
[13/299] timecost: 58.31, lr: 0.000100, Train: (LOSS: 0.0999, MAE: 0.0999, RMSE: 0.1432, R2: 0.6940), Valid: (LOSS: 0.0982, MAE: 0.0982, RMSE: 0.1337, R2: 0.7487), PNorm: 187.4056, GNorm: 0.5000
[14/299] timecost: 59.28, lr: 0.000100, Train: (LOSS: 0.0952, MAE: 0.0952, RMSE: 0.1387, R2: 0.7084), Valid: (LOSS: 0.0996, MAE: 0.0996, RMSE: 0.1367, R2: 0.7415), PNorm: 187.4287, GNorm: 0.5000
[15/299] timecost: 62.47, lr: 0.000100, Train: (LOSS: 0.0930, MAE: 0.0930, RMSE: 0.1354, R2: 0.7167), Valid: (LOSS: 0.0910, MAE: 0.0910, RMSE: 0.1243, R2: 0.7842), PNorm: 187.4484, GNorm: 0.5000
[16/299] timecost: 62.32, lr: 0.000100, Train: (LOSS: 0.0909, MAE: 0.0909, RMSE: 0.1331, R2: 0.7338), Valid: (LOSS: 0.0886, MAE: 0.0886, RMSE: 0.1199, R2: 0.7980), PNorm: 187.4639, GNorm: 0.5000
[17/299] timecost: 62.41, lr: 0.000100, Train: (LOSS: 0.0889, MAE: 0.0889, RMSE: 0.1303, R2: 0.7413), Valid: (LOSS: 0.0881, MAE: 0.0881, RMSE: 0.1220, R2: 0.7869), PNorm: 187.4834, GNorm: 0.5000
[18/299] timecost: 62.47, lr: 0.000100, Train: (LOSS: 0.0897, MAE: 0.0897, RMSE: 0.1322, R2: 0.7339), Valid: (LOSS: 0.0869, MAE: 0.0869, RMSE: 0.1169, R2: 0.8097), PNorm: 187.5059, GNorm: 0.5000
[19/299] timecost: 62.53, lr: 0.000100, Train: (LOSS: 0.0836, MAE: 0.0836, RMSE: 0.1239, R2: 0.7590), Valid: (LOSS: 0.0863, MAE: 0.0863, RMSE: 0.1221, R2: 0.7893), PNorm: 187.5270, GNorm: 0.5000
[20/299] timecost: 62.52, lr: 0.000100, Train: (LOSS: 0.0809, MAE: 0.0809, RMSE: 0.1199, R2: 0.7756), Valid: (LOSS: 0.0794, MAE: 0.0794, RMSE: 0.1139, R2: 0.8177), PNorm: 187.5480, GNorm: 0.5000
[21/299] timecost: 62.57, lr: 0.000100, Train: (LOSS: 0.0812, MAE: 0.0812, RMSE: 0.1219, R2: 0.7701), Valid: (LOSS: 0.0856, MAE: 0.0856, RMSE: 0.1175, R2: 0.8029), PNorm: 187.5678, GNorm: 0.5000
[22/299] timecost: 60.28, lr: 0.000100, Train: (LOSS: 0.0799, MAE: 0.0799, RMSE: 0.1184, R2: 0.7845), Valid: (LOSS: 0.0744, MAE: 0.0744, RMSE: 0.0998, R2: 0.8513), PNorm: 187.5878, GNorm: 0.5000
[23/299] timecost: 58.60, lr: 0.000100, Train: (LOSS: 0.0759, MAE: 0.0759, RMSE: 0.1165, R2: 0.7888), Valid: (LOSS: 0.0717, MAE: 0.0717, RMSE: 0.0960, R2: 0.8667), PNorm: 187.6099, GNorm: 0.5000
[24/299] timecost: 58.90, lr: 0.000100, Train: (LOSS: 0.0712, MAE: 0.0712, RMSE: 0.1079, R2: 0.8178), Valid: (LOSS: 0.0651, MAE: 0.0651, RMSE: 0.0892, R2: 0.8837), PNorm: 187.6315, GNorm: 0.5000
[25/299] timecost: 59.59, lr: 0.000100, Train: (LOSS: 0.0676, MAE: 0.0676, RMSE: 0.1018, R2: 0.8315), Valid: (LOSS: 0.0727, MAE: 0.0727, RMSE: 0.1023, R2: 0.8411), PNorm: 187.6508, GNorm: 0.5000
[26/299] timecost: 59.73, lr: 0.000100, Train: (LOSS: 0.0644, MAE: 0.0644, RMSE: 0.0985, R2: 0.8430), Valid: (LOSS: 0.0663, MAE: 0.0663, RMSE: 0.0928, R2: 0.8701), PNorm: 187.6681, GNorm: 0.5000
[27/299] timecost: 59.53, lr: 0.000100, Train: (LOSS: 0.0627, MAE: 0.0627, RMSE: 0.0975, R2: 0.8509), Valid: (LOSS: 0.0606, MAE: 0.0606, RMSE: 0.0875, R2: 0.8889), PNorm: 187.6863, GNorm: 0.5000
[28/299] timecost: 60.41, lr: 0.000100, Train: (LOSS: 0.0592, MAE: 0.0592, RMSE: 0.0910, R2: 0.8677), Valid: (LOSS: 0.0609, MAE: 0.0609, RMSE: 0.0845, R2: 0.8946), PNorm: 187.7074, GNorm: 0.5000
[29/299] timecost: 60.77, lr: 0.000100, Train: (LOSS: 0.0583, MAE: 0.0583, RMSE: 0.0898, R2: 0.8727), Valid: (LOSS: 0.0638, MAE: 0.0638, RMSE: 0.0861, R2: 0.8845), PNorm: 187.7246, GNorm: 0.5000
[30/299] timecost: 60.35, lr: 0.000100, Train: (LOSS: 0.0560, MAE: 0.0560, RMSE: 0.0853, R2: 0.8824), Valid: (LOSS: 0.0547, MAE: 0.0547, RMSE: 0.0775, R2: 0.9088), PNorm: 187.7441, GNorm: 0.5000
[31/299] timecost: 59.91, lr: 0.000100, Train: (LOSS: 0.0516, MAE: 0.0516, RMSE: 0.0797, R2: 0.8994), Valid: (LOSS: 0.0548, MAE: 0.0548, RMSE: 0.0756, R2: 0.9163), PNorm: 187.7594, GNorm: 0.5000
[32/299] timecost: 60.83, lr: 0.000100, Train: (LOSS: 0.0513, MAE: 0.0513, RMSE: 0.0796, R2: 0.9009), Valid: (LOSS: 0.0571, MAE: 0.0571, RMSE: 0.0793, R2: 0.9090), PNorm: 187.7759, GNorm: 0.5000
[33/299] timecost: 60.61, lr: 0.000100, Train: (LOSS: 0.0495, MAE: 0.0495, RMSE: 0.0769, R2: 0.9063), Valid: (LOSS: 0.0547, MAE: 0.0547, RMSE: 0.0793, R2: 0.9067), PNorm: 187.7933, GNorm: 0.5000
[34/299] timecost: 60.58, lr: 0.000100, Train: (LOSS: 0.0470, MAE: 0.0470, RMSE: 0.0729, R2: 0.9159), Valid: (LOSS: 0.0458, MAE: 0.0458, RMSE: 0.0647, R2: 0.9383), PNorm: 187.8108, GNorm: 0.5000
[35/299] timecost: 59.76, lr: 0.000100, Train: (LOSS: 0.0467, MAE: 0.0467, RMSE: 0.0735, R2: 0.9108), Valid: (LOSS: 0.0467, MAE: 0.0467, RMSE: 0.0639, R2: 0.9370), PNorm: 187.8252, GNorm: 0.5000
[36/299] timecost: 59.34, lr: 0.000100, Train: (LOSS: 0.0456, MAE: 0.0456, RMSE: 0.0708, R2: 0.9189), Valid: (LOSS: 0.0455, MAE: 0.0455, RMSE: 0.0653, R2: 0.9364), PNorm: 187.8434, GNorm: 0.4609
[37/299] timecost: 59.76, lr: 0.000100, Train: (LOSS: 0.0447, MAE: 0.0447, RMSE: 0.0699, R2: 0.9199), Valid: (LOSS: 0.0435, MAE: 0.0435, RMSE: 0.0623, R2: 0.9443), PNorm: 187.8577, GNorm: 0.5000
[38/299] timecost: 59.96, lr: 0.000100, Train: (LOSS: 0.0430, MAE: 0.0430, RMSE: 0.0674, R2: 0.9268), Valid: (LOSS: 0.0438, MAE: 0.0438, RMSE: 0.0611, R2: 0.9444), PNorm: 187.8715, GNorm: 0.5000
[39/299] timecost: 59.54, lr: 0.000100, Train: (LOSS: 0.0410, MAE: 0.0410, RMSE: 0.0662, R2: 0.9273), Valid: (LOSS: 0.0434, MAE: 0.0434, RMSE: 0.0613, R2: 0.9416), PNorm: 187.8881, GNorm: 0.5000
[40/299] timecost: 59.72, lr: 0.000100, Train: (LOSS: 0.0404, MAE: 0.0404, RMSE: 0.0643, R2: 0.9307), Valid: (LOSS: 0.0454, MAE: 0.0454, RMSE: 0.0618, R2: 0.9433), PNorm: 187.9013, GNorm: 0.5000
[41/299] timecost: 59.89, lr: 0.000100, Train: (LOSS: 0.0397, MAE: 0.0397, RMSE: 0.0632, R2: 0.9352), Valid: (LOSS: 0.0451, MAE: 0.0451, RMSE: 0.0629, R2: 0.9411), PNorm: 187.9153, GNorm: 0.5000
[42/299] timecost: 60.24, lr: 0.000100, Train: (LOSS: 0.0390, MAE: 0.0390, RMSE: 0.0621, R2: 0.9357), Valid: (LOSS: 0.0417, MAE: 0.0417, RMSE: 0.0585, R2: 0.9483), PNorm: 187.9299, GNorm: 0.5000
[43/299] timecost: 60.07, lr: 0.000100, Train: (LOSS: 0.0378, MAE: 0.0378, RMSE: 0.0601, R2: 0.9389), Valid: (LOSS: 0.0440, MAE: 0.0440, RMSE: 0.0614, R2: 0.9433), PNorm: 187.9419, GNorm: 0.5000
[44/299] timecost: 60.11, lr: 0.000100, Train: (LOSS: 0.0375, MAE: 0.0375, RMSE: 0.0603, R2: 0.9409), Valid: (LOSS: 0.0429, MAE: 0.0429, RMSE: 0.0604, R2: 0.9440), PNorm: 187.9581, GNorm: 0.5000
[45/299] timecost: 59.94, lr: 0.000100, Train: (LOSS: 0.0368, MAE: 0.0368, RMSE: 0.0584, R2: 0.9437), Valid: (LOSS: 0.0415, MAE: 0.0415, RMSE: 0.0579, R2: 0.9498), PNorm: 187.9720, GNorm: 0.4835
[46/299] timecost: 60.13, lr: 0.000100, Train: (LOSS: 0.0357, MAE: 0.0357, RMSE: 0.0571, R2: 0.9439), Valid: (LOSS: 0.0385, MAE: 0.0385, RMSE: 0.0538, R2: 0.9556), PNorm: 187.9850, GNorm: 0.5000
[47/299] timecost: 60.24, lr: 0.000100, Train: (LOSS: 0.0360, MAE: 0.0360, RMSE: 0.0578, R2: 0.9453), Valid: (LOSS: 0.0443, MAE: 0.0443, RMSE: 0.0622, R2: 0.9421), PNorm: 188.0007, GNorm: 0.4864
[48/299] timecost: 60.33, lr: 0.000100, Train: (LOSS: 0.0359, MAE: 0.0359, RMSE: 0.0570, R2: 0.9440), Valid: (LOSS: 0.0411, MAE: 0.0411, RMSE: 0.0571, R2: 0.9522), PNorm: 188.0129, GNorm: 0.4958
[49/299] timecost: 61.65, lr: 0.000100, Train: (LOSS: 0.0347, MAE: 0.0347, RMSE: 0.0559, R2: 0.9457), Valid: (LOSS: 0.0385, MAE: 0.0385, RMSE: 0.0542, R2: 0.9568), PNorm: 188.0285, GNorm: 0.5000
[50/299] timecost: 62.64, lr: 0.000100, Train: (LOSS: 0.0324, MAE: 0.0324, RMSE: 0.0521, R2: 0.9545), Valid: (LOSS: 0.0399, MAE: 0.0399, RMSE: 0.0573, R2: 0.9499), PNorm: 188.0394, GNorm: 0.4132
[51/299] timecost: 62.55, lr: 0.000100, Train: (LOSS: 0.0323, MAE: 0.0323, RMSE: 0.0524, R2: 0.9548), Valid: (LOSS: 0.0392, MAE: 0.0392, RMSE: 0.0573, R2: 0.9510), PNorm: 188.0524, GNorm: 0.4716
[52/299] timecost: 62.57, lr: 0.000100, Train: (LOSS: 0.0328, MAE: 0.0328, RMSE: 0.0528, R2: 0.9521), Valid: (LOSS: 0.0384, MAE: 0.0384, RMSE: 0.0561, R2: 0.9537), PNorm: 188.0647, GNorm: 0.4691
[53/299] timecost: 62.52, lr: 0.000100, Train: (LOSS: 0.0316, MAE: 0.0316, RMSE: 0.0519, R2: 0.9556), Valid: (LOSS: 0.0390, MAE: 0.0390, RMSE: 0.0555, R2: 0.9536), PNorm: 188.0784, GNorm: 0.4483
[54/299] timecost: 62.44, lr: 0.000100, Train: (LOSS: 0.0323, MAE: 0.0323, RMSE: 0.0528, R2: 0.9499), Valid: (LOSS: 0.0414, MAE: 0.0414, RMSE: 0.0592, R2: 0.9489), PNorm: 188.0920, GNorm: 0.5000
[55/299] timecost: 62.38, lr: 0.000100, Train: (LOSS: 0.0309, MAE: 0.0309, RMSE: 0.0507, R2: 0.9534), Valid: (LOSS: 0.0404, MAE: 0.0404, RMSE: 0.0593, R2: 0.9481), PNorm: 188.1049, GNorm: 0.5000
[56/299] timecost: 62.13, lr: 0.000100, Train: (LOSS: 0.0304, MAE: 0.0304, RMSE: 0.0495, R2: 0.9558), Valid: (LOSS: 0.0401, MAE: 0.0401, RMSE: 0.0584, R2: 0.9489), PNorm: 188.1184, GNorm: 0.5000
[57/299] timecost: 59.11, lr: 0.000100, Train: (LOSS: 0.0300, MAE: 0.0300, RMSE: 0.0493, R2: 0.9578), Valid: (LOSS: 0.0386, MAE: 0.0386, RMSE: 0.0542, R2: 0.9560), PNorm: 188.1274, GNorm: 0.5000
[58/299] timecost: 58.59, lr: 0.000100, Train: (LOSS: 0.0285, MAE: 0.0285, RMSE: 0.0460, R2: 0.9623), Valid: (LOSS: 0.0378, MAE: 0.0378, RMSE: 0.0533, R2: 0.9576), PNorm: 188.1390, GNorm: 0.3972
[59/299] timecost: 59.39, lr: 0.000100, Train: (LOSS: 0.0285, MAE: 0.0285, RMSE: 0.0473, R2: 0.9602), Valid: (LOSS: 0.0377, MAE: 0.0377, RMSE: 0.0549, R2: 0.9563), PNorm: 188.1497, GNorm: 0.5000
[60/299] timecost: 59.53, lr: 0.000100, Train: (LOSS: 0.0285, MAE: 0.0285, RMSE: 0.0473, R2: 0.9604), Valid: (LOSS: 0.0432, MAE: 0.0432, RMSE: 0.0616, R2: 0.9388), PNorm: 188.1645, GNorm: 0.5000
[61/299] timecost: 59.45, lr: 0.000100, Train: (LOSS: 0.0288, MAE: 0.0288, RMSE: 0.0476, R2: 0.9588), Valid: (LOSS: 0.0393, MAE: 0.0393, RMSE: 0.0562, R2: 0.9542), PNorm: 188.1763, GNorm: 0.4240
[62/299] timecost: 60.21, lr: 0.000100, Train: (LOSS: 0.0284, MAE: 0.0284, RMSE: 0.0463, R2: 0.9636), Valid: (LOSS: 0.0402, MAE: 0.0402, RMSE: 0.0575, R2: 0.9527), PNorm: 188.1879, GNorm: 0.5000
[63/299] timecost: 59.74, lr: 0.000100, Train: (LOSS: 0.0278, MAE: 0.0278, RMSE: 0.0462, R2: 0.9630), Valid: (LOSS: 0.0381, MAE: 0.0381, RMSE: 0.0571, R2: 0.9523), PNorm: 188.2003, GNorm: 0.4524
[64/299] timecost: 59.98, lr: 0.000100, Train: (LOSS: 0.0271, MAE: 0.0271, RMSE: 0.0449, R2: 0.9649), Valid: (LOSS: 0.0362, MAE: 0.0362, RMSE: 0.0530, R2: 0.9598), PNorm: 188.2128, GNorm: 0.5000
[65/299] timecost: 59.90, lr: 0.000100, Train: (LOSS: 0.0270, MAE: 0.0270, RMSE: 0.0447, R2: 0.9661), Valid: (LOSS: 0.0371, MAE: 0.0371, RMSE: 0.0538, R2: 0.9583), PNorm: 188.2239, GNorm: 0.4606
[66/299] timecost: 59.59, lr: 0.000100, Train: (LOSS: 0.0274, MAE: 0.0274, RMSE: 0.0455, R2: 0.9625), Valid: (LOSS: 0.0411, MAE: 0.0411, RMSE: 0.0592, R2: 0.9481), PNorm: 188.2377, GNorm: 0.5000
[67/299] timecost: 60.84, lr: 0.000100, Train: (LOSS: 0.0273, MAE: 0.0273, RMSE: 0.0451, R2: 0.9632), Valid: (LOSS: 0.0391, MAE: 0.0391, RMSE: 0.0576, R2: 0.9503), PNorm: 188.2503, GNorm: 0.5000
[68/299] timecost: 61.54, lr: 0.000100, Train: (LOSS: 0.0258, MAE: 0.0258, RMSE: 0.0427, R2: 0.9661), Valid: (LOSS: 0.0383, MAE: 0.0383, RMSE: 0.0570, R2: 0.9540), PNorm: 188.2630, GNorm: 0.4519
[69/299] timecost: 58.98, lr: 0.000100, Train: (LOSS: 0.0253, MAE: 0.0253, RMSE: 0.0422, R2: 0.9693), Valid: (LOSS: 0.0391, MAE: 0.0391, RMSE: 0.0590, R2: 0.9510), PNorm: 188.2755, GNorm: 0.4487
[70/299] timecost: 59.34, lr: 0.000100, Train: (LOSS: 0.0267, MAE: 0.0267, RMSE: 0.0441, R2: 0.9668), Valid: (LOSS: 0.0400, MAE: 0.0400, RMSE: 0.0584, R2: 0.9501), PNorm: 188.2881, GNorm: 0.5000
[71/299] timecost: 59.17, lr: 0.000100, Train: (LOSS: 0.0257, MAE: 0.0257, RMSE: 0.0425, R2: 0.9679), Valid: (LOSS: 0.0391, MAE: 0.0391, RMSE: 0.0581, R2: 0.9518), PNorm: 188.3032, GNorm: 0.4352
[72/299] timecost: 59.37, lr: 0.000100, Train: (LOSS: 0.0246, MAE: 0.0246, RMSE: 0.0417, R2: 0.9697), Valid: (LOSS: 0.0400, MAE: 0.0400, RMSE: 0.0574, R2: 0.9520), PNorm: 188.3151, GNorm: 0.5000
[73/299] timecost: 59.43, lr: 0.000100, Train: (LOSS: 0.0245, MAE: 0.0245, RMSE: 0.0412, R2: 0.9707), Valid: (LOSS: 0.0383, MAE: 0.0383, RMSE: 0.0565, R2: 0.9539), PNorm: 188.3270, GNorm: 0.5000
[74/299] timecost: 59.42, lr: 0.000100, Train: (LOSS: 0.0244, MAE: 0.0244, RMSE: 0.0400, R2: 0.9723), Valid: (LOSS: 0.0376, MAE: 0.0376, RMSE: 0.0544, R2: 0.9562), PNorm: 188.3395, GNorm: 0.5000
[75/299] timecost: 59.33, lr: 0.000100, Train: (LOSS: 0.0248, MAE: 0.0248, RMSE: 0.0408, R2: 0.9696), Valid: (LOSS: 0.0391, MAE: 0.0391, RMSE: 0.0568, R2: 0.9533), PNorm: 188.3536, GNorm: 0.5000
[76/299] timecost: 59.70, lr: 0.000100, Train: (LOSS: 0.0245, MAE: 0.0245, RMSE: 0.0401, R2: 0.9721), Valid: (LOSS: 0.0374, MAE: 0.0374, RMSE: 0.0568, R2: 0.9541), PNorm: 188.3667, GNorm: 0.5000
[77/299] timecost: 59.65, lr: 0.000100, Train: (LOSS: 0.0236, MAE: 0.0236, RMSE: 0.0381, R2: 0.9748), Valid: (LOSS: 0.0377, MAE: 0.0377, RMSE: 0.0574, R2: 0.9525), PNorm: 188.3812, GNorm: 0.5000
[78/299] timecost: 59.77, lr: 0.000100, Train: (LOSS: 0.0239, MAE: 0.0239, RMSE: 0.0380, R2: 0.9726), Valid: (LOSS: 0.0377, MAE: 0.0377, RMSE: 0.0550, R2: 0.9568), PNorm: 188.3893, GNorm: 0.4209
[79/299] timecost: 59.57, lr: 0.000100, Train: (LOSS: 0.0235, MAE: 0.0235, RMSE: 0.0380, R2: 0.9746), Valid: (LOSS: 0.0382, MAE: 0.0382, RMSE: 0.0566, R2: 0.9526), PNorm: 188.4026, GNorm: 0.5000
[80/299] timecost: 59.78, lr: 0.000100, Train: (LOSS: 0.0230, MAE: 0.0230, RMSE: 0.0369, R2: 0.9767), Valid: (LOSS: 0.0352, MAE: 0.0352, RMSE: 0.0540, R2: 0.9568), PNorm: 188.4161, GNorm: 0.4552
[81/299] timecost: 59.47, lr: 0.000100, Train: (LOSS: 0.0227, MAE: 0.0227, RMSE: 0.0365, R2: 0.9756), Valid: (LOSS: 0.0393, MAE: 0.0393, RMSE: 0.0591, R2: 0.9491), PNorm: 188.4287, GNorm: 0.5000
[82/299] timecost: 59.60, lr: 0.000100, Train: (LOSS: 0.0234, MAE: 0.0234, RMSE: 0.0369, R2: 0.9759), Valid: (LOSS: 0.0394, MAE: 0.0394, RMSE: 0.0605, R2: 0.9461), PNorm: 188.4417, GNorm: 0.5000
[83/299] timecost: 59.66, lr: 0.000100, Train: (LOSS: 0.0225, MAE: 0.0225, RMSE: 0.0361, R2: 0.9774), Valid: (LOSS: 0.0379, MAE: 0.0379, RMSE: 0.0563, R2: 0.9543), PNorm: 188.4562, GNorm: 0.5000
[84/299] timecost: 59.21, lr: 0.000100, Train: (LOSS: 0.0231, MAE: 0.0231, RMSE: 0.0360, R2: 0.9778), Valid: (LOSS: 0.0381, MAE: 0.0381, RMSE: 0.0538, R2: 0.9581), PNorm: 188.4690, GNorm: 0.5000
[85/299] timecost: 59.55, lr: 0.000100, Train: (LOSS: 0.0223, MAE: 0.0223, RMSE: 0.0358, R2: 0.9777), Valid: (LOSS: 0.0377, MAE: 0.0377, RMSE: 0.0561, R2: 0.9546), PNorm: 188.4813, GNorm: 0.5000
[86/299] timecost: 58.62, lr: 0.000100, Train: (LOSS: 0.0217, MAE: 0.0217, RMSE: 0.0337, R2: 0.9806), Valid: (LOSS: 0.0379, MAE: 0.0379, RMSE: 0.0567, R2: 0.9537), PNorm: 188.4969, GNorm: 0.5000
[87/299] timecost: 57.83, lr: 0.000100, Train: (LOSS: 0.0219, MAE: 0.0219, RMSE: 0.0346, R2: 0.9790), Valid: (LOSS: 0.0372, MAE: 0.0372, RMSE: 0.0551, R2: 0.9576), PNorm: 188.5098, GNorm: 0.5000
[88/299] timecost: 58.16, lr: 0.000100, Train: (LOSS: 0.0206, MAE: 0.0206, RMSE: 0.0325, R2: 0.9807), Valid: (LOSS: 0.0378, MAE: 0.0378, RMSE: 0.0583, R2: 0.9503), PNorm: 188.5212, GNorm: 0.5000
[89/299] timecost: 58.41, lr: 0.000100, Train: (LOSS: 0.0217, MAE: 0.0217, RMSE: 0.0338, R2: 0.9804), Valid: (LOSS: 0.0384, MAE: 0.0384, RMSE: 0.0571, R2: 0.9533), PNorm: 188.5378, GNorm: 0.5000
[90/299] timecost: 58.49, lr: 0.000100, Train: (LOSS: 0.0203, MAE: 0.0203, RMSE: 0.0321, R2: 0.9820), Valid: (LOSS: 0.0382, MAE: 0.0382, RMSE: 0.0580, R2: 0.9525), PNorm: 188.5493, GNorm: 0.5000
[91/299] timecost: 58.90, lr: 0.000100, Train: (LOSS: 0.0200, MAE: 0.0200, RMSE: 0.0316, R2: 0.9823), Valid: (LOSS: 0.0387, MAE: 0.0387, RMSE: 0.0569, R2: 0.9528), PNorm: 188.5623, GNorm: 0.4651
[92/299] timecost: 59.04, lr: 0.000100, Train: (LOSS: 0.0209, MAE: 0.0209, RMSE: 0.0321, R2: 0.9823), Valid: (LOSS: 0.0388, MAE: 0.0388, RMSE: 0.0575, R2: 0.9522), PNorm: 188.5750, GNorm: 0.4989
[93/299] timecost: 59.18, lr: 0.000100, Train: (LOSS: 0.0200, MAE: 0.0200, RMSE: 0.0308, R2: 0.9828), Valid: (LOSS: 0.0398, MAE: 0.0398, RMSE: 0.0590, R2: 0.9508), PNorm: 188.5884, GNorm: 0.5000
[94/299] timecost: 59.17, lr: 0.000100, Train: (LOSS: 0.0198, MAE: 0.0198, RMSE: 0.0309, R2: 0.9834), Valid: (LOSS: 0.0378, MAE: 0.0378, RMSE: 0.0559, R2: 0.9548), PNorm: 188.5997, GNorm: 0.4560
[95/299] timecost: 59.48, lr: 0.000100, Train: (LOSS: 0.0197, MAE: 0.0197, RMSE: 0.0298, R2: 0.9842), Valid: (LOSS: 0.0370, MAE: 0.0370, RMSE: 0.0540, R2: 0.9579), PNorm: 188.6122, GNorm: 0.4457
Epoch 00097: reducing learning rate of group 0 to 8.5000e-05.
[96/299] timecost: 59.15, lr: 0.000085, Train: (LOSS: 0.0196, MAE: 0.0196, RMSE: 0.0306, R2: 0.9827), Valid: (LOSS: 0.0390, MAE: 0.0390, RMSE: 0.0597, R2: 0.9478), PNorm: 188.6247, GNorm: 0.5000
[97/299] timecost: 58.84, lr: 0.000085, Train: (LOSS: 0.0189, MAE: 0.0189, RMSE: 0.0296, R2: 0.9843), Valid: (LOSS: 0.0372, MAE: 0.0372, RMSE: 0.0557, R2: 0.9553), PNorm: 188.6339, GNorm: 0.5000
[98/299] timecost: 59.36, lr: 0.000085, Train: (LOSS: 0.0178, MAE: 0.0178, RMSE: 0.0275, R2: 0.9870), Valid: (LOSS: 0.0389, MAE: 0.0389, RMSE: 0.0583, R2: 0.9508), PNorm: 188.6453, GNorm: 0.5000
[99/299] timecost: 58.73, lr: 0.000085, Train: (LOSS: 0.0179, MAE: 0.0179, RMSE: 0.0278, R2: 0.9861), Valid: (LOSS: 0.0369, MAE: 0.0369, RMSE: 0.0546, R2: 0.9573), PNorm: 188.6549, GNorm: 0.5000
[100/299] timecost: 58.95, lr: 0.000085, Train: (LOSS: 0.0179, MAE: 0.0179, RMSE: 0.0279, R2: 0.9870), Valid: (LOSS: 0.0388, MAE: 0.0388, RMSE: 0.0565, R2: 0.9545), PNorm: 188.6641, GNorm: 0.4384
[101/299] timecost: 59.08, lr: 0.000085, Train: (LOSS: 0.0179, MAE: 0.0179, RMSE: 0.0278, R2: 0.9870), Valid: (LOSS: 0.0378, MAE: 0.0378, RMSE: 0.0570, R2: 0.9533), PNorm: 188.6719, GNorm: 0.5000
[102/299] timecost: 59.28, lr: 0.000085, Train: (LOSS: 0.0172, MAE: 0.0172, RMSE: 0.0261, R2: 0.9889), Valid: (LOSS: 0.0366, MAE: 0.0366, RMSE: 0.0545, R2: 0.9574), PNorm: 188.6790, GNorm: 0.5000
[103/299] timecost: 58.89, lr: 0.000085, Train: (LOSS: 0.0165, MAE: 0.0165, RMSE: 0.0257, R2: 0.9886), Valid: (LOSS: 0.0353, MAE: 0.0353, RMSE: 0.0540, R2: 0.9582), PNorm: 188.6906, GNorm: 0.4286
[104/299] timecost: 59.45, lr: 0.000085, Train: (LOSS: 0.0163, MAE: 0.0163, RMSE: 0.0247, R2: 0.9894), Valid: (LOSS: 0.0366, MAE: 0.0366, RMSE: 0.0547, R2: 0.9561), PNorm: 188.6993, GNorm: 0.3925
[105/299] timecost: 58.69, lr: 0.000085, Train: (LOSS: 0.0163, MAE: 0.0163, RMSE: 0.0248, R2: 0.9896), Valid: (LOSS: 0.0358, MAE: 0.0358, RMSE: 0.0553, R2: 0.9557), PNorm: 188.7070, GNorm: 0.5000
[106/299] timecost: 59.33, lr: 0.000085, Train: (LOSS: 0.0163, MAE: 0.0163, RMSE: 0.0248, R2: 0.9889), Valid: (LOSS: 0.0378, MAE: 0.0378, RMSE: 0.0567, R2: 0.9539), PNorm: 188.7176, GNorm: 0.5000
[107/299] timecost: 59.24, lr: 0.000085, Train: (LOSS: 0.0164, MAE: 0.0164, RMSE: 0.0248, R2: 0.9894), Valid: (LOSS: 0.0391, MAE: 0.0391, RMSE: 0.0592, R2: 0.9493), PNorm: 188.7279, GNorm: 0.5000
[108/299] timecost: 59.16, lr: 0.000085, Train: (LOSS: 0.0155, MAE: 0.0155, RMSE: 0.0240, R2: 0.9903), Valid: (LOSS: 0.0379, MAE: 0.0379, RMSE: 0.0566, R2: 0.9541), PNorm: 188.7351, GNorm: 0.4824
[109/299] timecost: 59.06, lr: 0.000085, Train: (LOSS: 0.0155, MAE: 0.0155, RMSE: 0.0237, R2: 0.9894), Valid: (LOSS: 0.0386, MAE: 0.0386, RMSE: 0.0579, R2: 0.9513), PNorm: 188.7430, GNorm: 0.5000
[110/299] timecost: 59.79, lr: 0.000085, Train: (LOSS: 0.0157, MAE: 0.0157, RMSE: 0.0240, R2: 0.9903), Valid: (LOSS: 0.0375, MAE: 0.0375, RMSE: 0.0561, R2: 0.9545), PNorm: 188.7551, GNorm: 0.5000
[111/299] timecost: 59.32, lr: 0.000085, Train: (LOSS: 0.0163, MAE: 0.0163, RMSE: 0.0250, R2: 0.9898), Valid: (LOSS: 0.0379, MAE: 0.0379, RMSE: 0.0565, R2: 0.9547), PNorm: 188.7649, GNorm: 0.4675
Epoch 00113: reducing learning rate of group 0 to 7.2250e-05.
[112/299] timecost: 59.77, lr: 0.000072, Train: (LOSS: 0.0157, MAE: 0.0157, RMSE: 0.0240, R2: 0.9901), Valid: (LOSS: 0.0372, MAE: 0.0372, RMSE: 0.0548, R2: 0.9572), PNorm: 188.7723, GNorm: 0.5000
[113/299] timecost: 59.42, lr: 0.000072, Train: (LOSS: 0.0142, MAE: 0.0142, RMSE: 0.0222, R2: 0.9917), Valid: (LOSS: 0.0362, MAE: 0.0362, RMSE: 0.0572, R2: 0.9538), PNorm: 188.7789, GNorm: 0.5000
[114/299] timecost: 59.28, lr: 0.000072, Train: (LOSS: 0.0135, MAE: 0.0135, RMSE: 0.0210, R2: 0.9919), Valid: (LOSS: 0.0365, MAE: 0.0365, RMSE: 0.0555, R2: 0.9565), PNorm: 188.7848, GNorm: 0.5000
[115/299] timecost: 59.20, lr: 0.000072, Train: (LOSS: 0.0138, MAE: 0.0138, RMSE: 0.0214, R2: 0.9920), Valid: (LOSS: 0.0369, MAE: 0.0369, RMSE: 0.0552, R2: 0.9574), PNorm: 188.7908, GNorm: 0.4674
[116/299] timecost: 59.06, lr: 0.000072, Train: (LOSS: 0.0137, MAE: 0.0137, RMSE: 0.0211, R2: 0.9923), Valid: (LOSS: 0.0361, MAE: 0.0361, RMSE: 0.0561, R2: 0.9554), PNorm: 188.7977, GNorm: 0.5000
[117/299] timecost: 59.15, lr: 0.000072, Train: (LOSS: 0.0139, MAE: 0.0139, RMSE: 0.0216, R2: 0.9919), Valid: (LOSS: 0.0351, MAE: 0.0351, RMSE: 0.0531, R2: 0.9600), PNorm: 188.8064, GNorm: 0.3357
[118/299] timecost: 59.45, lr: 0.000072, Train: (LOSS: 0.0140, MAE: 0.0140, RMSE: 0.0217, R2: 0.9917), Valid: (LOSS: 0.0362, MAE: 0.0362, RMSE: 0.0547, R2: 0.9569), PNorm: 188.8131, GNorm: 0.5000
[119/299] timecost: 59.21, lr: 0.000072, Train: (LOSS: 0.0146, MAE: 0.0146, RMSE: 0.0227, R2: 0.9910), Valid: (LOSS: 0.0354, MAE: 0.0354, RMSE: 0.0525, R2: 0.9607), PNorm: 188.8228, GNorm: 0.5000
[120/299] timecost: 59.51, lr: 0.000072, Train: (LOSS: 0.0134, MAE: 0.0134, RMSE: 0.0214, R2: 0.9924), Valid: (LOSS: 0.0364, MAE: 0.0364, RMSE: 0.0539, R2: 0.9587), PNorm: 188.8294, GNorm: 0.4828
[121/299] timecost: 59.54, lr: 0.000072, Train: (LOSS: 0.0134, MAE: 0.0134, RMSE: 0.0209, R2: 0.9925), Valid: (LOSS: 0.0367, MAE: 0.0367, RMSE: 0.0549, R2: 0.9570), PNorm: 188.8345, GNorm: 0.3938
[122/299] timecost: 59.97, lr: 0.000072, Train: (LOSS: 0.0132, MAE: 0.0132, RMSE: 0.0208, R2: 0.9922), Valid: (LOSS: 0.0357, MAE: 0.0357, RMSE: 0.0538, R2: 0.9589), PNorm: 188.8421, GNorm: 0.4201
[123/299] timecost: 59.34, lr: 0.000072, Train: (LOSS: 0.0131, MAE: 0.0131, RMSE: 0.0205, R2: 0.9927), Valid: (LOSS: 0.0350, MAE: 0.0350, RMSE: 0.0524, R2: 0.9609), PNorm: 188.8496, GNorm: 0.4525
[124/299] timecost: 59.63, lr: 0.000072, Train: (LOSS: 0.0130, MAE: 0.0130, RMSE: 0.0204, R2: 0.9927), Valid: (LOSS: 0.0369, MAE: 0.0369, RMSE: 0.0566, R2: 0.9540), PNorm: 188.8574, GNorm: 0.5000
[125/299] timecost: 59.12, lr: 0.000072, Train: (LOSS: 0.0134, MAE: 0.0134, RMSE: 0.0209, R2: 0.9925), Valid: (LOSS: 0.0364, MAE: 0.0364, RMSE: 0.0549, R2: 0.9568), PNorm: 188.8653, GNorm: 0.5000
[126/299] timecost: 59.25, lr: 0.000072, Train: (LOSS: 0.0128, MAE: 0.0128, RMSE: 0.0198, R2: 0.9930), Valid: (LOSS: 0.0355, MAE: 0.0355, RMSE: 0.0521, R2: 0.9618), PNorm: 188.8723, GNorm: 0.5000
[127/299] timecost: 61.68, lr: 0.000072, Train: (LOSS: 0.0127, MAE: 0.0127, RMSE: 0.0198, R2: 0.9933), Valid: (LOSS: 0.0352, MAE: 0.0352, RMSE: 0.0522, R2: 0.9617), PNorm: 188.8810, GNorm: 0.4737
Epoch 00129: reducing learning rate of group 0 to 6.1413e-05.
[128/299] timecost: 61.72, lr: 0.000061, Train: (LOSS: 0.0131, MAE: 0.0131, RMSE: 0.0202, R2: 0.9930), Valid: (LOSS: 0.0363, MAE: 0.0363, RMSE: 0.0538, R2: 0.9596), PNorm: 188.8908, GNorm: 0.5000
[129/299] timecost: 60.86, lr: 0.000061, Train: (LOSS: 0.0117, MAE: 0.0117, RMSE: 0.0184, R2: 0.9940), Valid: (LOSS: 0.0346, MAE: 0.0346, RMSE: 0.0515, R2: 0.9626), PNorm: 188.8971, GNorm: 0.4763
[130/299] timecost: 60.74, lr: 0.000061, Train: (LOSS: 0.0117, MAE: 0.0117, RMSE: 0.0185, R2: 0.9938), Valid: (LOSS: 0.0343, MAE: 0.0343, RMSE: 0.0507, R2: 0.9632), PNorm: 188.9034, GNorm: 0.5000
[131/299] timecost: 59.36, lr: 0.000061, Train: (LOSS: 0.0113, MAE: 0.0113, RMSE: 0.0175, R2: 0.9947), Valid: (LOSS: 0.0353, MAE: 0.0353, RMSE: 0.0511, R2: 0.9628), PNorm: 188.9095, GNorm: 0.5000
[132/299] timecost: 61.42, lr: 0.000061, Train: (LOSS: 0.0113, MAE: 0.0113, RMSE: 0.0171, R2: 0.9949), Valid: (LOSS: 0.0339, MAE: 0.0339, RMSE: 0.0504, R2: 0.9637), PNorm: 188.9157, GNorm: 0.4313
[133/299] timecost: 62.05, lr: 0.000061, Train: (LOSS: 0.0113, MAE: 0.0113, RMSE: 0.0173, R2: 0.9948), Valid: (LOSS: 0.0339, MAE: 0.0339, RMSE: 0.0505, R2: 0.9634), PNorm: 188.9225, GNorm: 0.4557
[134/299] timecost: 62.06, lr: 0.000061, Train: (LOSS: 0.0111, MAE: 0.0111, RMSE: 0.0168, R2: 0.9950), Valid: (LOSS: 0.0355, MAE: 0.0355, RMSE: 0.0510, R2: 0.9632), PNorm: 188.9290, GNorm: 0.4106
[135/299] timecost: 61.99, lr: 0.000061, Train: (LOSS: 0.0103, MAE: 0.0103, RMSE: 0.0156, R2: 0.9955), Valid: (LOSS: 0.0379, MAE: 0.0379, RMSE: 0.0558, R2: 0.9551), PNorm: 188.9351, GNorm: 0.5000
[136/299] timecost: 62.04, lr: 0.000061, Train: (LOSS: 0.0117, MAE: 0.0117, RMSE: 0.0193, R2: 0.9935), Valid: (LOSS: 0.0357, MAE: 0.0357, RMSE: 0.0544, R2: 0.9583), PNorm: 188.9389, GNorm: 0.4102
[137/299] timecost: 62.02, lr: 0.000061, Train: (LOSS: 0.0117, MAE: 0.0117, RMSE: 0.0188, R2: 0.9937), Valid: (LOSS: 0.0353, MAE: 0.0353, RMSE: 0.0521, R2: 0.9621), PNorm: 188.9471, GNorm: 0.5000
[138/299] timecost: 62.01, lr: 0.000061, Train: (LOSS: 0.0111, MAE: 0.0111, RMSE: 0.0179, R2: 0.9943), Valid: (LOSS: 0.0353, MAE: 0.0353, RMSE: 0.0529, R2: 0.9603), PNorm: 188.9543, GNorm: 0.4804
[139/299] timecost: 62.00, lr: 0.000061, Train: (LOSS: 0.0111, MAE: 0.0111, RMSE: 0.0178, R2: 0.9940), Valid: (LOSS: 0.0352, MAE: 0.0352, RMSE: 0.0531, R2: 0.9600), PNorm: 188.9593, GNorm: 0.5000
[140/299] timecost: 61.78, lr: 0.000061, Train: (LOSS: 0.0111, MAE: 0.0111, RMSE: 0.0182, R2: 0.9938), Valid: (LOSS: 0.0346, MAE: 0.0346, RMSE: 0.0528, R2: 0.9603), PNorm: 188.9657, GNorm: 0.5000
[141/299] timecost: 60.68, lr: 0.000061, Train: (LOSS: 0.0110, MAE: 0.0110, RMSE: 0.0178, R2: 0.9937), Valid: (LOSS: 0.0361, MAE: 0.0361, RMSE: 0.0534, R2: 0.9600), PNorm: 188.9700, GNorm: 0.4678
[142/299] timecost: 60.50, lr: 0.000061, Train: (LOSS: 0.0116, MAE: 0.0116, RMSE: 0.0192, R2: 0.9930), Valid: (LOSS: 0.0359, MAE: 0.0359, RMSE: 0.0543, R2: 0.9584), PNorm: 188.9766, GNorm: 0.4939
[143/299] timecost: 60.97, lr: 0.000061, Train: (LOSS: 0.0112, MAE: 0.0112, RMSE: 0.0180, R2: 0.9943), Valid: (LOSS: 0.0355, MAE: 0.0355, RMSE: 0.0549, R2: 0.9570), PNorm: 188.9823, GNorm: 0.4737
[144/299] timecost: 58.78, lr: 0.000061, Train: (LOSS: 0.0107, MAE: 0.0107, RMSE: 0.0177, R2: 0.9941), Valid: (LOSS: 0.0359, MAE: 0.0359, RMSE: 0.0541, R2: 0.9585), PNorm: 188.9876, GNorm: 0.4886
[145/299] timecost: 58.41, lr: 0.000061, Train: (LOSS: 0.0115, MAE: 0.0115, RMSE: 0.0186, R2: 0.9938), Valid: (LOSS: 0.0358, MAE: 0.0358, RMSE: 0.0553, R2: 0.9567), PNorm: 188.9937, GNorm: 0.4578
[146/299] timecost: 58.19, lr: 0.000061, Train: (LOSS: 0.0106, MAE: 0.0106, RMSE: 0.0170, R2: 0.9943), Valid: (LOSS: 0.0347, MAE: 0.0347, RMSE: 0.0539, R2: 0.9593), PNorm: 189.0008, GNorm: 0.3927
[147/299] timecost: 58.19, lr: 0.000061, Train: (LOSS: 0.0109, MAE: 0.0109, RMSE: 0.0172, R2: 0.9945), Valid: (LOSS: 0.0344, MAE: 0.0344, RMSE: 0.0524, R2: 0.9608), PNorm: 189.0071, GNorm: 0.4381
Epoch 00149: reducing learning rate of group 0 to 5.2201e-05.
[148/299] timecost: 57.99, lr: 0.000052, Train: (LOSS: 0.0111, MAE: 0.0111, RMSE: 0.0172, R2: 0.9949), Valid: (LOSS: 0.0364, MAE: 0.0364, RMSE: 0.0548, R2: 0.9576), PNorm: 189.0150, GNorm: 0.3671
[149/299] timecost: 57.96, lr: 0.000052, Train: (LOSS: 0.0098, MAE: 0.0098, RMSE: 0.0157, R2: 0.9953), Valid: (LOSS: 0.0351, MAE: 0.0351, RMSE: 0.0531, R2: 0.9603), PNorm: 189.0196, GNorm: 0.3626
[150/299] timecost: 57.82, lr: 0.000052, Train: (LOSS: 0.0092, MAE: 0.0092, RMSE: 0.0145, R2: 0.9960), Valid: (LOSS: 0.0347, MAE: 0.0347, RMSE: 0.0517, R2: 0.9620), PNorm: 189.0244, GNorm: 0.4295
[151/299] timecost: 57.85, lr: 0.000052, Train: (LOSS: 0.0088, MAE: 0.0088, RMSE: 0.0138, R2: 0.9963), Valid: (LOSS: 0.0348, MAE: 0.0348, RMSE: 0.0514, R2: 0.9627), PNorm: 189.0288, GNorm: 0.4057
[152/299] timecost: 58.42, lr: 0.000052, Train: (LOSS: 0.0088, MAE: 0.0088, RMSE: 0.0139, R2: 0.9964), Valid: (LOSS: 0.0345, MAE: 0.0345, RMSE: 0.0513, R2: 0.9623), PNorm: 189.0336, GNorm: 0.4421
[153/299] timecost: 58.83, lr: 0.000052, Train: (LOSS: 0.0088, MAE: 0.0088, RMSE: 0.0139, R2: 0.9964), Valid: (LOSS: 0.0350, MAE: 0.0350, RMSE: 0.0510, R2: 0.9623), PNorm: 189.0390, GNorm: 0.5000
[154/299] timecost: 59.15, lr: 0.000052, Train: (LOSS: 0.0089, MAE: 0.0089, RMSE: 0.0137, R2: 0.9967), Valid: (LOSS: 0.0349, MAE: 0.0349, RMSE: 0.0522, R2: 0.9618), PNorm: 189.0423, GNorm: 0.4385
[155/299] timecost: 59.14, lr: 0.000052, Train: (LOSS: 0.0089, MAE: 0.0089, RMSE: 0.0142, R2: 0.9963), Valid: (LOSS: 0.0339, MAE: 0.0339, RMSE: 0.0507, R2: 0.9637), PNorm: 189.0463, GNorm: 0.4015
[156/299] timecost: 59.15, lr: 0.000052, Train: (LOSS: 0.0092, MAE: 0.0092, RMSE: 0.0141, R2: 0.9961), Valid: (LOSS: 0.0339, MAE: 0.0339, RMSE: 0.0503, R2: 0.9643), PNorm: 189.0522, GNorm: 0.5000
[157/299] timecost: 59.19, lr: 0.000052, Train: (LOSS: 0.0085, MAE: 0.0085, RMSE: 0.0129, R2: 0.9970), Valid: (LOSS: 0.0346, MAE: 0.0346, RMSE: 0.0504, R2: 0.9642), PNorm: 189.0566, GNorm: 0.5000
[158/299] timecost: 59.56, lr: 0.000052, Train: (LOSS: 0.0084, MAE: 0.0084, RMSE: 0.0129, R2: 0.9968), Valid: (LOSS: 0.0342, MAE: 0.0342, RMSE: 0.0497, R2: 0.9646), PNorm: 189.0620, GNorm: 0.4408
[159/299] timecost: 59.38, lr: 0.000052, Train: (LOSS: 0.0085, MAE: 0.0085, RMSE: 0.0131, R2: 0.9968), Valid: (LOSS: 0.0334, MAE: 0.0334, RMSE: 0.0493, R2: 0.9655), PNorm: 189.0659, GNorm: 0.4122
[160/299] timecost: 59.95, lr: 0.000052, Train: (LOSS: 0.0088, MAE: 0.0088, RMSE: 0.0135, R2: 0.9967), Valid: (LOSS: 0.0349, MAE: 0.0349, RMSE: 0.0516, R2: 0.9622), PNorm: 189.0723, GNorm: 0.3759
[161/299] timecost: 59.32, lr: 0.000052, Train: (LOSS: 0.0086, MAE: 0.0086, RMSE: 0.0130, R2: 0.9969), Valid: (LOSS: 0.0328, MAE: 0.0328, RMSE: 0.0478, R2: 0.9674), PNorm: 189.0750, GNorm: 0.3957
[162/299] timecost: 59.30, lr: 0.000052, Train: (LOSS: 0.0085, MAE: 0.0085, RMSE: 0.0129, R2: 0.9969), Valid: (LOSS: 0.0337, MAE: 0.0337, RMSE: 0.0509, R2: 0.9631), PNorm: 189.0789, GNorm: 0.3619
[163/299] timecost: 59.39, lr: 0.000052, Train: (LOSS: 0.0085, MAE: 0.0085, RMSE: 0.0130, R2: 0.9968), Valid: (LOSS: 0.0338, MAE: 0.0338, RMSE: 0.0496, R2: 0.9652), PNorm: 189.0842, GNorm: 0.4080
[164/299] timecost: 59.27, lr: 0.000052, Train: (LOSS: 0.0081, MAE: 0.0081, RMSE: 0.0123, R2: 0.9972), Valid: (LOSS: 0.0328, MAE: 0.0328, RMSE: 0.0493, R2: 0.9657), PNorm: 189.0880, GNorm: 0.4864
[165/299] timecost: 59.14, lr: 0.000052, Train: (LOSS: 0.0081, MAE: 0.0081, RMSE: 0.0124, R2: 0.9969), Valid: (LOSS: 0.0332, MAE: 0.0332, RMSE: 0.0499, R2: 0.9646), PNorm: 189.0935, GNorm: 0.4334
[166/299] timecost: 59.51, lr: 0.000052, Train: (LOSS: 0.0079, MAE: 0.0079, RMSE: 0.0121, R2: 0.9968), Valid: (LOSS: 0.0345, MAE: 0.0345, RMSE: 0.0501, R2: 0.9642), PNorm: 189.0968, GNorm: 0.5000
[167/299] timecost: 59.66, lr: 0.000052, Train: (LOSS: 0.0083, MAE: 0.0083, RMSE: 0.0125, R2: 0.9972), Valid: (LOSS: 0.0339, MAE: 0.0339, RMSE: 0.0498, R2: 0.9646), PNorm: 189.1007, GNorm: 0.3272
[168/299] timecost: 59.67, lr: 0.000052, Train: (LOSS: 0.0082, MAE: 0.0082, RMSE: 0.0124, R2: 0.9971), Valid: (LOSS: 0.0330, MAE: 0.0330, RMSE: 0.0495, R2: 0.9649), PNorm: 189.1052, GNorm: 0.5000
[169/299] timecost: 59.17, lr: 0.000052, Train: (LOSS: 0.0081, MAE: 0.0081, RMSE: 0.0124, R2: 0.9972), Valid: (LOSS: 0.0345, MAE: 0.0345, RMSE: 0.0508, R2: 0.9632), PNorm: 189.1102, GNorm: 0.3492
[170/299] timecost: 59.70, lr: 0.000052, Train: (LOSS: 0.0086, MAE: 0.0086, RMSE: 0.0129, R2: 0.9970), Valid: (LOSS: 0.0348, MAE: 0.0348, RMSE: 0.0507, R2: 0.9634), PNorm: 189.1143, GNorm: 0.4275
[171/299] timecost: 59.35, lr: 0.000052, Train: (LOSS: 0.0081, MAE: 0.0081, RMSE: 0.0123, R2: 0.9972), Valid: (LOSS: 0.0343, MAE: 0.0343, RMSE: 0.0502, R2: 0.9640), PNorm: 189.1197, GNorm: 0.5000
[172/299] timecost: 59.60, lr: 0.000052, Train: (LOSS: 0.0077, MAE: 0.0077, RMSE: 0.0118, R2: 0.9975), Valid: (LOSS: 0.0336, MAE: 0.0336, RMSE: 0.0503, R2: 0.9634), PNorm: 189.1223, GNorm: 0.4309
[173/299] timecost: 61.16, lr: 0.000052, Train: (LOSS: 0.0080, MAE: 0.0080, RMSE: 0.0124, R2: 0.9971), Valid: (LOSS: 0.0349, MAE: 0.0349, RMSE: 0.0517, R2: 0.9619), PNorm: 189.1267, GNorm: 0.4309
[174/299] timecost: 61.99, lr: 0.000052, Train: (LOSS: 0.0079, MAE: 0.0079, RMSE: 0.0119, R2: 0.9974), Valid: (LOSS: 0.0350, MAE: 0.0350, RMSE: 0.0504, R2: 0.9642), PNorm: 189.1311, GNorm: 0.5000
[175/299] timecost: 62.01, lr: 0.000052, Train: (LOSS: 0.0079, MAE: 0.0079, RMSE: 0.0120, R2: 0.9973), Valid: (LOSS: 0.0327, MAE: 0.0327, RMSE: 0.0482, R2: 0.9668), PNorm: 189.1367, GNorm: 0.5000
[176/299] timecost: 62.07, lr: 0.000052, Train: (LOSS: 0.0081, MAE: 0.0081, RMSE: 0.0119, R2: 0.9973), Valid: (LOSS: 0.0342, MAE: 0.0342, RMSE: 0.0512, R2: 0.9625), PNorm: 189.1404, GNorm: 0.4918
Epoch 00178: reducing learning rate of group 0 to 4.4371e-05.
[177/299] timecost: 59.89, lr: 0.000044, Train: (LOSS: 0.0082, MAE: 0.0082, RMSE: 0.0125, R2: 0.9970), Valid: (LOSS: 0.0337, MAE: 0.0337, RMSE: 0.0492, R2: 0.9655), PNorm: 189.1475, GNorm: 0.4501
[178/299] timecost: 59.14, lr: 0.000044, Train: (LOSS: 0.0072, MAE: 0.0072, RMSE: 0.0109, R2: 0.9978), Valid: (LOSS: 0.0334, MAE: 0.0334, RMSE: 0.0494, R2: 0.9654), PNorm: 189.1510, GNorm: 0.4660
[179/299] timecost: 59.10, lr: 0.000044, Train: (LOSS: 0.0066, MAE: 0.0066, RMSE: 0.0101, R2: 0.9979), Valid: (LOSS: 0.0327, MAE: 0.0327, RMSE: 0.0479, R2: 0.9675), PNorm: 189.1533, GNorm: 0.4883
[180/299] timecost: 61.36, lr: 0.000044, Train: (LOSS: 0.0067, MAE: 0.0067, RMSE: 0.0102, R2: 0.9981), Valid: (LOSS: 0.0335, MAE: 0.0335, RMSE: 0.0493, R2: 0.9655), PNorm: 189.1562, GNorm: 0.5000
[181/299] timecost: 62.02, lr: 0.000044, Train: (LOSS: 0.0068, MAE: 0.0068, RMSE: 0.0102, R2: 0.9981), Valid: (LOSS: 0.0337, MAE: 0.0337, RMSE: 0.0488, R2: 0.9663), PNorm: 189.1592, GNorm: 0.3769
[182/299] timecost: 61.98, lr: 0.000044, Train: (LOSS: 0.0068, MAE: 0.0068, RMSE: 0.0103, R2: 0.9979), Valid: (LOSS: 0.0333, MAE: 0.0333, RMSE: 0.0487, R2: 0.9665), PNorm: 189.1610, GNorm: 0.3825
[183/299] timecost: 62.18, lr: 0.000044, Train: (LOSS: 0.0070, MAE: 0.0070, RMSE: 0.0105, R2: 0.9975), Valid: (LOSS: 0.0338, MAE: 0.0338, RMSE: 0.0497, R2: 0.9650), PNorm: 189.1644, GNorm: 0.3909
[184/299] timecost: 62.10, lr: 0.000044, Train: (LOSS: 0.0066, MAE: 0.0066, RMSE: 0.0101, R2: 0.9981), Valid: (LOSS: 0.0344, MAE: 0.0344, RMSE: 0.0502, R2: 0.9647), PNorm: 189.1671, GNorm: 0.4795
[185/299] timecost: 62.05, lr: 0.000044, Train: (LOSS: 0.0070, MAE: 0.0070, RMSE: 0.0106, R2: 0.9979), Valid: (LOSS: 0.0338, MAE: 0.0338, RMSE: 0.0503, R2: 0.9642), PNorm: 189.1713, GNorm: 0.4399
[186/299] timecost: 62.03, lr: 0.000044, Train: (LOSS: 0.0071, MAE: 0.0071, RMSE: 0.0107, R2: 0.9978), Valid: (LOSS: 0.0337, MAE: 0.0337, RMSE: 0.0488, R2: 0.9665), PNorm: 189.1749, GNorm: 0.5000
[187/299] timecost: 62.26, lr: 0.000044, Train: (LOSS: 0.0069, MAE: 0.0069, RMSE: 0.0104, R2: 0.9978), Valid: (LOSS: 0.0330, MAE: 0.0330, RMSE: 0.0489, R2: 0.9662), PNorm: 189.1777, GNorm: 0.4021
[188/299] timecost: 62.05, lr: 0.000044, Train: (LOSS: 0.0065, MAE: 0.0065, RMSE: 0.0100, R2: 0.9979), Valid: (LOSS: 0.0329, MAE: 0.0329, RMSE: 0.0490, R2: 0.9657), PNorm: 189.1810, GNorm: 0.5000
[189/299] timecost: 62.16, lr: 0.000044, Train: (LOSS: 0.0065, MAE: 0.0065, RMSE: 0.0099, R2: 0.9982), Valid: (LOSS: 0.0335, MAE: 0.0335, RMSE: 0.0494, R2: 0.9651), PNorm: 189.1841, GNorm: 0.5000
[190/299] timecost: 62.04, lr: 0.000044, Train: (LOSS: 0.0067, MAE: 0.0067, RMSE: 0.0103, R2: 0.9980), Valid: (LOSS: 0.0338, MAE: 0.0338, RMSE: 0.0505, R2: 0.9636), PNorm: 189.1871, GNorm: 0.5000
[191/299] timecost: 62.12, lr: 0.000044, Train: (LOSS: 0.0065, MAE: 0.0065, RMSE: 0.0098, R2: 0.9980), Valid: (LOSS: 0.0336, MAE: 0.0336, RMSE: 0.0496, R2: 0.9650), PNorm: 189.1907, GNorm: 0.3963
[192/299] timecost: 62.03, lr: 0.000044, Train: (LOSS: 0.0063, MAE: 0.0063, RMSE: 0.0094, R2: 0.9982), Valid: (LOSS: 0.0336, MAE: 0.0336, RMSE: 0.0501, R2: 0.9641), PNorm: 189.1931, GNorm: 0.4539
Epoch 00194: reducing learning rate of group 0 to 3.7715e-05.
[193/299] timecost: 62.07, lr: 0.000038, Train: (LOSS: 0.0066, MAE: 0.0066, RMSE: 0.0097, R2: 0.9978), Valid: (LOSS: 0.0338, MAE: 0.0338, RMSE: 0.0500, R2: 0.9646), PNorm: 189.1969, GNorm: 0.5000
[194/299] timecost: 62.16, lr: 0.000038, Train: (LOSS: 0.0061, MAE: 0.0061, RMSE: 0.0094, R2: 0.9982), Valid: (LOSS: 0.0340, MAE: 0.0340, RMSE: 0.0495, R2: 0.9650), PNorm: 189.1997, GNorm: 0.3977
[195/299] timecost: 62.35, lr: 0.000038, Train: (LOSS: 0.0059, MAE: 0.0059, RMSE: 0.0092, R2: 0.9982), Valid: (LOSS: 0.0334, MAE: 0.0334, RMSE: 0.0486, R2: 0.9664), PNorm: 189.2016, GNorm: 0.3679
[196/299] timecost: 62.04, lr: 0.000038, Train: (LOSS: 0.0061, MAE: 0.0061, RMSE: 0.0095, R2: 0.9982), Valid: (LOSS: 0.0336, MAE: 0.0336, RMSE: 0.0496, R2: 0.9648), PNorm: 189.2045, GNorm: 0.5000
[197/299] timecost: 61.76, lr: 0.000038, Train: (LOSS: 0.0057, MAE: 0.0057, RMSE: 0.0087, R2: 0.9984), Valid: (LOSS: 0.0334, MAE: 0.0334, RMSE: 0.0498, R2: 0.9645), PNorm: 189.2068, GNorm: 0.4949
[198/299] timecost: 60.64, lr: 0.000038, Train: (LOSS: 0.0061, MAE: 0.0061, RMSE: 0.0093, R2: 0.9982), Valid: (LOSS: 0.0337, MAE: 0.0337, RMSE: 0.0508, R2: 0.9628), PNorm: 189.2095, GNorm: 0.5000
[199/299] timecost: 60.77, lr: 0.000038, Train: (LOSS: 0.0056, MAE: 0.0056, RMSE: 0.0086, R2: 0.9983), Valid: (LOSS: 0.0331, MAE: 0.0331, RMSE: 0.0493, R2: 0.9653), PNorm: 189.2116, GNorm: 0.4319
[200/299] timecost: 60.69, lr: 0.000038, Train: (LOSS: 0.0055, MAE: 0.0055, RMSE: 0.0087, R2: 0.9985), Valid: (LOSS: 0.0335, MAE: 0.0335, RMSE: 0.0491, R2: 0.9655), PNorm: 189.2142, GNorm: 0.5000
[201/299] timecost: 59.33, lr: 0.000038, Train: (LOSS: 0.0055, MAE: 0.0055, RMSE: 0.0086, R2: 0.9984), Valid: (LOSS: 0.0333, MAE: 0.0333, RMSE: 0.0493, R2: 0.9653), PNorm: 189.2167, GNorm: 0.4420
[202/299] timecost: 59.69, lr: 0.000038, Train: (LOSS: 0.0056, MAE: 0.0056, RMSE: 0.0086, R2: 0.9985), Valid: (LOSS: 0.0332, MAE: 0.0332, RMSE: 0.0489, R2: 0.9661), PNorm: 189.2185, GNorm: 0.5000
[203/299] timecost: 59.69, lr: 0.000038, Train: (LOSS: 0.0056, MAE: 0.0056, RMSE: 0.0087, R2: 0.9984), Valid: (LOSS: 0.0329, MAE: 0.0329, RMSE: 0.0482, R2: 0.9670), PNorm: 189.2212, GNorm: 0.3767
[204/299] timecost: 59.00, lr: 0.000038, Train: (LOSS: 0.0056, MAE: 0.0056, RMSE: 0.0086, R2: 0.9984), Valid: (LOSS: 0.0334, MAE: 0.0334, RMSE: 0.0500, R2: 0.9641), PNorm: 189.2232, GNorm: 0.5000
[205/299] timecost: 59.38, lr: 0.000038, Train: (LOSS: 0.0056, MAE: 0.0056, RMSE: 0.0087, R2: 0.9984), Valid: (LOSS: 0.0333, MAE: 0.0333, RMSE: 0.0485, R2: 0.9663), PNorm: 189.2261, GNorm: 0.3679
[206/299] timecost: 59.09, lr: 0.000038, Train: (LOSS: 0.0056, MAE: 0.0056, RMSE: 0.0085, R2: 0.9984), Valid: (LOSS: 0.0330, MAE: 0.0330, RMSE: 0.0482, R2: 0.9669), PNorm: 189.2273, GNorm: 0.5000
[207/299] timecost: 59.14, lr: 0.000038, Train: (LOSS: 0.0056, MAE: 0.0056, RMSE: 0.0084, R2: 0.9985), Valid: (LOSS: 0.0326, MAE: 0.0326, RMSE: 0.0475, R2: 0.9677), PNorm: 189.2298, GNorm: 0.5000
[208/299] timecost: 58.78, lr: 0.000038, Train: (LOSS: 0.0054, MAE: 0.0054, RMSE: 0.0083, R2: 0.9985), Valid: (LOSS: 0.0331, MAE: 0.0331, RMSE: 0.0485, R2: 0.9665), PNorm: 189.2312, GNorm: 0.3992
Epoch 00210: reducing learning rate of group 0 to 3.2058e-05.
[209/299] timecost: 58.16, lr: 0.000032, Train: (LOSS: 0.0055, MAE: 0.0055, RMSE: 0.0084, R2: 0.9985), Valid: (LOSS: 0.0334, MAE: 0.0334, RMSE: 0.0490, R2: 0.9659), PNorm: 189.2343, GNorm: 0.4502
[210/299] timecost: 58.86, lr: 0.000032, Train: (LOSS: 0.0054, MAE: 0.0054, RMSE: 0.0083, R2: 0.9986), Valid: (LOSS: 0.0339, MAE: 0.0339, RMSE: 0.0501, R2: 0.9643), PNorm: 189.2360, GNorm: 0.5000
[211/299] timecost: 61.30, lr: 0.000032, Train: (LOSS: 0.0049, MAE: 0.0049, RMSE: 0.0077, R2: 0.9988), Valid: (LOSS: 0.0334, MAE: 0.0334, RMSE: 0.0489, R2: 0.9662), PNorm: 189.2378, GNorm: 0.4631
[212/299] timecost: 58.69, lr: 0.000032, Train: (LOSS: 0.0048, MAE: 0.0048, RMSE: 0.0077, R2: 0.9988), Valid: (LOSS: 0.0341, MAE: 0.0341, RMSE: 0.0499, R2: 0.9646), PNorm: 189.2395, GNorm: 0.5000
[213/299] timecost: 58.37, lr: 0.000032, Train: (LOSS: 0.0049, MAE: 0.0049, RMSE: 0.0076, R2: 0.9988), Valid: (LOSS: 0.0334, MAE: 0.0334, RMSE: 0.0488, R2: 0.9660), PNorm: 189.2407, GNorm: 0.4495
[214/299] timecost: 58.75, lr: 0.000032, Train: (LOSS: 0.0049, MAE: 0.0049, RMSE: 0.0077, R2: 0.9987), Valid: (LOSS: 0.0330, MAE: 0.0330, RMSE: 0.0488, R2: 0.9662), PNorm: 189.2427, GNorm: 0.4432
[215/299] timecost: 60.03, lr: 0.000032, Train: (LOSS: 0.0048, MAE: 0.0048, RMSE: 0.0075, R2: 0.9987), Valid: (LOSS: 0.0336, MAE: 0.0336, RMSE: 0.0498, R2: 0.9647), PNorm: 189.2450, GNorm: 0.3793
[216/299] timecost: 60.00, lr: 0.000032, Train: (LOSS: 0.0050, MAE: 0.0050, RMSE: 0.0078, R2: 0.9987), Valid: (LOSS: 0.0328, MAE: 0.0328, RMSE: 0.0488, R2: 0.9661), PNorm: 189.2472, GNorm: 0.3864
[217/299] timecost: 60.13, lr: 0.000032, Train: (LOSS: 0.0049, MAE: 0.0049, RMSE: 0.0075, R2: 0.9987), Valid: (LOSS: 0.0334, MAE: 0.0334, RMSE: 0.0492, R2: 0.9655), PNorm: 189.2490, GNorm: 0.5000
[218/299] timecost: 60.02, lr: 0.000032, Train: (LOSS: 0.0049, MAE: 0.0049, RMSE: 0.0076, R2: 0.9987), Valid: (LOSS: 0.0329, MAE: 0.0329, RMSE: 0.0485, R2: 0.9666), PNorm: 189.2515, GNorm: 0.5000
[219/299] timecost: 59.92, lr: 0.000032, Train: (LOSS: 0.0049, MAE: 0.0049, RMSE: 0.0077, R2: 0.9987), Valid: (LOSS: 0.0327, MAE: 0.0327, RMSE: 0.0489, R2: 0.9657), PNorm: 189.2526, GNorm: 0.4412
[220/299] timecost: 59.42, lr: 0.000032, Train: (LOSS: 0.0049, MAE: 0.0049, RMSE: 0.0077, R2: 0.9986), Valid: (LOSS: 0.0334, MAE: 0.0334, RMSE: 0.0491, R2: 0.9656), PNorm: 189.2549, GNorm: 0.4837
[221/299] timecost: 60.25, lr: 0.000032, Train: (LOSS: 0.0050, MAE: 0.0050, RMSE: 0.0079, R2: 0.9987), Valid: (LOSS: 0.0328, MAE: 0.0328, RMSE: 0.0484, R2: 0.9666), PNorm: 189.2566, GNorm: 0.4746
[222/299] timecost: 58.91, lr: 0.000032, Train: (LOSS: 0.0048, MAE: 0.0048, RMSE: 0.0076, R2: 0.9987), Valid: (LOSS: 0.0339, MAE: 0.0339, RMSE: 0.0494, R2: 0.9654), PNorm: 189.2580, GNorm: 0.4231
[223/299] timecost: 58.87, lr: 0.000032, Train: (LOSS: 0.0049, MAE: 0.0049, RMSE: 0.0077, R2: 0.9987), Valid: (LOSS: 0.0325, MAE: 0.0325, RMSE: 0.0487, R2: 0.9663), PNorm: 189.2606, GNorm: 0.4534
[224/299] timecost: 59.46, lr: 0.000032, Train: (LOSS: 0.0048, MAE: 0.0048, RMSE: 0.0074, R2: 0.9989), Valid: (LOSS: 0.0335, MAE: 0.0335, RMSE: 0.0490, R2: 0.9658), PNorm: 189.2626, GNorm: 0.4077
[225/299] timecost: 58.55, lr: 0.000032, Train: (LOSS: 0.0048, MAE: 0.0048, RMSE: 0.0074, R2: 0.9989), Valid: (LOSS: 0.0330, MAE: 0.0330, RMSE: 0.0491, R2: 0.9659), PNorm: 189.2645, GNorm: 0.5000
[226/299] timecost: 59.75, lr: 0.000032, Train: (LOSS: 0.0048, MAE: 0.0048, RMSE: 0.0075, R2: 0.9988), Valid: (LOSS: 0.0335, MAE: 0.0335, RMSE: 0.0497, R2: 0.9647), PNorm: 189.2658, GNorm: 0.4469
[227/299] timecost: 59.91, lr: 0.000032, Train: (LOSS: 0.0050, MAE: 0.0050, RMSE: 0.0078, R2: 0.9988), Valid: (LOSS: 0.0328, MAE: 0.0328, RMSE: 0.0489, R2: 0.9658), PNorm: 189.2680, GNorm: 0.4820
[228/299] timecost: 59.87, lr: 0.000032, Train: (LOSS: 0.0048, MAE: 0.0048, RMSE: 0.0074, R2: 0.9989), Valid: (LOSS: 0.0335, MAE: 0.0335, RMSE: 0.0496, R2: 0.9649), PNorm: 189.2682, GNorm: 0.4233
[229/299] timecost: 60.00, lr: 0.000032, Train: (LOSS: 0.0048, MAE: 0.0048, RMSE: 0.0074, R2: 0.9988), Valid: (LOSS: 0.0334, MAE: 0.0334, RMSE: 0.0492, R2: 0.9655), PNorm: 189.2717, GNorm: 0.3696
[230/299] timecost: 60.05, lr: 0.000032, Train: (LOSS: 0.0049, MAE: 0.0049, RMSE: 0.0073, R2: 0.9988), Valid: (LOSS: 0.0329, MAE: 0.0329, RMSE: 0.0481, R2: 0.9670), PNorm: 189.2738, GNorm: 0.5000
[231/299] timecost: 60.04, lr: 0.000032, Train: (LOSS: 0.0046, MAE: 0.0046, RMSE: 0.0070, R2: 0.9989), Valid: (LOSS: 0.0333, MAE: 0.0333, RMSE: 0.0488, R2: 0.9658), PNorm: 189.2748, GNorm: 0.3525
[232/299] timecost: 60.01, lr: 0.000032, Train: (LOSS: 0.0046, MAE: 0.0046, RMSE: 0.0072, R2: 0.9988), Valid: (LOSS: 0.0326, MAE: 0.0326, RMSE: 0.0484, R2: 0.9667), PNorm: 189.2764, GNorm: 0.5000
[233/299] timecost: 60.08, lr: 0.000032, Train: (LOSS: 0.0046, MAE: 0.0046, RMSE: 0.0072, R2: 0.9987), Valid: (LOSS: 0.0322, MAE: 0.0322, RMSE: 0.0479, R2: 0.9673), PNorm: 189.2781, GNorm: 0.5000
[234/299] timecost: 59.93, lr: 0.000032, Train: (LOSS: 0.0047, MAE: 0.0047, RMSE: 0.0072, R2: 0.9987), Valid: (LOSS: 0.0330, MAE: 0.0330, RMSE: 0.0489, R2: 0.9660), PNorm: 189.2803, GNorm: 0.4833
[235/299] timecost: 59.77, lr: 0.000032, Train: (LOSS: 0.0046, MAE: 0.0046, RMSE: 0.0072, R2: 0.9989), Valid: (LOSS: 0.0335, MAE: 0.0335, RMSE: 0.0491, R2: 0.9660), PNorm: 189.2817, GNorm: 0.3885
[236/299] timecost: 58.35, lr: 0.000032, Train: (LOSS: 0.0047, MAE: 0.0047, RMSE: 0.0073, R2: 0.9989), Valid: (LOSS: 0.0336, MAE: 0.0336, RMSE: 0.0494, R2: 0.9653), PNorm: 189.2838, GNorm: 0.4790
[237/299] timecost: 58.37, lr: 0.000032, Train: (LOSS: 0.0047, MAE: 0.0047, RMSE: 0.0073, R2: 0.9989), Valid: (LOSS: 0.0333, MAE: 0.0333, RMSE: 0.0489, R2: 0.9659), PNorm: 189.2854, GNorm: 0.4036
[238/299] timecost: 58.02, lr: 0.000032, Train: (LOSS: 0.0050, MAE: 0.0050, RMSE: 0.0075, R2: 0.9989), Valid: (LOSS: 0.0331, MAE: 0.0331, RMSE: 0.0492, R2: 0.9657), PNorm: 189.2875, GNorm: 0.4565
Epoch 00240: reducing learning rate of group 0 to 2.7249e-05.
[239/299] timecost: 58.08, lr: 0.000027, Train: (LOSS: 0.0045, MAE: 0.0045, RMSE: 0.0070, R2: 0.9989), Valid: (LOSS: 0.0333, MAE: 0.0333, RMSE: 0.0497, R2: 0.9647), PNorm: 189.2883, GNorm: 0.4889
[240/299] timecost: 58.22, lr: 0.000027, Train: (LOSS: 0.0044, MAE: 0.0044, RMSE: 0.0069, R2: 0.9990), Valid: (LOSS: 0.0336, MAE: 0.0336, RMSE: 0.0498, R2: 0.9648), PNorm: 189.2904, GNorm: 0.5000
[241/299] timecost: 58.71, lr: 0.000027, Train: (LOSS: 0.0042, MAE: 0.0042, RMSE: 0.0066, R2: 0.9989), Valid: (LOSS: 0.0336, MAE: 0.0336, RMSE: 0.0497, R2: 0.9646), PNorm: 189.2915, GNorm: 0.4748
[242/299] timecost: 59.20, lr: 0.000027, Train: (LOSS: 0.0039, MAE: 0.0039, RMSE: 0.0063, R2: 0.9991), Valid: (LOSS: 0.0328, MAE: 0.0328, RMSE: 0.0486, R2: 0.9663), PNorm: 189.2929, GNorm: 0.4890
[243/299] timecost: 62.03, lr: 0.000027, Train: (LOSS: 0.0039, MAE: 0.0039, RMSE: 0.0062, R2: 0.9991), Valid: (LOSS: 0.0333, MAE: 0.0333, RMSE: 0.0486, R2: 0.9665), PNorm: 189.2942, GNorm: 0.3864
[244/299] timecost: 62.02, lr: 0.000027, Train: (LOSS: 0.0043, MAE: 0.0043, RMSE: 0.0068, R2: 0.9990), Valid: (LOSS: 0.0333, MAE: 0.0333, RMSE: 0.0494, R2: 0.9654), PNorm: 189.2953, GNorm: 0.5000
[245/299] timecost: 61.99, lr: 0.000027, Train: (LOSS: 0.0039, MAE: 0.0039, RMSE: 0.0063, R2: 0.9990), Valid: (LOSS: 0.0329, MAE: 0.0329, RMSE: 0.0486, R2: 0.9665), PNorm: 189.2964, GNorm: 0.4406
[246/299] timecost: 62.12, lr: 0.000027, Train: (LOSS: 0.0039, MAE: 0.0039, RMSE: 0.0062, R2: 0.9992), Valid: (LOSS: 0.0332, MAE: 0.0332, RMSE: 0.0488, R2: 0.9661), PNorm: 189.2978, GNorm: 0.5000
[247/299] timecost: 62.15, lr: 0.000027, Train: (LOSS: 0.0040, MAE: 0.0040, RMSE: 0.0064, R2: 0.9991), Valid: (LOSS: 0.0330, MAE: 0.0330, RMSE: 0.0489, R2: 0.9659), PNorm: 189.2988, GNorm: 0.4290
[248/299] timecost: 62.06, lr: 0.000027, Train: (LOSS: 0.0040, MAE: 0.0040, RMSE: 0.0063, R2: 0.9990), Valid: (LOSS: 0.0330, MAE: 0.0330, RMSE: 0.0490, R2: 0.9658), PNorm: 189.3002, GNorm: 0.4902
[249/299] timecost: 60.11, lr: 0.000027, Train: (LOSS: 0.0040, MAE: 0.0040, RMSE: 0.0065, R2: 0.9988), Valid: (LOSS: 0.0329, MAE: 0.0329, RMSE: 0.0487, R2: 0.9665), PNorm: 189.3014, GNorm: 0.3921
[250/299] timecost: 58.95, lr: 0.000027, Train: (LOSS: 0.0040, MAE: 0.0040, RMSE: 0.0064, R2: 0.9991), Valid: (LOSS: 0.0331, MAE: 0.0331, RMSE: 0.0488, R2: 0.9664), PNorm: 189.3029, GNorm: 0.4838
[251/299] timecost: 57.73, lr: 0.000027, Train: (LOSS: 0.0041, MAE: 0.0041, RMSE: 0.0064, R2: 0.9988), Valid: (LOSS: 0.0328, MAE: 0.0328, RMSE: 0.0484, R2: 0.9669), PNorm: 189.3039, GNorm: 0.3913
[252/299] timecost: 59.07, lr: 0.000027, Train: (LOSS: 0.0041, MAE: 0.0041, RMSE: 0.0065, R2: 0.9990), Valid: (LOSS: 0.0328, MAE: 0.0328, RMSE: 0.0482, R2: 0.9669), PNorm: 189.3057, GNorm: 0.4129
[253/299] timecost: 62.02, lr: 0.000027, Train: (LOSS: 0.0039, MAE: 0.0039, RMSE: 0.0063, R2: 0.9991), Valid: (LOSS: 0.0327, MAE: 0.0327, RMSE: 0.0482, R2: 0.9671), PNorm: 189.3064, GNorm: 0.4044
[254/299] timecost: 61.98, lr: 0.000027, Train: (LOSS: 0.0040, MAE: 0.0040, RMSE: 0.0063, R2: 0.9991), Valid: (LOSS: 0.0327, MAE: 0.0327, RMSE: 0.0477, R2: 0.9676), PNorm: 189.3085, GNorm: 0.4641
Epoch 00256: reducing learning rate of group 0 to 2.3162e-05.
[255/299] timecost: 62.08, lr: 0.000023, Train: (LOSS: 0.0042, MAE: 0.0042, RMSE: 0.0067, R2: 0.9990), Valid: (LOSS: 0.0330, MAE: 0.0330, RMSE: 0.0491, R2: 0.9657), PNorm: 189.3099, GNorm: 0.4352
[256/299] timecost: 62.07, lr: 0.000023, Train: (LOSS: 0.0038, MAE: 0.0038, RMSE: 0.0060, R2: 0.9989), Valid: (LOSS: 0.0328, MAE: 0.0328, RMSE: 0.0484, R2: 0.9669), PNorm: 189.3109, GNorm: 0.4842
[257/299] timecost: 62.21, lr: 0.000023, Train: (LOSS: 0.0035, MAE: 0.0035, RMSE: 0.0057, R2: 0.9990), Valid: (LOSS: 0.0327, MAE: 0.0327, RMSE: 0.0488, R2: 0.9663), PNorm: 189.3117, GNorm: 0.5000
[258/299] timecost: 62.07, lr: 0.000023, Train: (LOSS: 0.0036, MAE: 0.0036, RMSE: 0.0058, R2: 0.9990), Valid: (LOSS: 0.0329, MAE: 0.0329, RMSE: 0.0485, R2: 0.9666), PNorm: 189.3125, GNorm: 0.5000
[259/299] timecost: 62.17, lr: 0.000023, Train: (LOSS: 0.0036, MAE: 0.0036, RMSE: 0.0059, R2: 0.9992), Valid: (LOSS: 0.0327, MAE: 0.0327, RMSE: 0.0483, R2: 0.9671), PNorm: 189.3141, GNorm: 0.4641
[260/299] timecost: 62.04, lr: 0.000023, Train: (LOSS: 0.0035, MAE: 0.0035, RMSE: 0.0056, R2: 0.9992), Valid: (LOSS: 0.0332, MAE: 0.0332, RMSE: 0.0490, R2: 0.9659), PNorm: 189.3147, GNorm: 0.5000
[261/299] timecost: 62.17, lr: 0.000023, Train: (LOSS: 0.0035, MAE: 0.0035, RMSE: 0.0056, R2: 0.9992), Valid: (LOSS: 0.0332, MAE: 0.0332, RMSE: 0.0490, R2: 0.9657), PNorm: 189.3159, GNorm: 0.5000
[262/299] timecost: 62.19, lr: 0.000023, Train: (LOSS: 0.0037, MAE: 0.0037, RMSE: 0.0058, R2: 0.9992), Valid: (LOSS: 0.0330, MAE: 0.0330, RMSE: 0.0489, R2: 0.9660), PNorm: 189.3167, GNorm: 0.4184
[263/299] timecost: 62.24, lr: 0.000023, Train: (LOSS: 0.0033, MAE: 0.0033, RMSE: 0.0055, R2: 0.9993), Valid: (LOSS: 0.0328, MAE: 0.0328, RMSE: 0.0490, R2: 0.9658), PNorm: 189.3177, GNorm: 0.4861
[264/299] timecost: 62.07, lr: 0.000023, Train: (LOSS: 0.0035, MAE: 0.0035, RMSE: 0.0055, R2: 0.9992), Valid: (LOSS: 0.0337, MAE: 0.0337, RMSE: 0.0499, R2: 0.9648), PNorm: 189.3184, GNorm: 0.5000
[265/299] timecost: 62.22, lr: 0.000023, Train: (LOSS: 0.0035, MAE: 0.0035, RMSE: 0.0057, R2: 0.9993), Valid: (LOSS: 0.0331, MAE: 0.0331, RMSE: 0.0496, R2: 0.9652), PNorm: 189.3201, GNorm: 0.4858
[266/299] timecost: 62.06, lr: 0.000023, Train: (LOSS: 0.0035, MAE: 0.0035, RMSE: 0.0057, R2: 0.9992), Valid: (LOSS: 0.0336, MAE: 0.0336, RMSE: 0.0495, R2: 0.9655), PNorm: 189.3203, GNorm: 0.4141
[267/299] timecost: 62.12, lr: 0.000023, Train: (LOSS: 0.0039, MAE: 0.0039, RMSE: 0.0062, R2: 0.9991), Valid: (LOSS: 0.0330, MAE: 0.0330, RMSE: 0.0488, R2: 0.9664), PNorm: 189.3217, GNorm: 0.4351
[268/299] timecost: 62.06, lr: 0.000023, Train: (LOSS: 0.0035, MAE: 0.0035, RMSE: 0.0056, R2: 0.9992), Valid: (LOSS: 0.0332, MAE: 0.0332, RMSE: 0.0493, R2: 0.9656), PNorm: 189.3229, GNorm: 0.4756
[269/299] timecost: 61.99, lr: 0.000023, Train: (LOSS: 0.0035, MAE: 0.0035, RMSE: 0.0056, R2: 0.9992), Valid: (LOSS: 0.0329, MAE: 0.0329, RMSE: 0.0492, R2: 0.9655), PNorm: 189.3241, GNorm: 0.4627
[270/299] timecost: 62.09, lr: 0.000023, Train: (LOSS: 0.0036, MAE: 0.0036, RMSE: 0.0058, R2: 0.9993), Valid: (LOSS: 0.0329, MAE: 0.0329, RMSE: 0.0489, R2: 0.9660), PNorm: 189.3245, GNorm: 0.3927
Epoch 00272: reducing learning rate of group 0 to 1.9687e-05.
[271/299] timecost: 62.19, lr: 0.000020, Train: (LOSS: 0.0036, MAE: 0.0036, RMSE: 0.0057, R2: 0.9992), Valid: (LOSS: 0.0327, MAE: 0.0327, RMSE: 0.0486, R2: 0.9664), PNorm: 189.3255, GNorm: 0.4388
[272/299] timecost: 62.23, lr: 0.000020, Train: (LOSS: 0.0032, MAE: 0.0032, RMSE: 0.0054, R2: 0.9992), Valid: (LOSS: 0.0329, MAE: 0.0329, RMSE: 0.0489, R2: 0.9661), PNorm: 189.3266, GNorm: 0.3209
[273/299] timecost: 62.15, lr: 0.000020, Train: (LOSS: 0.0032, MAE: 0.0032, RMSE: 0.0053, R2: 0.9993), Valid: (LOSS: 0.0328, MAE: 0.0328, RMSE: 0.0488, R2: 0.9662), PNorm: 189.3275, GNorm: 0.4722
[274/299] timecost: 62.14, lr: 0.000020, Train: (LOSS: 0.0031, MAE: 0.0031, RMSE: 0.0052, R2: 0.9993), Valid: (LOSS: 0.0331, MAE: 0.0331, RMSE: 0.0494, R2: 0.9654), PNorm: 189.3282, GNorm: 0.4396
[275/299] timecost: 62.04, lr: 0.000020, Train: (LOSS: 0.0033, MAE: 0.0033, RMSE: 0.0054, R2: 0.9993), Valid: (LOSS: 0.0330, MAE: 0.0330, RMSE: 0.0490, R2: 0.9660), PNorm: 189.3293, GNorm: 0.5000
[276/299] timecost: 62.21, lr: 0.000020, Train: (LOSS: 0.0031, MAE: 0.0031, RMSE: 0.0052, R2: 0.9993), Valid: (LOSS: 0.0328, MAE: 0.0328, RMSE: 0.0490, R2: 0.9660), PNorm: 189.3294, GNorm: 0.5000
[277/299] timecost: 62.05, lr: 0.000020, Train: (LOSS: 0.0032, MAE: 0.0032, RMSE: 0.0052, R2: 0.9993), Valid: (LOSS: 0.0329, MAE: 0.0329, RMSE: 0.0488, R2: 0.9662), PNorm: 189.3299, GNorm: 0.5000
[278/299] timecost: 62.18, lr: 0.000020, Train: (LOSS: 0.0031, MAE: 0.0031, RMSE: 0.0051, R2: 0.9994), Valid: (LOSS: 0.0332, MAE: 0.0332, RMSE: 0.0493, R2: 0.9656), PNorm: 189.3310, GNorm: 0.4313
[279/299] timecost: 62.11, lr: 0.000020, Train: (LOSS: 0.0032, MAE: 0.0032, RMSE: 0.0052, R2: 0.9993), Valid: (LOSS: 0.0329, MAE: 0.0329, RMSE: 0.0493, R2: 0.9655), PNorm: 189.3317, GNorm: 0.5000
[280/299] timecost: 62.09, lr: 0.000020, Train: (LOSS: 0.0030, MAE: 0.0030, RMSE: 0.0050, R2: 0.9993), Valid: (LOSS: 0.0329, MAE: 0.0329, RMSE: 0.0486, R2: 0.9663), PNorm: 189.3323, GNorm: 0.4597
[281/299] timecost: 62.24, lr: 0.000020, Train: (LOSS: 0.0031, MAE: 0.0031, RMSE: 0.0051, R2: 0.9992), Valid: (LOSS: 0.0329, MAE: 0.0329, RMSE: 0.0489, R2: 0.9661), PNorm: 189.3332, GNorm: 0.5000
[282/299] timecost: 62.12, lr: 0.000020, Train: (LOSS: 0.0030, MAE: 0.0030, RMSE: 0.0049, R2: 0.9994), Valid: (LOSS: 0.0326, MAE: 0.0326, RMSE: 0.0486, R2: 0.9665), PNorm: 189.3329, GNorm: 0.4776
[283/299] timecost: 62.07, lr: 0.000020, Train: (LOSS: 0.0030, MAE: 0.0030, RMSE: 0.0050, R2: 0.9994), Valid: (LOSS: 0.0332, MAE: 0.0332, RMSE: 0.0493, R2: 0.9657), PNorm: 189.3345, GNorm: 0.5000
[284/299] timecost: 62.11, lr: 0.000020, Train: (LOSS: 0.0030, MAE: 0.0030, RMSE: 0.0050, R2: 0.9993), Valid: (LOSS: 0.0330, MAE: 0.0330, RMSE: 0.0489, R2: 0.9662), PNorm: 189.3351, GNorm: 0.4892
[285/299] timecost: 62.09, lr: 0.000020, Train: (LOSS: 0.0031, MAE: 0.0031, RMSE: 0.0051, R2: 0.9993), Valid: (LOSS: 0.0330, MAE: 0.0330, RMSE: 0.0493, R2: 0.9656), PNorm: 189.3362, GNorm: 0.4651
[286/299] timecost: 62.21, lr: 0.000020, Train: (LOSS: 0.0030, MAE: 0.0030, RMSE: 0.0051, R2: 0.9994), Valid: (LOSS: 0.0330, MAE: 0.0330, RMSE: 0.0493, R2: 0.9655), PNorm: 189.3369, GNorm: 0.5000
Epoch 00288: reducing learning rate of group 0 to 1.6734e-05.
[287/299] timecost: 62.05, lr: 0.000017, Train: (LOSS: 0.0031, MAE: 0.0031, RMSE: 0.0050, R2: 0.9994), Valid: (LOSS: 0.0329, MAE: 0.0329, RMSE: 0.0494, R2: 0.9654), PNorm: 189.3383, GNorm: 0.5000
[288/299] timecost: 62.07, lr: 0.000017, Train: (LOSS: 0.0029, MAE: 0.0029, RMSE: 0.0049, R2: 0.9994), Valid: (LOSS: 0.0330, MAE: 0.0330, RMSE: 0.0492, R2: 0.9655), PNorm: 189.3386, GNorm: 0.5000
[289/299] timecost: 62.11, lr: 0.000017, Train: (LOSS: 0.0027, MAE: 0.0027, RMSE: 0.0046, R2: 0.9994), Valid: (LOSS: 0.0329, MAE: 0.0329, RMSE: 0.0490, R2: 0.9659), PNorm: 189.3392, GNorm: 0.3789
[290/299] timecost: 62.07, lr: 0.000017, Train: (LOSS: 0.0027, MAE: 0.0027, RMSE: 0.0045, R2: 0.9994), Valid: (LOSS: 0.0329, MAE: 0.0329, RMSE: 0.0491, R2: 0.9658), PNorm: 189.3394, GNorm: 0.4592
[291/299] timecost: 62.16, lr: 0.000017, Train: (LOSS: 0.0027, MAE: 0.0027, RMSE: 0.0046, R2: 0.9994), Valid: (LOSS: 0.0330, MAE: 0.0330, RMSE: 0.0491, R2: 0.9658), PNorm: 189.3402, GNorm: 0.5000
[292/299] timecost: 62.02, lr: 0.000017, Train: (LOSS: 0.0026, MAE: 0.0026, RMSE: 0.0045, R2: 0.9994), Valid: (LOSS: 0.0328, MAE: 0.0328, RMSE: 0.0488, R2: 0.9662), PNorm: 189.3407, GNorm: 0.5000
[293/299] timecost: 62.01, lr: 0.000017, Train: (LOSS: 0.0027, MAE: 0.0027, RMSE: 0.0045, R2: 0.9994), Valid: (LOSS: 0.0328, MAE: 0.0328, RMSE: 0.0492, R2: 0.9656), PNorm: 189.3409, GNorm: 0.5000
[294/299] timecost: 62.09, lr: 0.000017, Train: (LOSS: 0.0026, MAE: 0.0026, RMSE: 0.0045, R2: 0.9994), Valid: (LOSS: 0.0328, MAE: 0.0328, RMSE: 0.0490, R2: 0.9660), PNorm: 189.3415, GNorm: 0.4916
[295/299] timecost: 62.20, lr: 0.000017, Train: (LOSS: 0.0028, MAE: 0.0028, RMSE: 0.0048, R2: 0.9990), Valid: (LOSS: 0.0331, MAE: 0.0331, RMSE: 0.0495, R2: 0.9651), PNorm: 189.3423, GNorm: 0.4919
[296/299] timecost: 61.44, lr: 0.000017, Train: (LOSS: 0.0028, MAE: 0.0028, RMSE: 0.0048, R2: 0.9994), Valid: (LOSS: 0.0329, MAE: 0.0329, RMSE: 0.0493, R2: 0.9655), PNorm: 189.3426, GNorm: 0.4484
[297/299] timecost: 59.23, lr: 0.000017, Train: (LOSS: 0.0026, MAE: 0.0026, RMSE: 0.0045, R2: 0.9994), Valid: (LOSS: 0.0328, MAE: 0.0328, RMSE: 0.0489, R2: 0.9660), PNorm: 189.3434, GNorm: 0.3790
[298/299] timecost: 59.47, lr: 0.000017, Train: (LOSS: 0.0027, MAE: 0.0027, RMSE: 0.0046, R2: 0.9994), Valid: (LOSS: 0.0331, MAE: 0.0331, RMSE: 0.0492, R2: 0.9656), PNorm: 189.3440, GNorm: 0.5000
[299/299] timecost: 59.12, lr: 0.000017, Train: (LOSS: 0.0027, MAE: 0.0027, RMSE: 0.0046, R2: 0.9993), Valid: (LOSS: 0.0327, MAE: 0.0327, RMSE: 0.0491, R2: 0.9658), PNorm: 189.3446, GNorm: 0.4583
==========Training End==========
==========Test Best Model==========
================Final Results=======================
mse: 0.0328 +- 0.0000:
rmse: 0.0479 +- 0.0000:
mae: 0.0328 +- 0.0000:
r2: 0.9673 +- 0.0000:
tensor([[0.0000, 0.0000],
        [0.1154, 0.1475],
        [0.1177, 0.1828],
        ...,
        [0.0000, 0.0000],
        [0.4406, 0.4354],
        [0.0000, 0.0000]], device='cuda:0')
