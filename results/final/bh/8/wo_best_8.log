cuda available with GPU: Tesla V100-PCIE-16GB
==========Load Seed==========
set_random_seed
0
==========Training Start==========
Training Graphs:  2491
Valid Graphs:  277
Test Graphs:  1187
============Not pretrained weights used============
============Creating new layers============
============Creating Model============
Training Graphs Batches:  78
Valid Graphs Batches:  9
Test Graphs Batches:  37
[0/299] timecost: 60.85, lr: 0.000100, Train: (LOSS: 0.2260, MAE: 0.2260, RMSE: 0.2780, R2: -0.1557), Valid: (LOSS: 0.1680, MAE: 0.1680, RMSE: 0.2071, R2: 0.3567), PNorm: 187.0331, GNorm: 0.5000
[1/299] timecost: 58.39, lr: 0.000100, Train: (LOSS: 0.1714, MAE: 0.1714, RMSE: 0.2224, R2: 0.2926), Valid: (LOSS: 0.1448, MAE: 0.1448, RMSE: 0.1935, R2: 0.4375), PNorm: 187.0577, GNorm: 0.5000
[2/299] timecost: 58.86, lr: 0.000100, Train: (LOSS: 0.1610, MAE: 0.1610, RMSE: 0.2095, R2: 0.3685), Valid: (LOSS: 0.1416, MAE: 0.1416, RMSE: 0.1882, R2: 0.4640), PNorm: 187.0990, GNorm: 0.5000
[3/299] timecost: 58.51, lr: 0.000100, Train: (LOSS: 0.1482, MAE: 0.1482, RMSE: 0.1937, R2: 0.4408), Valid: (LOSS: 0.1630, MAE: 0.1630, RMSE: 0.2054, R2: 0.3532), PNorm: 187.1283, GNorm: 0.5000
[4/299] timecost: 58.38, lr: 0.000100, Train: (LOSS: 0.1380, MAE: 0.1380, RMSE: 0.1870, R2: 0.4846), Valid: (LOSS: 0.1204, MAE: 0.1204, RMSE: 0.1660, R2: 0.5754), PNorm: 187.1484, GNorm: 0.5000
[5/299] timecost: 58.71, lr: 0.000100, Train: (LOSS: 0.1299, MAE: 0.1299, RMSE: 0.1779, R2: 0.5257), Valid: (LOSS: 0.1239, MAE: 0.1239, RMSE: 0.1714, R2: 0.5525), PNorm: 187.1686, GNorm: 0.2401
[6/299] timecost: 58.53, lr: 0.000100, Train: (LOSS: 0.1273, MAE: 0.1273, RMSE: 0.1765, R2: 0.5395), Valid: (LOSS: 0.1309, MAE: 0.1309, RMSE: 0.1826, R2: 0.4922), PNorm: 187.1823, GNorm: 0.5000
[7/299] timecost: 58.26, lr: 0.000100, Train: (LOSS: 0.1244, MAE: 0.1244, RMSE: 0.1741, R2: 0.5411), Valid: (LOSS: 0.1260, MAE: 0.1260, RMSE: 0.1687, R2: 0.5722), PNorm: 187.2010, GNorm: 0.5000
[8/299] timecost: 58.55, lr: 0.000100, Train: (LOSS: 0.1241, MAE: 0.1241, RMSE: 0.1707, R2: 0.5728), Valid: (LOSS: 0.1293, MAE: 0.1293, RMSE: 0.1765, R2: 0.5248), PNorm: 187.2275, GNorm: 0.3448
[9/299] timecost: 61.80, lr: 0.000100, Train: (LOSS: 0.1229, MAE: 0.1229, RMSE: 0.1719, R2: 0.5573), Valid: (LOSS: 0.1182, MAE: 0.1182, RMSE: 0.1585, R2: 0.6167), PNorm: 187.2517, GNorm: 0.5000
[10/299] timecost: 62.49, lr: 0.000100, Train: (LOSS: 0.1187, MAE: 0.1187, RMSE: 0.1670, R2: 0.5749), Valid: (LOSS: 0.1057, MAE: 0.1057, RMSE: 0.1489, R2: 0.6668), PNorm: 187.2847, GNorm: 0.5000
[11/299] timecost: 59.26, lr: 0.000100, Train: (LOSS: 0.1028, MAE: 0.1028, RMSE: 0.1479, R2: 0.6673), Valid: (LOSS: 0.0972, MAE: 0.0972, RMSE: 0.1386, R2: 0.7075), PNorm: 187.3298, GNorm: 0.5000
[12/299] timecost: 62.60, lr: 0.000100, Train: (LOSS: 0.0970, MAE: 0.0970, RMSE: 0.1403, R2: 0.6943), Valid: (LOSS: 0.0952, MAE: 0.0952, RMSE: 0.1317, R2: 0.7375), PNorm: 187.3581, GNorm: 0.5000
[13/299] timecost: 62.54, lr: 0.000100, Train: (LOSS: 0.0943, MAE: 0.0943, RMSE: 0.1380, R2: 0.7082), Valid: (LOSS: 0.0850, MAE: 0.0850, RMSE: 0.1203, R2: 0.7759), PNorm: 187.3878, GNorm: 0.5000
[14/299] timecost: 62.72, lr: 0.000100, Train: (LOSS: 0.0907, MAE: 0.0907, RMSE: 0.1333, R2: 0.7227), Valid: (LOSS: 0.0855, MAE: 0.0855, RMSE: 0.1173, R2: 0.7881), PNorm: 187.4163, GNorm: 0.5000
[15/299] timecost: 62.43, lr: 0.000100, Train: (LOSS: 0.0848, MAE: 0.0848, RMSE: 0.1278, R2: 0.7418), Valid: (LOSS: 0.0770, MAE: 0.0770, RMSE: 0.1141, R2: 0.8003), PNorm: 187.4337, GNorm: 0.3348
[16/299] timecost: 62.48, lr: 0.000100, Train: (LOSS: 0.0819, MAE: 0.0819, RMSE: 0.1264, R2: 0.7558), Valid: (LOSS: 0.0792, MAE: 0.0792, RMSE: 0.1131, R2: 0.8015), PNorm: 187.4503, GNorm: 0.5000
[17/299] timecost: 62.48, lr: 0.000100, Train: (LOSS: 0.0806, MAE: 0.0806, RMSE: 0.1228, R2: 0.7657), Valid: (LOSS: 0.0814, MAE: 0.0814, RMSE: 0.1140, R2: 0.7998), PNorm: 187.4697, GNorm: 0.5000
[18/299] timecost: 62.44, lr: 0.000100, Train: (LOSS: 0.0789, MAE: 0.0789, RMSE: 0.1215, R2: 0.7779), Valid: (LOSS: 0.0754, MAE: 0.0754, RMSE: 0.1100, R2: 0.8143), PNorm: 187.4866, GNorm: 0.5000
[19/299] timecost: 62.46, lr: 0.000100, Train: (LOSS: 0.0774, MAE: 0.0774, RMSE: 0.1184, R2: 0.7755), Valid: (LOSS: 0.0766, MAE: 0.0766, RMSE: 0.1138, R2: 0.8021), PNorm: 187.5048, GNorm: 0.5000
[20/299] timecost: 62.47, lr: 0.000100, Train: (LOSS: 0.0754, MAE: 0.0754, RMSE: 0.1172, R2: 0.7891), Valid: (LOSS: 0.0690, MAE: 0.0690, RMSE: 0.1045, R2: 0.8283), PNorm: 187.5240, GNorm: 0.5000
[21/299] timecost: 62.47, lr: 0.000100, Train: (LOSS: 0.0736, MAE: 0.0736, RMSE: 0.1134, R2: 0.7994), Valid: (LOSS: 0.0666, MAE: 0.0666, RMSE: 0.0985, R2: 0.8466), PNorm: 187.5476, GNorm: 0.4759
[22/299] timecost: 62.61, lr: 0.000100, Train: (LOSS: 0.0703, MAE: 0.0703, RMSE: 0.1108, R2: 0.8109), Valid: (LOSS: 0.0706, MAE: 0.0706, RMSE: 0.1050, R2: 0.8279), PNorm: 187.5657, GNorm: 0.5000
[23/299] timecost: 62.47, lr: 0.000100, Train: (LOSS: 0.0658, MAE: 0.0658, RMSE: 0.1031, R2: 0.8335), Valid: (LOSS: 0.0635, MAE: 0.0635, RMSE: 0.0973, R2: 0.8563), PNorm: 187.5882, GNorm: 0.5000
[24/299] timecost: 62.41, lr: 0.000100, Train: (LOSS: 0.0602, MAE: 0.0602, RMSE: 0.0944, R2: 0.8607), Valid: (LOSS: 0.0730, MAE: 0.0730, RMSE: 0.1092, R2: 0.8166), PNorm: 187.6071, GNorm: 0.5000
[25/299] timecost: 62.53, lr: 0.000100, Train: (LOSS: 0.0603, MAE: 0.0603, RMSE: 0.0952, R2: 0.8550), Valid: (LOSS: 0.0626, MAE: 0.0626, RMSE: 0.0959, R2: 0.8521), PNorm: 187.6275, GNorm: 0.4733
[26/299] timecost: 62.46, lr: 0.000100, Train: (LOSS: 0.0568, MAE: 0.0568, RMSE: 0.0904, R2: 0.8713), Valid: (LOSS: 0.0610, MAE: 0.0610, RMSE: 0.0940, R2: 0.8606), PNorm: 187.6500, GNorm: 0.5000
[27/299] timecost: 62.59, lr: 0.000100, Train: (LOSS: 0.0545, MAE: 0.0545, RMSE: 0.0871, R2: 0.8785), Valid: (LOSS: 0.0527, MAE: 0.0527, RMSE: 0.0815, R2: 0.8948), PNorm: 187.6657, GNorm: 0.4420
[28/299] timecost: 62.54, lr: 0.000100, Train: (LOSS: 0.0545, MAE: 0.0545, RMSE: 0.0882, R2: 0.8737), Valid: (LOSS: 0.0539, MAE: 0.0539, RMSE: 0.0818, R2: 0.8935), PNorm: 187.6864, GNorm: 0.5000
[29/299] timecost: 59.73, lr: 0.000100, Train: (LOSS: 0.0507, MAE: 0.0507, RMSE: 0.0825, R2: 0.8912), Valid: (LOSS: 0.0546, MAE: 0.0546, RMSE: 0.0855, R2: 0.8805), PNorm: 187.7013, GNorm: 0.5000
[30/299] timecost: 60.55, lr: 0.000100, Train: (LOSS: 0.0511, MAE: 0.0511, RMSE: 0.0833, R2: 0.8894), Valid: (LOSS: 0.0492, MAE: 0.0492, RMSE: 0.0800, R2: 0.8958), PNorm: 187.7169, GNorm: 0.4926
[31/299] timecost: 60.08, lr: 0.000100, Train: (LOSS: 0.0493, MAE: 0.0493, RMSE: 0.0807, R2: 0.8829), Valid: (LOSS: 0.0492, MAE: 0.0492, RMSE: 0.0808, R2: 0.8897), PNorm: 187.7349, GNorm: 0.4592
[32/299] timecost: 59.71, lr: 0.000100, Train: (LOSS: 0.0468, MAE: 0.0468, RMSE: 0.0747, R2: 0.9005), Valid: (LOSS: 0.0510, MAE: 0.0510, RMSE: 0.0836, R2: 0.8828), PNorm: 187.7462, GNorm: 0.4656
[33/299] timecost: 62.55, lr: 0.000100, Train: (LOSS: 0.0455, MAE: 0.0455, RMSE: 0.0752, R2: 0.9073), Valid: (LOSS: 0.0500, MAE: 0.0500, RMSE: 0.0836, R2: 0.8828), PNorm: 187.7635, GNorm: 0.4692
[34/299] timecost: 62.44, lr: 0.000100, Train: (LOSS: 0.0435, MAE: 0.0435, RMSE: 0.0724, R2: 0.9079), Valid: (LOSS: 0.0483, MAE: 0.0483, RMSE: 0.0762, R2: 0.8982), PNorm: 187.7766, GNorm: 0.5000
[35/299] timecost: 62.41, lr: 0.000100, Train: (LOSS: 0.0440, MAE: 0.0440, RMSE: 0.0738, R2: 0.9069), Valid: (LOSS: 0.0479, MAE: 0.0479, RMSE: 0.0787, R2: 0.8895), PNorm: 187.7905, GNorm: 0.5000
[36/299] timecost: 62.41, lr: 0.000100, Train: (LOSS: 0.0426, MAE: 0.0426, RMSE: 0.0703, R2: 0.9109), Valid: (LOSS: 0.0470, MAE: 0.0470, RMSE: 0.0746, R2: 0.9074), PNorm: 187.8040, GNorm: 0.4281
[37/299] timecost: 62.47, lr: 0.000100, Train: (LOSS: 0.0424, MAE: 0.0424, RMSE: 0.0702, R2: 0.9213), Valid: (LOSS: 0.0528, MAE: 0.0528, RMSE: 0.0831, R2: 0.8908), PNorm: 187.8199, GNorm: 0.5000
[38/299] timecost: 62.66, lr: 0.000100, Train: (LOSS: 0.0404, MAE: 0.0404, RMSE: 0.0674, R2: 0.9205), Valid: (LOSS: 0.0470, MAE: 0.0470, RMSE: 0.0768, R2: 0.9046), PNorm: 187.8350, GNorm: 0.5000
[39/299] timecost: 62.52, lr: 0.000100, Train: (LOSS: 0.0401, MAE: 0.0401, RMSE: 0.0656, R2: 0.9263), Valid: (LOSS: 0.0452, MAE: 0.0452, RMSE: 0.0729, R2: 0.9082), PNorm: 187.8488, GNorm: 0.5000
[40/299] timecost: 62.55, lr: 0.000100, Train: (LOSS: 0.0387, MAE: 0.0387, RMSE: 0.0622, R2: 0.9340), Valid: (LOSS: 0.0462, MAE: 0.0462, RMSE: 0.0697, R2: 0.9218), PNorm: 187.8641, GNorm: 0.5000
[41/299] timecost: 62.65, lr: 0.000100, Train: (LOSS: 0.0374, MAE: 0.0374, RMSE: 0.0602, R2: 0.9392), Valid: (LOSS: 0.0449, MAE: 0.0449, RMSE: 0.0702, R2: 0.9195), PNorm: 187.8786, GNorm: 0.5000
[42/299] timecost: 62.53, lr: 0.000100, Train: (LOSS: 0.0383, MAE: 0.0383, RMSE: 0.0607, R2: 0.9393), Valid: (LOSS: 0.0408, MAE: 0.0408, RMSE: 0.0643, R2: 0.9353), PNorm: 187.8926, GNorm: 0.5000
[43/299] timecost: 62.48, lr: 0.000100, Train: (LOSS: 0.0366, MAE: 0.0366, RMSE: 0.0580, R2: 0.9443), Valid: (LOSS: 0.0423, MAE: 0.0423, RMSE: 0.0675, R2: 0.9297), PNorm: 187.9070, GNorm: 0.5000
[44/299] timecost: 62.61, lr: 0.000100, Train: (LOSS: 0.0353, MAE: 0.0353, RMSE: 0.0552, R2: 0.9496), Valid: (LOSS: 0.0446, MAE: 0.0446, RMSE: 0.0684, R2: 0.9244), PNorm: 187.9212, GNorm: 0.5000
[45/299] timecost: 62.68, lr: 0.000100, Train: (LOSS: 0.0352, MAE: 0.0352, RMSE: 0.0560, R2: 0.9482), Valid: (LOSS: 0.0440, MAE: 0.0440, RMSE: 0.0684, R2: 0.9277), PNorm: 187.9342, GNorm: 0.5000
[46/299] timecost: 62.64, lr: 0.000100, Train: (LOSS: 0.0347, MAE: 0.0347, RMSE: 0.0557, R2: 0.9501), Valid: (LOSS: 0.0434, MAE: 0.0434, RMSE: 0.0671, R2: 0.9317), PNorm: 187.9488, GNorm: 0.5000
[47/299] timecost: 62.46, lr: 0.000100, Train: (LOSS: 0.0333, MAE: 0.0333, RMSE: 0.0538, R2: 0.9513), Valid: (LOSS: 0.0396, MAE: 0.0396, RMSE: 0.0612, R2: 0.9422), PNorm: 187.9602, GNorm: 0.5000
[48/299] timecost: 62.62, lr: 0.000100, Train: (LOSS: 0.0324, MAE: 0.0324, RMSE: 0.0507, R2: 0.9561), Valid: (LOSS: 0.0371, MAE: 0.0371, RMSE: 0.0561, R2: 0.9508), PNorm: 187.9745, GNorm: 0.4595
[49/299] timecost: 62.57, lr: 0.000100, Train: (LOSS: 0.0324, MAE: 0.0324, RMSE: 0.0509, R2: 0.9563), Valid: (LOSS: 0.0392, MAE: 0.0392, RMSE: 0.0628, R2: 0.9395), PNorm: 187.9861, GNorm: 0.5000
[50/299] timecost: 62.55, lr: 0.000100, Train: (LOSS: 0.0306, MAE: 0.0306, RMSE: 0.0482, R2: 0.9602), Valid: (LOSS: 0.0421, MAE: 0.0421, RMSE: 0.0630, R2: 0.9393), PNorm: 187.9991, GNorm: 0.5000
[51/299] timecost: 62.28, lr: 0.000100, Train: (LOSS: 0.0320, MAE: 0.0320, RMSE: 0.0500, R2: 0.9587), Valid: (LOSS: 0.0410, MAE: 0.0410, RMSE: 0.0629, R2: 0.9398), PNorm: 188.0139, GNorm: 0.4602
[52/299] timecost: 59.49, lr: 0.000100, Train: (LOSS: 0.0314, MAE: 0.0314, RMSE: 0.0494, R2: 0.9608), Valid: (LOSS: 0.0395, MAE: 0.0395, RMSE: 0.0616, R2: 0.9413), PNorm: 188.0272, GNorm: 0.3747
[53/299] timecost: 59.34, lr: 0.000100, Train: (LOSS: 0.0300, MAE: 0.0300, RMSE: 0.0475, R2: 0.9631), Valid: (LOSS: 0.0409, MAE: 0.0409, RMSE: 0.0641, R2: 0.9358), PNorm: 188.0398, GNorm: 0.3858
[54/299] timecost: 58.46, lr: 0.000100, Train: (LOSS: 0.0292, MAE: 0.0292, RMSE: 0.0461, R2: 0.9649), Valid: (LOSS: 0.0392, MAE: 0.0392, RMSE: 0.0612, R2: 0.9410), PNorm: 188.0532, GNorm: 0.3894
[55/299] timecost: 58.64, lr: 0.000100, Train: (LOSS: 0.0290, MAE: 0.0290, RMSE: 0.0466, R2: 0.9616), Valid: (LOSS: 0.0408, MAE: 0.0408, RMSE: 0.0612, R2: 0.9434), PNorm: 188.0661, GNorm: 0.4770
[56/299] timecost: 58.54, lr: 0.000100, Train: (LOSS: 0.0281, MAE: 0.0281, RMSE: 0.0450, R2: 0.9654), Valid: (LOSS: 0.0383, MAE: 0.0383, RMSE: 0.0585, R2: 0.9481), PNorm: 188.0783, GNorm: 0.5000
[57/299] timecost: 58.49, lr: 0.000100, Train: (LOSS: 0.0281, MAE: 0.0281, RMSE: 0.0446, R2: 0.9660), Valid: (LOSS: 0.0374, MAE: 0.0374, RMSE: 0.0600, R2: 0.9445), PNorm: 188.0897, GNorm: 0.5000
[58/299] timecost: 58.77, lr: 0.000100, Train: (LOSS: 0.0279, MAE: 0.0279, RMSE: 0.0443, R2: 0.9663), Valid: (LOSS: 0.0414, MAE: 0.0414, RMSE: 0.0648, R2: 0.9358), PNorm: 188.1032, GNorm: 0.5000
[59/299] timecost: 58.50, lr: 0.000100, Train: (LOSS: 0.0280, MAE: 0.0280, RMSE: 0.0450, R2: 0.9671), Valid: (LOSS: 0.0369, MAE: 0.0369, RMSE: 0.0553, R2: 0.9543), PNorm: 188.1154, GNorm: 0.5000
[60/299] timecost: 58.47, lr: 0.000100, Train: (LOSS: 0.0268, MAE: 0.0268, RMSE: 0.0432, R2: 0.9690), Valid: (LOSS: 0.0368, MAE: 0.0368, RMSE: 0.0591, R2: 0.9461), PNorm: 188.1292, GNorm: 0.5000
[61/299] timecost: 59.93, lr: 0.000100, Train: (LOSS: 0.0265, MAE: 0.0265, RMSE: 0.0422, R2: 0.9692), Valid: (LOSS: 0.0391, MAE: 0.0391, RMSE: 0.0601, R2: 0.9452), PNorm: 188.1406, GNorm: 0.5000
[62/299] timecost: 60.82, lr: 0.000100, Train: (LOSS: 0.0269, MAE: 0.0269, RMSE: 0.0429, R2: 0.9698), Valid: (LOSS: 0.0376, MAE: 0.0376, RMSE: 0.0597, R2: 0.9459), PNorm: 188.1530, GNorm: 0.5000
[63/299] timecost: 62.48, lr: 0.000100, Train: (LOSS: 0.0259, MAE: 0.0259, RMSE: 0.0414, R2: 0.9709), Valid: (LOSS: 0.0408, MAE: 0.0408, RMSE: 0.0643, R2: 0.9361), PNorm: 188.1649, GNorm: 0.4682
[64/299] timecost: 62.51, lr: 0.000100, Train: (LOSS: 0.0268, MAE: 0.0268, RMSE: 0.0426, R2: 0.9690), Valid: (LOSS: 0.0365, MAE: 0.0365, RMSE: 0.0573, R2: 0.9503), PNorm: 188.1768, GNorm: 0.5000
[65/299] timecost: 62.42, lr: 0.000100, Train: (LOSS: 0.0258, MAE: 0.0258, RMSE: 0.0406, R2: 0.9716), Valid: (LOSS: 0.0396, MAE: 0.0396, RMSE: 0.0623, R2: 0.9407), PNorm: 188.1905, GNorm: 0.4716
[66/299] timecost: 62.50, lr: 0.000100, Train: (LOSS: 0.0244, MAE: 0.0244, RMSE: 0.0392, R2: 0.9733), Valid: (LOSS: 0.0391, MAE: 0.0391, RMSE: 0.0642, R2: 0.9349), PNorm: 188.2006, GNorm: 0.4167
[67/299] timecost: 62.42, lr: 0.000100, Train: (LOSS: 0.0246, MAE: 0.0246, RMSE: 0.0390, R2: 0.9744), Valid: (LOSS: 0.0402, MAE: 0.0402, RMSE: 0.0622, R2: 0.9411), PNorm: 188.2126, GNorm: 0.5000
[68/299] timecost: 62.49, lr: 0.000100, Train: (LOSS: 0.0248, MAE: 0.0248, RMSE: 0.0389, R2: 0.9746), Valid: (LOSS: 0.0392, MAE: 0.0392, RMSE: 0.0608, R2: 0.9427), PNorm: 188.2238, GNorm: 0.5000
[69/299] timecost: 62.76, lr: 0.000100, Train: (LOSS: 0.0242, MAE: 0.0242, RMSE: 0.0383, R2: 0.9752), Valid: (LOSS: 0.0356, MAE: 0.0356, RMSE: 0.0562, R2: 0.9515), PNorm: 188.2388, GNorm: 0.5000
[70/299] timecost: 62.56, lr: 0.000100, Train: (LOSS: 0.0245, MAE: 0.0245, RMSE: 0.0394, R2: 0.9746), Valid: (LOSS: 0.0420, MAE: 0.0420, RMSE: 0.0628, R2: 0.9406), PNorm: 188.2502, GNorm: 0.5000
[71/299] timecost: 62.64, lr: 0.000100, Train: (LOSS: 0.0239, MAE: 0.0239, RMSE: 0.0372, R2: 0.9770), Valid: (LOSS: 0.0359, MAE: 0.0359, RMSE: 0.0539, R2: 0.9542), PNorm: 188.2607, GNorm: 0.5000
[72/299] timecost: 62.54, lr: 0.000100, Train: (LOSS: 0.0240, MAE: 0.0240, RMSE: 0.0377, R2: 0.9764), Valid: (LOSS: 0.0399, MAE: 0.0399, RMSE: 0.0621, R2: 0.9399), PNorm: 188.2722, GNorm: 0.5000
[73/299] timecost: 62.62, lr: 0.000100, Train: (LOSS: 0.0232, MAE: 0.0232, RMSE: 0.0371, R2: 0.9763), Valid: (LOSS: 0.0377, MAE: 0.0377, RMSE: 0.0592, R2: 0.9469), PNorm: 188.2875, GNorm: 0.5000
[74/299] timecost: 62.63, lr: 0.000100, Train: (LOSS: 0.0228, MAE: 0.0228, RMSE: 0.0359, R2: 0.9782), Valid: (LOSS: 0.0391, MAE: 0.0391, RMSE: 0.0595, R2: 0.9466), PNorm: 188.2985, GNorm: 0.5000
[75/299] timecost: 62.54, lr: 0.000100, Train: (LOSS: 0.0221, MAE: 0.0221, RMSE: 0.0354, R2: 0.9784), Valid: (LOSS: 0.0374, MAE: 0.0374, RMSE: 0.0598, R2: 0.9439), PNorm: 188.3097, GNorm: 0.5000
[76/299] timecost: 62.58, lr: 0.000100, Train: (LOSS: 0.0224, MAE: 0.0224, RMSE: 0.0358, R2: 0.9779), Valid: (LOSS: 0.0368, MAE: 0.0368, RMSE: 0.0580, R2: 0.9495), PNorm: 188.3200, GNorm: 0.5000
[77/299] timecost: 62.66, lr: 0.000100, Train: (LOSS: 0.0226, MAE: 0.0226, RMSE: 0.0358, R2: 0.9778), Valid: (LOSS: 0.0352, MAE: 0.0352, RMSE: 0.0540, R2: 0.9556), PNorm: 188.3341, GNorm: 0.5000
[78/299] timecost: 62.53, lr: 0.000100, Train: (LOSS: 0.0227, MAE: 0.0227, RMSE: 0.0373, R2: 0.9755), Valid: (LOSS: 0.0377, MAE: 0.0377, RMSE: 0.0602, R2: 0.9435), PNorm: 188.3447, GNorm: 0.2593
[79/299] timecost: 62.58, lr: 0.000100, Train: (LOSS: 0.0226, MAE: 0.0226, RMSE: 0.0361, R2: 0.9762), Valid: (LOSS: 0.0370, MAE: 0.0370, RMSE: 0.0585, R2: 0.9464), PNorm: 188.3567, GNorm: 0.3876
[80/299] timecost: 61.53, lr: 0.000100, Train: (LOSS: 0.0214, MAE: 0.0214, RMSE: 0.0341, R2: 0.9797), Valid: (LOSS: 0.0370, MAE: 0.0370, RMSE: 0.0544, R2: 0.9540), PNorm: 188.3683, GNorm: 0.4746
[81/299] timecost: 59.90, lr: 0.000100, Train: (LOSS: 0.0223, MAE: 0.0223, RMSE: 0.0352, R2: 0.9789), Valid: (LOSS: 0.0386, MAE: 0.0386, RMSE: 0.0624, R2: 0.9386), PNorm: 188.3816, GNorm: 0.5000
[82/299] timecost: 59.65, lr: 0.000100, Train: (LOSS: 0.0207, MAE: 0.0207, RMSE: 0.0329, R2: 0.9810), Valid: (LOSS: 0.0367, MAE: 0.0367, RMSE: 0.0572, R2: 0.9488), PNorm: 188.3939, GNorm: 0.3660
[83/299] timecost: 59.94, lr: 0.000100, Train: (LOSS: 0.0218, MAE: 0.0218, RMSE: 0.0351, R2: 0.9779), Valid: (LOSS: 0.0373, MAE: 0.0373, RMSE: 0.0595, R2: 0.9452), PNorm: 188.4087, GNorm: 0.3569
[84/299] timecost: 62.33, lr: 0.000100, Train: (LOSS: 0.0199, MAE: 0.0199, RMSE: 0.0323, R2: 0.9808), Valid: (LOSS: 0.0365, MAE: 0.0365, RMSE: 0.0569, R2: 0.9499), PNorm: 188.4168, GNorm: 0.4559
[85/299] timecost: 62.54, lr: 0.000100, Train: (LOSS: 0.0204, MAE: 0.0204, RMSE: 0.0331, R2: 0.9807), Valid: (LOSS: 0.0345, MAE: 0.0345, RMSE: 0.0525, R2: 0.9584), PNorm: 188.4273, GNorm: 0.4631
[86/299] timecost: 62.47, lr: 0.000100, Train: (LOSS: 0.0206, MAE: 0.0206, RMSE: 0.0333, R2: 0.9817), Valid: (LOSS: 0.0364, MAE: 0.0364, RMSE: 0.0545, R2: 0.9544), PNorm: 188.4405, GNorm: 0.5000
[87/299] timecost: 62.38, lr: 0.000100, Train: (LOSS: 0.0210, MAE: 0.0210, RMSE: 0.0337, R2: 0.9801), Valid: (LOSS: 0.0347, MAE: 0.0347, RMSE: 0.0550, R2: 0.9527), PNorm: 188.4495, GNorm: 0.4618
[88/299] timecost: 62.42, lr: 0.000100, Train: (LOSS: 0.0201, MAE: 0.0201, RMSE: 0.0325, R2: 0.9803), Valid: (LOSS: 0.0367, MAE: 0.0367, RMSE: 0.0573, R2: 0.9509), PNorm: 188.4622, GNorm: 0.4118
[89/299] timecost: 62.38, lr: 0.000100, Train: (LOSS: 0.0198, MAE: 0.0198, RMSE: 0.0321, R2: 0.9818), Valid: (LOSS: 0.0362, MAE: 0.0362, RMSE: 0.0572, R2: 0.9509), PNorm: 188.4735, GNorm: 0.5000
[90/299] timecost: 60.52, lr: 0.000100, Train: (LOSS: 0.0198, MAE: 0.0198, RMSE: 0.0321, R2: 0.9813), Valid: (LOSS: 0.0383, MAE: 0.0383, RMSE: 0.0602, R2: 0.9445), PNorm: 188.4829, GNorm: 0.3179
[91/299] timecost: 59.71, lr: 0.000100, Train: (LOSS: 0.0195, MAE: 0.0195, RMSE: 0.0316, R2: 0.9822), Valid: (LOSS: 0.0341, MAE: 0.0341, RMSE: 0.0540, R2: 0.9561), PNorm: 188.4949, GNorm: 0.5000
[92/299] timecost: 59.45, lr: 0.000100, Train: (LOSS: 0.0198, MAE: 0.0198, RMSE: 0.0316, R2: 0.9808), Valid: (LOSS: 0.0361, MAE: 0.0361, RMSE: 0.0601, R2: 0.9429), PNorm: 188.5063, GNorm: 0.4518
[93/299] timecost: 59.13, lr: 0.000100, Train: (LOSS: 0.0191, MAE: 0.0191, RMSE: 0.0306, R2: 0.9820), Valid: (LOSS: 0.0356, MAE: 0.0356, RMSE: 0.0555, R2: 0.9540), PNorm: 188.5181, GNorm: 0.4589
[94/299] timecost: 59.21, lr: 0.000100, Train: (LOSS: 0.0190, MAE: 0.0190, RMSE: 0.0307, R2: 0.9832), Valid: (LOSS: 0.0377, MAE: 0.0377, RMSE: 0.0589, R2: 0.9465), PNorm: 188.5285, GNorm: 0.4305
[95/299] timecost: 58.84, lr: 0.000100, Train: (LOSS: 0.0186, MAE: 0.0186, RMSE: 0.0294, R2: 0.9841), Valid: (LOSS: 0.0362, MAE: 0.0362, RMSE: 0.0559, R2: 0.9522), PNorm: 188.5396, GNorm: 0.5000
[96/299] timecost: 60.84, lr: 0.000100, Train: (LOSS: 0.0186, MAE: 0.0186, RMSE: 0.0298, R2: 0.9834), Valid: (LOSS: 0.0392, MAE: 0.0392, RMSE: 0.0593, R2: 0.9467), PNorm: 188.5483, GNorm: 0.5000
[97/299] timecost: 62.23, lr: 0.000100, Train: (LOSS: 0.0195, MAE: 0.0195, RMSE: 0.0310, R2: 0.9819), Valid: (LOSS: 0.0353, MAE: 0.0353, RMSE: 0.0544, R2: 0.9547), PNorm: 188.5635, GNorm: 0.5000
[98/299] timecost: 62.10, lr: 0.000100, Train: (LOSS: 0.0184, MAE: 0.0184, RMSE: 0.0298, R2: 0.9839), Valid: (LOSS: 0.0356, MAE: 0.0356, RMSE: 0.0551, R2: 0.9527), PNorm: 188.5749, GNorm: 0.3825
[99/299] timecost: 62.05, lr: 0.000100, Train: (LOSS: 0.0183, MAE: 0.0183, RMSE: 0.0302, R2: 0.9836), Valid: (LOSS: 0.0335, MAE: 0.0335, RMSE: 0.0517, R2: 0.9590), PNorm: 188.5857, GNorm: 0.4326
[100/299] timecost: 62.30, lr: 0.000100, Train: (LOSS: 0.0182, MAE: 0.0182, RMSE: 0.0296, R2: 0.9823), Valid: (LOSS: 0.0377, MAE: 0.0377, RMSE: 0.0610, R2: 0.9422), PNorm: 188.5988, GNorm: 0.4710
[101/299] timecost: 62.13, lr: 0.000100, Train: (LOSS: 0.0188, MAE: 0.0188, RMSE: 0.0298, R2: 0.9832), Valid: (LOSS: 0.0344, MAE: 0.0344, RMSE: 0.0531, R2: 0.9563), PNorm: 188.6083, GNorm: 0.4064
[102/299] timecost: 62.15, lr: 0.000100, Train: (LOSS: 0.0173, MAE: 0.0173, RMSE: 0.0281, R2: 0.9852), Valid: (LOSS: 0.0379, MAE: 0.0379, RMSE: 0.0571, R2: 0.9511), PNorm: 188.6207, GNorm: 0.3840
[103/299] timecost: 60.38, lr: 0.000100, Train: (LOSS: 0.0182, MAE: 0.0182, RMSE: 0.0294, R2: 0.9838), Valid: (LOSS: 0.0366, MAE: 0.0366, RMSE: 0.0572, R2: 0.9471), PNorm: 188.6326, GNorm: 0.5000
[104/299] timecost: 59.87, lr: 0.000100, Train: (LOSS: 0.0186, MAE: 0.0186, RMSE: 0.0309, R2: 0.9832), Valid: (LOSS: 0.0367, MAE: 0.0367, RMSE: 0.0547, R2: 0.9539), PNorm: 188.6475, GNorm: 0.4476
[105/299] timecost: 59.73, lr: 0.000100, Train: (LOSS: 0.0175, MAE: 0.0175, RMSE: 0.0288, R2: 0.9842), Valid: (LOSS: 0.0349, MAE: 0.0349, RMSE: 0.0552, R2: 0.9539), PNorm: 188.6601, GNorm: 0.3797
[106/299] timecost: 58.78, lr: 0.000100, Train: (LOSS: 0.0181, MAE: 0.0181, RMSE: 0.0297, R2: 0.9842), Valid: (LOSS: 0.0350, MAE: 0.0350, RMSE: 0.0543, R2: 0.9555), PNorm: 188.6720, GNorm: 0.5000
[107/299] timecost: 58.67, lr: 0.000100, Train: (LOSS: 0.0185, MAE: 0.0185, RMSE: 0.0296, R2: 0.9832), Valid: (LOSS: 0.0349, MAE: 0.0349, RMSE: 0.0564, R2: 0.9517), PNorm: 188.6855, GNorm: 0.4576
[108/299] timecost: 59.30, lr: 0.000100, Train: (LOSS: 0.0175, MAE: 0.0175, RMSE: 0.0284, R2: 0.9852), Valid: (LOSS: 0.0324, MAE: 0.0324, RMSE: 0.0503, R2: 0.9617), PNorm: 188.6962, GNorm: 0.5000
[109/299] timecost: 62.04, lr: 0.000100, Train: (LOSS: 0.0175, MAE: 0.0175, RMSE: 0.0286, R2: 0.9860), Valid: (LOSS: 0.0366, MAE: 0.0366, RMSE: 0.0555, R2: 0.9521), PNorm: 188.7084, GNorm: 0.5000
[110/299] timecost: 61.95, lr: 0.000100, Train: (LOSS: 0.0180, MAE: 0.0180, RMSE: 0.0303, R2: 0.9830), Valid: (LOSS: 0.0367, MAE: 0.0367, RMSE: 0.0550, R2: 0.9543), PNorm: 188.7219, GNorm: 0.4585
[111/299] timecost: 62.19, lr: 0.000100, Train: (LOSS: 0.0174, MAE: 0.0174, RMSE: 0.0288, R2: 0.9838), Valid: (LOSS: 0.0354, MAE: 0.0354, RMSE: 0.0553, R2: 0.9528), PNorm: 188.7328, GNorm: 0.3954
[112/299] timecost: 62.10, lr: 0.000100, Train: (LOSS: 0.0172, MAE: 0.0172, RMSE: 0.0285, R2: 0.9861), Valid: (LOSS: 0.0362, MAE: 0.0362, RMSE: 0.0551, R2: 0.9546), PNorm: 188.7431, GNorm: 0.5000
[113/299] timecost: 62.06, lr: 0.000100, Train: (LOSS: 0.0171, MAE: 0.0171, RMSE: 0.0281, R2: 0.9857), Valid: (LOSS: 0.0344, MAE: 0.0344, RMSE: 0.0536, R2: 0.9554), PNorm: 188.7564, GNorm: 0.5000
[114/299] timecost: 61.31, lr: 0.000100, Train: (LOSS: 0.0167, MAE: 0.0167, RMSE: 0.0271, R2: 0.9863), Valid: (LOSS: 0.0343, MAE: 0.0343, RMSE: 0.0527, R2: 0.9586), PNorm: 188.7659, GNorm: 0.4397
[115/299] timecost: 59.42, lr: 0.000100, Train: (LOSS: 0.0165, MAE: 0.0165, RMSE: 0.0271, R2: 0.9852), Valid: (LOSS: 0.0338, MAE: 0.0338, RMSE: 0.0524, R2: 0.9580), PNorm: 188.7760, GNorm: 0.4980
[116/299] timecost: 62.29, lr: 0.000100, Train: (LOSS: 0.0161, MAE: 0.0161, RMSE: 0.0271, R2: 0.9860), Valid: (LOSS: 0.0370, MAE: 0.0370, RMSE: 0.0567, R2: 0.9523), PNorm: 188.7862, GNorm: 0.3934
[117/299] timecost: 62.17, lr: 0.000100, Train: (LOSS: 0.0163, MAE: 0.0163, RMSE: 0.0273, R2: 0.9855), Valid: (LOSS: 0.0340, MAE: 0.0340, RMSE: 0.0494, R2: 0.9622), PNorm: 188.7947, GNorm: 0.4847
[118/299] timecost: 62.06, lr: 0.000100, Train: (LOSS: 0.0168, MAE: 0.0168, RMSE: 0.0282, R2: 0.9852), Valid: (LOSS: 0.0336, MAE: 0.0336, RMSE: 0.0523, R2: 0.9577), PNorm: 188.8088, GNorm: 0.4640
[119/299] timecost: 62.12, lr: 0.000100, Train: (LOSS: 0.0168, MAE: 0.0168, RMSE: 0.0275, R2: 0.9853), Valid: (LOSS: 0.0344, MAE: 0.0344, RMSE: 0.0526, R2: 0.9584), PNorm: 188.8240, GNorm: 0.3973
[120/299] timecost: 62.27, lr: 0.000100, Train: (LOSS: 0.0164, MAE: 0.0164, RMSE: 0.0272, R2: 0.9858), Valid: (LOSS: 0.0353, MAE: 0.0353, RMSE: 0.0537, R2: 0.9569), PNorm: 188.8362, GNorm: 0.4630
[121/299] timecost: 62.05, lr: 0.000100, Train: (LOSS: 0.0166, MAE: 0.0166, RMSE: 0.0274, R2: 0.9845), Valid: (LOSS: 0.0361, MAE: 0.0361, RMSE: 0.0566, R2: 0.9520), PNorm: 188.8487, GNorm: 0.3431
[122/299] timecost: 62.02, lr: 0.000100, Train: (LOSS: 0.0167, MAE: 0.0167, RMSE: 0.0276, R2: 0.9859), Valid: (LOSS: 0.0354, MAE: 0.0354, RMSE: 0.0548, R2: 0.9548), PNorm: 188.8614, GNorm: 0.4038
[123/299] timecost: 61.96, lr: 0.000100, Train: (LOSS: 0.0163, MAE: 0.0163, RMSE: 0.0265, R2: 0.9857), Valid: (LOSS: 0.0347, MAE: 0.0347, RMSE: 0.0529, R2: 0.9570), PNorm: 188.8705, GNorm: 0.4629
Epoch 00125: reducing learning rate of group 0 to 8.5000e-05.
[124/299] timecost: 62.28, lr: 0.000085, Train: (LOSS: 0.0161, MAE: 0.0161, RMSE: 0.0266, R2: 0.9860), Valid: (LOSS: 0.0360, MAE: 0.0360, RMSE: 0.0538, R2: 0.9554), PNorm: 188.8852, GNorm: 0.3645
[125/299] timecost: 62.18, lr: 0.000085, Train: (LOSS: 0.0151, MAE: 0.0151, RMSE: 0.0251, R2: 0.9871), Valid: (LOSS: 0.0363, MAE: 0.0363, RMSE: 0.0548, R2: 0.9547), PNorm: 188.8928, GNorm: 0.4393
[126/299] timecost: 62.04, lr: 0.000085, Train: (LOSS: 0.0146, MAE: 0.0146, RMSE: 0.0251, R2: 0.9872), Valid: (LOSS: 0.0349, MAE: 0.0349, RMSE: 0.0514, R2: 0.9594), PNorm: 188.9008, GNorm: 0.3697
[127/299] timecost: 62.20, lr: 0.000085, Train: (LOSS: 0.0160, MAE: 0.0160, RMSE: 0.0261, R2: 0.9875), Valid: (LOSS: 0.0337, MAE: 0.0337, RMSE: 0.0509, R2: 0.9599), PNorm: 188.9110, GNorm: 0.3356
[128/299] timecost: 62.18, lr: 0.000085, Train: (LOSS: 0.0149, MAE: 0.0149, RMSE: 0.0255, R2: 0.9870), Valid: (LOSS: 0.0342, MAE: 0.0342, RMSE: 0.0526, R2: 0.9581), PNorm: 188.9174, GNorm: 0.4021
[129/299] timecost: 62.22, lr: 0.000085, Train: (LOSS: 0.0146, MAE: 0.0146, RMSE: 0.0253, R2: 0.9871), Valid: (LOSS: 0.0348, MAE: 0.0348, RMSE: 0.0522, R2: 0.9588), PNorm: 188.9268, GNorm: 0.5000
[130/299] timecost: 62.19, lr: 0.000085, Train: (LOSS: 0.0143, MAE: 0.0143, RMSE: 0.0248, R2: 0.9879), Valid: (LOSS: 0.0330, MAE: 0.0330, RMSE: 0.0509, R2: 0.9605), PNorm: 188.9367, GNorm: 0.4113
[131/299] timecost: 62.10, lr: 0.000085, Train: (LOSS: 0.0139, MAE: 0.0139, RMSE: 0.0239, R2: 0.9865), Valid: (LOSS: 0.0329, MAE: 0.0329, RMSE: 0.0495, R2: 0.9624), PNorm: 188.9442, GNorm: 0.5000
[132/299] timecost: 62.20, lr: 0.000085, Train: (LOSS: 0.0146, MAE: 0.0146, RMSE: 0.0248, R2: 0.9879), Valid: (LOSS: 0.0330, MAE: 0.0330, RMSE: 0.0503, R2: 0.9614), PNorm: 188.9525, GNorm: 0.4771
[133/299] timecost: 61.60, lr: 0.000085, Train: (LOSS: 0.0138, MAE: 0.0138, RMSE: 0.0235, R2: 0.9880), Valid: (LOSS: 0.0352, MAE: 0.0352, RMSE: 0.0538, R2: 0.9565), PNorm: 188.9604, GNorm: 0.4595
[134/299] timecost: 58.91, lr: 0.000085, Train: (LOSS: 0.0138, MAE: 0.0138, RMSE: 0.0233, R2: 0.9882), Valid: (LOSS: 0.0334, MAE: 0.0334, RMSE: 0.0514, R2: 0.9600), PNorm: 188.9683, GNorm: 0.4150
[135/299] timecost: 58.61, lr: 0.000085, Train: (LOSS: 0.0137, MAE: 0.0137, RMSE: 0.0242, R2: 0.9867), Valid: (LOSS: 0.0339, MAE: 0.0339, RMSE: 0.0516, R2: 0.9597), PNorm: 188.9760, GNorm: 0.4391
[136/299] timecost: 58.52, lr: 0.000085, Train: (LOSS: 0.0138, MAE: 0.0138, RMSE: 0.0239, R2: 0.9874), Valid: (LOSS: 0.0338, MAE: 0.0338, RMSE: 0.0510, R2: 0.9608), PNorm: 188.9832, GNorm: 0.3373
[137/299] timecost: 58.50, lr: 0.000085, Train: (LOSS: 0.0137, MAE: 0.0137, RMSE: 0.0232, R2: 0.9882), Valid: (LOSS: 0.0340, MAE: 0.0340, RMSE: 0.0513, R2: 0.9591), PNorm: 188.9923, GNorm: 0.3463
[138/299] timecost: 58.73, lr: 0.000085, Train: (LOSS: 0.0139, MAE: 0.0139, RMSE: 0.0244, R2: 0.9875), Valid: (LOSS: 0.0336, MAE: 0.0336, RMSE: 0.0515, R2: 0.9597), PNorm: 189.0013, GNorm: 0.3256
[139/299] timecost: 58.72, lr: 0.000085, Train: (LOSS: 0.0135, MAE: 0.0135, RMSE: 0.0233, R2: 0.9885), Valid: (LOSS: 0.0350, MAE: 0.0350, RMSE: 0.0527, R2: 0.9575), PNorm: 189.0075, GNorm: 0.3643
Epoch 00141: reducing learning rate of group 0 to 7.2250e-05.
[140/299] timecost: 58.46, lr: 0.000072, Train: (LOSS: 0.0134, MAE: 0.0134, RMSE: 0.0235, R2: 0.9884), Valid: (LOSS: 0.0350, MAE: 0.0350, RMSE: 0.0519, R2: 0.9589), PNorm: 189.0191, GNorm: 0.3587
[141/299] timecost: 58.21, lr: 0.000072, Train: (LOSS: 0.0131, MAE: 0.0131, RMSE: 0.0230, R2: 0.9884), Valid: (LOSS: 0.0347, MAE: 0.0347, RMSE: 0.0520, R2: 0.9593), PNorm: 189.0256, GNorm: 0.4156
[142/299] timecost: 58.57, lr: 0.000072, Train: (LOSS: 0.0128, MAE: 0.0128, RMSE: 0.0227, R2: 0.9891), Valid: (LOSS: 0.0330, MAE: 0.0330, RMSE: 0.0494, R2: 0.9627), PNorm: 189.0335, GNorm: 0.3487
[143/299] timecost: 58.25, lr: 0.000072, Train: (LOSS: 0.0120, MAE: 0.0120, RMSE: 0.0219, R2: 0.9894), Valid: (LOSS: 0.0333, MAE: 0.0333, RMSE: 0.0495, R2: 0.9626), PNorm: 189.0393, GNorm: 0.3437
[144/299] timecost: 58.53, lr: 0.000072, Train: (LOSS: 0.0121, MAE: 0.0121, RMSE: 0.0219, R2: 0.9890), Valid: (LOSS: 0.0347, MAE: 0.0347, RMSE: 0.0512, R2: 0.9601), PNorm: 189.0462, GNorm: 0.4586
[145/299] timecost: 58.56, lr: 0.000072, Train: (LOSS: 0.0120, MAE: 0.0120, RMSE: 0.0219, R2: 0.9895), Valid: (LOSS: 0.0336, MAE: 0.0336, RMSE: 0.0511, R2: 0.9605), PNorm: 189.0510, GNorm: 0.4285
[146/299] timecost: 58.64, lr: 0.000072, Train: (LOSS: 0.0121, MAE: 0.0121, RMSE: 0.0223, R2: 0.9892), Valid: (LOSS: 0.0337, MAE: 0.0337, RMSE: 0.0512, R2: 0.9601), PNorm: 189.0587, GNorm: 0.5000
[147/299] timecost: 58.47, lr: 0.000072, Train: (LOSS: 0.0123, MAE: 0.0123, RMSE: 0.0218, R2: 0.9891), Valid: (LOSS: 0.0329, MAE: 0.0329, RMSE: 0.0516, R2: 0.9584), PNorm: 189.0666, GNorm: 0.4210
[148/299] timecost: 58.43, lr: 0.000072, Train: (LOSS: 0.0119, MAE: 0.0119, RMSE: 0.0221, R2: 0.9891), Valid: (LOSS: 0.0335, MAE: 0.0335, RMSE: 0.0512, R2: 0.9600), PNorm: 189.0706, GNorm: 0.4021
[149/299] timecost: 58.34, lr: 0.000072, Train: (LOSS: 0.0125, MAE: 0.0125, RMSE: 0.0224, R2: 0.9892), Valid: (LOSS: 0.0347, MAE: 0.0347, RMSE: 0.0510, R2: 0.9605), PNorm: 189.0777, GNorm: 0.4029
[150/299] timecost: 58.39, lr: 0.000072, Train: (LOSS: 0.0117, MAE: 0.0117, RMSE: 0.0218, R2: 0.9896), Valid: (LOSS: 0.0335, MAE: 0.0335, RMSE: 0.0504, R2: 0.9613), PNorm: 189.0863, GNorm: 0.3244
[151/299] timecost: 60.54, lr: 0.000072, Train: (LOSS: 0.0120, MAE: 0.0120, RMSE: 0.0213, R2: 0.9884), Valid: (LOSS: 0.0346, MAE: 0.0346, RMSE: 0.0508, R2: 0.9603), PNorm: 189.0940, GNorm: 0.4454
[152/299] timecost: 62.11, lr: 0.000072, Train: (LOSS: 0.0123, MAE: 0.0123, RMSE: 0.0224, R2: 0.9898), Valid: (LOSS: 0.0326, MAE: 0.0326, RMSE: 0.0497, R2: 0.9623), PNorm: 189.1005, GNorm: 0.2822
[153/299] timecost: 60.19, lr: 0.000072, Train: (LOSS: 0.0120, MAE: 0.0120, RMSE: 0.0213, R2: 0.9895), Valid: (LOSS: 0.0335, MAE: 0.0335, RMSE: 0.0506, R2: 0.9609), PNorm: 189.1109, GNorm: 0.3066
[154/299] timecost: 59.23, lr: 0.000072, Train: (LOSS: 0.0123, MAE: 0.0123, RMSE: 0.0219, R2: 0.9900), Valid: (LOSS: 0.0346, MAE: 0.0346, RMSE: 0.0523, R2: 0.9582), PNorm: 189.1164, GNorm: 0.5000
[155/299] timecost: 58.74, lr: 0.000072, Train: (LOSS: 0.0124, MAE: 0.0124, RMSE: 0.0224, R2: 0.9884), Valid: (LOSS: 0.0326, MAE: 0.0326, RMSE: 0.0488, R2: 0.9623), PNorm: 189.1272, GNorm: 0.3169
Epoch 00157: reducing learning rate of group 0 to 6.1413e-05.
[156/299] timecost: 58.26, lr: 0.000061, Train: (LOSS: 0.0123, MAE: 0.0123, RMSE: 0.0223, R2: 0.9884), Valid: (LOSS: 0.0347, MAE: 0.0347, RMSE: 0.0511, R2: 0.9598), PNorm: 189.1360, GNorm: 0.3560
[157/299] timecost: 58.73, lr: 0.000061, Train: (LOSS: 0.0114, MAE: 0.0114, RMSE: 0.0209, R2: 0.9895), Valid: (LOSS: 0.0332, MAE: 0.0332, RMSE: 0.0503, R2: 0.9617), PNorm: 189.1435, GNorm: 0.4926
[158/299] timecost: 60.75, lr: 0.000061, Train: (LOSS: 0.0109, MAE: 0.0109, RMSE: 0.0199, R2: 0.9910), Valid: (LOSS: 0.0324, MAE: 0.0324, RMSE: 0.0497, R2: 0.9622), PNorm: 189.1495, GNorm: 0.5000
[159/299] timecost: 62.00, lr: 0.000061, Train: (LOSS: 0.0109, MAE: 0.0109, RMSE: 0.0201, R2: 0.9903), Valid: (LOSS: 0.0322, MAE: 0.0322, RMSE: 0.0487, R2: 0.9639), PNorm: 189.1568, GNorm: 0.3112
[160/299] timecost: 61.98, lr: 0.000061, Train: (LOSS: 0.0103, MAE: 0.0103, RMSE: 0.0185, R2: 0.9919), Valid: (LOSS: 0.0329, MAE: 0.0329, RMSE: 0.0489, R2: 0.9634), PNorm: 189.1642, GNorm: 0.3926
[161/299] timecost: 62.13, lr: 0.000061, Train: (LOSS: 0.0101, MAE: 0.0101, RMSE: 0.0189, R2: 0.9914), Valid: (LOSS: 0.0336, MAE: 0.0336, RMSE: 0.0506, R2: 0.9614), PNorm: 189.1697, GNorm: 0.3979
[162/299] timecost: 62.02, lr: 0.000061, Train: (LOSS: 0.0108, MAE: 0.0108, RMSE: 0.0199, R2: 0.9909), Valid: (LOSS: 0.0311, MAE: 0.0311, RMSE: 0.0471, R2: 0.9664), PNorm: 189.1768, GNorm: 0.4836
[163/299] timecost: 59.35, lr: 0.000061, Train: (LOSS: 0.0104, MAE: 0.0104, RMSE: 0.0193, R2: 0.9913), Valid: (LOSS: 0.0311, MAE: 0.0311, RMSE: 0.0473, R2: 0.9654), PNorm: 189.1846, GNorm: 0.3774
[164/299] timecost: 58.42, lr: 0.000061, Train: (LOSS: 0.0097, MAE: 0.0097, RMSE: 0.0180, R2: 0.9919), Valid: (LOSS: 0.0318, MAE: 0.0318, RMSE: 0.0496, R2: 0.9622), PNorm: 189.1920, GNorm: 0.3351
[165/299] timecost: 58.11, lr: 0.000061, Train: (LOSS: 0.0105, MAE: 0.0105, RMSE: 0.0199, R2: 0.9904), Valid: (LOSS: 0.0328, MAE: 0.0328, RMSE: 0.0501, R2: 0.9617), PNorm: 189.1979, GNorm: 0.4638
[166/299] timecost: 58.75, lr: 0.000061, Train: (LOSS: 0.0101, MAE: 0.0101, RMSE: 0.0179, R2: 0.9925), Valid: (LOSS: 0.0312, MAE: 0.0312, RMSE: 0.0473, R2: 0.9655), PNorm: 189.2062, GNorm: 0.4217
[167/299] timecost: 58.65, lr: 0.000061, Train: (LOSS: 0.0102, MAE: 0.0102, RMSE: 0.0180, R2: 0.9922), Valid: (LOSS: 0.0319, MAE: 0.0319, RMSE: 0.0471, R2: 0.9656), PNorm: 189.2129, GNorm: 0.3878
[168/299] timecost: 58.04, lr: 0.000061, Train: (LOSS: 0.0096, MAE: 0.0096, RMSE: 0.0173, R2: 0.9922), Valid: (LOSS: 0.0321, MAE: 0.0321, RMSE: 0.0478, R2: 0.9654), PNorm: 189.2191, GNorm: 0.4605
[169/299] timecost: 58.74, lr: 0.000061, Train: (LOSS: 0.0096, MAE: 0.0096, RMSE: 0.0172, R2: 0.9927), Valid: (LOSS: 0.0314, MAE: 0.0314, RMSE: 0.0475, R2: 0.9653), PNorm: 189.2240, GNorm: 0.3556
[170/299] timecost: 58.96, lr: 0.000061, Train: (LOSS: 0.0094, MAE: 0.0094, RMSE: 0.0168, R2: 0.9934), Valid: (LOSS: 0.0325, MAE: 0.0325, RMSE: 0.0487, R2: 0.9640), PNorm: 189.2317, GNorm: 0.4417
[171/299] timecost: 58.83, lr: 0.000061, Train: (LOSS: 0.0097, MAE: 0.0097, RMSE: 0.0170, R2: 0.9929), Valid: (LOSS: 0.0312, MAE: 0.0312, RMSE: 0.0469, R2: 0.9661), PNorm: 189.2379, GNorm: 0.3583
[172/299] timecost: 58.92, lr: 0.000061, Train: (LOSS: 0.0093, MAE: 0.0093, RMSE: 0.0165, R2: 0.9933), Valid: (LOSS: 0.0317, MAE: 0.0317, RMSE: 0.0473, R2: 0.9660), PNorm: 189.2451, GNorm: 0.4280
[173/299] timecost: 59.17, lr: 0.000061, Train: (LOSS: 0.0113, MAE: 0.0113, RMSE: 0.0212, R2: 0.9899), Valid: (LOSS: 0.0328, MAE: 0.0328, RMSE: 0.0495, R2: 0.9626), PNorm: 189.2520, GNorm: 0.4382
[174/299] timecost: 59.59, lr: 0.000061, Train: (LOSS: 0.0101, MAE: 0.0101, RMSE: 0.0185, R2: 0.9918), Valid: (LOSS: 0.0321, MAE: 0.0321, RMSE: 0.0489, R2: 0.9642), PNorm: 189.2599, GNorm: 0.4115
[175/299] timecost: 58.49, lr: 0.000061, Train: (LOSS: 0.0098, MAE: 0.0098, RMSE: 0.0170, R2: 0.9925), Valid: (LOSS: 0.0319, MAE: 0.0319, RMSE: 0.0478, R2: 0.9654), PNorm: 189.2676, GNorm: 0.2980
[176/299] timecost: 58.52, lr: 0.000061, Train: (LOSS: 0.0091, MAE: 0.0091, RMSE: 0.0161, R2: 0.9934), Valid: (LOSS: 0.0307, MAE: 0.0307, RMSE: 0.0453, R2: 0.9686), PNorm: 189.2742, GNorm: 0.4072
[177/299] timecost: 58.29, lr: 0.000061, Train: (LOSS: 0.0093, MAE: 0.0093, RMSE: 0.0165, R2: 0.9932), Valid: (LOSS: 0.0307, MAE: 0.0307, RMSE: 0.0462, R2: 0.9672), PNorm: 189.2776, GNorm: 0.3998
[178/299] timecost: 58.56, lr: 0.000061, Train: (LOSS: 0.0090, MAE: 0.0090, RMSE: 0.0156, R2: 0.9942), Valid: (LOSS: 0.0310, MAE: 0.0310, RMSE: 0.0476, R2: 0.9656), PNorm: 189.2837, GNorm: 0.5000
[179/299] timecost: 58.23, lr: 0.000061, Train: (LOSS: 0.0086, MAE: 0.0086, RMSE: 0.0147, R2: 0.9941), Valid: (LOSS: 0.0324, MAE: 0.0324, RMSE: 0.0497, R2: 0.9622), PNorm: 189.2888, GNorm: 0.4065
[180/299] timecost: 58.55, lr: 0.000061, Train: (LOSS: 0.0090, MAE: 0.0090, RMSE: 0.0156, R2: 0.9936), Valid: (LOSS: 0.0315, MAE: 0.0315, RMSE: 0.0476, R2: 0.9658), PNorm: 189.2954, GNorm: 0.3618
[181/299] timecost: 58.45, lr: 0.000061, Train: (LOSS: 0.0091, MAE: 0.0091, RMSE: 0.0155, R2: 0.9938), Valid: (LOSS: 0.0307, MAE: 0.0307, RMSE: 0.0464, R2: 0.9672), PNorm: 189.2993, GNorm: 0.4189
[182/299] timecost: 58.73, lr: 0.000061, Train: (LOSS: 0.0087, MAE: 0.0087, RMSE: 0.0152, R2: 0.9942), Valid: (LOSS: 0.0304, MAE: 0.0304, RMSE: 0.0462, R2: 0.9671), PNorm: 189.3034, GNorm: 0.4058
[183/299] timecost: 58.49, lr: 0.000061, Train: (LOSS: 0.0087, MAE: 0.0087, RMSE: 0.0150, R2: 0.9947), Valid: (LOSS: 0.0312, MAE: 0.0312, RMSE: 0.0470, R2: 0.9663), PNorm: 189.3090, GNorm: 0.3331
[184/299] timecost: 58.26, lr: 0.000061, Train: (LOSS: 0.0084, MAE: 0.0084, RMSE: 0.0146, R2: 0.9938), Valid: (LOSS: 0.0318, MAE: 0.0318, RMSE: 0.0474, R2: 0.9655), PNorm: 189.3129, GNorm: 0.3857
[185/299] timecost: 58.44, lr: 0.000061, Train: (LOSS: 0.0088, MAE: 0.0088, RMSE: 0.0153, R2: 0.9941), Valid: (LOSS: 0.0320, MAE: 0.0320, RMSE: 0.0478, R2: 0.9653), PNorm: 189.3182, GNorm: 0.4055
[186/299] timecost: 58.85, lr: 0.000061, Train: (LOSS: 0.0084, MAE: 0.0084, RMSE: 0.0145, R2: 0.9936), Valid: (LOSS: 0.0316, MAE: 0.0316, RMSE: 0.0478, R2: 0.9653), PNorm: 189.3238, GNorm: 0.3831
[187/299] timecost: 60.28, lr: 0.000061, Train: (LOSS: 0.0082, MAE: 0.0082, RMSE: 0.0143, R2: 0.9948), Valid: (LOSS: 0.0309, MAE: 0.0309, RMSE: 0.0472, R2: 0.9658), PNorm: 189.3278, GNorm: 0.3370
[188/299] timecost: 61.91, lr: 0.000061, Train: (LOSS: 0.0083, MAE: 0.0083, RMSE: 0.0145, R2: 0.9944), Valid: (LOSS: 0.0329, MAE: 0.0329, RMSE: 0.0487, R2: 0.9641), PNorm: 189.3323, GNorm: 0.3465
[189/299] timecost: 61.94, lr: 0.000061, Train: (LOSS: 0.0085, MAE: 0.0085, RMSE: 0.0148, R2: 0.9944), Valid: (LOSS: 0.0314, MAE: 0.0314, RMSE: 0.0475, R2: 0.9657), PNorm: 189.3386, GNorm: 0.4082
[190/299] timecost: 62.08, lr: 0.000061, Train: (LOSS: 0.0082, MAE: 0.0082, RMSE: 0.0142, R2: 0.9945), Valid: (LOSS: 0.0319, MAE: 0.0319, RMSE: 0.0476, R2: 0.9655), PNorm: 189.3447, GNorm: 0.3991
[191/299] timecost: 62.04, lr: 0.000061, Train: (LOSS: 0.0085, MAE: 0.0085, RMSE: 0.0144, R2: 0.9946), Valid: (LOSS: 0.0306, MAE: 0.0306, RMSE: 0.0458, R2: 0.9680), PNorm: 189.3486, GNorm: 0.3928
Epoch 00193: reducing learning rate of group 0 to 5.2201e-05.
[192/299] timecost: 62.18, lr: 0.000052, Train: (LOSS: 0.0082, MAE: 0.0082, RMSE: 0.0140, R2: 0.9945), Valid: (LOSS: 0.0308, MAE: 0.0308, RMSE: 0.0472, R2: 0.9662), PNorm: 189.3534, GNorm: 0.3451
[193/299] timecost: 62.17, lr: 0.000052, Train: (LOSS: 0.0082, MAE: 0.0082, RMSE: 0.0139, R2: 0.9941), Valid: (LOSS: 0.0303, MAE: 0.0303, RMSE: 0.0459, R2: 0.9676), PNorm: 189.3586, GNorm: 0.4973
[194/299] timecost: 60.71, lr: 0.000052, Train: (LOSS: 0.0076, MAE: 0.0076, RMSE: 0.0131, R2: 0.9947), Valid: (LOSS: 0.0313, MAE: 0.0313, RMSE: 0.0478, R2: 0.9654), PNorm: 189.3624, GNorm: 0.3776
[195/299] timecost: 59.46, lr: 0.000052, Train: (LOSS: 0.0071, MAE: 0.0071, RMSE: 0.0127, R2: 0.9954), Valid: (LOSS: 0.0313, MAE: 0.0313, RMSE: 0.0471, R2: 0.9664), PNorm: 189.3651, GNorm: 0.4114
[196/299] timecost: 61.80, lr: 0.000052, Train: (LOSS: 0.0075, MAE: 0.0075, RMSE: 0.0129, R2: 0.9950), Valid: (LOSS: 0.0309, MAE: 0.0309, RMSE: 0.0467, R2: 0.9667), PNorm: 189.3680, GNorm: 0.3995
[197/299] timecost: 62.10, lr: 0.000052, Train: (LOSS: 0.0072, MAE: 0.0072, RMSE: 0.0125, R2: 0.9950), Valid: (LOSS: 0.0304, MAE: 0.0304, RMSE: 0.0462, R2: 0.9674), PNorm: 189.3727, GNorm: 0.4297
[198/299] timecost: 62.14, lr: 0.000052, Train: (LOSS: 0.0072, MAE: 0.0072, RMSE: 0.0126, R2: 0.9953), Valid: (LOSS: 0.0306, MAE: 0.0306, RMSE: 0.0462, R2: 0.9674), PNorm: 189.3754, GNorm: 0.3807
[199/299] timecost: 62.22, lr: 0.000052, Train: (LOSS: 0.0070, MAE: 0.0070, RMSE: 0.0123, R2: 0.9950), Valid: (LOSS: 0.0304, MAE: 0.0304, RMSE: 0.0463, R2: 0.9675), PNorm: 189.3788, GNorm: 0.3286
[200/299] timecost: 62.08, lr: 0.000052, Train: (LOSS: 0.0069, MAE: 0.0069, RMSE: 0.0121, R2: 0.9949), Valid: (LOSS: 0.0313, MAE: 0.0313, RMSE: 0.0470, R2: 0.9666), PNorm: 189.3822, GNorm: 0.2725
[201/299] timecost: 62.10, lr: 0.000052, Train: (LOSS: 0.0069, MAE: 0.0069, RMSE: 0.0122, R2: 0.9948), Valid: (LOSS: 0.0308, MAE: 0.0308, RMSE: 0.0462, R2: 0.9675), PNorm: 189.3857, GNorm: 0.5000
[202/299] timecost: 62.00, lr: 0.000052, Train: (LOSS: 0.0071, MAE: 0.0071, RMSE: 0.0122, R2: 0.9955), Valid: (LOSS: 0.0313, MAE: 0.0313, RMSE: 0.0462, R2: 0.9674), PNorm: 189.3888, GNorm: 0.4511
[203/299] timecost: 62.18, lr: 0.000052, Train: (LOSS: 0.0070, MAE: 0.0070, RMSE: 0.0123, R2: 0.9957), Valid: (LOSS: 0.0306, MAE: 0.0306, RMSE: 0.0465, R2: 0.9669), PNorm: 189.3943, GNorm: 0.3802
[204/299] timecost: 62.14, lr: 0.000052, Train: (LOSS: 0.0070, MAE: 0.0070, RMSE: 0.0120, R2: 0.9952), Valid: (LOSS: 0.0300, MAE: 0.0300, RMSE: 0.0460, R2: 0.9678), PNorm: 189.3966, GNorm: 0.3212
[205/299] timecost: 58.19, lr: 0.000052, Train: (LOSS: 0.0068, MAE: 0.0068, RMSE: 0.0120, R2: 0.9953), Valid: (LOSS: 0.0315, MAE: 0.0315, RMSE: 0.0473, R2: 0.9662), PNorm: 189.4012, GNorm: 0.4542
[206/299] timecost: 59.57, lr: 0.000052, Train: (LOSS: 0.0072, MAE: 0.0072, RMSE: 0.0125, R2: 0.9954), Valid: (LOSS: 0.0309, MAE: 0.0309, RMSE: 0.0470, R2: 0.9660), PNorm: 189.4055, GNorm: 0.3137
[207/299] timecost: 61.54, lr: 0.000052, Train: (LOSS: 0.0071, MAE: 0.0071, RMSE: 0.0127, R2: 0.9953), Valid: (LOSS: 0.0320, MAE: 0.0320, RMSE: 0.0481, R2: 0.9647), PNorm: 189.4093, GNorm: 0.4536
[208/299] timecost: 58.46, lr: 0.000052, Train: (LOSS: 0.0068, MAE: 0.0068, RMSE: 0.0119, R2: 0.9961), Valid: (LOSS: 0.0312, MAE: 0.0312, RMSE: 0.0472, R2: 0.9662), PNorm: 189.4123, GNorm: 0.4142
Epoch 00210: reducing learning rate of group 0 to 4.4371e-05.
[209/299] timecost: 58.44, lr: 0.000044, Train: (LOSS: 0.0069, MAE: 0.0069, RMSE: 0.0119, R2: 0.9957), Valid: (LOSS: 0.0318, MAE: 0.0318, RMSE: 0.0483, R2: 0.9648), PNorm: 189.4174, GNorm: 0.3427
[210/299] timecost: 58.44, lr: 0.000044, Train: (LOSS: 0.0066, MAE: 0.0066, RMSE: 0.0115, R2: 0.9958), Valid: (LOSS: 0.0306, MAE: 0.0306, RMSE: 0.0461, R2: 0.9673), PNorm: 189.4209, GNorm: 0.3840
[211/299] timecost: 58.37, lr: 0.000044, Train: (LOSS: 0.0062, MAE: 0.0062, RMSE: 0.0110, R2: 0.9959), Valid: (LOSS: 0.0315, MAE: 0.0315, RMSE: 0.0473, R2: 0.9661), PNorm: 189.4245, GNorm: 0.3894
[212/299] timecost: 59.74, lr: 0.000044, Train: (LOSS: 0.0060, MAE: 0.0060, RMSE: 0.0110, R2: 0.9953), Valid: (LOSS: 0.0308, MAE: 0.0308, RMSE: 0.0471, R2: 0.9662), PNorm: 189.4262, GNorm: 0.4546
[213/299] timecost: 58.98, lr: 0.000044, Train: (LOSS: 0.0059, MAE: 0.0059, RMSE: 0.0108, R2: 0.9955), Valid: (LOSS: 0.0312, MAE: 0.0312, RMSE: 0.0474, R2: 0.9655), PNorm: 189.4298, GNorm: 0.3080
[214/299] timecost: 58.45, lr: 0.000044, Train: (LOSS: 0.0059, MAE: 0.0059, RMSE: 0.0108, R2: 0.9953), Valid: (LOSS: 0.0308, MAE: 0.0308, RMSE: 0.0475, R2: 0.9654), PNorm: 189.4310, GNorm: 0.4601
[215/299] timecost: 58.33, lr: 0.000044, Train: (LOSS: 0.0060, MAE: 0.0060, RMSE: 0.0108, R2: 0.9953), Valid: (LOSS: 0.0318, MAE: 0.0318, RMSE: 0.0485, R2: 0.9641), PNorm: 189.4346, GNorm: 0.4416
[216/299] timecost: 58.44, lr: 0.000044, Train: (LOSS: 0.0058, MAE: 0.0058, RMSE: 0.0106, R2: 0.9962), Valid: (LOSS: 0.0315, MAE: 0.0315, RMSE: 0.0481, R2: 0.9647), PNorm: 189.4375, GNorm: 0.3165
[217/299] timecost: 58.41, lr: 0.000044, Train: (LOSS: 0.0058, MAE: 0.0058, RMSE: 0.0104, R2: 0.9964), Valid: (LOSS: 0.0307, MAE: 0.0307, RMSE: 0.0467, R2: 0.9667), PNorm: 189.4398, GNorm: 0.4021
[218/299] timecost: 58.60, lr: 0.000044, Train: (LOSS: 0.0058, MAE: 0.0058, RMSE: 0.0105, R2: 0.9956), Valid: (LOSS: 0.0303, MAE: 0.0303, RMSE: 0.0466, R2: 0.9669), PNorm: 189.4423, GNorm: 0.4484
[219/299] timecost: 58.10, lr: 0.000044, Train: (LOSS: 0.0061, MAE: 0.0061, RMSE: 0.0111, R2: 0.9958), Valid: (LOSS: 0.0312, MAE: 0.0312, RMSE: 0.0482, R2: 0.9648), PNorm: 189.4448, GNorm: 0.5000
[220/299] timecost: 59.20, lr: 0.000044, Train: (LOSS: 0.0060, MAE: 0.0060, RMSE: 0.0107, R2: 0.9958), Valid: (LOSS: 0.0313, MAE: 0.0313, RMSE: 0.0479, R2: 0.9650), PNorm: 189.4485, GNorm: 0.3023
[221/299] timecost: 58.89, lr: 0.000044, Train: (LOSS: 0.0059, MAE: 0.0059, RMSE: 0.0107, R2: 0.9959), Valid: (LOSS: 0.0319, MAE: 0.0319, RMSE: 0.0484, R2: 0.9644), PNorm: 189.4509, GNorm: 0.5000
[222/299] timecost: 59.69, lr: 0.000044, Train: (LOSS: 0.0059, MAE: 0.0059, RMSE: 0.0105, R2: 0.9958), Valid: (LOSS: 0.0319, MAE: 0.0319, RMSE: 0.0487, R2: 0.9641), PNorm: 189.4546, GNorm: 0.3414
[223/299] timecost: 58.95, lr: 0.000044, Train: (LOSS: 0.0059, MAE: 0.0059, RMSE: 0.0101, R2: 0.9959), Valid: (LOSS: 0.0316, MAE: 0.0316, RMSE: 0.0476, R2: 0.9656), PNorm: 189.4584, GNorm: 0.3493
[224/299] timecost: 58.92, lr: 0.000044, Train: (LOSS: 0.0060, MAE: 0.0060, RMSE: 0.0108, R2: 0.9964), Valid: (LOSS: 0.0313, MAE: 0.0313, RMSE: 0.0473, R2: 0.9659), PNorm: 189.4604, GNorm: 0.4940
Epoch 00226: reducing learning rate of group 0 to 3.7715e-05.
[225/299] timecost: 59.25, lr: 0.000038, Train: (LOSS: 0.0058, MAE: 0.0058, RMSE: 0.0105, R2: 0.9958), Valid: (LOSS: 0.0313, MAE: 0.0313, RMSE: 0.0479, R2: 0.9651), PNorm: 189.4629, GNorm: 0.5000
[226/299] timecost: 60.80, lr: 0.000038, Train: (LOSS: 0.0056, MAE: 0.0056, RMSE: 0.0102, R2: 0.9953), Valid: (LOSS: 0.0311, MAE: 0.0311, RMSE: 0.0469, R2: 0.9664), PNorm: 189.4657, GNorm: 0.4244
[227/299] timecost: 62.07, lr: 0.000038, Train: (LOSS: 0.0056, MAE: 0.0056, RMSE: 0.0101, R2: 0.9962), Valid: (LOSS: 0.0306, MAE: 0.0306, RMSE: 0.0469, R2: 0.9664), PNorm: 189.4669, GNorm: 0.3275
[228/299] timecost: 62.12, lr: 0.000038, Train: (LOSS: 0.0051, MAE: 0.0051, RMSE: 0.0096, R2: 0.9937), Valid: (LOSS: 0.0313, MAE: 0.0313, RMSE: 0.0479, R2: 0.9651), PNorm: 189.4697, GNorm: 0.5000
[229/299] timecost: 62.17, lr: 0.000038, Train: (LOSS: 0.0053, MAE: 0.0053, RMSE: 0.0093, R2: 0.9959), Valid: (LOSS: 0.0308, MAE: 0.0308, RMSE: 0.0464, R2: 0.9668), PNorm: 189.4707, GNorm: 0.3717
[230/299] timecost: 62.04, lr: 0.000038, Train: (LOSS: 0.0053, MAE: 0.0053, RMSE: 0.0098, R2: 0.9962), Valid: (LOSS: 0.0316, MAE: 0.0316, RMSE: 0.0473, R2: 0.9658), PNorm: 189.4721, GNorm: 0.4860
[231/299] timecost: 62.06, lr: 0.000038, Train: (LOSS: 0.0053, MAE: 0.0053, RMSE: 0.0098, R2: 0.9960), Valid: (LOSS: 0.0311, MAE: 0.0311, RMSE: 0.0473, R2: 0.9658), PNorm: 189.4745, GNorm: 0.4362
[232/299] timecost: 60.03, lr: 0.000038, Train: (LOSS: 0.0050, MAE: 0.0050, RMSE: 0.0096, R2: 0.9962), Valid: (LOSS: 0.0314, MAE: 0.0314, RMSE: 0.0475, R2: 0.9658), PNorm: 189.4765, GNorm: 0.3845
[233/299] timecost: 59.14, lr: 0.000038, Train: (LOSS: 0.0051, MAE: 0.0051, RMSE: 0.0096, R2: 0.9963), Valid: (LOSS: 0.0304, MAE: 0.0304, RMSE: 0.0466, R2: 0.9670), PNorm: 189.4788, GNorm: 0.3318
[234/299] timecost: 59.25, lr: 0.000038, Train: (LOSS: 0.0051, MAE: 0.0051, RMSE: 0.0092, R2: 0.9958), Valid: (LOSS: 0.0303, MAE: 0.0303, RMSE: 0.0469, R2: 0.9665), PNorm: 189.4805, GNorm: 0.3722
[235/299] timecost: 58.56, lr: 0.000038, Train: (LOSS: 0.0049, MAE: 0.0049, RMSE: 0.0095, R2: 0.9965), Valid: (LOSS: 0.0308, MAE: 0.0308, RMSE: 0.0473, R2: 0.9660), PNorm: 189.4826, GNorm: 0.3732
[236/299] timecost: 58.83, lr: 0.000038, Train: (LOSS: 0.0050, MAE: 0.0050, RMSE: 0.0094, R2: 0.9958), Valid: (LOSS: 0.0311, MAE: 0.0311, RMSE: 0.0476, R2: 0.9655), PNorm: 189.4853, GNorm: 0.3649
[237/299] timecost: 58.84, lr: 0.000038, Train: (LOSS: 0.0050, MAE: 0.0050, RMSE: 0.0095, R2: 0.9963), Valid: (LOSS: 0.0314, MAE: 0.0314, RMSE: 0.0476, R2: 0.9654), PNorm: 189.4878, GNorm: 0.2951
[238/299] timecost: 59.01, lr: 0.000038, Train: (LOSS: 0.0050, MAE: 0.0050, RMSE: 0.0094, R2: 0.9959), Valid: (LOSS: 0.0303, MAE: 0.0303, RMSE: 0.0466, R2: 0.9665), PNorm: 189.4902, GNorm: 0.4579
[239/299] timecost: 58.95, lr: 0.000038, Train: (LOSS: 0.0048, MAE: 0.0048, RMSE: 0.0093, R2: 0.9960), Valid: (LOSS: 0.0308, MAE: 0.0308, RMSE: 0.0467, R2: 0.9666), PNorm: 189.4914, GNorm: 0.3349
[240/299] timecost: 58.75, lr: 0.000038, Train: (LOSS: 0.0050, MAE: 0.0050, RMSE: 0.0095, R2: 0.9960), Valid: (LOSS: 0.0309, MAE: 0.0309, RMSE: 0.0467, R2: 0.9669), PNorm: 189.4941, GNorm: 0.3536
Epoch 00242: reducing learning rate of group 0 to 3.2058e-05.
[241/299] timecost: 58.24, lr: 0.000032, Train: (LOSS: 0.0051, MAE: 0.0051, RMSE: 0.0090, R2: 0.9968), Valid: (LOSS: 0.0304, MAE: 0.0304, RMSE: 0.0461, R2: 0.9677), PNorm: 189.4966, GNorm: 0.2800
[242/299] timecost: 58.49, lr: 0.000032, Train: (LOSS: 0.0046, MAE: 0.0046, RMSE: 0.0089, R2: 0.9965), Valid: (LOSS: 0.0306, MAE: 0.0306, RMSE: 0.0467, R2: 0.9665), PNorm: 189.4975, GNorm: 0.4177
[243/299] timecost: 58.19, lr: 0.000032, Train: (LOSS: 0.0045, MAE: 0.0045, RMSE: 0.0088, R2: 0.9965), Valid: (LOSS: 0.0304, MAE: 0.0304, RMSE: 0.0463, R2: 0.9670), PNorm: 189.4998, GNorm: 0.3105
[244/299] timecost: 58.40, lr: 0.000032, Train: (LOSS: 0.0044, MAE: 0.0044, RMSE: 0.0087, R2: 0.9962), Valid: (LOSS: 0.0315, MAE: 0.0315, RMSE: 0.0472, R2: 0.9661), PNorm: 189.5016, GNorm: 0.4378
[245/299] timecost: 59.38, lr: 0.000032, Train: (LOSS: 0.0043, MAE: 0.0043, RMSE: 0.0086, R2: 0.9961), Valid: (LOSS: 0.0311, MAE: 0.0311, RMSE: 0.0470, R2: 0.9663), PNorm: 189.5032, GNorm: 0.5000
[246/299] timecost: 59.80, lr: 0.000032, Train: (LOSS: 0.0045, MAE: 0.0045, RMSE: 0.0087, R2: 0.9965), Valid: (LOSS: 0.0308, MAE: 0.0308, RMSE: 0.0466, R2: 0.9667), PNorm: 189.5043, GNorm: 0.3897
[247/299] timecost: 59.50, lr: 0.000032, Train: (LOSS: 0.0045, MAE: 0.0045, RMSE: 0.0089, R2: 0.9957), Valid: (LOSS: 0.0308, MAE: 0.0308, RMSE: 0.0470, R2: 0.9665), PNorm: 189.5063, GNorm: 0.4730
[248/299] timecost: 58.47, lr: 0.000032, Train: (LOSS: 0.0044, MAE: 0.0044, RMSE: 0.0088, R2: 0.9965), Valid: (LOSS: 0.0309, MAE: 0.0309, RMSE: 0.0469, R2: 0.9665), PNorm: 189.5074, GNorm: 0.5000
[249/299] timecost: 58.58, lr: 0.000032, Train: (LOSS: 0.0044, MAE: 0.0044, RMSE: 0.0087, R2: 0.9960), Valid: (LOSS: 0.0308, MAE: 0.0308, RMSE: 0.0469, R2: 0.9666), PNorm: 189.5088, GNorm: 0.3978
[250/299] timecost: 59.67, lr: 0.000032, Train: (LOSS: 0.0045, MAE: 0.0045, RMSE: 0.0088, R2: 0.9964), Valid: (LOSS: 0.0307, MAE: 0.0307, RMSE: 0.0470, R2: 0.9662), PNorm: 189.5110, GNorm: 0.4271
[251/299] timecost: 62.00, lr: 0.000032, Train: (LOSS: 0.0045, MAE: 0.0045, RMSE: 0.0084, R2: 0.9961), Valid: (LOSS: 0.0307, MAE: 0.0307, RMSE: 0.0461, R2: 0.9675), PNorm: 189.5119, GNorm: 0.4468
[252/299] timecost: 62.04, lr: 0.000032, Train: (LOSS: 0.0043, MAE: 0.0043, RMSE: 0.0082, R2: 0.9971), Valid: (LOSS: 0.0306, MAE: 0.0306, RMSE: 0.0462, R2: 0.9674), PNorm: 189.5147, GNorm: 0.4219
[253/299] timecost: 62.06, lr: 0.000032, Train: (LOSS: 0.0043, MAE: 0.0043, RMSE: 0.0086, R2: 0.9965), Valid: (LOSS: 0.0313, MAE: 0.0313, RMSE: 0.0475, R2: 0.9657), PNorm: 189.5155, GNorm: 0.4540
[254/299] timecost: 62.00, lr: 0.000032, Train: (LOSS: 0.0042, MAE: 0.0042, RMSE: 0.0085, R2: 0.9964), Valid: (LOSS: 0.0309, MAE: 0.0309, RMSE: 0.0472, R2: 0.9661), PNorm: 189.5166, GNorm: 0.3866
[255/299] timecost: 61.87, lr: 0.000032, Train: (LOSS: 0.0042, MAE: 0.0042, RMSE: 0.0085, R2: 0.9963), Valid: (LOSS: 0.0307, MAE: 0.0307, RMSE: 0.0468, R2: 0.9666), PNorm: 189.5187, GNorm: 0.4426
[256/299] timecost: 61.97, lr: 0.000032, Train: (LOSS: 0.0044, MAE: 0.0044, RMSE: 0.0087, R2: 0.9962), Valid: (LOSS: 0.0307, MAE: 0.0307, RMSE: 0.0467, R2: 0.9670), PNorm: 189.5205, GNorm: 0.5000
Epoch 00258: reducing learning rate of group 0 to 2.7249e-05.
[257/299] timecost: 62.13, lr: 0.000027, Train: (LOSS: 0.0044, MAE: 0.0044, RMSE: 0.0087, R2: 0.9961), Valid: (LOSS: 0.0305, MAE: 0.0305, RMSE: 0.0464, R2: 0.9670), PNorm: 189.5216, GNorm: 0.4909
[258/299] timecost: 62.05, lr: 0.000027, Train: (LOSS: 0.0041, MAE: 0.0041, RMSE: 0.0083, R2: 0.9962), Valid: (LOSS: 0.0308, MAE: 0.0308, RMSE: 0.0468, R2: 0.9666), PNorm: 189.5231, GNorm: 0.3624
[259/299] timecost: 58.94, lr: 0.000027, Train: (LOSS: 0.0040, MAE: 0.0040, RMSE: 0.0083, R2: 0.9961), Valid: (LOSS: 0.0308, MAE: 0.0308, RMSE: 0.0472, R2: 0.9661), PNorm: 189.5241, GNorm: 0.3414
[260/299] timecost: 58.61, lr: 0.000027, Train: (LOSS: 0.0040, MAE: 0.0040, RMSE: 0.0082, R2: 0.9958), Valid: (LOSS: 0.0310, MAE: 0.0310, RMSE: 0.0468, R2: 0.9667), PNorm: 189.5259, GNorm: 0.3279
[261/299] timecost: 58.11, lr: 0.000027, Train: (LOSS: 0.0039, MAE: 0.0039, RMSE: 0.0081, R2: 0.9965), Valid: (LOSS: 0.0308, MAE: 0.0308, RMSE: 0.0468, R2: 0.9666), PNorm: 189.5271, GNorm: 0.3441
[262/299] timecost: 58.64, lr: 0.000027, Train: (LOSS: 0.0040, MAE: 0.0040, RMSE: 0.0081, R2: 0.9958), Valid: (LOSS: 0.0309, MAE: 0.0309, RMSE: 0.0471, R2: 0.9663), PNorm: 189.5285, GNorm: 0.4719
[263/299] timecost: 58.27, lr: 0.000027, Train: (LOSS: 0.0038, MAE: 0.0038, RMSE: 0.0081, R2: 0.9964), Valid: (LOSS: 0.0310, MAE: 0.0310, RMSE: 0.0472, R2: 0.9660), PNorm: 189.5295, GNorm: 0.5000
[264/299] timecost: 58.31, lr: 0.000027, Train: (LOSS: 0.0039, MAE: 0.0039, RMSE: 0.0080, R2: 0.9962), Valid: (LOSS: 0.0311, MAE: 0.0311, RMSE: 0.0469, R2: 0.9662), PNorm: 189.5306, GNorm: 0.5000
[265/299] timecost: 58.53, lr: 0.000027, Train: (LOSS: 0.0038, MAE: 0.0038, RMSE: 0.0078, R2: 0.9968), Valid: (LOSS: 0.0311, MAE: 0.0311, RMSE: 0.0467, R2: 0.9667), PNorm: 189.5317, GNorm: 0.4361
[266/299] timecost: 58.45, lr: 0.000027, Train: (LOSS: 0.0040, MAE: 0.0040, RMSE: 0.0080, R2: 0.9962), Valid: (LOSS: 0.0311, MAE: 0.0311, RMSE: 0.0469, R2: 0.9665), PNorm: 189.5331, GNorm: 0.3569
[267/299] timecost: 58.41, lr: 0.000027, Train: (LOSS: 0.0039, MAE: 0.0039, RMSE: 0.0080, R2: 0.9965), Valid: (LOSS: 0.0310, MAE: 0.0310, RMSE: 0.0470, R2: 0.9663), PNorm: 189.5340, GNorm: 0.3405
[268/299] timecost: 58.47, lr: 0.000027, Train: (LOSS: 0.0040, MAE: 0.0040, RMSE: 0.0081, R2: 0.9963), Valid: (LOSS: 0.0310, MAE: 0.0310, RMSE: 0.0469, R2: 0.9664), PNorm: 189.5362, GNorm: 0.3934
[269/299] timecost: 58.08, lr: 0.000027, Train: (LOSS: 0.0038, MAE: 0.0038, RMSE: 0.0080, R2: 0.9965), Valid: (LOSS: 0.0307, MAE: 0.0307, RMSE: 0.0463, R2: 0.9672), PNorm: 189.5375, GNorm: 0.3236
[270/299] timecost: 58.65, lr: 0.000027, Train: (LOSS: 0.0038, MAE: 0.0038, RMSE: 0.0079, R2: 0.9965), Valid: (LOSS: 0.0307, MAE: 0.0307, RMSE: 0.0466, R2: 0.9670), PNorm: 189.5379, GNorm: 0.3674
[271/299] timecost: 58.40, lr: 0.000027, Train: (LOSS: 0.0038, MAE: 0.0038, RMSE: 0.0080, R2: 0.9963), Valid: (LOSS: 0.0304, MAE: 0.0304, RMSE: 0.0461, R2: 0.9674), PNorm: 189.5390, GNorm: 0.4604
[272/299] timecost: 58.60, lr: 0.000027, Train: (LOSS: 0.0039, MAE: 0.0039, RMSE: 0.0079, R2: 0.9970), Valid: (LOSS: 0.0304, MAE: 0.0304, RMSE: 0.0463, R2: 0.9674), PNorm: 189.5406, GNorm: 0.3824
Epoch 00274: reducing learning rate of group 0 to 2.3162e-05.
[273/299] timecost: 58.22, lr: 0.000023, Train: (LOSS: 0.0037, MAE: 0.0037, RMSE: 0.0077, R2: 0.9966), Valid: (LOSS: 0.0306, MAE: 0.0306, RMSE: 0.0463, R2: 0.9674), PNorm: 189.5417, GNorm: 0.3248
[274/299] timecost: 58.59, lr: 0.000023, Train: (LOSS: 0.0037, MAE: 0.0037, RMSE: 0.0076, R2: 0.9968), Valid: (LOSS: 0.0310, MAE: 0.0310, RMSE: 0.0471, R2: 0.9662), PNorm: 189.5427, GNorm: 0.4570
[275/299] timecost: 59.70, lr: 0.000023, Train: (LOSS: 0.0035, MAE: 0.0035, RMSE: 0.0076, R2: 0.9959), Valid: (LOSS: 0.0305, MAE: 0.0305, RMSE: 0.0464, R2: 0.9672), PNorm: 189.5436, GNorm: 0.4482
[276/299] timecost: 60.05, lr: 0.000023, Train: (LOSS: 0.0035, MAE: 0.0035, RMSE: 0.0075, R2: 0.9963), Valid: (LOSS: 0.0305, MAE: 0.0305, RMSE: 0.0466, R2: 0.9668), PNorm: 189.5448, GNorm: 0.3846
[277/299] timecost: 60.01, lr: 0.000023, Train: (LOSS: 0.0034, MAE: 0.0034, RMSE: 0.0075, R2: 0.9956), Valid: (LOSS: 0.0307, MAE: 0.0307, RMSE: 0.0467, R2: 0.9666), PNorm: 189.5448, GNorm: 0.3918
[278/299] timecost: 59.05, lr: 0.000023, Train: (LOSS: 0.0035, MAE: 0.0035, RMSE: 0.0075, R2: 0.9965), Valid: (LOSS: 0.0303, MAE: 0.0303, RMSE: 0.0460, R2: 0.9677), PNorm: 189.5461, GNorm: 0.5000
[279/299] timecost: 58.56, lr: 0.000023, Train: (LOSS: 0.0034, MAE: 0.0034, RMSE: 0.0075, R2: 0.9965), Valid: (LOSS: 0.0305, MAE: 0.0305, RMSE: 0.0461, R2: 0.9676), PNorm: 189.5473, GNorm: 0.3809
[280/299] timecost: 59.20, lr: 0.000023, Train: (LOSS: 0.0035, MAE: 0.0035, RMSE: 0.0074, R2: 0.9968), Valid: (LOSS: 0.0304, MAE: 0.0304, RMSE: 0.0461, R2: 0.9675), PNorm: 189.5482, GNorm: 0.3905
[281/299] timecost: 58.60, lr: 0.000023, Train: (LOSS: 0.0035, MAE: 0.0035, RMSE: 0.0076, R2: 0.9965), Valid: (LOSS: 0.0308, MAE: 0.0308, RMSE: 0.0465, R2: 0.9670), PNorm: 189.5487, GNorm: 0.4301
[282/299] timecost: 59.57, lr: 0.000023, Train: (LOSS: 0.0034, MAE: 0.0034, RMSE: 0.0074, R2: 0.9967), Valid: (LOSS: 0.0305, MAE: 0.0305, RMSE: 0.0464, R2: 0.9673), PNorm: 189.5496, GNorm: 0.5000
[283/299] timecost: 62.21, lr: 0.000023, Train: (LOSS: 0.0033, MAE: 0.0033, RMSE: 0.0073, R2: 0.9962), Valid: (LOSS: 0.0309, MAE: 0.0309, RMSE: 0.0467, R2: 0.9668), PNorm: 189.5504, GNorm: 0.3899
[284/299] timecost: 62.19, lr: 0.000023, Train: (LOSS: 0.0033, MAE: 0.0033, RMSE: 0.0073, R2: 0.9970), Valid: (LOSS: 0.0305, MAE: 0.0305, RMSE: 0.0464, R2: 0.9671), PNorm: 189.5520, GNorm: 0.3093
[285/299] timecost: 62.17, lr: 0.000023, Train: (LOSS: 0.0035, MAE: 0.0035, RMSE: 0.0074, R2: 0.9963), Valid: (LOSS: 0.0304, MAE: 0.0304, RMSE: 0.0462, R2: 0.9675), PNorm: 189.5523, GNorm: 0.3732
[286/299] timecost: 62.10, lr: 0.000023, Train: (LOSS: 0.0034, MAE: 0.0034, RMSE: 0.0072, R2: 0.9971), Valid: (LOSS: 0.0305, MAE: 0.0305, RMSE: 0.0460, R2: 0.9676), PNorm: 189.5536, GNorm: 0.4290
[287/299] timecost: 62.17, lr: 0.000023, Train: (LOSS: 0.0033, MAE: 0.0033, RMSE: 0.0068, R2: 0.9958), Valid: (LOSS: 0.0308, MAE: 0.0308, RMSE: 0.0472, R2: 0.9663), PNorm: 189.5545, GNorm: 0.3335
[288/299] timecost: 62.08, lr: 0.000023, Train: (LOSS: 0.0034, MAE: 0.0034, RMSE: 0.0073, R2: 0.9965), Valid: (LOSS: 0.0301, MAE: 0.0301, RMSE: 0.0458, R2: 0.9679), PNorm: 189.5548, GNorm: 0.4564
Epoch 00290: reducing learning rate of group 0 to 1.9687e-05.
[289/299] timecost: 62.36, lr: 0.000020, Train: (LOSS: 0.0035, MAE: 0.0035, RMSE: 0.0074, R2: 0.9963), Valid: (LOSS: 0.0307, MAE: 0.0307, RMSE: 0.0466, R2: 0.9669), PNorm: 189.5560, GNorm: 0.4190
[290/299] timecost: 60.21, lr: 0.000020, Train: (LOSS: 0.0032, MAE: 0.0032, RMSE: 0.0072, R2: 0.9962), Valid: (LOSS: 0.0308, MAE: 0.0308, RMSE: 0.0468, R2: 0.9665), PNorm: 189.5563, GNorm: 0.4901
[291/299] timecost: 58.26, lr: 0.000020, Train: (LOSS: 0.0033, MAE: 0.0033, RMSE: 0.0072, R2: 0.9955), Valid: (LOSS: 0.0306, MAE: 0.0306, RMSE: 0.0465, R2: 0.9671), PNorm: 189.5573, GNorm: 0.3283
[292/299] timecost: 58.32, lr: 0.000020, Train: (LOSS: 0.0032, MAE: 0.0032, RMSE: 0.0071, R2: 0.9962), Valid: (LOSS: 0.0302, MAE: 0.0302, RMSE: 0.0465, R2: 0.9670), PNorm: 189.5577, GNorm: 0.4922
[293/299] timecost: 58.10, lr: 0.000020, Train: (LOSS: 0.0032, MAE: 0.0032, RMSE: 0.0072, R2: 0.9964), Valid: (LOSS: 0.0304, MAE: 0.0304, RMSE: 0.0461, R2: 0.9675), PNorm: 189.5586, GNorm: 0.5000
[294/299] timecost: 58.45, lr: 0.000020, Train: (LOSS: 0.0032, MAE: 0.0032, RMSE: 0.0072, R2: 0.9967), Valid: (LOSS: 0.0308, MAE: 0.0308, RMSE: 0.0466, R2: 0.9669), PNorm: 189.5592, GNorm: 0.2931
[295/299] timecost: 59.74, lr: 0.000020, Train: (LOSS: 0.0031, MAE: 0.0031, RMSE: 0.0070, R2: 0.9957), Valid: (LOSS: 0.0306, MAE: 0.0306, RMSE: 0.0465, R2: 0.9671), PNorm: 189.5597, GNorm: 0.3415
[296/299] timecost: 58.51, lr: 0.000020, Train: (LOSS: 0.0030, MAE: 0.0030, RMSE: 0.0070, R2: 0.9969), Valid: (LOSS: 0.0312, MAE: 0.0312, RMSE: 0.0473, R2: 0.9660), PNorm: 189.5600, GNorm: 0.3554
[297/299] timecost: 58.85, lr: 0.000020, Train: (LOSS: 0.0031, MAE: 0.0031, RMSE: 0.0071, R2: 0.9969), Valid: (LOSS: 0.0303, MAE: 0.0303, RMSE: 0.0460, R2: 0.9677), PNorm: 189.5609, GNorm: 0.3345
[298/299] timecost: 58.78, lr: 0.000020, Train: (LOSS: 0.0030, MAE: 0.0030, RMSE: 0.0069, R2: 0.9969), Valid: (LOSS: 0.0307, MAE: 0.0307, RMSE: 0.0467, R2: 0.9668), PNorm: 189.5616, GNorm: 0.3106
[299/299] timecost: 58.17, lr: 0.000020, Train: (LOSS: 0.0030, MAE: 0.0030, RMSE: 0.0064, R2: 0.9968), Valid: (LOSS: 0.0306, MAE: 0.0306, RMSE: 0.0465, R2: 0.9670), PNorm: 189.5621, GNorm: 0.4154
==========Training End==========
==========Test Best Model==========
================Final Results=======================
mse: 0.0379 +- 0.0000:
rmse: 0.0614 +- 0.0000:
mae: 0.0379 +- 0.0000:
r2: 0.9412 +- 0.0000:
tensor([[0.1131, 0.1066],
        [0.0000, 0.0000],
        [0.0000, 0.0000],
        ...,
        [0.0000, 0.0000],
        [0.0000, 0.0000],
        [0.0000, 0.0000]], device='cuda:0')
