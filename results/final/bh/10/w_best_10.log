cuda available with GPU: Tesla V100-PCIE-16GB
==========Load Seed==========
set_random_seed
0
==========Training Start==========
Training Graphs:  2491
Valid Graphs:  277
Test Graphs:  1187
============Loading pretrained weights to generate initialization============
============Creating new layers============
============Creating Model============
Training Graphs Batches:  78
Valid Graphs Batches:  9
Test Graphs Batches:  37
[0/299] timecost: 69.31, lr: 0.000030, Train: (LOSS: 0.2346, MAE: 0.2346, RMSE: 0.2780, R2: -0.1053), Valid: (LOSS: 0.2200, MAE: 0.2200, RMSE: 0.2570, R2: -0.0195), PNorm: 174.4144, GNorm: 0.4339
[1/299] timecost: 62.75, lr: 0.000030, Train: (LOSS: 0.2189, MAE: 0.2189, RMSE: 0.2627, R2: 0.0216), Valid: (LOSS: 0.2115, MAE: 0.2115, RMSE: 0.2472, R2: 0.0557), PNorm: 173.7529, GNorm: 0.7775
[2/299] timecost: 62.84, lr: 0.000030, Train: (LOSS: 0.2099, MAE: 0.2099, RMSE: 0.2534, R2: 0.0884), Valid: (LOSS: 0.1964, MAE: 0.1964, RMSE: 0.2453, R2: 0.0805), PNorm: 173.1922, GNorm: 0.5945
[3/299] timecost: 63.04, lr: 0.000030, Train: (LOSS: 0.1935, MAE: 0.1935, RMSE: 0.2386, R2: 0.1837), Valid: (LOSS: 0.1815, MAE: 0.1815, RMSE: 0.2181, R2: 0.2686), PNorm: 172.7244, GNorm: 2.9163
[4/299] timecost: 63.28, lr: 0.000030, Train: (LOSS: 0.1670, MAE: 0.1670, RMSE: 0.2126, R2: 0.3482), Valid: (LOSS: 0.1411, MAE: 0.1411, RMSE: 0.1817, R2: 0.4924), PNorm: 172.3555, GNorm: 3.3518
[5/299] timecost: 63.05, lr: 0.000030, Train: (LOSS: 0.1483, MAE: 0.1483, RMSE: 0.1956, R2: 0.4413), Valid: (LOSS: 0.1337, MAE: 0.1337, RMSE: 0.1791, R2: 0.5068), PNorm: 172.0374, GNorm: 3.5367
[6/299] timecost: 62.97, lr: 0.000030, Train: (LOSS: 0.1353, MAE: 0.1353, RMSE: 0.1832, R2: 0.5132), Valid: (LOSS: 0.1297, MAE: 0.1297, RMSE: 0.1767, R2: 0.5160), PNorm: 171.7409, GNorm: 1.2712
[7/299] timecost: 63.10, lr: 0.000030, Train: (LOSS: 0.1278, MAE: 0.1278, RMSE: 0.1761, R2: 0.5469), Valid: (LOSS: 0.1406, MAE: 0.1406, RMSE: 0.1820, R2: 0.4805), PNorm: 171.4613, GNorm: 0.9804
[8/299] timecost: 63.05, lr: 0.000030, Train: (LOSS: 0.1284, MAE: 0.1284, RMSE: 0.1772, R2: 0.5393), Valid: (LOSS: 0.1508, MAE: 0.1508, RMSE: 0.1990, R2: 0.3809), PNorm: 171.2046, GNorm: 5.9787
[9/299] timecost: 63.25, lr: 0.000030, Train: (LOSS: 0.1233, MAE: 0.1233, RMSE: 0.1715, R2: 0.5622), Valid: (LOSS: 0.1176, MAE: 0.1176, RMSE: 0.1665, R2: 0.5665), PNorm: 170.9611, GNorm: 2.0359
[10/299] timecost: 65.16, lr: 0.000030, Train: (LOSS: 0.1186, MAE: 0.1186, RMSE: 0.1664, R2: 0.5895), Valid: (LOSS: 0.1258, MAE: 0.1258, RMSE: 0.1713, R2: 0.5416), PNorm: 170.7271, GNorm: 2.4319
[11/299] timecost: 64.98, lr: 0.000030, Train: (LOSS: 0.1156, MAE: 0.1156, RMSE: 0.1619, R2: 0.6168), Valid: (LOSS: 0.1153, MAE: 0.1153, RMSE: 0.1550, R2: 0.6273), PNorm: 170.5030, GNorm: 1.4902
[12/299] timecost: 64.89, lr: 0.000030, Train: (LOSS: 0.1160, MAE: 0.1160, RMSE: 0.1642, R2: 0.6022), Valid: (LOSS: 0.1314, MAE: 0.1314, RMSE: 0.1856, R2: 0.4496), PNorm: 170.2884, GNorm: 1.4474
[13/299] timecost: 65.19, lr: 0.000030, Train: (LOSS: 0.1125, MAE: 0.1125, RMSE: 0.1608, R2: 0.6172), Valid: (LOSS: 0.1076, MAE: 0.1076, RMSE: 0.1502, R2: 0.6339), PNorm: 170.0790, GNorm: 1.1106
[14/299] timecost: 65.17, lr: 0.000030, Train: (LOSS: 0.1094, MAE: 0.1094, RMSE: 0.1574, R2: 0.6379), Valid: (LOSS: 0.1136, MAE: 0.1136, RMSE: 0.1624, R2: 0.5911), PNorm: 169.8760, GNorm: 1.2761
[15/299] timecost: 64.94, lr: 0.000030, Train: (LOSS: 0.1060, MAE: 0.1060, RMSE: 0.1536, R2: 0.6437), Valid: (LOSS: 0.1148, MAE: 0.1148, RMSE: 0.1549, R2: 0.6188), PNorm: 169.6777, GNorm: 1.3132
[16/299] timecost: 64.89, lr: 0.000030, Train: (LOSS: 0.1103, MAE: 0.1103, RMSE: 0.1582, R2: 0.6345), Valid: (LOSS: 0.1172, MAE: 0.1172, RMSE: 0.1567, R2: 0.6118), PNorm: 169.4865, GNorm: 3.0558
[17/299] timecost: 65.17, lr: 0.000030, Train: (LOSS: 0.1052, MAE: 0.1052, RMSE: 0.1514, R2: 0.6550), Valid: (LOSS: 0.1077, MAE: 0.1077, RMSE: 0.1520, R2: 0.6278), PNorm: 169.3018, GNorm: 1.9958
[18/299] timecost: 65.03, lr: 0.000030, Train: (LOSS: 0.1020, MAE: 0.1020, RMSE: 0.1490, R2: 0.6672), Valid: (LOSS: 0.1101, MAE: 0.1101, RMSE: 0.1536, R2: 0.6182), PNorm: 169.1211, GNorm: 2.3741
[19/299] timecost: 64.55, lr: 0.000030, Train: (LOSS: 0.1153, MAE: 0.1153, RMSE: 0.1632, R2: 0.6020), Valid: (LOSS: 0.1190, MAE: 0.1190, RMSE: 0.1576, R2: 0.6084), PNorm: 168.9453, GNorm: 4.8352
[20/299] timecost: 64.76, lr: 0.000030, Train: (LOSS: 0.1013, MAE: 0.1013, RMSE: 0.1454, R2: 0.6809), Valid: (LOSS: 0.1158, MAE: 0.1158, RMSE: 0.1711, R2: 0.5270), PNorm: 168.7752, GNorm: 2.1207
[21/299] timecost: 64.18, lr: 0.000030, Train: (LOSS: 0.0967, MAE: 0.0967, RMSE: 0.1405, R2: 0.7081), Valid: (LOSS: 0.1004, MAE: 0.1004, RMSE: 0.1419, R2: 0.6644), PNorm: 168.6076, GNorm: 2.4875
[22/299] timecost: 63.37, lr: 0.000030, Train: (LOSS: 0.0969, MAE: 0.0969, RMSE: 0.1420, R2: 0.7007), Valid: (LOSS: 0.0966, MAE: 0.0966, RMSE: 0.1333, R2: 0.7216), PNorm: 168.4431, GNorm: 1.5573
[23/299] timecost: 63.63, lr: 0.000030, Train: (LOSS: 0.0894, MAE: 0.0894, RMSE: 0.1307, R2: 0.7439), Valid: (LOSS: 0.0975, MAE: 0.0975, RMSE: 0.1326, R2: 0.7176), PNorm: 168.2805, GNorm: 2.6028
[24/299] timecost: 63.44, lr: 0.000030, Train: (LOSS: 0.0908, MAE: 0.0908, RMSE: 0.1339, R2: 0.7320), Valid: (LOSS: 0.0940, MAE: 0.0940, RMSE: 0.1362, R2: 0.7062), PNorm: 168.1225, GNorm: 1.1640
[25/299] timecost: 63.51, lr: 0.000030, Train: (LOSS: 0.0931, MAE: 0.0931, RMSE: 0.1367, R2: 0.7162), Valid: (LOSS: 0.0909, MAE: 0.0909, RMSE: 0.1289, R2: 0.7202), PNorm: 167.9690, GNorm: 3.3706
[26/299] timecost: 63.72, lr: 0.000030, Train: (LOSS: 0.0829, MAE: 0.0829, RMSE: 0.1232, R2: 0.7740), Valid: (LOSS: 0.0868, MAE: 0.0868, RMSE: 0.1225, R2: 0.7591), PNorm: 167.8150, GNorm: 1.6114
[27/299] timecost: 64.17, lr: 0.000030, Train: (LOSS: 0.0803, MAE: 0.0803, RMSE: 0.1206, R2: 0.7840), Valid: (LOSS: 0.0871, MAE: 0.0871, RMSE: 0.1219, R2: 0.7568), PNorm: 167.6639, GNorm: 1.6399
[28/299] timecost: 64.63, lr: 0.000030, Train: (LOSS: 0.0774, MAE: 0.0774, RMSE: 0.1178, R2: 0.7896), Valid: (LOSS: 0.0842, MAE: 0.0842, RMSE: 0.1213, R2: 0.7689), PNorm: 167.5135, GNorm: 1.6005
[29/299] timecost: 64.49, lr: 0.000030, Train: (LOSS: 0.0800, MAE: 0.0800, RMSE: 0.1213, R2: 0.7739), Valid: (LOSS: 0.0802, MAE: 0.0802, RMSE: 0.1132, R2: 0.7882), PNorm: 167.3655, GNorm: 1.7757
[30/299] timecost: 64.35, lr: 0.000030, Train: (LOSS: 0.0798, MAE: 0.0798, RMSE: 0.1215, R2: 0.7743), Valid: (LOSS: 0.0795, MAE: 0.0795, RMSE: 0.1148, R2: 0.7928), PNorm: 167.2203, GNorm: 1.2156
[31/299] timecost: 64.62, lr: 0.000030, Train: (LOSS: 0.0733, MAE: 0.0733, RMSE: 0.1113, R2: 0.8124), Valid: (LOSS: 0.0772, MAE: 0.0772, RMSE: 0.1152, R2: 0.7784), PNorm: 167.0761, GNorm: 3.2915
[32/299] timecost: 64.63, lr: 0.000030, Train: (LOSS: 0.0766, MAE: 0.0766, RMSE: 0.1164, R2: 0.7950), Valid: (LOSS: 0.0809, MAE: 0.0809, RMSE: 0.1186, R2: 0.7692), PNorm: 166.9355, GNorm: 1.3887
[33/299] timecost: 64.51, lr: 0.000030, Train: (LOSS: 0.0730, MAE: 0.0730, RMSE: 0.1117, R2: 0.8107), Valid: (LOSS: 0.0725, MAE: 0.0725, RMSE: 0.1074, R2: 0.8085), PNorm: 166.7974, GNorm: 4.6965
[34/299] timecost: 64.59, lr: 0.000030, Train: (LOSS: 0.0713, MAE: 0.0713, RMSE: 0.1088, R2: 0.8177), Valid: (LOSS: 0.0737, MAE: 0.0737, RMSE: 0.1071, R2: 0.8091), PNorm: 166.6605, GNorm: 3.3219
[35/299] timecost: 64.43, lr: 0.000030, Train: (LOSS: 0.0666, MAE: 0.0666, RMSE: 0.1022, R2: 0.8399), Valid: (LOSS: 0.0783, MAE: 0.0783, RMSE: 0.1179, R2: 0.7761), PNorm: 166.5252, GNorm: 1.3231
[36/299] timecost: 64.43, lr: 0.000030, Train: (LOSS: 0.0672, MAE: 0.0672, RMSE: 0.1037, R2: 0.8370), Valid: (LOSS: 0.0715, MAE: 0.0715, RMSE: 0.1032, R2: 0.8227), PNorm: 166.3915, GNorm: 1.1475
[37/299] timecost: 64.79, lr: 0.000030, Train: (LOSS: 0.0636, MAE: 0.0636, RMSE: 0.0988, R2: 0.8489), Valid: (LOSS: 0.0627, MAE: 0.0627, RMSE: 0.0965, R2: 0.8432), PNorm: 166.2590, GNorm: 4.4677
[38/299] timecost: 64.98, lr: 0.000030, Train: (LOSS: 0.0592, MAE: 0.0592, RMSE: 0.0933, R2: 0.8652), Valid: (LOSS: 0.0575, MAE: 0.0575, RMSE: 0.0835, R2: 0.8791), PNorm: 166.1259, GNorm: 1.2727
[39/299] timecost: 64.83, lr: 0.000030, Train: (LOSS: 0.0601, MAE: 0.0601, RMSE: 0.0941, R2: 0.8630), Valid: (LOSS: 0.0595, MAE: 0.0595, RMSE: 0.0886, R2: 0.8667), PNorm: 165.9963, GNorm: 2.0717
[40/299] timecost: 64.23, lr: 0.000030, Train: (LOSS: 0.0583, MAE: 0.0583, RMSE: 0.0911, R2: 0.8698), Valid: (LOSS: 0.0586, MAE: 0.0586, RMSE: 0.0867, R2: 0.8725), PNorm: 165.8662, GNorm: 1.2792
[41/299] timecost: 63.73, lr: 0.000030, Train: (LOSS: 0.0571, MAE: 0.0571, RMSE: 0.0896, R2: 0.8740), Valid: (LOSS: 0.0620, MAE: 0.0620, RMSE: 0.0900, R2: 0.8635), PNorm: 165.7379, GNorm: 2.4230
[42/299] timecost: 64.63, lr: 0.000030, Train: (LOSS: 0.0586, MAE: 0.0586, RMSE: 0.0899, R2: 0.8726), Valid: (LOSS: 0.0606, MAE: 0.0606, RMSE: 0.0934, R2: 0.8446), PNorm: 165.6108, GNorm: 1.9170
[43/299] timecost: 64.47, lr: 0.000030, Train: (LOSS: 0.0560, MAE: 0.0560, RMSE: 0.0890, R2: 0.8764), Valid: (LOSS: 0.0609, MAE: 0.0609, RMSE: 0.0927, R2: 0.8491), PNorm: 165.4851, GNorm: 1.2378
[44/299] timecost: 64.74, lr: 0.000030, Train: (LOSS: 0.0515, MAE: 0.0515, RMSE: 0.0828, R2: 0.8940), Valid: (LOSS: 0.0538, MAE: 0.0538, RMSE: 0.0797, R2: 0.8923), PNorm: 165.3569, GNorm: 1.5893
[45/299] timecost: 64.57, lr: 0.000030, Train: (LOSS: 0.0530, MAE: 0.0530, RMSE: 0.0854, R2: 0.8866), Valid: (LOSS: 0.0588, MAE: 0.0588, RMSE: 0.0894, R2: 0.8670), PNorm: 165.2320, GNorm: 1.4070
[46/299] timecost: 63.99, lr: 0.000030, Train: (LOSS: 0.0522, MAE: 0.0522, RMSE: 0.0820, R2: 0.8941), Valid: (LOSS: 0.0556, MAE: 0.0556, RMSE: 0.0837, R2: 0.8780), PNorm: 165.1072, GNorm: 1.2070
[47/299] timecost: 63.92, lr: 0.000030, Train: (LOSS: 0.0499, MAE: 0.0499, RMSE: 0.0810, R2: 0.8968), Valid: (LOSS: 0.0533, MAE: 0.0533, RMSE: 0.0813, R2: 0.8822), PNorm: 164.9812, GNorm: 2.6551
[48/299] timecost: 63.12, lr: 0.000030, Train: (LOSS: 0.0507, MAE: 0.0507, RMSE: 0.0793, R2: 0.9030), Valid: (LOSS: 0.0517, MAE: 0.0517, RMSE: 0.0791, R2: 0.8891), PNorm: 164.8563, GNorm: 1.7795
[49/299] timecost: 62.86, lr: 0.000030, Train: (LOSS: 0.0492, MAE: 0.0492, RMSE: 0.0776, R2: 0.9010), Valid: (LOSS: 0.0578, MAE: 0.0578, RMSE: 0.0870, R2: 0.8704), PNorm: 164.7327, GNorm: 1.9871
[50/299] timecost: 62.96, lr: 0.000030, Train: (LOSS: 0.0494, MAE: 0.0494, RMSE: 0.0788, R2: 0.9028), Valid: (LOSS: 0.0507, MAE: 0.0507, RMSE: 0.0767, R2: 0.8986), PNorm: 164.6079, GNorm: 1.6135
[51/299] timecost: 64.15, lr: 0.000030, Train: (LOSS: 0.0494, MAE: 0.0494, RMSE: 0.0787, R2: 0.9001), Valid: (LOSS: 0.0526, MAE: 0.0526, RMSE: 0.0791, R2: 0.8913), PNorm: 164.4828, GNorm: 5.1690
[52/299] timecost: 64.30, lr: 0.000030, Train: (LOSS: 0.0470, MAE: 0.0470, RMSE: 0.0750, R2: 0.9095), Valid: (LOSS: 0.0488, MAE: 0.0488, RMSE: 0.0775, R2: 0.8935), PNorm: 164.3595, GNorm: 1.0984
[53/299] timecost: 64.68, lr: 0.000030, Train: (LOSS: 0.0466, MAE: 0.0466, RMSE: 0.0742, R2: 0.9110), Valid: (LOSS: 0.0500, MAE: 0.0500, RMSE: 0.0738, R2: 0.9072), PNorm: 164.2349, GNorm: 1.2009
[54/299] timecost: 63.94, lr: 0.000030, Train: (LOSS: 0.0448, MAE: 0.0448, RMSE: 0.0726, R2: 0.9151), Valid: (LOSS: 0.0488, MAE: 0.0488, RMSE: 0.0772, R2: 0.8966), PNorm: 164.1094, GNorm: 1.6681
[55/299] timecost: 63.35, lr: 0.000030, Train: (LOSS: 0.0438, MAE: 0.0438, RMSE: 0.0711, R2: 0.9137), Valid: (LOSS: 0.0506, MAE: 0.0506, RMSE: 0.0775, R2: 0.8950), PNorm: 163.9840, GNorm: 0.8342
[56/299] timecost: 63.52, lr: 0.000030, Train: (LOSS: 0.0452, MAE: 0.0452, RMSE: 0.0729, R2: 0.9167), Valid: (LOSS: 0.0477, MAE: 0.0477, RMSE: 0.0727, R2: 0.9070), PNorm: 163.8591, GNorm: 1.0302
[57/299] timecost: 64.29, lr: 0.000030, Train: (LOSS: 0.0430, MAE: 0.0430, RMSE: 0.0704, R2: 0.9191), Valid: (LOSS: 0.0495, MAE: 0.0495, RMSE: 0.0753, R2: 0.8987), PNorm: 163.7352, GNorm: 1.8955
[58/299] timecost: 64.56, lr: 0.000030, Train: (LOSS: 0.0425, MAE: 0.0425, RMSE: 0.0697, R2: 0.9228), Valid: (LOSS: 0.0489, MAE: 0.0489, RMSE: 0.0734, R2: 0.9075), PNorm: 163.6098, GNorm: 1.3321
[59/299] timecost: 64.72, lr: 0.000030, Train: (LOSS: 0.0430, MAE: 0.0430, RMSE: 0.0691, R2: 0.9209), Valid: (LOSS: 0.0471, MAE: 0.0471, RMSE: 0.0718, R2: 0.9085), PNorm: 163.4858, GNorm: 1.2937
[60/299] timecost: 64.56, lr: 0.000030, Train: (LOSS: 0.0417, MAE: 0.0417, RMSE: 0.0677, R2: 0.9234), Valid: (LOSS: 0.0516, MAE: 0.0516, RMSE: 0.0790, R2: 0.8938), PNorm: 163.3614, GNorm: 1.3174
[61/299] timecost: 63.92, lr: 0.000030, Train: (LOSS: 0.0414, MAE: 0.0414, RMSE: 0.0675, R2: 0.9258), Valid: (LOSS: 0.0465, MAE: 0.0465, RMSE: 0.0712, R2: 0.9104), PNorm: 163.2371, GNorm: 2.3087
[62/299] timecost: 64.53, lr: 0.000030, Train: (LOSS: 0.0427, MAE: 0.0427, RMSE: 0.0675, R2: 0.9275), Valid: (LOSS: 0.0465, MAE: 0.0465, RMSE: 0.0726, R2: 0.9073), PNorm: 163.1135, GNorm: 1.3110
[63/299] timecost: 64.23, lr: 0.000030, Train: (LOSS: 0.0425, MAE: 0.0425, RMSE: 0.0683, R2: 0.9227), Valid: (LOSS: 0.0476, MAE: 0.0476, RMSE: 0.0710, R2: 0.9126), PNorm: 162.9895, GNorm: 1.4282
[64/299] timecost: 64.36, lr: 0.000030, Train: (LOSS: 0.0405, MAE: 0.0405, RMSE: 0.0665, R2: 0.9281), Valid: (LOSS: 0.0465, MAE: 0.0465, RMSE: 0.0710, R2: 0.9100), PNorm: 162.8646, GNorm: 1.7705
[65/299] timecost: 65.02, lr: 0.000030, Train: (LOSS: 0.0403, MAE: 0.0403, RMSE: 0.0658, R2: 0.9313), Valid: (LOSS: 0.0461, MAE: 0.0461, RMSE: 0.0693, R2: 0.9155), PNorm: 162.7413, GNorm: 1.2171
[66/299] timecost: 64.60, lr: 0.000030, Train: (LOSS: 0.0410, MAE: 0.0410, RMSE: 0.0651, R2: 0.9318), Valid: (LOSS: 0.0486, MAE: 0.0486, RMSE: 0.0737, R2: 0.9076), PNorm: 162.6171, GNorm: 1.8956
[67/299] timecost: 64.31, lr: 0.000030, Train: (LOSS: 0.0421, MAE: 0.0421, RMSE: 0.0673, R2: 0.9291), Valid: (LOSS: 0.0467, MAE: 0.0467, RMSE: 0.0711, R2: 0.9090), PNorm: 162.4939, GNorm: 2.4992
[68/299] timecost: 63.44, lr: 0.000030, Train: (LOSS: 0.0385, MAE: 0.0385, RMSE: 0.0624, R2: 0.9367), Valid: (LOSS: 0.0465, MAE: 0.0465, RMSE: 0.0687, R2: 0.9169), PNorm: 162.3700, GNorm: 0.9103
[69/299] timecost: 63.72, lr: 0.000030, Train: (LOSS: 0.0387, MAE: 0.0387, RMSE: 0.0626, R2: 0.9365), Valid: (LOSS: 0.0441, MAE: 0.0441, RMSE: 0.0668, R2: 0.9213), PNorm: 162.2463, GNorm: 1.2279
[70/299] timecost: 63.33, lr: 0.000030, Train: (LOSS: 0.0386, MAE: 0.0386, RMSE: 0.0643, R2: 0.9341), Valid: (LOSS: 0.0501, MAE: 0.0501, RMSE: 0.0758, R2: 0.9009), PNorm: 162.1218, GNorm: 1.4035
[71/299] timecost: 63.32, lr: 0.000030, Train: (LOSS: 0.0387, MAE: 0.0387, RMSE: 0.0629, R2: 0.9314), Valid: (LOSS: 0.0453, MAE: 0.0453, RMSE: 0.0690, R2: 0.9181), PNorm: 161.9987, GNorm: 3.1059
[72/299] timecost: 64.92, lr: 0.000030, Train: (LOSS: 0.0409, MAE: 0.0409, RMSE: 0.0661, R2: 0.9233), Valid: (LOSS: 0.0451, MAE: 0.0451, RMSE: 0.0676, R2: 0.9191), PNorm: 161.8755, GNorm: 0.9474
[73/299] timecost: 64.67, lr: 0.000030, Train: (LOSS: 0.0383, MAE: 0.0383, RMSE: 0.0617, R2: 0.9398), Valid: (LOSS: 0.0443, MAE: 0.0443, RMSE: 0.0660, R2: 0.9231), PNorm: 161.7529, GNorm: 1.0589
[74/299] timecost: 64.88, lr: 0.000030, Train: (LOSS: 0.0360, MAE: 0.0360, RMSE: 0.0592, R2: 0.9418), Valid: (LOSS: 0.0436, MAE: 0.0436, RMSE: 0.0667, R2: 0.9223), PNorm: 161.6290, GNorm: 0.8918
[75/299] timecost: 64.48, lr: 0.000030, Train: (LOSS: 0.0372, MAE: 0.0372, RMSE: 0.0604, R2: 0.9413), Valid: (LOSS: 0.0426, MAE: 0.0426, RMSE: 0.0653, R2: 0.9235), PNorm: 161.5053, GNorm: 2.5919
[76/299] timecost: 64.74, lr: 0.000030, Train: (LOSS: 0.0358, MAE: 0.0358, RMSE: 0.0578, R2: 0.9465), Valid: (LOSS: 0.0447, MAE: 0.0447, RMSE: 0.0672, R2: 0.9223), PNorm: 161.3817, GNorm: 0.6602
[77/299] timecost: 64.41, lr: 0.000030, Train: (LOSS: 0.0376, MAE: 0.0376, RMSE: 0.0603, R2: 0.9395), Valid: (LOSS: 0.0409, MAE: 0.0409, RMSE: 0.0633, R2: 0.9275), PNorm: 161.2585, GNorm: 0.9352
[78/299] timecost: 64.31, lr: 0.000030, Train: (LOSS: 0.0367, MAE: 0.0367, RMSE: 0.0587, R2: 0.9437), Valid: (LOSS: 0.0417, MAE: 0.0417, RMSE: 0.0633, R2: 0.9296), PNorm: 161.1344, GNorm: 2.2405
[79/299] timecost: 64.65, lr: 0.000030, Train: (LOSS: 0.0348, MAE: 0.0348, RMSE: 0.0565, R2: 0.9483), Valid: (LOSS: 0.0416, MAE: 0.0416, RMSE: 0.0627, R2: 0.9325), PNorm: 161.0112, GNorm: 1.9124
[80/299] timecost: 64.51, lr: 0.000030, Train: (LOSS: 0.0360, MAE: 0.0360, RMSE: 0.0575, R2: 0.9427), Valid: (LOSS: 0.0434, MAE: 0.0434, RMSE: 0.0670, R2: 0.9197), PNorm: 160.8873, GNorm: 1.0072
[81/299] timecost: 64.47, lr: 0.000030, Train: (LOSS: 0.0357, MAE: 0.0357, RMSE: 0.0574, R2: 0.9476), Valid: (LOSS: 0.0406, MAE: 0.0406, RMSE: 0.0634, R2: 0.9288), PNorm: 160.7634, GNorm: 1.0001
[82/299] timecost: 62.98, lr: 0.000030, Train: (LOSS: 0.0335, MAE: 0.0335, RMSE: 0.0553, R2: 0.9490), Valid: (LOSS: 0.0416, MAE: 0.0416, RMSE: 0.0641, R2: 0.9267), PNorm: 160.6387, GNorm: 0.8456
[83/299] timecost: 63.43, lr: 0.000030, Train: (LOSS: 0.0333, MAE: 0.0333, RMSE: 0.0544, R2: 0.9514), Valid: (LOSS: 0.0414, MAE: 0.0414, RMSE: 0.0636, R2: 0.9239), PNorm: 160.5152, GNorm: 1.4538
[84/299] timecost: 63.96, lr: 0.000030, Train: (LOSS: 0.0346, MAE: 0.0346, RMSE: 0.0559, R2: 0.9489), Valid: (LOSS: 0.0437, MAE: 0.0437, RMSE: 0.0670, R2: 0.9216), PNorm: 160.3911, GNorm: 1.1264
[85/299] timecost: 64.40, lr: 0.000030, Train: (LOSS: 0.0352, MAE: 0.0352, RMSE: 0.0555, R2: 0.9495), Valid: (LOSS: 0.0406, MAE: 0.0406, RMSE: 0.0644, R2: 0.9249), PNorm: 160.2682, GNorm: 1.3652
[86/299] timecost: 64.01, lr: 0.000030, Train: (LOSS: 0.0336, MAE: 0.0336, RMSE: 0.0536, R2: 0.9526), Valid: (LOSS: 0.0411, MAE: 0.0411, RMSE: 0.0639, R2: 0.9267), PNorm: 160.1448, GNorm: 1.1016
[87/299] timecost: 64.08, lr: 0.000030, Train: (LOSS: 0.0328, MAE: 0.0328, RMSE: 0.0538, R2: 0.9534), Valid: (LOSS: 0.0426, MAE: 0.0426, RMSE: 0.0643, R2: 0.9277), PNorm: 160.0205, GNorm: 1.4905
[88/299] timecost: 64.10, lr: 0.000030, Train: (LOSS: 0.0345, MAE: 0.0345, RMSE: 0.0558, R2: 0.9496), Valid: (LOSS: 0.0375, MAE: 0.0375, RMSE: 0.0576, R2: 0.9399), PNorm: 159.8975, GNorm: 1.2239
[89/299] timecost: 63.89, lr: 0.000030, Train: (LOSS: 0.0334, MAE: 0.0334, RMSE: 0.0537, R2: 0.9515), Valid: (LOSS: 0.0415, MAE: 0.0415, RMSE: 0.0635, R2: 0.9269), PNorm: 159.7739, GNorm: 1.3888
[90/299] timecost: 62.96, lr: 0.000030, Train: (LOSS: 0.0325, MAE: 0.0325, RMSE: 0.0528, R2: 0.9516), Valid: (LOSS: 0.0418, MAE: 0.0418, RMSE: 0.0641, R2: 0.9272), PNorm: 159.6511, GNorm: 1.0618
[91/299] timecost: 62.43, lr: 0.000030, Train: (LOSS: 0.0329, MAE: 0.0329, RMSE: 0.0536, R2: 0.9515), Valid: (LOSS: 0.0412, MAE: 0.0412, RMSE: 0.0629, R2: 0.9295), PNorm: 159.5266, GNorm: 1.1426
[92/299] timecost: 62.58, lr: 0.000030, Train: (LOSS: 0.0319, MAE: 0.0319, RMSE: 0.0513, R2: 0.9565), Valid: (LOSS: 0.0387, MAE: 0.0387, RMSE: 0.0600, R2: 0.9342), PNorm: 159.4033, GNorm: 0.7078
[93/299] timecost: 62.55, lr: 0.000030, Train: (LOSS: 0.0324, MAE: 0.0324, RMSE: 0.0524, R2: 0.9531), Valid: (LOSS: 0.0397, MAE: 0.0397, RMSE: 0.0631, R2: 0.9287), PNorm: 159.2791, GNorm: 0.8376
[94/299] timecost: 62.26, lr: 0.000030, Train: (LOSS: 0.0324, MAE: 0.0324, RMSE: 0.0524, R2: 0.9518), Valid: (LOSS: 0.0392, MAE: 0.0392, RMSE: 0.0606, R2: 0.9327), PNorm: 159.1552, GNorm: 0.9709
[95/299] timecost: 62.43, lr: 0.000030, Train: (LOSS: 0.0304, MAE: 0.0304, RMSE: 0.0500, R2: 0.9570), Valid: (LOSS: 0.0377, MAE: 0.0377, RMSE: 0.0581, R2: 0.9371), PNorm: 159.0297, GNorm: 1.0749
[96/299] timecost: 62.56, lr: 0.000030, Train: (LOSS: 0.0304, MAE: 0.0304, RMSE: 0.0500, R2: 0.9565), Valid: (LOSS: 0.0403, MAE: 0.0403, RMSE: 0.0616, R2: 0.9318), PNorm: 158.9046, GNorm: 1.5723
[97/299] timecost: 62.55, lr: 0.000030, Train: (LOSS: 0.0312, MAE: 0.0312, RMSE: 0.0516, R2: 0.9539), Valid: (LOSS: 0.0399, MAE: 0.0399, RMSE: 0.0612, R2: 0.9330), PNorm: 158.7805, GNorm: 1.0544
[98/299] timecost: 62.94, lr: 0.000030, Train: (LOSS: 0.0310, MAE: 0.0310, RMSE: 0.0503, R2: 0.9574), Valid: (LOSS: 0.0388, MAE: 0.0388, RMSE: 0.0605, R2: 0.9345), PNorm: 158.6555, GNorm: 0.9391
[99/299] timecost: 62.38, lr: 0.000030, Train: (LOSS: 0.0291, MAE: 0.0291, RMSE: 0.0478, R2: 0.9588), Valid: (LOSS: 0.0389, MAE: 0.0389, RMSE: 0.0613, R2: 0.9311), PNorm: 158.5297, GNorm: 0.8414
[100/299] timecost: 62.58, lr: 0.000030, Train: (LOSS: 0.0312, MAE: 0.0312, RMSE: 0.0513, R2: 0.9550), Valid: (LOSS: 0.0366, MAE: 0.0366, RMSE: 0.0576, R2: 0.9379), PNorm: 158.4041, GNorm: 1.6234
[101/299] timecost: 63.73, lr: 0.000030, Train: (LOSS: 0.0295, MAE: 0.0295, RMSE: 0.0486, R2: 0.9608), Valid: (LOSS: 0.0399, MAE: 0.0399, RMSE: 0.0622, R2: 0.9307), PNorm: 158.2794, GNorm: 1.2392
[102/299] timecost: 63.26, lr: 0.000030, Train: (LOSS: 0.0283, MAE: 0.0283, RMSE: 0.0467, R2: 0.9616), Valid: (LOSS: 0.0396, MAE: 0.0396, RMSE: 0.0610, R2: 0.9342), PNorm: 158.1529, GNorm: 1.0135
[103/299] timecost: 62.55, lr: 0.000030, Train: (LOSS: 0.0311, MAE: 0.0311, RMSE: 0.0498, R2: 0.9564), Valid: (LOSS: 0.0425, MAE: 0.0425, RMSE: 0.0674, R2: 0.9203), PNorm: 158.0266, GNorm: 2.2447
[104/299] timecost: 62.61, lr: 0.000030, Train: (LOSS: 0.0289, MAE: 0.0289, RMSE: 0.0472, R2: 0.9605), Valid: (LOSS: 0.0389, MAE: 0.0389, RMSE: 0.0609, R2: 0.9343), PNorm: 157.9018, GNorm: 1.1186
[105/299] timecost: 62.48, lr: 0.000030, Train: (LOSS: 0.0285, MAE: 0.0285, RMSE: 0.0470, R2: 0.9611), Valid: (LOSS: 0.0373, MAE: 0.0373, RMSE: 0.0598, R2: 0.9335), PNorm: 157.7752, GNorm: 1.5268
[106/299] timecost: 62.31, lr: 0.000030, Train: (LOSS: 0.0279, MAE: 0.0279, RMSE: 0.0459, R2: 0.9642), Valid: (LOSS: 0.0364, MAE: 0.0364, RMSE: 0.0581, R2: 0.9395), PNorm: 157.6493, GNorm: 0.9114
[107/299] timecost: 62.99, lr: 0.000030, Train: (LOSS: 0.0279, MAE: 0.0279, RMSE: 0.0462, R2: 0.9650), Valid: (LOSS: 0.0388, MAE: 0.0388, RMSE: 0.0628, R2: 0.9270), PNorm: 157.5228, GNorm: 1.3434
[108/299] timecost: 62.46, lr: 0.000030, Train: (LOSS: 0.0293, MAE: 0.0293, RMSE: 0.0476, R2: 0.9616), Valid: (LOSS: 0.0397, MAE: 0.0397, RMSE: 0.0625, R2: 0.9302), PNorm: 157.3966, GNorm: 1.0155
[109/299] timecost: 62.42, lr: 0.000030, Train: (LOSS: 0.0318, MAE: 0.0318, RMSE: 0.0505, R2: 0.9566), Valid: (LOSS: 0.0384, MAE: 0.0384, RMSE: 0.0627, R2: 0.9294), PNorm: 157.2728, GNorm: 1.3921
[110/299] timecost: 62.36, lr: 0.000030, Train: (LOSS: 0.0281, MAE: 0.0281, RMSE: 0.0459, R2: 0.9642), Valid: (LOSS: 0.0390, MAE: 0.0390, RMSE: 0.0615, R2: 0.9315), PNorm: 157.1466, GNorm: 0.9648
[111/299] timecost: 62.83, lr: 0.000030, Train: (LOSS: 0.0290, MAE: 0.0290, RMSE: 0.0463, R2: 0.9620), Valid: (LOSS: 0.0402, MAE: 0.0402, RMSE: 0.0665, R2: 0.9195), PNorm: 157.0209, GNorm: 0.9645
[112/299] timecost: 62.54, lr: 0.000030, Train: (LOSS: 0.0278, MAE: 0.0278, RMSE: 0.0460, R2: 0.9634), Valid: (LOSS: 0.0384, MAE: 0.0384, RMSE: 0.0590, R2: 0.9357), PNorm: 156.8959, GNorm: 1.4666
[113/299] timecost: 62.69, lr: 0.000030, Train: (LOSS: 0.0283, MAE: 0.0283, RMSE: 0.0444, R2: 0.9632), Valid: (LOSS: 0.0388, MAE: 0.0388, RMSE: 0.0612, R2: 0.9342), PNorm: 156.7700, GNorm: 1.2071
[114/299] timecost: 62.57, lr: 0.000030, Train: (LOSS: 0.0273, MAE: 0.0273, RMSE: 0.0445, R2: 0.9648), Valid: (LOSS: 0.0378, MAE: 0.0378, RMSE: 0.0592, R2: 0.9371), PNorm: 156.6434, GNorm: 0.8524
[115/299] timecost: 62.52, lr: 0.000030, Train: (LOSS: 0.0379, MAE: 0.0379, RMSE: 0.0670, R2: 0.9187), Valid: (LOSS: 0.0555, MAE: 0.0555, RMSE: 0.0915, R2: 0.8574), PNorm: 156.5227, GNorm: 1.4269
[116/299] timecost: 62.41, lr: 0.000030, Train: (LOSS: 0.0321, MAE: 0.0321, RMSE: 0.0514, R2: 0.9543), Valid: (LOSS: 0.0378, MAE: 0.0378, RMSE: 0.0602, R2: 0.9352), PNorm: 156.4008, GNorm: 0.7626
[117/299] timecost: 62.68, lr: 0.000030, Train: (LOSS: 0.0265, MAE: 0.0265, RMSE: 0.0432, R2: 0.9672), Valid: (LOSS: 0.0365, MAE: 0.0365, RMSE: 0.0580, R2: 0.9385), PNorm: 156.2774, GNorm: 0.8336
[118/299] timecost: 62.59, lr: 0.000030, Train: (LOSS: 0.0265, MAE: 0.0265, RMSE: 0.0431, R2: 0.9674), Valid: (LOSS: 0.0382, MAE: 0.0382, RMSE: 0.0609, R2: 0.9327), PNorm: 156.1527, GNorm: 1.3311
[119/299] timecost: 62.57, lr: 0.000030, Train: (LOSS: 0.0262, MAE: 0.0262, RMSE: 0.0428, R2: 0.9683), Valid: (LOSS: 0.0400, MAE: 0.0400, RMSE: 0.0629, R2: 0.9307), PNorm: 156.0286, GNorm: 1.6244
[120/299] timecost: 62.82, lr: 0.000030, Train: (LOSS: 0.0269, MAE: 0.0269, RMSE: 0.0428, R2: 0.9696), Valid: (LOSS: 0.0387, MAE: 0.0387, RMSE: 0.0615, R2: 0.9328), PNorm: 155.9052, GNorm: 1.7774
[121/299] timecost: 62.93, lr: 0.000030, Train: (LOSS: 0.0263, MAE: 0.0263, RMSE: 0.0419, R2: 0.9688), Valid: (LOSS: 0.0355, MAE: 0.0355, RMSE: 0.0549, R2: 0.9426), PNorm: 155.7816, GNorm: 0.8344
[122/299] timecost: 62.53, lr: 0.000030, Train: (LOSS: 0.0266, MAE: 0.0266, RMSE: 0.0424, R2: 0.9689), Valid: (LOSS: 0.0367, MAE: 0.0367, RMSE: 0.0584, R2: 0.9390), PNorm: 155.6582, GNorm: 1.1938
[123/299] timecost: 62.31, lr: 0.000030, Train: (LOSS: 0.0243, MAE: 0.0243, RMSE: 0.0398, R2: 0.9719), Valid: (LOSS: 0.0409, MAE: 0.0409, RMSE: 0.0616, R2: 0.9334), PNorm: 155.5339, GNorm: 1.0499
[124/299] timecost: 62.36, lr: 0.000030, Train: (LOSS: 0.0263, MAE: 0.0263, RMSE: 0.0420, R2: 0.9695), Valid: (LOSS: 0.0361, MAE: 0.0361, RMSE: 0.0572, R2: 0.9406), PNorm: 155.4098, GNorm: 1.2365
[125/299] timecost: 63.49, lr: 0.000030, Train: (LOSS: 0.0248, MAE: 0.0248, RMSE: 0.0398, R2: 0.9724), Valid: (LOSS: 0.0369, MAE: 0.0369, RMSE: 0.0579, R2: 0.9405), PNorm: 155.2860, GNorm: 0.7738
[126/299] timecost: 63.49, lr: 0.000030, Train: (LOSS: 0.0248, MAE: 0.0248, RMSE: 0.0393, R2: 0.9739), Valid: (LOSS: 0.0377, MAE: 0.0377, RMSE: 0.0583, R2: 0.9384), PNorm: 155.1610, GNorm: 0.9790
[127/299] timecost: 63.95, lr: 0.000030, Train: (LOSS: 0.0245, MAE: 0.0245, RMSE: 0.0395, R2: 0.9730), Valid: (LOSS: 0.0358, MAE: 0.0358, RMSE: 0.0569, R2: 0.9413), PNorm: 155.0367, GNorm: 0.9484
[128/299] timecost: 63.66, lr: 0.000030, Train: (LOSS: 0.0233, MAE: 0.0233, RMSE: 0.0372, R2: 0.9747), Valid: (LOSS: 0.0362, MAE: 0.0362, RMSE: 0.0575, R2: 0.9401), PNorm: 154.9121, GNorm: 1.5794
[129/299] timecost: 63.93, lr: 0.000030, Train: (LOSS: 0.0232, MAE: 0.0232, RMSE: 0.0366, R2: 0.9758), Valid: (LOSS: 0.0376, MAE: 0.0376, RMSE: 0.0593, R2: 0.9376), PNorm: 154.7864, GNorm: 0.9357
[130/299] timecost: 62.87, lr: 0.000030, Train: (LOSS: 0.0238, MAE: 0.0238, RMSE: 0.0379, R2: 0.9748), Valid: (LOSS: 0.0368, MAE: 0.0368, RMSE: 0.0595, R2: 0.9359), PNorm: 154.6624, GNorm: 0.8435
[131/299] timecost: 62.58, lr: 0.000030, Train: (LOSS: 0.0237, MAE: 0.0237, RMSE: 0.0371, R2: 0.9751), Valid: (LOSS: 0.0348, MAE: 0.0348, RMSE: 0.0559, R2: 0.9433), PNorm: 154.5378, GNorm: 1.0094
[132/299] timecost: 62.65, lr: 0.000030, Train: (LOSS: 0.0236, MAE: 0.0236, RMSE: 0.0379, R2: 0.9738), Valid: (LOSS: 0.0363, MAE: 0.0363, RMSE: 0.0576, R2: 0.9407), PNorm: 154.4131, GNorm: 1.5464
[133/299] timecost: 62.82, lr: 0.000030, Train: (LOSS: 0.0228, MAE: 0.0228, RMSE: 0.0364, R2: 0.9765), Valid: (LOSS: 0.0363, MAE: 0.0363, RMSE: 0.0576, R2: 0.9401), PNorm: 154.2889, GNorm: 0.9140
[134/299] timecost: 62.65, lr: 0.000030, Train: (LOSS: 0.0228, MAE: 0.0228, RMSE: 0.0361, R2: 0.9771), Valid: (LOSS: 0.0356, MAE: 0.0356, RMSE: 0.0577, R2: 0.9395), PNorm: 154.1637, GNorm: 1.6706
[135/299] timecost: 63.06, lr: 0.000030, Train: (LOSS: 0.0225, MAE: 0.0225, RMSE: 0.0359, R2: 0.9754), Valid: (LOSS: 0.0347, MAE: 0.0347, RMSE: 0.0521, R2: 0.9500), PNorm: 154.0395, GNorm: 1.2359
[136/299] timecost: 64.39, lr: 0.000030, Train: (LOSS: 0.0223, MAE: 0.0223, RMSE: 0.0351, R2: 0.9786), Valid: (LOSS: 0.0367, MAE: 0.0367, RMSE: 0.0560, R2: 0.9436), PNorm: 153.9145, GNorm: 0.8007
[137/299] timecost: 64.09, lr: 0.000030, Train: (LOSS: 0.0244, MAE: 0.0244, RMSE: 0.0394, R2: 0.9717), Valid: (LOSS: 0.0362, MAE: 0.0362, RMSE: 0.0579, R2: 0.9380), PNorm: 153.7927, GNorm: 1.2412
[138/299] timecost: 63.86, lr: 0.000030, Train: (LOSS: 0.0226, MAE: 0.0226, RMSE: 0.0367, R2: 0.9762), Valid: (LOSS: 0.0358, MAE: 0.0358, RMSE: 0.0547, R2: 0.9464), PNorm: 153.6707, GNorm: 0.9440
[139/299] timecost: 64.03, lr: 0.000030, Train: (LOSS: 0.0216, MAE: 0.0216, RMSE: 0.0348, R2: 0.9779), Valid: (LOSS: 0.0390, MAE: 0.0390, RMSE: 0.0603, R2: 0.9370), PNorm: 153.5479, GNorm: 1.0500
[140/299] timecost: 63.31, lr: 0.000030, Train: (LOSS: 0.0214, MAE: 0.0214, RMSE: 0.0346, R2: 0.9790), Valid: (LOSS: 0.0374, MAE: 0.0374, RMSE: 0.0561, R2: 0.9436), PNorm: 153.4249, GNorm: 1.4821
[141/299] timecost: 63.41, lr: 0.000030, Train: (LOSS: 0.0214, MAE: 0.0214, RMSE: 0.0337, R2: 0.9782), Valid: (LOSS: 0.0360, MAE: 0.0360, RMSE: 0.0557, R2: 0.9422), PNorm: 153.3023, GNorm: 1.1077
[142/299] timecost: 63.41, lr: 0.000030, Train: (LOSS: 0.0210, MAE: 0.0210, RMSE: 0.0330, R2: 0.9793), Valid: (LOSS: 0.0353, MAE: 0.0353, RMSE: 0.0564, R2: 0.9425), PNorm: 153.1792, GNorm: 1.2463
[143/299] timecost: 63.05, lr: 0.000030, Train: (LOSS: 0.0203, MAE: 0.0203, RMSE: 0.0322, R2: 0.9800), Valid: (LOSS: 0.0337, MAE: 0.0337, RMSE: 0.0528, R2: 0.9490), PNorm: 153.0560, GNorm: 0.7274
[144/299] timecost: 62.60, lr: 0.000030, Train: (LOSS: 0.0207, MAE: 0.0207, RMSE: 0.0333, R2: 0.9804), Valid: (LOSS: 0.0344, MAE: 0.0344, RMSE: 0.0543, R2: 0.9460), PNorm: 152.9332, GNorm: 0.7494
[145/299] timecost: 64.08, lr: 0.000030, Train: (LOSS: 0.0217, MAE: 0.0217, RMSE: 0.0342, R2: 0.9787), Valid: (LOSS: 0.0339, MAE: 0.0339, RMSE: 0.0531, R2: 0.9492), PNorm: 152.8098, GNorm: 1.3227
[146/299] timecost: 64.86, lr: 0.000030, Train: (LOSS: 0.0225, MAE: 0.0225, RMSE: 0.0359, R2: 0.9767), Valid: (LOSS: 0.0345, MAE: 0.0345, RMSE: 0.0547, R2: 0.9452), PNorm: 152.6877, GNorm: 0.8369
[147/299] timecost: 64.73, lr: 0.000030, Train: (LOSS: 0.0208, MAE: 0.0208, RMSE: 0.0330, R2: 0.9799), Valid: (LOSS: 0.0383, MAE: 0.0383, RMSE: 0.0580, R2: 0.9403), PNorm: 152.5651, GNorm: 0.8960
[148/299] timecost: 64.53, lr: 0.000030, Train: (LOSS: 0.0199, MAE: 0.0199, RMSE: 0.0325, R2: 0.9807), Valid: (LOSS: 0.0361, MAE: 0.0361, RMSE: 0.0582, R2: 0.9383), PNorm: 152.4415, GNorm: 1.3650
[149/299] timecost: 63.94, lr: 0.000030, Train: (LOSS: 0.0202, MAE: 0.0202, RMSE: 0.0322, R2: 0.9797), Valid: (LOSS: 0.0338, MAE: 0.0338, RMSE: 0.0521, R2: 0.9502), PNorm: 152.3178, GNorm: 1.0661
[150/299] timecost: 63.71, lr: 0.000030, Train: (LOSS: 0.0208, MAE: 0.0208, RMSE: 0.0330, R2: 0.9810), Valid: (LOSS: 0.0341, MAE: 0.0341, RMSE: 0.0536, R2: 0.9479), PNorm: 152.1945, GNorm: 0.9244
[151/299] timecost: 63.77, lr: 0.000030, Train: (LOSS: 0.0192, MAE: 0.0192, RMSE: 0.0312, R2: 0.9824), Valid: (LOSS: 0.0357, MAE: 0.0357, RMSE: 0.0567, R2: 0.9432), PNorm: 152.0699, GNorm: 0.9382
[152/299] timecost: 64.04, lr: 0.000030, Train: (LOSS: 0.0200, MAE: 0.0200, RMSE: 0.0316, R2: 0.9799), Valid: (LOSS: 0.0361, MAE: 0.0361, RMSE: 0.0561, R2: 0.9437), PNorm: 151.9448, GNorm: 0.9141
[153/299] timecost: 64.01, lr: 0.000030, Train: (LOSS: 0.0199, MAE: 0.0199, RMSE: 0.0318, R2: 0.9822), Valid: (LOSS: 0.0341, MAE: 0.0341, RMSE: 0.0551, R2: 0.9451), PNorm: 151.8210, GNorm: 1.1364
[154/299] timecost: 64.10, lr: 0.000030, Train: (LOSS: 0.0198, MAE: 0.0198, RMSE: 0.0308, R2: 0.9818), Valid: (LOSS: 0.0353, MAE: 0.0353, RMSE: 0.0562, R2: 0.9426), PNorm: 151.6967, GNorm: 0.8378
[155/299] timecost: 64.21, lr: 0.000030, Train: (LOSS: 0.0200, MAE: 0.0200, RMSE: 0.0314, R2: 0.9820), Valid: (LOSS: 0.0331, MAE: 0.0331, RMSE: 0.0526, R2: 0.9492), PNorm: 151.5722, GNorm: 1.1782
[156/299] timecost: 63.90, lr: 0.000030, Train: (LOSS: 0.0190, MAE: 0.0190, RMSE: 0.0300, R2: 0.9828), Valid: (LOSS: 0.0354, MAE: 0.0354, RMSE: 0.0568, R2: 0.9422), PNorm: 151.4479, GNorm: 1.0429
[157/299] timecost: 63.39, lr: 0.000030, Train: (LOSS: 0.0188, MAE: 0.0188, RMSE: 0.0306, R2: 0.9821), Valid: (LOSS: 0.0357, MAE: 0.0357, RMSE: 0.0560, R2: 0.9426), PNorm: 151.3237, GNorm: 0.7835
[158/299] timecost: 64.11, lr: 0.000030, Train: (LOSS: 0.0195, MAE: 0.0195, RMSE: 0.0308, R2: 0.9808), Valid: (LOSS: 0.0341, MAE: 0.0341, RMSE: 0.0537, R2: 0.9473), PNorm: 151.1984, GNorm: 1.1599
[159/299] timecost: 64.10, lr: 0.000030, Train: (LOSS: 0.0188, MAE: 0.0188, RMSE: 0.0296, R2: 0.9840), Valid: (LOSS: 0.0335, MAE: 0.0335, RMSE: 0.0519, R2: 0.9506), PNorm: 151.0743, GNorm: 0.9812
[160/299] timecost: 64.17, lr: 0.000030, Train: (LOSS: 0.0200, MAE: 0.0200, RMSE: 0.0314, R2: 0.9810), Valid: (LOSS: 0.0340, MAE: 0.0340, RMSE: 0.0511, R2: 0.9529), PNorm: 150.9492, GNorm: 1.1712
[161/299] timecost: 64.12, lr: 0.000030, Train: (LOSS: 0.0193, MAE: 0.0193, RMSE: 0.0309, R2: 0.9825), Valid: (LOSS: 0.0332, MAE: 0.0332, RMSE: 0.0525, R2: 0.9492), PNorm: 150.8261, GNorm: 1.0611
[162/299] timecost: 63.68, lr: 0.000030, Train: (LOSS: 0.0176, MAE: 0.0176, RMSE: 0.0288, R2: 0.9831), Valid: (LOSS: 0.0332, MAE: 0.0332, RMSE: 0.0514, R2: 0.9525), PNorm: 150.7007, GNorm: 1.0128
[163/299] timecost: 62.51, lr: 0.000030, Train: (LOSS: 0.0180, MAE: 0.0180, RMSE: 0.0289, R2: 0.9831), Valid: (LOSS: 0.0344, MAE: 0.0344, RMSE: 0.0558, R2: 0.9438), PNorm: 150.5759, GNorm: 0.8743
[164/299] timecost: 63.29, lr: 0.000030, Train: (LOSS: 0.0182, MAE: 0.0182, RMSE: 0.0294, R2: 0.9836), Valid: (LOSS: 0.0335, MAE: 0.0335, RMSE: 0.0544, R2: 0.9468), PNorm: 150.4512, GNorm: 1.0216
[165/299] timecost: 63.39, lr: 0.000030, Train: (LOSS: 0.0182, MAE: 0.0182, RMSE: 0.0298, R2: 0.9838), Valid: (LOSS: 0.0343, MAE: 0.0343, RMSE: 0.0547, R2: 0.9461), PNorm: 150.3265, GNorm: 0.7619
[166/299] timecost: 62.97, lr: 0.000030, Train: (LOSS: 0.0177, MAE: 0.0177, RMSE: 0.0283, R2: 0.9844), Valid: (LOSS: 0.0345, MAE: 0.0345, RMSE: 0.0537, R2: 0.9494), PNorm: 150.2001, GNorm: 0.8712
[167/299] timecost: 62.61, lr: 0.000030, Train: (LOSS: 0.0171, MAE: 0.0171, RMSE: 0.0275, R2: 0.9851), Valid: (LOSS: 0.0338, MAE: 0.0338, RMSE: 0.0527, R2: 0.9511), PNorm: 150.0752, GNorm: 0.9003
[168/299] timecost: 62.43, lr: 0.000030, Train: (LOSS: 0.0198, MAE: 0.0198, RMSE: 0.0323, R2: 0.9803), Valid: (LOSS: 0.0340, MAE: 0.0340, RMSE: 0.0544, R2: 0.9477), PNorm: 149.9521, GNorm: 0.6697
[169/299] timecost: 63.84, lr: 0.000030, Train: (LOSS: 0.0178, MAE: 0.0178, RMSE: 0.0291, R2: 0.9833), Valid: (LOSS: 0.0343, MAE: 0.0343, RMSE: 0.0540, R2: 0.9467), PNorm: 149.8277, GNorm: 0.9511
[170/299] timecost: 64.22, lr: 0.000030, Train: (LOSS: 0.0173, MAE: 0.0173, RMSE: 0.0278, R2: 0.9851), Valid: (LOSS: 0.0340, MAE: 0.0340, RMSE: 0.0539, R2: 0.9480), PNorm: 149.7039, GNorm: 0.9016
[171/299] timecost: 63.84, lr: 0.000030, Train: (LOSS: 0.0179, MAE: 0.0179, RMSE: 0.0287, R2: 0.9852), Valid: (LOSS: 0.0359, MAE: 0.0359, RMSE: 0.0587, R2: 0.9381), PNorm: 149.5792, GNorm: 0.7407
[172/299] timecost: 64.25, lr: 0.000030, Train: (LOSS: 0.0174, MAE: 0.0174, RMSE: 0.0280, R2: 0.9857), Valid: (LOSS: 0.0334, MAE: 0.0334, RMSE: 0.0510, R2: 0.9537), PNorm: 149.4557, GNorm: 1.2875
[173/299] timecost: 63.98, lr: 0.000030, Train: (LOSS: 0.0173, MAE: 0.0173, RMSE: 0.0279, R2: 0.9847), Valid: (LOSS: 0.0356, MAE: 0.0356, RMSE: 0.0554, R2: 0.9462), PNorm: 149.3319, GNorm: 0.8392
[174/299] timecost: 64.20, lr: 0.000030, Train: (LOSS: 0.0172, MAE: 0.0172, RMSE: 0.0276, R2: 0.9855), Valid: (LOSS: 0.0341, MAE: 0.0341, RMSE: 0.0534, R2: 0.9493), PNorm: 149.2074, GNorm: 1.0786
[175/299] timecost: 64.02, lr: 0.000030, Train: (LOSS: 0.0171, MAE: 0.0171, RMSE: 0.0278, R2: 0.9854), Valid: (LOSS: 0.0333, MAE: 0.0333, RMSE: 0.0517, R2: 0.9515), PNorm: 149.0834, GNorm: 0.8464
Epoch 00177: reducing learning rate of group 0 to 2.7000e-05.
[176/299] timecost: 64.04, lr: 0.000027, Train: (LOSS: 0.0168, MAE: 0.0168, RMSE: 0.0268, R2: 0.9858), Valid: (LOSS: 0.0359, MAE: 0.0359, RMSE: 0.0547, R2: 0.9470), PNorm: 148.9596, GNorm: 0.7519
[177/299] timecost: 64.12, lr: 0.000027, Train: (LOSS: 0.0168, MAE: 0.0168, RMSE: 0.0272, R2: 0.9862), Valid: (LOSS: 0.0348, MAE: 0.0348, RMSE: 0.0546, R2: 0.9465), PNorm: 148.8480, GNorm: 0.7452
[178/299] timecost: 63.38, lr: 0.000027, Train: (LOSS: 0.0156, MAE: 0.0156, RMSE: 0.0257, R2: 0.9867), Valid: (LOSS: 0.0317, MAE: 0.0317, RMSE: 0.0517, R2: 0.9515), PNorm: 148.7355, GNorm: 0.9265
[179/299] timecost: 62.53, lr: 0.000027, Train: (LOSS: 0.0156, MAE: 0.0156, RMSE: 0.0254, R2: 0.9858), Valid: (LOSS: 0.0346, MAE: 0.0346, RMSE: 0.0546, R2: 0.9463), PNorm: 148.6232, GNorm: 0.9200
[180/299] timecost: 62.48, lr: 0.000027, Train: (LOSS: 0.0158, MAE: 0.0158, RMSE: 0.0255, R2: 0.9878), Valid: (LOSS: 0.0342, MAE: 0.0342, RMSE: 0.0521, R2: 0.9509), PNorm: 148.5103, GNorm: 1.3942
[181/299] timecost: 62.51, lr: 0.000027, Train: (LOSS: 0.0159, MAE: 0.0159, RMSE: 0.0257, R2: 0.9863), Valid: (LOSS: 0.0350, MAE: 0.0350, RMSE: 0.0546, R2: 0.9472), PNorm: 148.3976, GNorm: 1.2239
[182/299] timecost: 62.88, lr: 0.000027, Train: (LOSS: 0.0152, MAE: 0.0152, RMSE: 0.0247, R2: 0.9882), Valid: (LOSS: 0.0340, MAE: 0.0340, RMSE: 0.0534, R2: 0.9492), PNorm: 148.2853, GNorm: 0.7793
[183/299] timecost: 63.01, lr: 0.000027, Train: (LOSS: 0.0155, MAE: 0.0155, RMSE: 0.0255, R2: 0.9865), Valid: (LOSS: 0.0332, MAE: 0.0332, RMSE: 0.0503, R2: 0.9539), PNorm: 148.1733, GNorm: 0.9202
[184/299] timecost: 62.69, lr: 0.000027, Train: (LOSS: 0.0160, MAE: 0.0160, RMSE: 0.0259, R2: 0.9862), Valid: (LOSS: 0.0340, MAE: 0.0340, RMSE: 0.0517, R2: 0.9531), PNorm: 148.0614, GNorm: 0.7365
[185/299] timecost: 63.12, lr: 0.000027, Train: (LOSS: 0.0153, MAE: 0.0153, RMSE: 0.0250, R2: 0.9868), Valid: (LOSS: 0.0323, MAE: 0.0323, RMSE: 0.0505, R2: 0.9538), PNorm: 147.9491, GNorm: 0.6726
[186/299] timecost: 62.99, lr: 0.000027, Train: (LOSS: 0.0148, MAE: 0.0148, RMSE: 0.0244, R2: 0.9881), Valid: (LOSS: 0.0332, MAE: 0.0332, RMSE: 0.0520, R2: 0.9538), PNorm: 147.8369, GNorm: 0.8829
[187/299] timecost: 63.47, lr: 0.000027, Train: (LOSS: 0.0151, MAE: 0.0151, RMSE: 0.0239, R2: 0.9887), Valid: (LOSS: 0.0358, MAE: 0.0358, RMSE: 0.0559, R2: 0.9448), PNorm: 147.7248, GNorm: 0.9303
[188/299] timecost: 62.96, lr: 0.000027, Train: (LOSS: 0.0151, MAE: 0.0151, RMSE: 0.0249, R2: 0.9878), Valid: (LOSS: 0.0341, MAE: 0.0341, RMSE: 0.0536, R2: 0.9499), PNorm: 147.6143, GNorm: 1.3155
[189/299] timecost: 63.96, lr: 0.000027, Train: (LOSS: 0.0146, MAE: 0.0146, RMSE: 0.0241, R2: 0.9886), Valid: (LOSS: 0.0330, MAE: 0.0330, RMSE: 0.0520, R2: 0.9529), PNorm: 147.5019, GNorm: 0.7611
[190/299] timecost: 63.96, lr: 0.000027, Train: (LOSS: 0.0149, MAE: 0.0149, RMSE: 0.0245, R2: 0.9871), Valid: (LOSS: 0.0338, MAE: 0.0338, RMSE: 0.0537, R2: 0.9502), PNorm: 147.3898, GNorm: 0.7358
[191/299] timecost: 64.13, lr: 0.000027, Train: (LOSS: 0.0145, MAE: 0.0145, RMSE: 0.0236, R2: 0.9886), Valid: (LOSS: 0.0335, MAE: 0.0335, RMSE: 0.0531, R2: 0.9497), PNorm: 147.2779, GNorm: 0.9986
[192/299] timecost: 64.27, lr: 0.000027, Train: (LOSS: 0.0150, MAE: 0.0150, RMSE: 0.0241, R2: 0.9884), Valid: (LOSS: 0.0351, MAE: 0.0351, RMSE: 0.0532, R2: 0.9509), PNorm: 147.1658, GNorm: 1.6050
[193/299] timecost: 64.16, lr: 0.000027, Train: (LOSS: 0.0155, MAE: 0.0155, RMSE: 0.0253, R2: 0.9863), Valid: (LOSS: 0.0340, MAE: 0.0340, RMSE: 0.0525, R2: 0.9518), PNorm: 147.0555, GNorm: 0.9295
[194/299] timecost: 63.91, lr: 0.000027, Train: (LOSS: 0.0144, MAE: 0.0144, RMSE: 0.0234, R2: 0.9871), Valid: (LOSS: 0.0322, MAE: 0.0322, RMSE: 0.0494, R2: 0.9567), PNorm: 146.9447, GNorm: 0.9579
[195/299] timecost: 64.08, lr: 0.000027, Train: (LOSS: 0.0143, MAE: 0.0143, RMSE: 0.0238, R2: 0.9882), Valid: (LOSS: 0.0332, MAE: 0.0332, RMSE: 0.0522, R2: 0.9506), PNorm: 146.8343, GNorm: 0.8123
[196/299] timecost: 63.61, lr: 0.000027, Train: (LOSS: 0.0152, MAE: 0.0152, RMSE: 0.0252, R2: 0.9868), Valid: (LOSS: 0.0341, MAE: 0.0341, RMSE: 0.0521, R2: 0.9527), PNorm: 146.7248, GNorm: 0.9553
[197/299] timecost: 62.51, lr: 0.000027, Train: (LOSS: 0.0144, MAE: 0.0144, RMSE: 0.0232, R2: 0.9889), Valid: (LOSS: 0.0336, MAE: 0.0336, RMSE: 0.0521, R2: 0.9531), PNorm: 146.6154, GNorm: 0.9236
[198/299] timecost: 62.70, lr: 0.000027, Train: (LOSS: 0.0139, MAE: 0.0139, RMSE: 0.0230, R2: 0.9894), Valid: (LOSS: 0.0332, MAE: 0.0332, RMSE: 0.0528, R2: 0.9524), PNorm: 146.5050, GNorm: 0.7521
Epoch 00200: reducing learning rate of group 0 to 2.4300e-05.
[199/299] timecost: 63.91, lr: 0.000024, Train: (LOSS: 0.0140, MAE: 0.0140, RMSE: 0.0226, R2: 0.9896), Valid: (LOSS: 0.0331, MAE: 0.0331, RMSE: 0.0511, R2: 0.9552), PNorm: 146.3957, GNorm: 0.8978
[200/299] timecost: 63.93, lr: 0.000024, Train: (LOSS: 0.0131, MAE: 0.0131, RMSE: 0.0217, R2: 0.9898), Valid: (LOSS: 0.0338, MAE: 0.0338, RMSE: 0.0514, R2: 0.9535), PNorm: 146.2961, GNorm: 0.5553
[201/299] timecost: 64.06, lr: 0.000024, Train: (LOSS: 0.0136, MAE: 0.0136, RMSE: 0.0222, R2: 0.9894), Valid: (LOSS: 0.0331, MAE: 0.0331, RMSE: 0.0510, R2: 0.9542), PNorm: 146.1972, GNorm: 0.8952
[202/299] timecost: 64.52, lr: 0.000024, Train: (LOSS: 0.0135, MAE: 0.0135, RMSE: 0.0224, R2: 0.9894), Valid: (LOSS: 0.0343, MAE: 0.0343, RMSE: 0.0546, R2: 0.9488), PNorm: 146.0985, GNorm: 0.8292
[203/299] timecost: 64.13, lr: 0.000024, Train: (LOSS: 0.0139, MAE: 0.0139, RMSE: 0.0226, R2: 0.9895), Valid: (LOSS: 0.0346, MAE: 0.0346, RMSE: 0.0544, R2: 0.9484), PNorm: 145.9997, GNorm: 1.1272
[204/299] timecost: 63.93, lr: 0.000024, Train: (LOSS: 0.0136, MAE: 0.0136, RMSE: 0.0223, R2: 0.9898), Valid: (LOSS: 0.0342, MAE: 0.0342, RMSE: 0.0528, R2: 0.9511), PNorm: 145.9011, GNorm: 0.9472
[205/299] timecost: 64.07, lr: 0.000024, Train: (LOSS: 0.0126, MAE: 0.0126, RMSE: 0.0210, R2: 0.9906), Valid: (LOSS: 0.0325, MAE: 0.0325, RMSE: 0.0511, R2: 0.9537), PNorm: 145.8019, GNorm: 1.3380
[206/299] timecost: 64.49, lr: 0.000024, Train: (LOSS: 0.0130, MAE: 0.0130, RMSE: 0.0211, R2: 0.9903), Valid: (LOSS: 0.0327, MAE: 0.0327, RMSE: 0.0501, R2: 0.9568), PNorm: 145.7027, GNorm: 0.6616
[207/299] timecost: 63.95, lr: 0.000024, Train: (LOSS: 0.0127, MAE: 0.0127, RMSE: 0.0209, R2: 0.9904), Valid: (LOSS: 0.0314, MAE: 0.0314, RMSE: 0.0479, R2: 0.9601), PNorm: 145.6029, GNorm: 1.2902
[208/299] timecost: 64.14, lr: 0.000024, Train: (LOSS: 0.0126, MAE: 0.0126, RMSE: 0.0209, R2: 0.9909), Valid: (LOSS: 0.0330, MAE: 0.0330, RMSE: 0.0508, R2: 0.9549), PNorm: 145.5030, GNorm: 0.8972
[209/299] timecost: 64.33, lr: 0.000024, Train: (LOSS: 0.0123, MAE: 0.0123, RMSE: 0.0206, R2: 0.9907), Valid: (LOSS: 0.0334, MAE: 0.0334, RMSE: 0.0514, R2: 0.9542), PNorm: 145.4034, GNorm: 0.6502
[210/299] timecost: 63.98, lr: 0.000024, Train: (LOSS: 0.0120, MAE: 0.0120, RMSE: 0.0198, R2: 0.9914), Valid: (LOSS: 0.0339, MAE: 0.0339, RMSE: 0.0527, R2: 0.9517), PNorm: 145.3041, GNorm: 0.8511
[211/299] timecost: 64.41, lr: 0.000024, Train: (LOSS: 0.0133, MAE: 0.0133, RMSE: 0.0217, R2: 0.9908), Valid: (LOSS: 0.0343, MAE: 0.0343, RMSE: 0.0529, R2: 0.9536), PNorm: 145.2049, GNorm: 1.1800
[212/299] timecost: 64.20, lr: 0.000024, Train: (LOSS: 0.0128, MAE: 0.0128, RMSE: 0.0207, R2: 0.9912), Valid: (LOSS: 0.0343, MAE: 0.0343, RMSE: 0.0527, R2: 0.9528), PNorm: 145.1062, GNorm: 0.8934
[213/299] timecost: 63.97, lr: 0.000024, Train: (LOSS: 0.0119, MAE: 0.0119, RMSE: 0.0196, R2: 0.9918), Valid: (LOSS: 0.0344, MAE: 0.0344, RMSE: 0.0523, R2: 0.9538), PNorm: 145.0059, GNorm: 0.9997
[214/299] timecost: 63.99, lr: 0.000024, Train: (LOSS: 0.0123, MAE: 0.0123, RMSE: 0.0201, R2: 0.9920), Valid: (LOSS: 0.0343, MAE: 0.0343, RMSE: 0.0543, R2: 0.9500), PNorm: 144.9055, GNorm: 0.9150
[215/299] timecost: 64.29, lr: 0.000024, Train: (LOSS: 0.0130, MAE: 0.0130, RMSE: 0.0213, R2: 0.9903), Valid: (LOSS: 0.0343, MAE: 0.0343, RMSE: 0.0522, R2: 0.9535), PNorm: 144.8067, GNorm: 0.9125
[216/299] timecost: 63.92, lr: 0.000024, Train: (LOSS: 0.0121, MAE: 0.0121, RMSE: 0.0199, R2: 0.9912), Valid: (LOSS: 0.0331, MAE: 0.0331, RMSE: 0.0506, R2: 0.9561), PNorm: 144.7078, GNorm: 0.6245
[217/299] timecost: 64.28, lr: 0.000024, Train: (LOSS: 0.0119, MAE: 0.0119, RMSE: 0.0193, R2: 0.9919), Valid: (LOSS: 0.0325, MAE: 0.0325, RMSE: 0.0498, R2: 0.9572), PNorm: 144.6081, GNorm: 0.8515
[218/299] timecost: 64.30, lr: 0.000024, Train: (LOSS: 0.0120, MAE: 0.0120, RMSE: 0.0199, R2: 0.9924), Valid: (LOSS: 0.0333, MAE: 0.0333, RMSE: 0.0495, R2: 0.9579), PNorm: 144.5086, GNorm: 0.9314
[219/299] timecost: 64.49, lr: 0.000024, Train: (LOSS: 0.0121, MAE: 0.0121, RMSE: 0.0199, R2: 0.9918), Valid: (LOSS: 0.0335, MAE: 0.0335, RMSE: 0.0505, R2: 0.9564), PNorm: 144.4103, GNorm: 1.1876
Epoch 00221: reducing learning rate of group 0 to 2.1870e-05.
[220/299] timecost: 64.45, lr: 0.000022, Train: (LOSS: 0.0118, MAE: 0.0118, RMSE: 0.0193, R2: 0.9926), Valid: (LOSS: 0.0323, MAE: 0.0323, RMSE: 0.0490, R2: 0.9584), PNorm: 144.3109, GNorm: 0.8811
[221/299] timecost: 64.09, lr: 0.000022, Train: (LOSS: 0.0121, MAE: 0.0121, RMSE: 0.0196, R2: 0.9924), Valid: (LOSS: 0.0328, MAE: 0.0328, RMSE: 0.0507, R2: 0.9570), PNorm: 144.2218, GNorm: 1.0283
[222/299] timecost: 64.03, lr: 0.000022, Train: (LOSS: 0.0110, MAE: 0.0110, RMSE: 0.0184, R2: 0.9926), Valid: (LOSS: 0.0323, MAE: 0.0323, RMSE: 0.0483, R2: 0.9610), PNorm: 144.1326, GNorm: 0.6996
[223/299] timecost: 64.20, lr: 0.000022, Train: (LOSS: 0.0112, MAE: 0.0112, RMSE: 0.0186, R2: 0.9922), Valid: (LOSS: 0.0336, MAE: 0.0336, RMSE: 0.0511, R2: 0.9555), PNorm: 144.0436, GNorm: 0.9477
[224/299] timecost: 63.94, lr: 0.000022, Train: (LOSS: 0.0115, MAE: 0.0115, RMSE: 0.0185, R2: 0.9927), Valid: (LOSS: 0.0342, MAE: 0.0342, RMSE: 0.0530, R2: 0.9535), PNorm: 143.9548, GNorm: 1.2464
[225/299] timecost: 64.15, lr: 0.000022, Train: (LOSS: 0.0113, MAE: 0.0113, RMSE: 0.0186, R2: 0.9930), Valid: (LOSS: 0.0333, MAE: 0.0333, RMSE: 0.0518, R2: 0.9538), PNorm: 143.8657, GNorm: 1.2520
[226/299] timecost: 64.39, lr: 0.000022, Train: (LOSS: 0.0118, MAE: 0.0118, RMSE: 0.0191, R2: 0.9927), Valid: (LOSS: 0.0339, MAE: 0.0339, RMSE: 0.0510, R2: 0.9568), PNorm: 143.7773, GNorm: 0.9864
[227/299] timecost: 63.85, lr: 0.000022, Train: (LOSS: 0.0109, MAE: 0.0109, RMSE: 0.0177, R2: 0.9933), Valid: (LOSS: 0.0324, MAE: 0.0324, RMSE: 0.0490, R2: 0.9598), PNorm: 143.6881, GNorm: 0.7627
[228/299] timecost: 63.98, lr: 0.000022, Train: (LOSS: 0.0112, MAE: 0.0112, RMSE: 0.0183, R2: 0.9929), Valid: (LOSS: 0.0333, MAE: 0.0333, RMSE: 0.0505, R2: 0.9577), PNorm: 143.5986, GNorm: 0.9222
[229/299] timecost: 63.88, lr: 0.000022, Train: (LOSS: 0.0111, MAE: 0.0111, RMSE: 0.0183, R2: 0.9932), Valid: (LOSS: 0.0332, MAE: 0.0332, RMSE: 0.0504, R2: 0.9577), PNorm: 143.5100, GNorm: 0.7099
[230/299] timecost: 63.95, lr: 0.000022, Train: (LOSS: 0.0112, MAE: 0.0112, RMSE: 0.0183, R2: 0.9929), Valid: (LOSS: 0.0324, MAE: 0.0324, RMSE: 0.0486, R2: 0.9609), PNorm: 143.4212, GNorm: 1.0806
[231/299] timecost: 63.93, lr: 0.000022, Train: (LOSS: 0.0114, MAE: 0.0114, RMSE: 0.0184, R2: 0.9924), Valid: (LOSS: 0.0323, MAE: 0.0323, RMSE: 0.0492, R2: 0.9591), PNorm: 143.3320, GNorm: 1.2396
[232/299] timecost: 63.70, lr: 0.000022, Train: (LOSS: 0.0113, MAE: 0.0113, RMSE: 0.0182, R2: 0.9932), Valid: (LOSS: 0.0337, MAE: 0.0337, RMSE: 0.0507, R2: 0.9570), PNorm: 143.2442, GNorm: 0.9991
[233/299] timecost: 63.59, lr: 0.000022, Train: (LOSS: 0.0104, MAE: 0.0104, RMSE: 0.0175, R2: 0.9937), Valid: (LOSS: 0.0349, MAE: 0.0349, RMSE: 0.0535, R2: 0.9529), PNorm: 143.1563, GNorm: 0.7062
[234/299] timecost: 62.70, lr: 0.000022, Train: (LOSS: 0.0112, MAE: 0.0112, RMSE: 0.0183, R2: 0.9932), Valid: (LOSS: 0.0334, MAE: 0.0334, RMSE: 0.0510, R2: 0.9566), PNorm: 143.0678, GNorm: 0.8045
[235/299] timecost: 64.05, lr: 0.000022, Train: (LOSS: 0.0105, MAE: 0.0105, RMSE: 0.0176, R2: 0.9935), Valid: (LOSS: 0.0341, MAE: 0.0341, RMSE: 0.0520, R2: 0.9551), PNorm: 142.9794, GNorm: 0.7392
[236/299] timecost: 63.28, lr: 0.000022, Train: (LOSS: 0.0107, MAE: 0.0107, RMSE: 0.0175, R2: 0.9938), Valid: (LOSS: 0.0336, MAE: 0.0336, RMSE: 0.0511, R2: 0.9569), PNorm: 142.8919, GNorm: 0.7994
[237/299] timecost: 64.00, lr: 0.000022, Train: (LOSS: 0.0106, MAE: 0.0106, RMSE: 0.0176, R2: 0.9936), Valid: (LOSS: 0.0344, MAE: 0.0344, RMSE: 0.0521, R2: 0.9556), PNorm: 142.8046, GNorm: 1.0695
[238/299] timecost: 63.85, lr: 0.000022, Train: (LOSS: 0.0113, MAE: 0.0113, RMSE: 0.0181, R2: 0.9929), Valid: (LOSS: 0.0337, MAE: 0.0337, RMSE: 0.0500, R2: 0.9590), PNorm: 142.7179, GNorm: 0.7381
[239/299] timecost: 64.08, lr: 0.000022, Train: (LOSS: 0.0105, MAE: 0.0105, RMSE: 0.0172, R2: 0.9936), Valid: (LOSS: 0.0339, MAE: 0.0339, RMSE: 0.0507, R2: 0.9579), PNorm: 142.6315, GNorm: 0.8357
[240/299] timecost: 63.65, lr: 0.000022, Train: (LOSS: 0.0109, MAE: 0.0109, RMSE: 0.0176, R2: 0.9939), Valid: (LOSS: 0.0338, MAE: 0.0338, RMSE: 0.0507, R2: 0.9579), PNorm: 142.5448, GNorm: 0.7593
Epoch 00242: reducing learning rate of group 0 to 1.9683e-05.
[241/299] timecost: 62.92, lr: 0.000020, Train: (LOSS: 0.0105, MAE: 0.0105, RMSE: 0.0172, R2: 0.9937), Valid: (LOSS: 0.0328, MAE: 0.0328, RMSE: 0.0501, R2: 0.9586), PNorm: 142.4586, GNorm: 0.6514
[242/299] timecost: 62.97, lr: 0.000020, Train: (LOSS: 0.0103, MAE: 0.0103, RMSE: 0.0167, R2: 0.9942), Valid: (LOSS: 0.0339, MAE: 0.0339, RMSE: 0.0494, R2: 0.9596), PNorm: 142.3804, GNorm: 0.6918
[243/299] timecost: 62.97, lr: 0.000020, Train: (LOSS: 0.0103, MAE: 0.0103, RMSE: 0.0166, R2: 0.9945), Valid: (LOSS: 0.0331, MAE: 0.0331, RMSE: 0.0505, R2: 0.9583), PNorm: 142.3034, GNorm: 0.8176
[244/299] timecost: 62.85, lr: 0.000020, Train: (LOSS: 0.0095, MAE: 0.0095, RMSE: 0.0157, R2: 0.9944), Valid: (LOSS: 0.0327, MAE: 0.0327, RMSE: 0.0484, R2: 0.9613), PNorm: 142.2252, GNorm: 0.8407
[245/299] timecost: 62.81, lr: 0.000020, Train: (LOSS: 0.0096, MAE: 0.0096, RMSE: 0.0158, R2: 0.9948), Valid: (LOSS: 0.0326, MAE: 0.0326, RMSE: 0.0488, R2: 0.9610), PNorm: 142.1466, GNorm: 0.9182
[246/299] timecost: 62.97, lr: 0.000020, Train: (LOSS: 0.0096, MAE: 0.0096, RMSE: 0.0157, R2: 0.9943), Valid: (LOSS: 0.0334, MAE: 0.0334, RMSE: 0.0502, R2: 0.9581), PNorm: 142.0681, GNorm: 0.9311
[247/299] timecost: 63.07, lr: 0.000020, Train: (LOSS: 0.0095, MAE: 0.0095, RMSE: 0.0156, R2: 0.9950), Valid: (LOSS: 0.0337, MAE: 0.0337, RMSE: 0.0508, R2: 0.9580), PNorm: 141.9900, GNorm: 0.7976
[248/299] timecost: 64.03, lr: 0.000020, Train: (LOSS: 0.0096, MAE: 0.0096, RMSE: 0.0157, R2: 0.9952), Valid: (LOSS: 0.0329, MAE: 0.0329, RMSE: 0.0495, R2: 0.9603), PNorm: 141.9127, GNorm: 0.8054
[249/299] timecost: 64.51, lr: 0.000020, Train: (LOSS: 0.0096, MAE: 0.0096, RMSE: 0.0154, R2: 0.9952), Valid: (LOSS: 0.0324, MAE: 0.0324, RMSE: 0.0490, R2: 0.9603), PNorm: 141.8344, GNorm: 0.5092
[250/299] timecost: 64.07, lr: 0.000020, Train: (LOSS: 0.0094, MAE: 0.0094, RMSE: 0.0157, R2: 0.9950), Valid: (LOSS: 0.0334, MAE: 0.0334, RMSE: 0.0506, R2: 0.9579), PNorm: 141.7558, GNorm: 0.8469
[251/299] timecost: 64.49, lr: 0.000020, Train: (LOSS: 0.0093, MAE: 0.0093, RMSE: 0.0153, R2: 0.9951), Valid: (LOSS: 0.0328, MAE: 0.0328, RMSE: 0.0489, R2: 0.9608), PNorm: 141.6780, GNorm: 0.7983
[252/299] timecost: 63.96, lr: 0.000020, Train: (LOSS: 0.0093, MAE: 0.0093, RMSE: 0.0152, R2: 0.9943), Valid: (LOSS: 0.0322, MAE: 0.0322, RMSE: 0.0476, R2: 0.9626), PNorm: 141.6002, GNorm: 1.1595
[253/299] timecost: 62.70, lr: 0.000020, Train: (LOSS: 0.0092, MAE: 0.0092, RMSE: 0.0151, R2: 0.9950), Valid: (LOSS: 0.0323, MAE: 0.0323, RMSE: 0.0476, R2: 0.9627), PNorm: 141.5230, GNorm: 0.8572
[254/299] timecost: 63.24, lr: 0.000020, Train: (LOSS: 0.0097, MAE: 0.0097, RMSE: 0.0154, R2: 0.9950), Valid: (LOSS: 0.0330, MAE: 0.0330, RMSE: 0.0501, R2: 0.9588), PNorm: 141.4454, GNorm: 0.8168
[255/299] timecost: 62.69, lr: 0.000020, Train: (LOSS: 0.0092, MAE: 0.0092, RMSE: 0.0152, R2: 0.9948), Valid: (LOSS: 0.0321, MAE: 0.0321, RMSE: 0.0478, R2: 0.9630), PNorm: 141.3681, GNorm: 0.8489
[256/299] timecost: 62.83, lr: 0.000020, Train: (LOSS: 0.0092, MAE: 0.0092, RMSE: 0.0150, R2: 0.9951), Valid: (LOSS: 0.0321, MAE: 0.0321, RMSE: 0.0480, R2: 0.9623), PNorm: 141.2908, GNorm: 0.7934
[257/299] timecost: 63.10, lr: 0.000020, Train: (LOSS: 0.0100, MAE: 0.0100, RMSE: 0.0158, R2: 0.9950), Valid: (LOSS: 0.0338, MAE: 0.0338, RMSE: 0.0501, R2: 0.9593), PNorm: 141.2143, GNorm: 1.3044
[258/299] timecost: 63.88, lr: 0.000020, Train: (LOSS: 0.0100, MAE: 0.0100, RMSE: 0.0158, R2: 0.9948), Valid: (LOSS: 0.0337, MAE: 0.0337, RMSE: 0.0493, R2: 0.9604), PNorm: 141.1381, GNorm: 1.0962
[259/299] timecost: 63.90, lr: 0.000020, Train: (LOSS: 0.0100, MAE: 0.0100, RMSE: 0.0155, R2: 0.9952), Valid: (LOSS: 0.0322, MAE: 0.0322, RMSE: 0.0475, R2: 0.9634), PNorm: 141.0620, GNorm: 0.7506
[260/299] timecost: 63.86, lr: 0.000020, Train: (LOSS: 0.0091, MAE: 0.0091, RMSE: 0.0147, R2: 0.9954), Valid: (LOSS: 0.0330, MAE: 0.0330, RMSE: 0.0488, R2: 0.9612), PNorm: 140.9857, GNorm: 0.7134
[261/299] timecost: 64.03, lr: 0.000020, Train: (LOSS: 0.0091, MAE: 0.0091, RMSE: 0.0146, R2: 0.9955), Valid: (LOSS: 0.0332, MAE: 0.0332, RMSE: 0.0488, R2: 0.9607), PNorm: 140.9092, GNorm: 0.5424
Epoch 00263: reducing learning rate of group 0 to 1.7715e-05.
[262/299] timecost: 64.44, lr: 0.000018, Train: (LOSS: 0.0088, MAE: 0.0088, RMSE: 0.0143, R2: 0.9956), Valid: (LOSS: 0.0325, MAE: 0.0325, RMSE: 0.0481, R2: 0.9623), PNorm: 140.8324, GNorm: 0.7674
[263/299] timecost: 63.46, lr: 0.000018, Train: (LOSS: 0.0088, MAE: 0.0088, RMSE: 0.0143, R2: 0.9956), Valid: (LOSS: 0.0326, MAE: 0.0326, RMSE: 0.0485, R2: 0.9621), PNorm: 140.7636, GNorm: 0.4990
[264/299] timecost: 62.95, lr: 0.000018, Train: (LOSS: 0.0083, MAE: 0.0083, RMSE: 0.0138, R2: 0.9956), Valid: (LOSS: 0.0332, MAE: 0.0332, RMSE: 0.0487, R2: 0.9611), PNorm: 140.6946, GNorm: 1.0745
[265/299] timecost: 63.07, lr: 0.000018, Train: (LOSS: 0.0086, MAE: 0.0086, RMSE: 0.0139, R2: 0.9958), Valid: (LOSS: 0.0337, MAE: 0.0337, RMSE: 0.0502, R2: 0.9585), PNorm: 140.6254, GNorm: 0.9561
[266/299] timecost: 62.96, lr: 0.000018, Train: (LOSS: 0.0081, MAE: 0.0081, RMSE: 0.0134, R2: 0.9957), Valid: (LOSS: 0.0315, MAE: 0.0315, RMSE: 0.0469, R2: 0.9644), PNorm: 140.5563, GNorm: 0.9146
[267/299] timecost: 62.83, lr: 0.000018, Train: (LOSS: 0.0089, MAE: 0.0089, RMSE: 0.0142, R2: 0.9958), Valid: (LOSS: 0.0328, MAE: 0.0328, RMSE: 0.0479, R2: 0.9628), PNorm: 140.4875, GNorm: 0.9476
[268/299] timecost: 63.32, lr: 0.000018, Train: (LOSS: 0.0080, MAE: 0.0080, RMSE: 0.0131, R2: 0.9962), Valid: (LOSS: 0.0326, MAE: 0.0326, RMSE: 0.0471, R2: 0.9646), PNorm: 140.4188, GNorm: 0.8044
[269/299] timecost: 63.10, lr: 0.000018, Train: (LOSS: 0.0084, MAE: 0.0084, RMSE: 0.0135, R2: 0.9962), Valid: (LOSS: 0.0328, MAE: 0.0328, RMSE: 0.0476, R2: 0.9634), PNorm: 140.3492, GNorm: 1.3855
[270/299] timecost: 62.79, lr: 0.000018, Train: (LOSS: 0.0084, MAE: 0.0084, RMSE: 0.0136, R2: 0.9960), Valid: (LOSS: 0.0331, MAE: 0.0331, RMSE: 0.0497, R2: 0.9597), PNorm: 140.2807, GNorm: 0.8734
[271/299] timecost: 63.71, lr: 0.000018, Train: (LOSS: 0.0086, MAE: 0.0086, RMSE: 0.0137, R2: 0.9951), Valid: (LOSS: 0.0333, MAE: 0.0333, RMSE: 0.0491, R2: 0.9612), PNorm: 140.2120, GNorm: 0.7876
[272/299] timecost: 63.61, lr: 0.000018, Train: (LOSS: 0.0086, MAE: 0.0086, RMSE: 0.0140, R2: 0.9954), Valid: (LOSS: 0.0327, MAE: 0.0327, RMSE: 0.0469, R2: 0.9642), PNorm: 140.1433, GNorm: 0.6173
[273/299] timecost: 62.90, lr: 0.000018, Train: (LOSS: 0.0086, MAE: 0.0086, RMSE: 0.0138, R2: 0.9959), Valid: (LOSS: 0.0330, MAE: 0.0330, RMSE: 0.0484, R2: 0.9618), PNorm: 140.0752, GNorm: 0.8838
[274/299] timecost: 63.74, lr: 0.000018, Train: (LOSS: 0.0082, MAE: 0.0082, RMSE: 0.0136, R2: 0.9962), Valid: (LOSS: 0.0330, MAE: 0.0330, RMSE: 0.0488, R2: 0.9617), PNorm: 140.0064, GNorm: 0.7167
[275/299] timecost: 63.51, lr: 0.000018, Train: (LOSS: 0.0080, MAE: 0.0080, RMSE: 0.0131, R2: 0.9953), Valid: (LOSS: 0.0326, MAE: 0.0326, RMSE: 0.0477, R2: 0.9636), PNorm: 139.9383, GNorm: 0.7684
[276/299] timecost: 63.92, lr: 0.000018, Train: (LOSS: 0.0079, MAE: 0.0079, RMSE: 0.0131, R2: 0.9958), Valid: (LOSS: 0.0331, MAE: 0.0331, RMSE: 0.0501, R2: 0.9592), PNorm: 139.8694, GNorm: 0.8699
[277/299] timecost: 64.05, lr: 0.000018, Train: (LOSS: 0.0081, MAE: 0.0081, RMSE: 0.0132, R2: 0.9962), Valid: (LOSS: 0.0328, MAE: 0.0328, RMSE: 0.0484, R2: 0.9617), PNorm: 139.8008, GNorm: 0.8367
[278/299] timecost: 64.17, lr: 0.000018, Train: (LOSS: 0.0080, MAE: 0.0080, RMSE: 0.0130, R2: 0.9959), Valid: (LOSS: 0.0323, MAE: 0.0323, RMSE: 0.0477, R2: 0.9633), PNorm: 139.7326, GNorm: 0.8997
[279/299] timecost: 63.83, lr: 0.000018, Train: (LOSS: 0.0082, MAE: 0.0082, RMSE: 0.0133, R2: 0.9960), Valid: (LOSS: 0.0336, MAE: 0.0336, RMSE: 0.0496, R2: 0.9605), PNorm: 139.6640, GNorm: 0.7269
[280/299] timecost: 63.23, lr: 0.000018, Train: (LOSS: 0.0082, MAE: 0.0082, RMSE: 0.0132, R2: 0.9957), Valid: (LOSS: 0.0328, MAE: 0.0328, RMSE: 0.0484, R2: 0.9622), PNorm: 139.5957, GNorm: 0.7982
[281/299] timecost: 62.58, lr: 0.000018, Train: (LOSS: 0.0081, MAE: 0.0081, RMSE: 0.0132, R2: 0.9961), Valid: (LOSS: 0.0333, MAE: 0.0333, RMSE: 0.0494, R2: 0.9599), PNorm: 139.5271, GNorm: 0.8187
[282/299] timecost: 63.06, lr: 0.000018, Train: (LOSS: 0.0082, MAE: 0.0082, RMSE: 0.0128, R2: 0.9964), Valid: (LOSS: 0.0328, MAE: 0.0328, RMSE: 0.0480, R2: 0.9635), PNorm: 139.4592, GNorm: 0.7995
Epoch 00284: reducing learning rate of group 0 to 1.5943e-05.
[283/299] timecost: 63.77, lr: 0.000016, Train: (LOSS: 0.0083, MAE: 0.0083, RMSE: 0.0133, R2: 0.9956), Valid: (LOSS: 0.0322, MAE: 0.0322, RMSE: 0.0471, R2: 0.9642), PNorm: 139.3918, GNorm: 0.8209
[284/299] timecost: 64.19, lr: 0.000016, Train: (LOSS: 0.0078, MAE: 0.0078, RMSE: 0.0126, R2: 0.9968), Valid: (LOSS: 0.0329, MAE: 0.0329, RMSE: 0.0481, R2: 0.9627), PNorm: 139.3312, GNorm: 0.8932
[285/299] timecost: 64.01, lr: 0.000016, Train: (LOSS: 0.0079, MAE: 0.0079, RMSE: 0.0128, R2: 0.9965), Valid: (LOSS: 0.0330, MAE: 0.0330, RMSE: 0.0482, R2: 0.9624), PNorm: 139.2705, GNorm: 0.6374
[286/299] timecost: 64.03, lr: 0.000016, Train: (LOSS: 0.0077, MAE: 0.0077, RMSE: 0.0125, R2: 0.9965), Valid: (LOSS: 0.0327, MAE: 0.0327, RMSE: 0.0482, R2: 0.9626), PNorm: 139.2101, GNorm: 0.6652
[287/299] timecost: 64.30, lr: 0.000016, Train: (LOSS: 0.0078, MAE: 0.0078, RMSE: 0.0127, R2: 0.9962), Valid: (LOSS: 0.0323, MAE: 0.0323, RMSE: 0.0471, R2: 0.9643), PNorm: 139.1497, GNorm: 0.8325
[288/299] timecost: 63.98, lr: 0.000016, Train: (LOSS: 0.0075, MAE: 0.0075, RMSE: 0.0123, R2: 0.9967), Valid: (LOSS: 0.0325, MAE: 0.0325, RMSE: 0.0478, R2: 0.9630), PNorm: 139.0896, GNorm: 0.7338
[289/299] timecost: 64.00, lr: 0.000016, Train: (LOSS: 0.0080, MAE: 0.0080, RMSE: 0.0129, R2: 0.9963), Valid: (LOSS: 0.0328, MAE: 0.0328, RMSE: 0.0483, R2: 0.9627), PNorm: 139.0302, GNorm: 0.9986
[290/299] timecost: 63.82, lr: 0.000016, Train: (LOSS: 0.0077, MAE: 0.0077, RMSE: 0.0125, R2: 0.9960), Valid: (LOSS: 0.0325, MAE: 0.0325, RMSE: 0.0475, R2: 0.9638), PNorm: 138.9704, GNorm: 0.9017
[291/299] timecost: 63.65, lr: 0.000016, Train: (LOSS: 0.0078, MAE: 0.0078, RMSE: 0.0126, R2: 0.9965), Valid: (LOSS: 0.0326, MAE: 0.0326, RMSE: 0.0483, R2: 0.9627), PNorm: 138.9111, GNorm: 0.9274
[292/299] timecost: 63.72, lr: 0.000016, Train: (LOSS: 0.0072, MAE: 0.0072, RMSE: 0.0119, R2: 0.9965), Valid: (LOSS: 0.0321, MAE: 0.0321, RMSE: 0.0469, R2: 0.9647), PNorm: 138.8515, GNorm: 0.8996
[293/299] timecost: 64.08, lr: 0.000016, Train: (LOSS: 0.0072, MAE: 0.0072, RMSE: 0.0118, R2: 0.9964), Valid: (LOSS: 0.0320, MAE: 0.0320, RMSE: 0.0469, R2: 0.9645), PNorm: 138.7919, GNorm: 1.1811
[294/299] timecost: 64.18, lr: 0.000016, Train: (LOSS: 0.0073, MAE: 0.0073, RMSE: 0.0119, R2: 0.9968), Valid: (LOSS: 0.0329, MAE: 0.0329, RMSE: 0.0487, R2: 0.9618), PNorm: 138.7320, GNorm: 0.9076
[295/299] timecost: 63.63, lr: 0.000016, Train: (LOSS: 0.0072, MAE: 0.0072, RMSE: 0.0119, R2: 0.9965), Valid: (LOSS: 0.0323, MAE: 0.0323, RMSE: 0.0476, R2: 0.9634), PNorm: 138.6726, GNorm: 0.6899
[296/299] timecost: 63.98, lr: 0.000016, Train: (LOSS: 0.0080, MAE: 0.0080, RMSE: 0.0127, R2: 0.9965), Valid: (LOSS: 0.0321, MAE: 0.0321, RMSE: 0.0484, R2: 0.9621), PNorm: 138.6136, GNorm: 1.0645
[297/299] timecost: 63.41, lr: 0.000016, Train: (LOSS: 0.0074, MAE: 0.0074, RMSE: 0.0117, R2: 0.9967), Valid: (LOSS: 0.0331, MAE: 0.0331, RMSE: 0.0483, R2: 0.9625), PNorm: 138.5546, GNorm: 0.7632
[298/299] timecost: 63.96, lr: 0.000016, Train: (LOSS: 0.0073, MAE: 0.0073, RMSE: 0.0118, R2: 0.9969), Valid: (LOSS: 0.0328, MAE: 0.0328, RMSE: 0.0485, R2: 0.9618), PNorm: 138.4954, GNorm: 0.6424
[299/299] timecost: 63.70, lr: 0.000016, Train: (LOSS: 0.0078, MAE: 0.0078, RMSE: 0.0127, R2: 0.9961), Valid: (LOSS: 0.0331, MAE: 0.0331, RMSE: 0.0492, R2: 0.9612), PNorm: 138.4375, GNorm: 0.7531
==========Training End==========
==========Test Best Model==========
================Final Results=======================
mse: 0.0343 +- 0.0000:
rmse: 0.0533 +- 0.0000:
mae: 0.0343 +- 0.0000:
r2: 0.9577 +- 0.0000:
tensor([[0.0000, 0.0000],
        [0.0000, 0.0000],
        [0.1352, 0.1828],
        ...,
        [0.0123, 0.0144],
        [0.0000, 0.0000],
        [0.0000, 0.0000]], device='cuda:0')
