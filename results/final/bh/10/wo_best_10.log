cuda available with GPU: Tesla V100-PCIE-16GB
==========Load Seed==========
set_random_seed
0
==========Training Start==========
Training Graphs:  2491
Valid Graphs:  277
Test Graphs:  1187
============Not pretrained weights used============
============Creating new layers============
============Creating Model============
Training Graphs Batches:  78
Valid Graphs Batches:  9
Test Graphs Batches:  37
[0/299] timecost: 64.87, lr: 0.000100, Train: (LOSS: 0.3269, MAE: 0.3269, RMSE: 0.4179, R2: -1.4813), Valid: (LOSS: 0.3114, MAE: 0.3114, RMSE: 0.4032, R2: -1.5016), PNorm: 187.0195, GNorm: 0.0000
[1/299] timecost: 59.82, lr: 0.000100, Train: (LOSS: 0.2629, MAE: 0.2629, RMSE: 0.3225, R2: -0.5549), Valid: (LOSS: 0.1776, MAE: 0.1776, RMSE: 0.2242, R2: 0.2169), PNorm: 187.0639, GNorm: 0.5000
[2/299] timecost: 59.22, lr: 0.000100, Train: (LOSS: 0.1710, MAE: 0.1710, RMSE: 0.2198, R2: 0.3136), Valid: (LOSS: 0.1401, MAE: 0.1401, RMSE: 0.1864, R2: 0.4542), PNorm: 187.1238, GNorm: 0.5000
[3/299] timecost: 59.24, lr: 0.000100, Train: (LOSS: 0.1426, MAE: 0.1426, RMSE: 0.1918, R2: 0.4642), Valid: (LOSS: 0.1387, MAE: 0.1387, RMSE: 0.1917, R2: 0.4200), PNorm: 187.1760, GNorm: 0.5000
[4/299] timecost: 60.42, lr: 0.000100, Train: (LOSS: 0.1395, MAE: 0.1395, RMSE: 0.1882, R2: 0.4884), Valid: (LOSS: 0.1306, MAE: 0.1306, RMSE: 0.1807, R2: 0.4859), PNorm: 187.2071, GNorm: 0.5000
[5/299] timecost: 60.75, lr: 0.000100, Train: (LOSS: 0.1350, MAE: 0.1350, RMSE: 0.1841, R2: 0.5115), Valid: (LOSS: 0.1278, MAE: 0.1278, RMSE: 0.1853, R2: 0.4477), PNorm: 187.2320, GNorm: 0.2666
[6/299] timecost: 59.33, lr: 0.000100, Train: (LOSS: 0.1299, MAE: 0.1299, RMSE: 0.1787, R2: 0.5284), Valid: (LOSS: 0.1197, MAE: 0.1197, RMSE: 0.1678, R2: 0.5550), PNorm: 187.2616, GNorm: 0.4012
[7/299] timecost: 59.46, lr: 0.000100, Train: (LOSS: 0.1283, MAE: 0.1283, RMSE: 0.1779, R2: 0.5413), Valid: (LOSS: 0.1212, MAE: 0.1212, RMSE: 0.1733, R2: 0.5141), PNorm: 187.2863, GNorm: 0.5000
[8/299] timecost: 58.66, lr: 0.000100, Train: (LOSS: 0.1252, MAE: 0.1252, RMSE: 0.1758, R2: 0.5439), Valid: (LOSS: 0.1163, MAE: 0.1163, RMSE: 0.1650, R2: 0.5662), PNorm: 187.3042, GNorm: 0.3883
[9/299] timecost: 59.15, lr: 0.000100, Train: (LOSS: 0.1280, MAE: 0.1280, RMSE: 0.1770, R2: 0.5454), Valid: (LOSS: 0.1286, MAE: 0.1286, RMSE: 0.1810, R2: 0.4804), PNorm: 187.3241, GNorm: 0.2875
[10/299] timecost: 59.22, lr: 0.000100, Train: (LOSS: 0.1260, MAE: 0.1260, RMSE: 0.1760, R2: 0.5489), Valid: (LOSS: 0.1181, MAE: 0.1181, RMSE: 0.1706, R2: 0.5396), PNorm: 187.3470, GNorm: 0.5000
[11/299] timecost: 59.20, lr: 0.000100, Train: (LOSS: 0.1244, MAE: 0.1244, RMSE: 0.1744, R2: 0.5449), Valid: (LOSS: 0.1131, MAE: 0.1131, RMSE: 0.1628, R2: 0.5744), PNorm: 187.3774, GNorm: 0.2987
[12/299] timecost: 59.53, lr: 0.000100, Train: (LOSS: 0.1204, MAE: 0.1204, RMSE: 0.1710, R2: 0.5667), Valid: (LOSS: 0.1179, MAE: 0.1179, RMSE: 0.1697, R2: 0.5421), PNorm: 187.3960, GNorm: 0.5000
[13/299] timecost: 59.22, lr: 0.000100, Train: (LOSS: 0.1202, MAE: 0.1202, RMSE: 0.1687, R2: 0.5737), Valid: (LOSS: 0.1097, MAE: 0.1097, RMSE: 0.1568, R2: 0.6044), PNorm: 187.4144, GNorm: 0.3693
[14/299] timecost: 59.08, lr: 0.000100, Train: (LOSS: 0.1170, MAE: 0.1170, RMSE: 0.1657, R2: 0.5910), Valid: (LOSS: 0.1170, MAE: 0.1170, RMSE: 0.1652, R2: 0.5525), PNorm: 187.4342, GNorm: 0.5000
[15/299] timecost: 59.05, lr: 0.000100, Train: (LOSS: 0.1173, MAE: 0.1173, RMSE: 0.1660, R2: 0.5941), Valid: (LOSS: 0.1140, MAE: 0.1140, RMSE: 0.1607, R2: 0.5795), PNorm: 187.4493, GNorm: 0.4593
[16/299] timecost: 59.33, lr: 0.000100, Train: (LOSS: 0.1186, MAE: 0.1186, RMSE: 0.1685, R2: 0.5726), Valid: (LOSS: 0.1135, MAE: 0.1135, RMSE: 0.1632, R2: 0.5715), PNorm: 187.4704, GNorm: 0.3197
[17/299] timecost: 59.39, lr: 0.000100, Train: (LOSS: 0.1168, MAE: 0.1168, RMSE: 0.1672, R2: 0.5876), Valid: (LOSS: 0.1120, MAE: 0.1120, RMSE: 0.1634, R2: 0.5764), PNorm: 187.4884, GNorm: 0.3179
[18/299] timecost: 59.08, lr: 0.000100, Train: (LOSS: 0.1129, MAE: 0.1129, RMSE: 0.1620, R2: 0.6077), Valid: (LOSS: 0.1070, MAE: 0.1070, RMSE: 0.1497, R2: 0.6505), PNorm: 187.5136, GNorm: 0.5000
[19/299] timecost: 58.96, lr: 0.000100, Train: (LOSS: 0.0975, MAE: 0.0975, RMSE: 0.1417, R2: 0.6951), Valid: (LOSS: 0.0898, MAE: 0.0898, RMSE: 0.1268, R2: 0.7443), PNorm: 187.5497, GNorm: 0.5000
[20/299] timecost: 58.89, lr: 0.000100, Train: (LOSS: 0.0928, MAE: 0.0928, RMSE: 0.1367, R2: 0.7199), Valid: (LOSS: 0.0918, MAE: 0.0918, RMSE: 0.1313, R2: 0.7208), PNorm: 187.5754, GNorm: 0.3202
[21/299] timecost: 61.00, lr: 0.000100, Train: (LOSS: 0.0899, MAE: 0.0899, RMSE: 0.1324, R2: 0.7331), Valid: (LOSS: 0.0873, MAE: 0.0873, RMSE: 0.1251, R2: 0.7465), PNorm: 187.5952, GNorm: 0.3338
[22/299] timecost: 62.88, lr: 0.000100, Train: (LOSS: 0.0839, MAE: 0.0839, RMSE: 0.1273, R2: 0.7565), Valid: (LOSS: 0.0830, MAE: 0.0830, RMSE: 0.1224, R2: 0.7603), PNorm: 187.6182, GNorm: 0.5000
[23/299] timecost: 62.11, lr: 0.000100, Train: (LOSS: 0.0827, MAE: 0.0827, RMSE: 0.1251, R2: 0.7631), Valid: (LOSS: 0.0789, MAE: 0.0789, RMSE: 0.1150, R2: 0.7831), PNorm: 187.6367, GNorm: 0.3788
[24/299] timecost: 58.86, lr: 0.000100, Train: (LOSS: 0.0811, MAE: 0.0811, RMSE: 0.1240, R2: 0.7614), Valid: (LOSS: 0.0825, MAE: 0.0825, RMSE: 0.1180, R2: 0.7645), PNorm: 187.6590, GNorm: 0.3763
[25/299] timecost: 58.87, lr: 0.000100, Train: (LOSS: 0.0908, MAE: 0.0908, RMSE: 0.1366, R2: 0.7083), Valid: (LOSS: 0.0874, MAE: 0.0874, RMSE: 0.1265, R2: 0.7394), PNorm: 187.6837, GNorm: 0.5000
[26/299] timecost: 58.71, lr: 0.000100, Train: (LOSS: 0.0802, MAE: 0.0802, RMSE: 0.1191, R2: 0.7835), Valid: (LOSS: 0.0823, MAE: 0.0823, RMSE: 0.1199, R2: 0.7674), PNorm: 187.7085, GNorm: 0.5000
[27/299] timecost: 59.54, lr: 0.000100, Train: (LOSS: 0.0739, MAE: 0.0739, RMSE: 0.1114, R2: 0.8124), Valid: (LOSS: 0.0765, MAE: 0.0765, RMSE: 0.1085, R2: 0.8080), PNorm: 187.7337, GNorm: 0.5000
[28/299] timecost: 61.27, lr: 0.000100, Train: (LOSS: 0.0668, MAE: 0.0668, RMSE: 0.0998, R2: 0.8450), Valid: (LOSS: 0.0688, MAE: 0.0688, RMSE: 0.0964, R2: 0.8479), PNorm: 187.7591, GNorm: 0.5000
[29/299] timecost: 60.94, lr: 0.000100, Train: (LOSS: 0.0651, MAE: 0.0651, RMSE: 0.0973, R2: 0.8562), Valid: (LOSS: 0.0656, MAE: 0.0656, RMSE: 0.0979, R2: 0.8352), PNorm: 187.7812, GNorm: 0.4561
[30/299] timecost: 61.06, lr: 0.000100, Train: (LOSS: 0.0618, MAE: 0.0618, RMSE: 0.0930, R2: 0.8670), Valid: (LOSS: 0.0621, MAE: 0.0621, RMSE: 0.0973, R2: 0.8438), PNorm: 187.8055, GNorm: 0.5000
[31/299] timecost: 60.78, lr: 0.000100, Train: (LOSS: 0.0572, MAE: 0.0572, RMSE: 0.0861, R2: 0.8853), Valid: (LOSS: 0.0575, MAE: 0.0575, RMSE: 0.0850, R2: 0.8765), PNorm: 187.8303, GNorm: 0.5000
[32/299] timecost: 60.93, lr: 0.000100, Train: (LOSS: 0.0549, MAE: 0.0549, RMSE: 0.0850, R2: 0.8898), Valid: (LOSS: 0.0579, MAE: 0.0579, RMSE: 0.0855, R2: 0.8728), PNorm: 187.8493, GNorm: 0.5000
[33/299] timecost: 60.68, lr: 0.000100, Train: (LOSS: 0.0521, MAE: 0.0521, RMSE: 0.0804, R2: 0.8980), Valid: (LOSS: 0.0560, MAE: 0.0560, RMSE: 0.0851, R2: 0.8737), PNorm: 187.8712, GNorm: 0.5000
[34/299] timecost: 60.70, lr: 0.000100, Train: (LOSS: 0.0515, MAE: 0.0515, RMSE: 0.0777, R2: 0.9053), Valid: (LOSS: 0.0529, MAE: 0.0529, RMSE: 0.0818, R2: 0.8843), PNorm: 187.8887, GNorm: 0.5000
[35/299] timecost: 60.61, lr: 0.000100, Train: (LOSS: 0.0494, MAE: 0.0494, RMSE: 0.0761, R2: 0.9084), Valid: (LOSS: 0.0512, MAE: 0.0512, RMSE: 0.0796, R2: 0.8905), PNorm: 187.9054, GNorm: 0.5000
[36/299] timecost: 60.49, lr: 0.000100, Train: (LOSS: 0.0483, MAE: 0.0483, RMSE: 0.0726, R2: 0.9159), Valid: (LOSS: 0.0576, MAE: 0.0576, RMSE: 0.0875, R2: 0.8682), PNorm: 187.9242, GNorm: 0.4807
[37/299] timecost: 60.19, lr: 0.000100, Train: (LOSS: 0.0471, MAE: 0.0471, RMSE: 0.0724, R2: 0.9143), Valid: (LOSS: 0.0541, MAE: 0.0541, RMSE: 0.0885, R2: 0.8624), PNorm: 187.9405, GNorm: 0.5000
[38/299] timecost: 60.41, lr: 0.000100, Train: (LOSS: 0.0463, MAE: 0.0463, RMSE: 0.0718, R2: 0.9224), Valid: (LOSS: 0.0529, MAE: 0.0529, RMSE: 0.0818, R2: 0.8763), PNorm: 187.9595, GNorm: 0.5000
[39/299] timecost: 61.00, lr: 0.000100, Train: (LOSS: 0.0439, MAE: 0.0439, RMSE: 0.0673, R2: 0.9277), Valid: (LOSS: 0.0510, MAE: 0.0510, RMSE: 0.0766, R2: 0.8977), PNorm: 187.9781, GNorm: 0.3633
[40/299] timecost: 60.48, lr: 0.000100, Train: (LOSS: 0.0444, MAE: 0.0444, RMSE: 0.0687, R2: 0.9259), Valid: (LOSS: 0.0479, MAE: 0.0479, RMSE: 0.0748, R2: 0.8947), PNorm: 187.9948, GNorm: 0.5000
[41/299] timecost: 59.89, lr: 0.000100, Train: (LOSS: 0.0433, MAE: 0.0433, RMSE: 0.0668, R2: 0.9282), Valid: (LOSS: 0.0459, MAE: 0.0459, RMSE: 0.0707, R2: 0.9129), PNorm: 188.0125, GNorm: 0.5000
[42/299] timecost: 59.85, lr: 0.000100, Train: (LOSS: 0.0416, MAE: 0.0416, RMSE: 0.0647, R2: 0.9305), Valid: (LOSS: 0.0481, MAE: 0.0481, RMSE: 0.0741, R2: 0.9085), PNorm: 188.0294, GNorm: 0.5000
[43/299] timecost: 59.58, lr: 0.000100, Train: (LOSS: 0.0416, MAE: 0.0416, RMSE: 0.0633, R2: 0.9364), Valid: (LOSS: 0.0540, MAE: 0.0540, RMSE: 0.0844, R2: 0.8760), PNorm: 188.0457, GNorm: 0.5000
[44/299] timecost: 59.46, lr: 0.000100, Train: (LOSS: 0.0411, MAE: 0.0411, RMSE: 0.0643, R2: 0.9340), Valid: (LOSS: 0.0488, MAE: 0.0488, RMSE: 0.0793, R2: 0.8898), PNorm: 188.0617, GNorm: 0.5000
[45/299] timecost: 58.79, lr: 0.000100, Train: (LOSS: 0.0390, MAE: 0.0390, RMSE: 0.0607, R2: 0.9418), Valid: (LOSS: 0.0454, MAE: 0.0454, RMSE: 0.0716, R2: 0.9110), PNorm: 188.0750, GNorm: 0.5000
[46/299] timecost: 58.54, lr: 0.000100, Train: (LOSS: 0.0382, MAE: 0.0382, RMSE: 0.0589, R2: 0.9450), Valid: (LOSS: 0.0469, MAE: 0.0469, RMSE: 0.0761, R2: 0.8999), PNorm: 188.0917, GNorm: 0.4785
[47/299] timecost: 58.82, lr: 0.000100, Train: (LOSS: 0.0387, MAE: 0.0387, RMSE: 0.0604, R2: 0.9430), Valid: (LOSS: 0.0440, MAE: 0.0440, RMSE: 0.0686, R2: 0.9186), PNorm: 188.1071, GNorm: 0.5000
[48/299] timecost: 58.73, lr: 0.000100, Train: (LOSS: 0.0380, MAE: 0.0380, RMSE: 0.0582, R2: 0.9443), Valid: (LOSS: 0.0421, MAE: 0.0421, RMSE: 0.0648, R2: 0.9260), PNorm: 188.1215, GNorm: 0.4372
[49/299] timecost: 60.36, lr: 0.000100, Train: (LOSS: 0.0367, MAE: 0.0367, RMSE: 0.0569, R2: 0.9465), Valid: (LOSS: 0.0531, MAE: 0.0531, RMSE: 0.0820, R2: 0.8834), PNorm: 188.1374, GNorm: 0.4541
[50/299] timecost: 60.76, lr: 0.000100, Train: (LOSS: 0.0374, MAE: 0.0374, RMSE: 0.0584, R2: 0.9460), Valid: (LOSS: 0.0426, MAE: 0.0426, RMSE: 0.0680, R2: 0.9189), PNorm: 188.1528, GNorm: 0.4187
[51/299] timecost: 59.93, lr: 0.000100, Train: (LOSS: 0.0351, MAE: 0.0351, RMSE: 0.0553, R2: 0.9525), Valid: (LOSS: 0.0437, MAE: 0.0437, RMSE: 0.0698, R2: 0.9158), PNorm: 188.1666, GNorm: 0.5000
[52/299] timecost: 60.35, lr: 0.000100, Train: (LOSS: 0.0364, MAE: 0.0364, RMSE: 0.0556, R2: 0.9480), Valid: (LOSS: 0.0485, MAE: 0.0485, RMSE: 0.0788, R2: 0.8878), PNorm: 188.1865, GNorm: 0.4185
[53/299] timecost: 61.20, lr: 0.000100, Train: (LOSS: 0.0352, MAE: 0.0352, RMSE: 0.0550, R2: 0.9508), Valid: (LOSS: 0.0418, MAE: 0.0418, RMSE: 0.0696, R2: 0.9143), PNorm: 188.1989, GNorm: 0.5000
[54/299] timecost: 60.94, lr: 0.000100, Train: (LOSS: 0.0333, MAE: 0.0333, RMSE: 0.0535, R2: 0.9538), Valid: (LOSS: 0.0477, MAE: 0.0477, RMSE: 0.0764, R2: 0.8979), PNorm: 188.2118, GNorm: 0.3991
[55/299] timecost: 61.17, lr: 0.000100, Train: (LOSS: 0.0338, MAE: 0.0338, RMSE: 0.0530, R2: 0.9553), Valid: (LOSS: 0.0399, MAE: 0.0399, RMSE: 0.0643, R2: 0.9267), PNorm: 188.2267, GNorm: 0.5000
[56/299] timecost: 61.09, lr: 0.000100, Train: (LOSS: 0.0341, MAE: 0.0341, RMSE: 0.0541, R2: 0.9515), Valid: (LOSS: 0.0405, MAE: 0.0405, RMSE: 0.0638, R2: 0.9275), PNorm: 188.2406, GNorm: 0.5000
[57/299] timecost: 60.73, lr: 0.000100, Train: (LOSS: 0.0323, MAE: 0.0323, RMSE: 0.0518, R2: 0.9564), Valid: (LOSS: 0.0412, MAE: 0.0412, RMSE: 0.0676, R2: 0.9164), PNorm: 188.2537, GNorm: 0.5000
[58/299] timecost: 61.07, lr: 0.000100, Train: (LOSS: 0.0319, MAE: 0.0319, RMSE: 0.0493, R2: 0.9588), Valid: (LOSS: 0.0415, MAE: 0.0415, RMSE: 0.0642, R2: 0.9289), PNorm: 188.2669, GNorm: 0.5000
[59/299] timecost: 60.50, lr: 0.000100, Train: (LOSS: 0.0329, MAE: 0.0329, RMSE: 0.0518, R2: 0.9556), Valid: (LOSS: 0.0410, MAE: 0.0410, RMSE: 0.0668, R2: 0.9199), PNorm: 188.2794, GNorm: 0.3897
[60/299] timecost: 60.62, lr: 0.000100, Train: (LOSS: 0.0314, MAE: 0.0314, RMSE: 0.0499, R2: 0.9601), Valid: (LOSS: 0.0413, MAE: 0.0413, RMSE: 0.0654, R2: 0.9243), PNorm: 188.2917, GNorm: 0.4822
[61/299] timecost: 60.92, lr: 0.000100, Train: (LOSS: 0.0315, MAE: 0.0315, RMSE: 0.0498, R2: 0.9587), Valid: (LOSS: 0.0454, MAE: 0.0454, RMSE: 0.0759, R2: 0.8995), PNorm: 188.3086, GNorm: 0.4416
[62/299] timecost: 60.59, lr: 0.000100, Train: (LOSS: 0.0315, MAE: 0.0315, RMSE: 0.0502, R2: 0.9589), Valid: (LOSS: 0.0409, MAE: 0.0409, RMSE: 0.0657, R2: 0.9242), PNorm: 188.3250, GNorm: 0.3924
[63/299] timecost: 60.41, lr: 0.000100, Train: (LOSS: 0.0312, MAE: 0.0312, RMSE: 0.0496, R2: 0.9583), Valid: (LOSS: 0.0416, MAE: 0.0416, RMSE: 0.0646, R2: 0.9273), PNorm: 188.3373, GNorm: 0.5000
[64/299] timecost: 60.59, lr: 0.000100, Train: (LOSS: 0.0300, MAE: 0.0300, RMSE: 0.0479, R2: 0.9623), Valid: (LOSS: 0.0418, MAE: 0.0418, RMSE: 0.0669, R2: 0.9230), PNorm: 188.3515, GNorm: 0.4793
[65/299] timecost: 60.03, lr: 0.000100, Train: (LOSS: 0.0296, MAE: 0.0296, RMSE: 0.0477, R2: 0.9625), Valid: (LOSS: 0.0400, MAE: 0.0400, RMSE: 0.0669, R2: 0.9174), PNorm: 188.3647, GNorm: 0.4829
[66/299] timecost: 60.02, lr: 0.000100, Train: (LOSS: 0.0295, MAE: 0.0295, RMSE: 0.0468, R2: 0.9617), Valid: (LOSS: 0.0400, MAE: 0.0400, RMSE: 0.0668, R2: 0.9195), PNorm: 188.3795, GNorm: 0.5000
[67/299] timecost: 59.14, lr: 0.000100, Train: (LOSS: 0.0281, MAE: 0.0281, RMSE: 0.0452, R2: 0.9655), Valid: (LOSS: 0.0364, MAE: 0.0364, RMSE: 0.0574, R2: 0.9385), PNorm: 188.3947, GNorm: 0.3184
[68/299] timecost: 58.97, lr: 0.000100, Train: (LOSS: 0.0278, MAE: 0.0278, RMSE: 0.0444, R2: 0.9657), Valid: (LOSS: 0.0423, MAE: 0.0423, RMSE: 0.0690, R2: 0.9153), PNorm: 188.4108, GNorm: 0.3800
[69/299] timecost: 59.68, lr: 0.000100, Train: (LOSS: 0.0278, MAE: 0.0278, RMSE: 0.0448, R2: 0.9672), Valid: (LOSS: 0.0406, MAE: 0.0406, RMSE: 0.0652, R2: 0.9243), PNorm: 188.4256, GNorm: 0.3865
[70/299] timecost: 60.51, lr: 0.000100, Train: (LOSS: 0.0274, MAE: 0.0274, RMSE: 0.0437, R2: 0.9663), Valid: (LOSS: 0.0382, MAE: 0.0382, RMSE: 0.0631, R2: 0.9279), PNorm: 188.4376, GNorm: 0.5000
[71/299] timecost: 58.91, lr: 0.000100, Train: (LOSS: 0.0271, MAE: 0.0271, RMSE: 0.0431, R2: 0.9682), Valid: (LOSS: 0.0395, MAE: 0.0395, RMSE: 0.0668, R2: 0.9214), PNorm: 188.4507, GNorm: 0.5000
[72/299] timecost: 59.24, lr: 0.000100, Train: (LOSS: 0.0262, MAE: 0.0262, RMSE: 0.0421, R2: 0.9697), Valid: (LOSS: 0.0379, MAE: 0.0379, RMSE: 0.0574, R2: 0.9410), PNorm: 188.4629, GNorm: 0.4989
[73/299] timecost: 59.30, lr: 0.000100, Train: (LOSS: 0.0262, MAE: 0.0262, RMSE: 0.0424, R2: 0.9677), Valid: (LOSS: 0.0404, MAE: 0.0404, RMSE: 0.0716, R2: 0.9012), PNorm: 188.4751, GNorm: 0.5000
[74/299] timecost: 59.29, lr: 0.000100, Train: (LOSS: 0.0267, MAE: 0.0267, RMSE: 0.0438, R2: 0.9683), Valid: (LOSS: 0.0407, MAE: 0.0407, RMSE: 0.0634, R2: 0.9301), PNorm: 188.4888, GNorm: 0.3812
[75/299] timecost: 59.22, lr: 0.000100, Train: (LOSS: 0.0252, MAE: 0.0252, RMSE: 0.0413, R2: 0.9697), Valid: (LOSS: 0.0380, MAE: 0.0380, RMSE: 0.0582, R2: 0.9374), PNorm: 188.5014, GNorm: 0.4024
[76/299] timecost: 58.72, lr: 0.000100, Train: (LOSS: 0.0252, MAE: 0.0252, RMSE: 0.0410, R2: 0.9694), Valid: (LOSS: 0.0414, MAE: 0.0414, RMSE: 0.0661, R2: 0.9239), PNorm: 188.5128, GNorm: 0.3681
[77/299] timecost: 59.56, lr: 0.000100, Train: (LOSS: 0.0261, MAE: 0.0261, RMSE: 0.0419, R2: 0.9715), Valid: (LOSS: 0.0395, MAE: 0.0395, RMSE: 0.0627, R2: 0.9309), PNorm: 188.5284, GNorm: 0.5000
[78/299] timecost: 60.83, lr: 0.000100, Train: (LOSS: 0.0259, MAE: 0.0259, RMSE: 0.0419, R2: 0.9707), Valid: (LOSS: 0.0389, MAE: 0.0389, RMSE: 0.0631, R2: 0.9299), PNorm: 188.5413, GNorm: 0.5000
[79/299] timecost: 59.68, lr: 0.000100, Train: (LOSS: 0.0244, MAE: 0.0244, RMSE: 0.0405, R2: 0.9719), Valid: (LOSS: 0.0386, MAE: 0.0386, RMSE: 0.0611, R2: 0.9348), PNorm: 188.5546, GNorm: 0.5000
[80/299] timecost: 59.76, lr: 0.000100, Train: (LOSS: 0.0252, MAE: 0.0252, RMSE: 0.0415, R2: 0.9691), Valid: (LOSS: 0.0399, MAE: 0.0399, RMSE: 0.0651, R2: 0.9267), PNorm: 188.5717, GNorm: 0.5000
[81/299] timecost: 59.47, lr: 0.000100, Train: (LOSS: 0.0251, MAE: 0.0251, RMSE: 0.0406, R2: 0.9711), Valid: (LOSS: 0.0394, MAE: 0.0394, RMSE: 0.0610, R2: 0.9346), PNorm: 188.5858, GNorm: 0.4005
[82/299] timecost: 59.95, lr: 0.000100, Train: (LOSS: 0.0250, MAE: 0.0250, RMSE: 0.0402, R2: 0.9716), Valid: (LOSS: 0.0382, MAE: 0.0382, RMSE: 0.0638, R2: 0.9252), PNorm: 188.6008, GNorm: 0.3649
Epoch 00084: reducing learning rate of group 0 to 8.5000e-05.
[83/299] timecost: 60.93, lr: 0.000085, Train: (LOSS: 0.0235, MAE: 0.0235, RMSE: 0.0384, R2: 0.9749), Valid: (LOSS: 0.0383, MAE: 0.0383, RMSE: 0.0605, R2: 0.9367), PNorm: 188.6135, GNorm: 0.3714
[84/299] timecost: 61.03, lr: 0.000085, Train: (LOSS: 0.0228, MAE: 0.0228, RMSE: 0.0374, R2: 0.9742), Valid: (LOSS: 0.0393, MAE: 0.0393, RMSE: 0.0646, R2: 0.9263), PNorm: 188.6235, GNorm: 0.4222
[85/299] timecost: 60.98, lr: 0.000085, Train: (LOSS: 0.0244, MAE: 0.0244, RMSE: 0.0399, R2: 0.9719), Valid: (LOSS: 0.0383, MAE: 0.0383, RMSE: 0.0602, R2: 0.9393), PNorm: 188.6370, GNorm: 0.4977
[86/299] timecost: 60.59, lr: 0.000085, Train: (LOSS: 0.0226, MAE: 0.0226, RMSE: 0.0368, R2: 0.9765), Valid: (LOSS: 0.0374, MAE: 0.0374, RMSE: 0.0576, R2: 0.9405), PNorm: 188.6491, GNorm: 0.4311
[87/299] timecost: 60.08, lr: 0.000085, Train: (LOSS: 0.0227, MAE: 0.0227, RMSE: 0.0373, R2: 0.9746), Valid: (LOSS: 0.0376, MAE: 0.0376, RMSE: 0.0597, R2: 0.9366), PNorm: 188.6582, GNorm: 0.4228
[88/299] timecost: 59.82, lr: 0.000085, Train: (LOSS: 0.0219, MAE: 0.0219, RMSE: 0.0367, R2: 0.9758), Valid: (LOSS: 0.0373, MAE: 0.0373, RMSE: 0.0632, R2: 0.9288), PNorm: 188.6693, GNorm: 0.5000
[89/299] timecost: 60.10, lr: 0.000085, Train: (LOSS: 0.0217, MAE: 0.0217, RMSE: 0.0360, R2: 0.9753), Valid: (LOSS: 0.0366, MAE: 0.0366, RMSE: 0.0588, R2: 0.9391), PNorm: 188.6819, GNorm: 0.4453
[90/299] timecost: 60.62, lr: 0.000085, Train: (LOSS: 0.0218, MAE: 0.0218, RMSE: 0.0360, R2: 0.9763), Valid: (LOSS: 0.0394, MAE: 0.0394, RMSE: 0.0638, R2: 0.9265), PNorm: 188.6910, GNorm: 0.5000
[91/299] timecost: 61.47, lr: 0.000085, Train: (LOSS: 0.0209, MAE: 0.0209, RMSE: 0.0349, R2: 0.9780), Valid: (LOSS: 0.0354, MAE: 0.0354, RMSE: 0.0586, R2: 0.9384), PNorm: 188.7008, GNorm: 0.4505
[92/299] timecost: 62.35, lr: 0.000085, Train: (LOSS: 0.0215, MAE: 0.0215, RMSE: 0.0358, R2: 0.9759), Valid: (LOSS: 0.0396, MAE: 0.0396, RMSE: 0.0642, R2: 0.9274), PNorm: 188.7089, GNorm: 0.5000
[93/299] timecost: 62.34, lr: 0.000085, Train: (LOSS: 0.0212, MAE: 0.0212, RMSE: 0.0354, R2: 0.9777), Valid: (LOSS: 0.0358, MAE: 0.0358, RMSE: 0.0564, R2: 0.9426), PNorm: 188.7197, GNorm: 0.4435
[94/299] timecost: 62.35, lr: 0.000085, Train: (LOSS: 0.0216, MAE: 0.0216, RMSE: 0.0354, R2: 0.9756), Valid: (LOSS: 0.0365, MAE: 0.0365, RMSE: 0.0595, R2: 0.9378), PNorm: 188.7309, GNorm: 0.4989
[95/299] timecost: 62.31, lr: 0.000085, Train: (LOSS: 0.0202, MAE: 0.0202, RMSE: 0.0343, R2: 0.9775), Valid: (LOSS: 0.0368, MAE: 0.0368, RMSE: 0.0588, R2: 0.9374), PNorm: 188.7403, GNorm: 0.5000
[96/299] timecost: 62.37, lr: 0.000085, Train: (LOSS: 0.0207, MAE: 0.0207, RMSE: 0.0344, R2: 0.9764), Valid: (LOSS: 0.0359, MAE: 0.0359, RMSE: 0.0581, R2: 0.9385), PNorm: 188.7490, GNorm: 0.4913
[97/299] timecost: 62.48, lr: 0.000085, Train: (LOSS: 0.0210, MAE: 0.0210, RMSE: 0.0349, R2: 0.9767), Valid: (LOSS: 0.0370, MAE: 0.0370, RMSE: 0.0588, R2: 0.9391), PNorm: 188.7584, GNorm: 0.3459
[98/299] timecost: 62.24, lr: 0.000085, Train: (LOSS: 0.0196, MAE: 0.0196, RMSE: 0.0332, R2: 0.9792), Valid: (LOSS: 0.0358, MAE: 0.0358, RMSE: 0.0558, R2: 0.9447), PNorm: 188.7701, GNorm: 0.3700
[99/299] timecost: 62.44, lr: 0.000085, Train: (LOSS: 0.0201, MAE: 0.0201, RMSE: 0.0336, R2: 0.9797), Valid: (LOSS: 0.0367, MAE: 0.0367, RMSE: 0.0583, R2: 0.9411), PNorm: 188.7805, GNorm: 0.4993
[100/299] timecost: 62.46, lr: 0.000085, Train: (LOSS: 0.0194, MAE: 0.0194, RMSE: 0.0323, R2: 0.9799), Valid: (LOSS: 0.0387, MAE: 0.0387, RMSE: 0.0629, R2: 0.9273), PNorm: 188.7916, GNorm: 0.5000
[101/299] timecost: 62.23, lr: 0.000085, Train: (LOSS: 0.0195, MAE: 0.0195, RMSE: 0.0337, R2: 0.9781), Valid: (LOSS: 0.0372, MAE: 0.0372, RMSE: 0.0587, R2: 0.9383), PNorm: 188.8020, GNorm: 0.4117
[102/299] timecost: 62.42, lr: 0.000085, Train: (LOSS: 0.0192, MAE: 0.0192, RMSE: 0.0329, R2: 0.9807), Valid: (LOSS: 0.0365, MAE: 0.0365, RMSE: 0.0567, R2: 0.9414), PNorm: 188.8097, GNorm: 0.3658
[103/299] timecost: 62.21, lr: 0.000085, Train: (LOSS: 0.0201, MAE: 0.0201, RMSE: 0.0331, R2: 0.9794), Valid: (LOSS: 0.0357, MAE: 0.0357, RMSE: 0.0554, R2: 0.9445), PNorm: 188.8198, GNorm: 0.3676
[104/299] timecost: 62.34, lr: 0.000085, Train: (LOSS: 0.0192, MAE: 0.0192, RMSE: 0.0321, R2: 0.9815), Valid: (LOSS: 0.0365, MAE: 0.0365, RMSE: 0.0594, R2: 0.9370), PNorm: 188.8298, GNorm: 0.4284
[105/299] timecost: 62.31, lr: 0.000085, Train: (LOSS: 0.0192, MAE: 0.0192, RMSE: 0.0327, R2: 0.9805), Valid: (LOSS: 0.0365, MAE: 0.0365, RMSE: 0.0575, R2: 0.9414), PNorm: 188.8384, GNorm: 0.5000
[106/299] timecost: 62.41, lr: 0.000085, Train: (LOSS: 0.0186, MAE: 0.0186, RMSE: 0.0317, R2: 0.9803), Valid: (LOSS: 0.0376, MAE: 0.0376, RMSE: 0.0607, R2: 0.9327), PNorm: 188.8473, GNorm: 0.5000
Epoch 00108: reducing learning rate of group 0 to 7.2250e-05.
[107/299] timecost: 62.29, lr: 0.000072, Train: (LOSS: 0.0185, MAE: 0.0185, RMSE: 0.0313, R2: 0.9814), Valid: (LOSS: 0.0360, MAE: 0.0360, RMSE: 0.0551, R2: 0.9484), PNorm: 188.8592, GNorm: 0.4521
[108/299] timecost: 62.50, lr: 0.000072, Train: (LOSS: 0.0172, MAE: 0.0172, RMSE: 0.0300, R2: 0.9813), Valid: (LOSS: 0.0351, MAE: 0.0351, RMSE: 0.0597, R2: 0.9354), PNorm: 188.8662, GNorm: 0.4072
[109/299] timecost: 62.43, lr: 0.000072, Train: (LOSS: 0.0171, MAE: 0.0171, RMSE: 0.0305, R2: 0.9814), Valid: (LOSS: 0.0345, MAE: 0.0345, RMSE: 0.0541, R2: 0.9486), PNorm: 188.8726, GNorm: 0.5000
[110/299] timecost: 61.52, lr: 0.000072, Train: (LOSS: 0.0170, MAE: 0.0170, RMSE: 0.0297, R2: 0.9824), Valid: (LOSS: 0.0360, MAE: 0.0360, RMSE: 0.0578, R2: 0.9410), PNorm: 188.8798, GNorm: 0.3865
[111/299] timecost: 60.64, lr: 0.000072, Train: (LOSS: 0.0174, MAE: 0.0174, RMSE: 0.0299, R2: 0.9790), Valid: (LOSS: 0.0363, MAE: 0.0363, RMSE: 0.0594, R2: 0.9380), PNorm: 188.8889, GNorm: 0.4157
[112/299] timecost: 60.40, lr: 0.000072, Train: (LOSS: 0.0165, MAE: 0.0165, RMSE: 0.0293, R2: 0.9834), Valid: (LOSS: 0.0365, MAE: 0.0365, RMSE: 0.0596, R2: 0.9376), PNorm: 188.8960, GNorm: 0.5000
[113/299] timecost: 60.45, lr: 0.000072, Train: (LOSS: 0.0167, MAE: 0.0167, RMSE: 0.0292, R2: 0.9830), Valid: (LOSS: 0.0339, MAE: 0.0339, RMSE: 0.0530, R2: 0.9505), PNorm: 188.9035, GNorm: 0.5000
[114/299] timecost: 59.88, lr: 0.000072, Train: (LOSS: 0.0160, MAE: 0.0160, RMSE: 0.0285, R2: 0.9833), Valid: (LOSS: 0.0353, MAE: 0.0353, RMSE: 0.0553, R2: 0.9452), PNorm: 188.9105, GNorm: 0.5000
[115/299] timecost: 60.12, lr: 0.000072, Train: (LOSS: 0.0162, MAE: 0.0162, RMSE: 0.0289, R2: 0.9828), Valid: (LOSS: 0.0344, MAE: 0.0344, RMSE: 0.0551, R2: 0.9464), PNorm: 188.9164, GNorm: 0.3742
[116/299] timecost: 60.25, lr: 0.000072, Train: (LOSS: 0.0167, MAE: 0.0167, RMSE: 0.0285, R2: 0.9827), Valid: (LOSS: 0.0391, MAE: 0.0391, RMSE: 0.0609, R2: 0.9367), PNorm: 188.9246, GNorm: 0.5000
[117/299] timecost: 59.79, lr: 0.000072, Train: (LOSS: 0.0169, MAE: 0.0169, RMSE: 0.0296, R2: 0.9828), Valid: (LOSS: 0.0361, MAE: 0.0361, RMSE: 0.0560, R2: 0.9464), PNorm: 188.9332, GNorm: 0.3280
[118/299] timecost: 60.05, lr: 0.000072, Train: (LOSS: 0.0161, MAE: 0.0161, RMSE: 0.0281, R2: 0.9837), Valid: (LOSS: 0.0352, MAE: 0.0352, RMSE: 0.0574, R2: 0.9413), PNorm: 188.9417, GNorm: 0.4833
[119/299] timecost: 58.61, lr: 0.000072, Train: (LOSS: 0.0165, MAE: 0.0165, RMSE: 0.0288, R2: 0.9836), Valid: (LOSS: 0.0351, MAE: 0.0351, RMSE: 0.0531, R2: 0.9509), PNorm: 188.9507, GNorm: 0.4627
[120/299] timecost: 58.13, lr: 0.000072, Train: (LOSS: 0.0158, MAE: 0.0158, RMSE: 0.0274, R2: 0.9832), Valid: (LOSS: 0.0353, MAE: 0.0353, RMSE: 0.0540, R2: 0.9492), PNorm: 188.9587, GNorm: 0.3661
[121/299] timecost: 58.82, lr: 0.000072, Train: (LOSS: 0.0162, MAE: 0.0162, RMSE: 0.0294, R2: 0.9831), Valid: (LOSS: 0.0331, MAE: 0.0331, RMSE: 0.0521, R2: 0.9518), PNorm: 188.9669, GNorm: 0.5000
[122/299] timecost: 60.87, lr: 0.000072, Train: (LOSS: 0.0155, MAE: 0.0155, RMSE: 0.0278, R2: 0.9844), Valid: (LOSS: 0.0337, MAE: 0.0337, RMSE: 0.0539, R2: 0.9487), PNorm: 188.9738, GNorm: 0.3959
[123/299] timecost: 59.84, lr: 0.000072, Train: (LOSS: 0.0160, MAE: 0.0160, RMSE: 0.0285, R2: 0.9833), Valid: (LOSS: 0.0362, MAE: 0.0362, RMSE: 0.0573, R2: 0.9428), PNorm: 188.9813, GNorm: 0.5000
[124/299] timecost: 59.44, lr: 0.000072, Train: (LOSS: 0.0163, MAE: 0.0163, RMSE: 0.0284, R2: 0.9845), Valid: (LOSS: 0.0358, MAE: 0.0358, RMSE: 0.0563, R2: 0.9442), PNorm: 188.9890, GNorm: 0.4362
[125/299] timecost: 59.48, lr: 0.000072, Train: (LOSS: 0.0152, MAE: 0.0152, RMSE: 0.0273, R2: 0.9844), Valid: (LOSS: 0.0364, MAE: 0.0364, RMSE: 0.0577, R2: 0.9427), PNorm: 188.9974, GNorm: 0.3953
[126/299] timecost: 59.67, lr: 0.000072, Train: (LOSS: 0.0153, MAE: 0.0153, RMSE: 0.0272, R2: 0.9857), Valid: (LOSS: 0.0341, MAE: 0.0341, RMSE: 0.0552, R2: 0.9461), PNorm: 189.0049, GNorm: 0.4774
[127/299] timecost: 58.83, lr: 0.000072, Train: (LOSS: 0.0153, MAE: 0.0153, RMSE: 0.0274, R2: 0.9836), Valid: (LOSS: 0.0354, MAE: 0.0354, RMSE: 0.0568, R2: 0.9433), PNorm: 189.0120, GNorm: 0.4519
[128/299] timecost: 58.12, lr: 0.000072, Train: (LOSS: 0.0153, MAE: 0.0153, RMSE: 0.0276, R2: 0.9844), Valid: (LOSS: 0.0357, MAE: 0.0357, RMSE: 0.0580, R2: 0.9417), PNorm: 189.0221, GNorm: 0.3986
[129/299] timecost: 58.34, lr: 0.000072, Train: (LOSS: 0.0151, MAE: 0.0151, RMSE: 0.0272, R2: 0.9843), Valid: (LOSS: 0.0344, MAE: 0.0344, RMSE: 0.0532, R2: 0.9507), PNorm: 189.0293, GNorm: 0.5000
[130/299] timecost: 57.87, lr: 0.000072, Train: (LOSS: 0.0155, MAE: 0.0155, RMSE: 0.0271, R2: 0.9851), Valid: (LOSS: 0.0352, MAE: 0.0352, RMSE: 0.0570, R2: 0.9437), PNorm: 189.0375, GNorm: 0.4470
[131/299] timecost: 58.41, lr: 0.000072, Train: (LOSS: 0.0151, MAE: 0.0151, RMSE: 0.0272, R2: 0.9842), Valid: (LOSS: 0.0347, MAE: 0.0347, RMSE: 0.0563, R2: 0.9449), PNorm: 189.0458, GNorm: 0.4377
[132/299] timecost: 58.56, lr: 0.000072, Train: (LOSS: 0.0151, MAE: 0.0151, RMSE: 0.0272, R2: 0.9849), Valid: (LOSS: 0.0345, MAE: 0.0345, RMSE: 0.0536, R2: 0.9505), PNorm: 189.0546, GNorm: 0.5000
[133/299] timecost: 58.10, lr: 0.000072, Train: (LOSS: 0.0147, MAE: 0.0147, RMSE: 0.0273, R2: 0.9850), Valid: (LOSS: 0.0364, MAE: 0.0364, RMSE: 0.0562, R2: 0.9456), PNorm: 189.0628, GNorm: 0.5000
[134/299] timecost: 58.17, lr: 0.000072, Train: (LOSS: 0.0153, MAE: 0.0153, RMSE: 0.0275, R2: 0.9840), Valid: (LOSS: 0.0354, MAE: 0.0354, RMSE: 0.0555, R2: 0.9460), PNorm: 189.0721, GNorm: 0.4005
[135/299] timecost: 58.26, lr: 0.000072, Train: (LOSS: 0.0145, MAE: 0.0145, RMSE: 0.0265, R2: 0.9857), Valid: (LOSS: 0.0362, MAE: 0.0362, RMSE: 0.0558, R2: 0.9461), PNorm: 189.0804, GNorm: 0.4461
[136/299] timecost: 58.32, lr: 0.000072, Train: (LOSS: 0.0151, MAE: 0.0151, RMSE: 0.0267, R2: 0.9853), Valid: (LOSS: 0.0359, MAE: 0.0359, RMSE: 0.0572, R2: 0.9430), PNorm: 189.0882, GNorm: 0.3884
Epoch 00138: reducing learning rate of group 0 to 6.1413e-05.
[137/299] timecost: 58.22, lr: 0.000061, Train: (LOSS: 0.0147, MAE: 0.0147, RMSE: 0.0260, R2: 0.9841), Valid: (LOSS: 0.0343, MAE: 0.0343, RMSE: 0.0532, R2: 0.9502), PNorm: 189.0968, GNorm: 0.3680
[138/299] timecost: 58.25, lr: 0.000061, Train: (LOSS: 0.0145, MAE: 0.0145, RMSE: 0.0267, R2: 0.9858), Valid: (LOSS: 0.0367, MAE: 0.0367, RMSE: 0.0558, R2: 0.9457), PNorm: 189.1030, GNorm: 0.3878
[139/299] timecost: 60.27, lr: 0.000061, Train: (LOSS: 0.0138, MAE: 0.0138, RMSE: 0.0257, R2: 0.9858), Valid: (LOSS: 0.0343, MAE: 0.0343, RMSE: 0.0525, R2: 0.9505), PNorm: 189.1081, GNorm: 0.3746
[140/299] timecost: 60.07, lr: 0.000061, Train: (LOSS: 0.0132, MAE: 0.0132, RMSE: 0.0247, R2: 0.9851), Valid: (LOSS: 0.0356, MAE: 0.0356, RMSE: 0.0551, R2: 0.9477), PNorm: 189.1134, GNorm: 0.3757
[141/299] timecost: 60.19, lr: 0.000061, Train: (LOSS: 0.0136, MAE: 0.0136, RMSE: 0.0249, R2: 0.9865), Valid: (LOSS: 0.0363, MAE: 0.0363, RMSE: 0.0564, R2: 0.9447), PNorm: 189.1198, GNorm: 0.4904
[142/299] timecost: 60.15, lr: 0.000061, Train: (LOSS: 0.0134, MAE: 0.0134, RMSE: 0.0246, R2: 0.9852), Valid: (LOSS: 0.0359, MAE: 0.0359, RMSE: 0.0551, R2: 0.9479), PNorm: 189.1242, GNorm: 0.5000
[143/299] timecost: 60.43, lr: 0.000061, Train: (LOSS: 0.0130, MAE: 0.0130, RMSE: 0.0245, R2: 0.9874), Valid: (LOSS: 0.0351, MAE: 0.0351, RMSE: 0.0556, R2: 0.9458), PNorm: 189.1312, GNorm: 0.3857
[144/299] timecost: 60.63, lr: 0.000061, Train: (LOSS: 0.0133, MAE: 0.0133, RMSE: 0.0250, R2: 0.9866), Valid: (LOSS: 0.0338, MAE: 0.0338, RMSE: 0.0534, R2: 0.9505), PNorm: 189.1366, GNorm: 0.3766
[145/299] timecost: 60.15, lr: 0.000061, Train: (LOSS: 0.0133, MAE: 0.0133, RMSE: 0.0248, R2: 0.9858), Valid: (LOSS: 0.0344, MAE: 0.0344, RMSE: 0.0543, R2: 0.9478), PNorm: 189.1415, GNorm: 0.4236
[146/299] timecost: 60.21, lr: 0.000061, Train: (LOSS: 0.0128, MAE: 0.0128, RMSE: 0.0240, R2: 0.9870), Valid: (LOSS: 0.0352, MAE: 0.0352, RMSE: 0.0565, R2: 0.9433), PNorm: 189.1474, GNorm: 0.5000
[147/299] timecost: 60.02, lr: 0.000061, Train: (LOSS: 0.0130, MAE: 0.0130, RMSE: 0.0242, R2: 0.9866), Valid: (LOSS: 0.0345, MAE: 0.0345, RMSE: 0.0539, R2: 0.9496), PNorm: 189.1539, GNorm: 0.4184
[148/299] timecost: 60.07, lr: 0.000061, Train: (LOSS: 0.0126, MAE: 0.0126, RMSE: 0.0244, R2: 0.9865), Valid: (LOSS: 0.0331, MAE: 0.0331, RMSE: 0.0522, R2: 0.9522), PNorm: 189.1598, GNorm: 0.5000
[149/299] timecost: 59.72, lr: 0.000061, Train: (LOSS: 0.0128, MAE: 0.0128, RMSE: 0.0243, R2: 0.9873), Valid: (LOSS: 0.0345, MAE: 0.0345, RMSE: 0.0538, R2: 0.9496), PNorm: 189.1671, GNorm: 0.4939
[150/299] timecost: 61.72, lr: 0.000061, Train: (LOSS: 0.0128, MAE: 0.0128, RMSE: 0.0240, R2: 0.9870), Valid: (LOSS: 0.0354, MAE: 0.0354, RMSE: 0.0542, R2: 0.9481), PNorm: 189.1725, GNorm: 0.3995
[151/299] timecost: 60.62, lr: 0.000061, Train: (LOSS: 0.0127, MAE: 0.0127, RMSE: 0.0242, R2: 0.9863), Valid: (LOSS: 0.0349, MAE: 0.0349, RMSE: 0.0535, R2: 0.9495), PNorm: 189.1791, GNorm: 0.4587
[152/299] timecost: 60.89, lr: 0.000061, Train: (LOSS: 0.0128, MAE: 0.0128, RMSE: 0.0240, R2: 0.9870), Valid: (LOSS: 0.0356, MAE: 0.0356, RMSE: 0.0557, R2: 0.9469), PNorm: 189.1859, GNorm: 0.3604
Epoch 00154: reducing learning rate of group 0 to 5.2201e-05.
[153/299] timecost: 60.71, lr: 0.000052, Train: (LOSS: 0.0128, MAE: 0.0128, RMSE: 0.0241, R2: 0.9870), Valid: (LOSS: 0.0358, MAE: 0.0358, RMSE: 0.0544, R2: 0.9486), PNorm: 189.1927, GNorm: 0.4236
[154/299] timecost: 59.49, lr: 0.000052, Train: (LOSS: 0.0120, MAE: 0.0120, RMSE: 0.0234, R2: 0.9881), Valid: (LOSS: 0.0347, MAE: 0.0347, RMSE: 0.0542, R2: 0.9489), PNorm: 189.1981, GNorm: 0.3527
[155/299] timecost: 59.23, lr: 0.000052, Train: (LOSS: 0.0117, MAE: 0.0117, RMSE: 0.0232, R2: 0.9873), Valid: (LOSS: 0.0335, MAE: 0.0335, RMSE: 0.0518, R2: 0.9527), PNorm: 189.2021, GNorm: 0.3777
[156/299] timecost: 60.95, lr: 0.000052, Train: (LOSS: 0.0119, MAE: 0.0119, RMSE: 0.0228, R2: 0.9880), Valid: (LOSS: 0.0333, MAE: 0.0333, RMSE: 0.0511, R2: 0.9544), PNorm: 189.2057, GNorm: 0.5000
[157/299] timecost: 60.64, lr: 0.000052, Train: (LOSS: 0.0118, MAE: 0.0118, RMSE: 0.0233, R2: 0.9880), Valid: (LOSS: 0.0339, MAE: 0.0339, RMSE: 0.0536, R2: 0.9499), PNorm: 189.2112, GNorm: 0.5000
[158/299] timecost: 60.41, lr: 0.000052, Train: (LOSS: 0.0118, MAE: 0.0118, RMSE: 0.0224, R2: 0.9876), Valid: (LOSS: 0.0341, MAE: 0.0341, RMSE: 0.0523, R2: 0.9525), PNorm: 189.2170, GNorm: 0.4251
[159/299] timecost: 60.17, lr: 0.000052, Train: (LOSS: 0.0117, MAE: 0.0117, RMSE: 0.0228, R2: 0.9877), Valid: (LOSS: 0.0338, MAE: 0.0338, RMSE: 0.0523, R2: 0.9526), PNorm: 189.2198, GNorm: 0.4304
[160/299] timecost: 59.95, lr: 0.000052, Train: (LOSS: 0.0117, MAE: 0.0117, RMSE: 0.0231, R2: 0.9882), Valid: (LOSS: 0.0342, MAE: 0.0342, RMSE: 0.0533, R2: 0.9506), PNorm: 189.2252, GNorm: 0.5000
[161/299] timecost: 60.30, lr: 0.000052, Train: (LOSS: 0.0117, MAE: 0.0117, RMSE: 0.0221, R2: 0.9878), Valid: (LOSS: 0.0334, MAE: 0.0334, RMSE: 0.0511, R2: 0.9548), PNorm: 189.2302, GNorm: 0.3998
[162/299] timecost: 60.59, lr: 0.000052, Train: (LOSS: 0.0116, MAE: 0.0116, RMSE: 0.0222, R2: 0.9875), Valid: (LOSS: 0.0360, MAE: 0.0360, RMSE: 0.0561, R2: 0.9457), PNorm: 189.2345, GNorm: 0.4850
[163/299] timecost: 60.77, lr: 0.000052, Train: (LOSS: 0.0110, MAE: 0.0110, RMSE: 0.0224, R2: 0.9881), Valid: (LOSS: 0.0356, MAE: 0.0356, RMSE: 0.0542, R2: 0.9486), PNorm: 189.2396, GNorm: 0.3690
[164/299] timecost: 60.98, lr: 0.000052, Train: (LOSS: 0.0111, MAE: 0.0111, RMSE: 0.0221, R2: 0.9877), Valid: (LOSS: 0.0350, MAE: 0.0350, RMSE: 0.0526, R2: 0.9516), PNorm: 189.2429, GNorm: 0.3823
[165/299] timecost: 60.84, lr: 0.000052, Train: (LOSS: 0.0115, MAE: 0.0115, RMSE: 0.0229, R2: 0.9882), Valid: (LOSS: 0.0342, MAE: 0.0342, RMSE: 0.0522, R2: 0.9532), PNorm: 189.2477, GNorm: 0.4376
[166/299] timecost: 60.89, lr: 0.000052, Train: (LOSS: 0.0112, MAE: 0.0112, RMSE: 0.0222, R2: 0.9873), Valid: (LOSS: 0.0335, MAE: 0.0335, RMSE: 0.0511, R2: 0.9533), PNorm: 189.2516, GNorm: 0.5000
[167/299] timecost: 60.56, lr: 0.000052, Train: (LOSS: 0.0111, MAE: 0.0111, RMSE: 0.0220, R2: 0.9884), Valid: (LOSS: 0.0350, MAE: 0.0350, RMSE: 0.0530, R2: 0.9519), PNorm: 189.2574, GNorm: 0.4294
[168/299] timecost: 60.38, lr: 0.000052, Train: (LOSS: 0.0111, MAE: 0.0111, RMSE: 0.0217, R2: 0.9886), Valid: (LOSS: 0.0333, MAE: 0.0333, RMSE: 0.0524, R2: 0.9517), PNorm: 189.2616, GNorm: 0.4770
Epoch 00170: reducing learning rate of group 0 to 4.4371e-05.
[169/299] timecost: 58.41, lr: 0.000044, Train: (LOSS: 0.0114, MAE: 0.0114, RMSE: 0.0225, R2: 0.9885), Valid: (LOSS: 0.0351, MAE: 0.0351, RMSE: 0.0537, R2: 0.9505), PNorm: 189.2666, GNorm: 0.4176
[170/299] timecost: 58.88, lr: 0.000044, Train: (LOSS: 0.0105, MAE: 0.0105, RMSE: 0.0210, R2: 0.9893), Valid: (LOSS: 0.0336, MAE: 0.0336, RMSE: 0.0526, R2: 0.9528), PNorm: 189.2699, GNorm: 0.5000
[171/299] timecost: 59.07, lr: 0.000044, Train: (LOSS: 0.0103, MAE: 0.0103, RMSE: 0.0210, R2: 0.9887), Valid: (LOSS: 0.0341, MAE: 0.0341, RMSE: 0.0524, R2: 0.9520), PNorm: 189.2738, GNorm: 0.4511
[172/299] timecost: 59.08, lr: 0.000044, Train: (LOSS: 0.0101, MAE: 0.0101, RMSE: 0.0214, R2: 0.9891), Valid: (LOSS: 0.0335, MAE: 0.0335, RMSE: 0.0518, R2: 0.9533), PNorm: 189.2780, GNorm: 0.3405
[173/299] timecost: 58.73, lr: 0.000044, Train: (LOSS: 0.0102, MAE: 0.0102, RMSE: 0.0215, R2: 0.9889), Valid: (LOSS: 0.0346, MAE: 0.0346, RMSE: 0.0532, R2: 0.9505), PNorm: 189.2812, GNorm: 0.5000
[174/299] timecost: 58.69, lr: 0.000044, Train: (LOSS: 0.0101, MAE: 0.0101, RMSE: 0.0208, R2: 0.9889), Valid: (LOSS: 0.0340, MAE: 0.0340, RMSE: 0.0521, R2: 0.9528), PNorm: 189.2846, GNorm: 0.3989
[175/299] timecost: 58.38, lr: 0.000044, Train: (LOSS: 0.0101, MAE: 0.0101, RMSE: 0.0215, R2: 0.9875), Valid: (LOSS: 0.0348, MAE: 0.0348, RMSE: 0.0535, R2: 0.9500), PNorm: 189.2884, GNorm: 0.4721
[176/299] timecost: 58.53, lr: 0.000044, Train: (LOSS: 0.0101, MAE: 0.0101, RMSE: 0.0212, R2: 0.9880), Valid: (LOSS: 0.0348, MAE: 0.0348, RMSE: 0.0534, R2: 0.9513), PNorm: 189.2911, GNorm: 0.3658
[177/299] timecost: 58.67, lr: 0.000044, Train: (LOSS: 0.0100, MAE: 0.0100, RMSE: 0.0210, R2: 0.9894), Valid: (LOSS: 0.0345, MAE: 0.0345, RMSE: 0.0551, R2: 0.9478), PNorm: 189.2938, GNorm: 0.3620
[178/299] timecost: 58.67, lr: 0.000044, Train: (LOSS: 0.0100, MAE: 0.0100, RMSE: 0.0211, R2: 0.9892), Valid: (LOSS: 0.0342, MAE: 0.0342, RMSE: 0.0533, R2: 0.9511), PNorm: 189.2986, GNorm: 0.4957
[179/299] timecost: 59.04, lr: 0.000044, Train: (LOSS: 0.0100, MAE: 0.0100, RMSE: 0.0209, R2: 0.9893), Valid: (LOSS: 0.0346, MAE: 0.0346, RMSE: 0.0530, R2: 0.9506), PNorm: 189.3016, GNorm: 0.4170
[180/299] timecost: 58.82, lr: 0.000044, Train: (LOSS: 0.0100, MAE: 0.0100, RMSE: 0.0208, R2: 0.9895), Valid: (LOSS: 0.0341, MAE: 0.0341, RMSE: 0.0518, R2: 0.9535), PNorm: 189.3054, GNorm: 0.5000
[181/299] timecost: 59.26, lr: 0.000044, Train: (LOSS: 0.0097, MAE: 0.0097, RMSE: 0.0207, R2: 0.9892), Valid: (LOSS: 0.0337, MAE: 0.0337, RMSE: 0.0516, R2: 0.9537), PNorm: 189.3086, GNorm: 0.3452
[182/299] timecost: 60.40, lr: 0.000044, Train: (LOSS: 0.0099, MAE: 0.0099, RMSE: 0.0211, R2: 0.9896), Valid: (LOSS: 0.0344, MAE: 0.0344, RMSE: 0.0534, R2: 0.9512), PNorm: 189.3121, GNorm: 0.4966
[183/299] timecost: 59.21, lr: 0.000044, Train: (LOSS: 0.0097, MAE: 0.0097, RMSE: 0.0211, R2: 0.9895), Valid: (LOSS: 0.0343, MAE: 0.0343, RMSE: 0.0526, R2: 0.9525), PNorm: 189.3155, GNorm: 0.3291
[184/299] timecost: 60.05, lr: 0.000044, Train: (LOSS: 0.0099, MAE: 0.0099, RMSE: 0.0208, R2: 0.9887), Valid: (LOSS: 0.0337, MAE: 0.0337, RMSE: 0.0533, R2: 0.9505), PNorm: 189.3187, GNorm: 0.4750
Epoch 00186: reducing learning rate of group 0 to 3.7715e-05.
[185/299] timecost: 59.70, lr: 0.000038, Train: (LOSS: 0.0100, MAE: 0.0100, RMSE: 0.0203, R2: 0.9886), Valid: (LOSS: 0.0344, MAE: 0.0344, RMSE: 0.0514, R2: 0.9546), PNorm: 189.3225, GNorm: 0.4031
[186/299] timecost: 58.45, lr: 0.000038, Train: (LOSS: 0.0096, MAE: 0.0096, RMSE: 0.0207, R2: 0.9896), Valid: (LOSS: 0.0348, MAE: 0.0348, RMSE: 0.0536, R2: 0.9510), PNorm: 189.3264, GNorm: 0.4463
[187/299] timecost: 59.64, lr: 0.000038, Train: (LOSS: 0.0090, MAE: 0.0090, RMSE: 0.0199, R2: 0.9902), Valid: (LOSS: 0.0347, MAE: 0.0347, RMSE: 0.0531, R2: 0.9510), PNorm: 189.3286, GNorm: 0.5000
[188/299] timecost: 60.23, lr: 0.000038, Train: (LOSS: 0.0089, MAE: 0.0089, RMSE: 0.0198, R2: 0.9895), Valid: (LOSS: 0.0342, MAE: 0.0342, RMSE: 0.0523, R2: 0.9528), PNorm: 189.3308, GNorm: 0.5000
[189/299] timecost: 60.74, lr: 0.000038, Train: (LOSS: 0.0091, MAE: 0.0091, RMSE: 0.0193, R2: 0.9895), Valid: (LOSS: 0.0341, MAE: 0.0341, RMSE: 0.0521, R2: 0.9527), PNorm: 189.3326, GNorm: 0.4973
[190/299] timecost: 60.98, lr: 0.000038, Train: (LOSS: 0.0088, MAE: 0.0088, RMSE: 0.0194, R2: 0.9899), Valid: (LOSS: 0.0332, MAE: 0.0332, RMSE: 0.0516, R2: 0.9539), PNorm: 189.3357, GNorm: 0.3536
[191/299] timecost: 59.93, lr: 0.000038, Train: (LOSS: 0.0090, MAE: 0.0090, RMSE: 0.0199, R2: 0.9904), Valid: (LOSS: 0.0340, MAE: 0.0340, RMSE: 0.0527, R2: 0.9522), PNorm: 189.3378, GNorm: 0.4476
[192/299] timecost: 59.36, lr: 0.000038, Train: (LOSS: 0.0089, MAE: 0.0089, RMSE: 0.0196, R2: 0.9898), Valid: (LOSS: 0.0337, MAE: 0.0337, RMSE: 0.0517, R2: 0.9536), PNorm: 189.3398, GNorm: 0.4270
[193/299] timecost: 59.24, lr: 0.000038, Train: (LOSS: 0.0089, MAE: 0.0089, RMSE: 0.0197, R2: 0.9895), Valid: (LOSS: 0.0337, MAE: 0.0337, RMSE: 0.0519, R2: 0.9535), PNorm: 189.3431, GNorm: 0.3797
[194/299] timecost: 59.26, lr: 0.000038, Train: (LOSS: 0.0090, MAE: 0.0090, RMSE: 0.0200, R2: 0.9905), Valid: (LOSS: 0.0341, MAE: 0.0341, RMSE: 0.0527, R2: 0.9523), PNorm: 189.3459, GNorm: 0.3096
[195/299] timecost: 59.40, lr: 0.000038, Train: (LOSS: 0.0088, MAE: 0.0088, RMSE: 0.0194, R2: 0.9895), Valid: (LOSS: 0.0349, MAE: 0.0349, RMSE: 0.0528, R2: 0.9514), PNorm: 189.3489, GNorm: 0.4412
[196/299] timecost: 58.56, lr: 0.000038, Train: (LOSS: 0.0089, MAE: 0.0089, RMSE: 0.0190, R2: 0.9899), Valid: (LOSS: 0.0332, MAE: 0.0332, RMSE: 0.0509, R2: 0.9551), PNorm: 189.3508, GNorm: 0.4944
[197/299] timecost: 58.77, lr: 0.000038, Train: (LOSS: 0.0089, MAE: 0.0089, RMSE: 0.0193, R2: 0.9897), Valid: (LOSS: 0.0343, MAE: 0.0343, RMSE: 0.0526, R2: 0.9524), PNorm: 189.3528, GNorm: 0.3861
[198/299] timecost: 58.55, lr: 0.000038, Train: (LOSS: 0.0090, MAE: 0.0090, RMSE: 0.0191, R2: 0.9900), Valid: (LOSS: 0.0348, MAE: 0.0348, RMSE: 0.0528, R2: 0.9518), PNorm: 189.3560, GNorm: 0.4295
[199/299] timecost: 58.61, lr: 0.000038, Train: (LOSS: 0.0090, MAE: 0.0090, RMSE: 0.0187, R2: 0.9895), Valid: (LOSS: 0.0334, MAE: 0.0334, RMSE: 0.0514, R2: 0.9541), PNorm: 189.3599, GNorm: 0.4897
[200/299] timecost: 58.51, lr: 0.000038, Train: (LOSS: 0.0088, MAE: 0.0088, RMSE: 0.0192, R2: 0.9903), Valid: (LOSS: 0.0338, MAE: 0.0338, RMSE: 0.0512, R2: 0.9551), PNorm: 189.3621, GNorm: 0.5000
Epoch 00202: reducing learning rate of group 0 to 3.2058e-05.
[201/299] timecost: 58.49, lr: 0.000032, Train: (LOSS: 0.0089, MAE: 0.0089, RMSE: 0.0194, R2: 0.9903), Valid: (LOSS: 0.0335, MAE: 0.0335, RMSE: 0.0509, R2: 0.9550), PNorm: 189.3645, GNorm: 0.4144
[202/299] timecost: 60.44, lr: 0.000032, Train: (LOSS: 0.0085, MAE: 0.0085, RMSE: 0.0188, R2: 0.9895), Valid: (LOSS: 0.0335, MAE: 0.0335, RMSE: 0.0516, R2: 0.9540), PNorm: 189.3669, GNorm: 0.3278
[203/299] timecost: 61.67, lr: 0.000032, Train: (LOSS: 0.0082, MAE: 0.0082, RMSE: 0.0193, R2: 0.9901), Valid: (LOSS: 0.0345, MAE: 0.0345, RMSE: 0.0521, R2: 0.9532), PNorm: 189.3677, GNorm: 0.5000
[204/299] timecost: 60.30, lr: 0.000032, Train: (LOSS: 0.0083, MAE: 0.0083, RMSE: 0.0190, R2: 0.9907), Valid: (LOSS: 0.0335, MAE: 0.0335, RMSE: 0.0511, R2: 0.9547), PNorm: 189.3708, GNorm: 0.4659
[205/299] timecost: 59.89, lr: 0.000032, Train: (LOSS: 0.0082, MAE: 0.0082, RMSE: 0.0184, R2: 0.9907), Valid: (LOSS: 0.0333, MAE: 0.0333, RMSE: 0.0511, R2: 0.9548), PNorm: 189.3723, GNorm: 0.3399
[206/299] timecost: 58.45, lr: 0.000032, Train: (LOSS: 0.0083, MAE: 0.0083, RMSE: 0.0186, R2: 0.9903), Valid: (LOSS: 0.0343, MAE: 0.0343, RMSE: 0.0510, R2: 0.9551), PNorm: 189.3738, GNorm: 0.3967
[207/299] timecost: 58.90, lr: 0.000032, Train: (LOSS: 0.0082, MAE: 0.0082, RMSE: 0.0186, R2: 0.9906), Valid: (LOSS: 0.0329, MAE: 0.0329, RMSE: 0.0502, R2: 0.9563), PNorm: 189.3767, GNorm: 0.3425
[208/299] timecost: 58.35, lr: 0.000032, Train: (LOSS: 0.0080, MAE: 0.0080, RMSE: 0.0182, R2: 0.9906), Valid: (LOSS: 0.0338, MAE: 0.0338, RMSE: 0.0514, R2: 0.9538), PNorm: 189.3782, GNorm: 0.4624
[209/299] timecost: 59.95, lr: 0.000032, Train: (LOSS: 0.0080, MAE: 0.0080, RMSE: 0.0173, R2: 0.9911), Valid: (LOSS: 0.0338, MAE: 0.0338, RMSE: 0.0518, R2: 0.9538), PNorm: 189.3803, GNorm: 0.4403
[210/299] timecost: 62.44, lr: 0.000032, Train: (LOSS: 0.0083, MAE: 0.0083, RMSE: 0.0186, R2: 0.9909), Valid: (LOSS: 0.0343, MAE: 0.0343, RMSE: 0.0523, R2: 0.9525), PNorm: 189.3823, GNorm: 0.5000
[211/299] timecost: 58.89, lr: 0.000032, Train: (LOSS: 0.0084, MAE: 0.0084, RMSE: 0.0187, R2: 0.9913), Valid: (LOSS: 0.0339, MAE: 0.0339, RMSE: 0.0520, R2: 0.9534), PNorm: 189.3855, GNorm: 0.4177
[212/299] timecost: 59.59, lr: 0.000032, Train: (LOSS: 0.0081, MAE: 0.0081, RMSE: 0.0187, R2: 0.9912), Valid: (LOSS: 0.0341, MAE: 0.0341, RMSE: 0.0519, R2: 0.9535), PNorm: 189.3874, GNorm: 0.4916
[213/299] timecost: 59.36, lr: 0.000032, Train: (LOSS: 0.0083, MAE: 0.0083, RMSE: 0.0186, R2: 0.9904), Valid: (LOSS: 0.0342, MAE: 0.0342, RMSE: 0.0516, R2: 0.9545), PNorm: 189.3896, GNorm: 0.4626
[214/299] timecost: 58.77, lr: 0.000032, Train: (LOSS: 0.0082, MAE: 0.0082, RMSE: 0.0182, R2: 0.9913), Valid: (LOSS: 0.0338, MAE: 0.0338, RMSE: 0.0513, R2: 0.9545), PNorm: 189.3918, GNorm: 0.4512
[215/299] timecost: 58.40, lr: 0.000032, Train: (LOSS: 0.0081, MAE: 0.0081, RMSE: 0.0186, R2: 0.9913), Valid: (LOSS: 0.0346, MAE: 0.0346, RMSE: 0.0529, R2: 0.9516), PNorm: 189.3932, GNorm: 0.3923
[216/299] timecost: 58.04, lr: 0.000032, Train: (LOSS: 0.0079, MAE: 0.0079, RMSE: 0.0177, R2: 0.9918), Valid: (LOSS: 0.0338, MAE: 0.0338, RMSE: 0.0511, R2: 0.9548), PNorm: 189.3961, GNorm: 0.3108
Epoch 00218: reducing learning rate of group 0 to 2.7249e-05.
[217/299] timecost: 58.72, lr: 0.000027, Train: (LOSS: 0.0081, MAE: 0.0081, RMSE: 0.0181, R2: 0.9910), Valid: (LOSS: 0.0336, MAE: 0.0336, RMSE: 0.0511, R2: 0.9549), PNorm: 189.3977, GNorm: 0.3830
[218/299] timecost: 58.27, lr: 0.000027, Train: (LOSS: 0.0076, MAE: 0.0076, RMSE: 0.0173, R2: 0.9910), Valid: (LOSS: 0.0336, MAE: 0.0336, RMSE: 0.0513, R2: 0.9546), PNorm: 189.3997, GNorm: 0.4154
[219/299] timecost: 61.48, lr: 0.000027, Train: (LOSS: 0.0073, MAE: 0.0073, RMSE: 0.0169, R2: 0.9910), Valid: (LOSS: 0.0331, MAE: 0.0331, RMSE: 0.0499, R2: 0.9564), PNorm: 189.4005, GNorm: 0.3806
[220/299] timecost: 61.97, lr: 0.000027, Train: (LOSS: 0.0073, MAE: 0.0073, RMSE: 0.0177, R2: 0.9919), Valid: (LOSS: 0.0342, MAE: 0.0342, RMSE: 0.0515, R2: 0.9540), PNorm: 189.4027, GNorm: 0.5000
[221/299] timecost: 60.28, lr: 0.000027, Train: (LOSS: 0.0075, MAE: 0.0075, RMSE: 0.0172, R2: 0.9915), Valid: (LOSS: 0.0344, MAE: 0.0344, RMSE: 0.0523, R2: 0.9530), PNorm: 189.4027, GNorm: 0.4701
[222/299] timecost: 60.23, lr: 0.000027, Train: (LOSS: 0.0072, MAE: 0.0072, RMSE: 0.0174, R2: 0.9910), Valid: (LOSS: 0.0332, MAE: 0.0332, RMSE: 0.0508, R2: 0.9551), PNorm: 189.4052, GNorm: 0.4587
[223/299] timecost: 59.90, lr: 0.000027, Train: (LOSS: 0.0073, MAE: 0.0073, RMSE: 0.0169, R2: 0.9918), Valid: (LOSS: 0.0334, MAE: 0.0334, RMSE: 0.0509, R2: 0.9548), PNorm: 189.4063, GNorm: 0.3392
[224/299] timecost: 58.52, lr: 0.000027, Train: (LOSS: 0.0073, MAE: 0.0073, RMSE: 0.0174, R2: 0.9912), Valid: (LOSS: 0.0335, MAE: 0.0335, RMSE: 0.0510, R2: 0.9548), PNorm: 189.4083, GNorm: 0.5000
[225/299] timecost: 58.52, lr: 0.000027, Train: (LOSS: 0.0074, MAE: 0.0074, RMSE: 0.0172, R2: 0.9914), Valid: (LOSS: 0.0342, MAE: 0.0342, RMSE: 0.0514, R2: 0.9546), PNorm: 189.4098, GNorm: 0.4894
[226/299] timecost: 59.01, lr: 0.000027, Train: (LOSS: 0.0074, MAE: 0.0074, RMSE: 0.0170, R2: 0.9921), Valid: (LOSS: 0.0336, MAE: 0.0336, RMSE: 0.0512, R2: 0.9548), PNorm: 189.4112, GNorm: 0.3994
[227/299] timecost: 59.56, lr: 0.000027, Train: (LOSS: 0.0072, MAE: 0.0072, RMSE: 0.0172, R2: 0.9918), Valid: (LOSS: 0.0335, MAE: 0.0335, RMSE: 0.0508, R2: 0.9555), PNorm: 189.4124, GNorm: 0.4946
[228/299] timecost: 58.75, lr: 0.000027, Train: (LOSS: 0.0072, MAE: 0.0072, RMSE: 0.0169, R2: 0.9929), Valid: (LOSS: 0.0334, MAE: 0.0334, RMSE: 0.0507, R2: 0.9557), PNorm: 189.4143, GNorm: 0.4315
[229/299] timecost: 58.33, lr: 0.000027, Train: (LOSS: 0.0072, MAE: 0.0072, RMSE: 0.0166, R2: 0.9915), Valid: (LOSS: 0.0342, MAE: 0.0342, RMSE: 0.0518, R2: 0.9541), PNorm: 189.4149, GNorm: 0.3765
[230/299] timecost: 58.49, lr: 0.000027, Train: (LOSS: 0.0073, MAE: 0.0073, RMSE: 0.0168, R2: 0.9916), Valid: (LOSS: 0.0337, MAE: 0.0337, RMSE: 0.0508, R2: 0.9555), PNorm: 189.4175, GNorm: 0.4592
[231/299] timecost: 58.80, lr: 0.000027, Train: (LOSS: 0.0074, MAE: 0.0074, RMSE: 0.0176, R2: 0.9922), Valid: (LOSS: 0.0338, MAE: 0.0338, RMSE: 0.0508, R2: 0.9554), PNorm: 189.4194, GNorm: 0.5000
[232/299] timecost: 62.50, lr: 0.000027, Train: (LOSS: 0.0073, MAE: 0.0073, RMSE: 0.0174, R2: 0.9916), Valid: (LOSS: 0.0337, MAE: 0.0337, RMSE: 0.0511, R2: 0.9551), PNorm: 189.4202, GNorm: 0.3670
Epoch 00234: reducing learning rate of group 0 to 2.3162e-05.
[233/299] timecost: 61.99, lr: 0.000023, Train: (LOSS: 0.0073, MAE: 0.0073, RMSE: 0.0171, R2: 0.9924), Valid: (LOSS: 0.0335, MAE: 0.0335, RMSE: 0.0502, R2: 0.9566), PNorm: 189.4208, GNorm: 0.4658
[234/299] timecost: 60.79, lr: 0.000023, Train: (LOSS: 0.0068, MAE: 0.0068, RMSE: 0.0166, R2: 0.9920), Valid: (LOSS: 0.0336, MAE: 0.0336, RMSE: 0.0505, R2: 0.9561), PNorm: 189.4227, GNorm: 0.5000
[235/299] timecost: 60.16, lr: 0.000023, Train: (LOSS: 0.0068, MAE: 0.0068, RMSE: 0.0168, R2: 0.9917), Valid: (LOSS: 0.0333, MAE: 0.0333, RMSE: 0.0504, R2: 0.9562), PNorm: 189.4244, GNorm: 0.3800
[236/299] timecost: 59.86, lr: 0.000023, Train: (LOSS: 0.0068, MAE: 0.0068, RMSE: 0.0169, R2: 0.9924), Valid: (LOSS: 0.0334, MAE: 0.0334, RMSE: 0.0501, R2: 0.9568), PNorm: 189.4255, GNorm: 0.3417
[237/299] timecost: 60.47, lr: 0.000023, Train: (LOSS: 0.0067, MAE: 0.0067, RMSE: 0.0164, R2: 0.9924), Valid: (LOSS: 0.0339, MAE: 0.0339, RMSE: 0.0509, R2: 0.9551), PNorm: 189.4257, GNorm: 0.5000
[238/299] timecost: 59.89, lr: 0.000023, Train: (LOSS: 0.0068, MAE: 0.0068, RMSE: 0.0162, R2: 0.9928), Valid: (LOSS: 0.0333, MAE: 0.0333, RMSE: 0.0508, R2: 0.9557), PNorm: 189.4274, GNorm: 0.4417
[239/299] timecost: 60.87, lr: 0.000023, Train: (LOSS: 0.0065, MAE: 0.0065, RMSE: 0.0160, R2: 0.9916), Valid: (LOSS: 0.0333, MAE: 0.0333, RMSE: 0.0503, R2: 0.9563), PNorm: 189.4283, GNorm: 0.5000
[240/299] timecost: 62.43, lr: 0.000023, Train: (LOSS: 0.0065, MAE: 0.0065, RMSE: 0.0162, R2: 0.9927), Valid: (LOSS: 0.0334, MAE: 0.0334, RMSE: 0.0504, R2: 0.9561), PNorm: 189.4298, GNorm: 0.3569
[241/299] timecost: 62.38, lr: 0.000023, Train: (LOSS: 0.0066, MAE: 0.0066, RMSE: 0.0160, R2: 0.9920), Valid: (LOSS: 0.0331, MAE: 0.0331, RMSE: 0.0505, R2: 0.9558), PNorm: 189.4312, GNorm: 0.5000
[242/299] timecost: 62.58, lr: 0.000023, Train: (LOSS: 0.0066, MAE: 0.0066, RMSE: 0.0158, R2: 0.9929), Valid: (LOSS: 0.0343, MAE: 0.0343, RMSE: 0.0518, R2: 0.9538), PNorm: 189.4321, GNorm: 0.5000
[243/299] timecost: 62.36, lr: 0.000023, Train: (LOSS: 0.0067, MAE: 0.0067, RMSE: 0.0163, R2: 0.9918), Valid: (LOSS: 0.0340, MAE: 0.0340, RMSE: 0.0516, R2: 0.9541), PNorm: 189.4336, GNorm: 0.4382
[244/299] timecost: 62.37, lr: 0.000023, Train: (LOSS: 0.0067, MAE: 0.0067, RMSE: 0.0167, R2: 0.9922), Valid: (LOSS: 0.0341, MAE: 0.0341, RMSE: 0.0507, R2: 0.9556), PNorm: 189.4353, GNorm: 0.4390
[245/299] timecost: 62.40, lr: 0.000023, Train: (LOSS: 0.0066, MAE: 0.0066, RMSE: 0.0162, R2: 0.9923), Valid: (LOSS: 0.0338, MAE: 0.0338, RMSE: 0.0513, R2: 0.9547), PNorm: 189.4361, GNorm: 0.4459
[246/299] timecost: 62.43, lr: 0.000023, Train: (LOSS: 0.0065, MAE: 0.0065, RMSE: 0.0163, R2: 0.9921), Valid: (LOSS: 0.0336, MAE: 0.0336, RMSE: 0.0513, R2: 0.9547), PNorm: 189.4369, GNorm: 0.4437
[247/299] timecost: 62.39, lr: 0.000023, Train: (LOSS: 0.0064, MAE: 0.0064, RMSE: 0.0162, R2: 0.9924), Valid: (LOSS: 0.0338, MAE: 0.0338, RMSE: 0.0516, R2: 0.9540), PNorm: 189.4380, GNorm: 0.4593
[248/299] timecost: 62.40, lr: 0.000023, Train: (LOSS: 0.0064, MAE: 0.0064, RMSE: 0.0157, R2: 0.9933), Valid: (LOSS: 0.0329, MAE: 0.0329, RMSE: 0.0501, R2: 0.9563), PNorm: 189.4391, GNorm: 0.5000
Epoch 00250: reducing learning rate of group 0 to 1.9687e-05.
[249/299] timecost: 62.45, lr: 0.000020, Train: (LOSS: 0.0065, MAE: 0.0065, RMSE: 0.0160, R2: 0.9925), Valid: (LOSS: 0.0343, MAE: 0.0343, RMSE: 0.0517, R2: 0.9542), PNorm: 189.4405, GNorm: 0.3687
[250/299] timecost: 62.34, lr: 0.000020, Train: (LOSS: 0.0063, MAE: 0.0063, RMSE: 0.0161, R2: 0.9928), Valid: (LOSS: 0.0335, MAE: 0.0335, RMSE: 0.0510, R2: 0.9550), PNorm: 189.4409, GNorm: 0.4259
[251/299] timecost: 62.37, lr: 0.000020, Train: (LOSS: 0.0063, MAE: 0.0063, RMSE: 0.0154, R2: 0.9913), Valid: (LOSS: 0.0337, MAE: 0.0337, RMSE: 0.0506, R2: 0.9558), PNorm: 189.4418, GNorm: 0.5000
[252/299] timecost: 62.49, lr: 0.000020, Train: (LOSS: 0.0063, MAE: 0.0063, RMSE: 0.0159, R2: 0.9926), Valid: (LOSS: 0.0333, MAE: 0.0333, RMSE: 0.0509, R2: 0.9557), PNorm: 189.4421, GNorm: 0.3124
[253/299] timecost: 62.16, lr: 0.000020, Train: (LOSS: 0.0061, MAE: 0.0061, RMSE: 0.0157, R2: 0.9921), Valid: (LOSS: 0.0335, MAE: 0.0335, RMSE: 0.0507, R2: 0.9555), PNorm: 189.4434, GNorm: 0.5000
[254/299] timecost: 62.35, lr: 0.000020, Train: (LOSS: 0.0061, MAE: 0.0061, RMSE: 0.0155, R2: 0.9925), Valid: (LOSS: 0.0335, MAE: 0.0335, RMSE: 0.0507, R2: 0.9559), PNorm: 189.4437, GNorm: 0.3800
[255/299] timecost: 62.28, lr: 0.000020, Train: (LOSS: 0.0062, MAE: 0.0062, RMSE: 0.0157, R2: 0.9926), Valid: (LOSS: 0.0332, MAE: 0.0332, RMSE: 0.0510, R2: 0.9548), PNorm: 189.4443, GNorm: 0.4673
[256/299] timecost: 62.24, lr: 0.000020, Train: (LOSS: 0.0063, MAE: 0.0063, RMSE: 0.0155, R2: 0.9925), Valid: (LOSS: 0.0336, MAE: 0.0336, RMSE: 0.0512, R2: 0.9549), PNorm: 189.4454, GNorm: 0.4992
[257/299] timecost: 62.46, lr: 0.000020, Train: (LOSS: 0.0061, MAE: 0.0061, RMSE: 0.0155, R2: 0.9921), Valid: (LOSS: 0.0338, MAE: 0.0338, RMSE: 0.0511, R2: 0.9549), PNorm: 189.4468, GNorm: 0.3960
[258/299] timecost: 62.43, lr: 0.000020, Train: (LOSS: 0.0060, MAE: 0.0060, RMSE: 0.0159, R2: 0.9929), Valid: (LOSS: 0.0338, MAE: 0.0338, RMSE: 0.0511, R2: 0.9551), PNorm: 189.4478, GNorm: 0.4315
[259/299] timecost: 62.40, lr: 0.000020, Train: (LOSS: 0.0061, MAE: 0.0061, RMSE: 0.0155, R2: 0.9932), Valid: (LOSS: 0.0342, MAE: 0.0342, RMSE: 0.0516, R2: 0.9542), PNorm: 189.4482, GNorm: 0.3856
[260/299] timecost: 62.26, lr: 0.000020, Train: (LOSS: 0.0060, MAE: 0.0060, RMSE: 0.0152, R2: 0.9930), Valid: (LOSS: 0.0338, MAE: 0.0338, RMSE: 0.0514, R2: 0.9543), PNorm: 189.4494, GNorm: 0.4295
[261/299] timecost: 62.47, lr: 0.000020, Train: (LOSS: 0.0060, MAE: 0.0060, RMSE: 0.0149, R2: 0.9928), Valid: (LOSS: 0.0336, MAE: 0.0336, RMSE: 0.0510, R2: 0.9553), PNorm: 189.4498, GNorm: 0.5000
[262/299] timecost: 62.37, lr: 0.000020, Train: (LOSS: 0.0060, MAE: 0.0060, RMSE: 0.0154, R2: 0.9931), Valid: (LOSS: 0.0338, MAE: 0.0338, RMSE: 0.0511, R2: 0.9551), PNorm: 189.4505, GNorm: 0.3697
[263/299] timecost: 62.32, lr: 0.000020, Train: (LOSS: 0.0059, MAE: 0.0059, RMSE: 0.0156, R2: 0.9927), Valid: (LOSS: 0.0336, MAE: 0.0336, RMSE: 0.0507, R2: 0.9557), PNorm: 189.4517, GNorm: 0.4674
[264/299] timecost: 62.34, lr: 0.000020, Train: (LOSS: 0.0059, MAE: 0.0059, RMSE: 0.0152, R2: 0.9927), Valid: (LOSS: 0.0337, MAE: 0.0337, RMSE: 0.0513, R2: 0.9548), PNorm: 189.4522, GNorm: 0.4858
Epoch 00266: reducing learning rate of group 0 to 1.6734e-05.
[265/299] timecost: 62.48, lr: 0.000017, Train: (LOSS: 0.0060, MAE: 0.0060, RMSE: 0.0156, R2: 0.9929), Valid: (LOSS: 0.0335, MAE: 0.0335, RMSE: 0.0510, R2: 0.9552), PNorm: 189.4530, GNorm: 0.3819
[266/299] timecost: 62.36, lr: 0.000017, Train: (LOSS: 0.0058, MAE: 0.0058, RMSE: 0.0153, R2: 0.9925), Valid: (LOSS: 0.0335, MAE: 0.0335, RMSE: 0.0508, R2: 0.9555), PNorm: 189.4535, GNorm: 0.5000
[267/299] timecost: 62.42, lr: 0.000017, Train: (LOSS: 0.0058, MAE: 0.0058, RMSE: 0.0152, R2: 0.9931), Valid: (LOSS: 0.0336, MAE: 0.0336, RMSE: 0.0507, R2: 0.9556), PNorm: 189.4542, GNorm: 0.5000
[268/299] timecost: 62.10, lr: 0.000017, Train: (LOSS: 0.0057, MAE: 0.0057, RMSE: 0.0150, R2: 0.9927), Valid: (LOSS: 0.0338, MAE: 0.0338, RMSE: 0.0515, R2: 0.9543), PNorm: 189.4545, GNorm: 0.4201
[269/299] timecost: 60.46, lr: 0.000017, Train: (LOSS: 0.0058, MAE: 0.0058, RMSE: 0.0156, R2: 0.9933), Valid: (LOSS: 0.0337, MAE: 0.0337, RMSE: 0.0513, R2: 0.9545), PNorm: 189.4553, GNorm: 0.5000
[270/299] timecost: 60.70, lr: 0.000017, Train: (LOSS: 0.0056, MAE: 0.0056, RMSE: 0.0157, R2: 0.9929), Valid: (LOSS: 0.0333, MAE: 0.0333, RMSE: 0.0510, R2: 0.9550), PNorm: 189.4559, GNorm: 0.3376
[271/299] timecost: 60.86, lr: 0.000017, Train: (LOSS: 0.0057, MAE: 0.0057, RMSE: 0.0150, R2: 0.9934), Valid: (LOSS: 0.0334, MAE: 0.0334, RMSE: 0.0505, R2: 0.9557), PNorm: 189.4568, GNorm: 0.4135
[272/299] timecost: 59.73, lr: 0.000017, Train: (LOSS: 0.0057, MAE: 0.0057, RMSE: 0.0151, R2: 0.9929), Valid: (LOSS: 0.0338, MAE: 0.0338, RMSE: 0.0511, R2: 0.9550), PNorm: 189.4575, GNorm: 0.5000
[273/299] timecost: 59.65, lr: 0.000017, Train: (LOSS: 0.0057, MAE: 0.0057, RMSE: 0.0154, R2: 0.9929), Valid: (LOSS: 0.0336, MAE: 0.0336, RMSE: 0.0508, R2: 0.9554), PNorm: 189.4579, GNorm: 0.4181
[274/299] timecost: 59.98, lr: 0.000017, Train: (LOSS: 0.0057, MAE: 0.0057, RMSE: 0.0148, R2: 0.9930), Valid: (LOSS: 0.0334, MAE: 0.0334, RMSE: 0.0505, R2: 0.9558), PNorm: 189.4589, GNorm: 0.4510
[275/299] timecost: 60.08, lr: 0.000017, Train: (LOSS: 0.0056, MAE: 0.0056, RMSE: 0.0151, R2: 0.9926), Valid: (LOSS: 0.0337, MAE: 0.0337, RMSE: 0.0510, R2: 0.9545), PNorm: 189.4591, GNorm: 0.5000
[276/299] timecost: 59.83, lr: 0.000017, Train: (LOSS: 0.0057, MAE: 0.0057, RMSE: 0.0154, R2: 0.9929), Valid: (LOSS: 0.0339, MAE: 0.0339, RMSE: 0.0507, R2: 0.9555), PNorm: 189.4602, GNorm: 0.5000
[277/299] timecost: 62.39, lr: 0.000017, Train: (LOSS: 0.0057, MAE: 0.0057, RMSE: 0.0154, R2: 0.9927), Valid: (LOSS: 0.0335, MAE: 0.0335, RMSE: 0.0510, R2: 0.9550), PNorm: 189.4604, GNorm: 0.4321
[278/299] timecost: 62.31, lr: 0.000017, Train: (LOSS: 0.0057, MAE: 0.0057, RMSE: 0.0143, R2: 0.9932), Valid: (LOSS: 0.0340, MAE: 0.0340, RMSE: 0.0513, R2: 0.9547), PNorm: 189.4613, GNorm: 0.4826
[279/299] timecost: 62.33, lr: 0.000017, Train: (LOSS: 0.0056, MAE: 0.0056, RMSE: 0.0148, R2: 0.9933), Valid: (LOSS: 0.0339, MAE: 0.0339, RMSE: 0.0513, R2: 0.9545), PNorm: 189.4622, GNorm: 0.4100
[280/299] timecost: 62.40, lr: 0.000017, Train: (LOSS: 0.0056, MAE: 0.0056, RMSE: 0.0150, R2: 0.9936), Valid: (LOSS: 0.0337, MAE: 0.0337, RMSE: 0.0511, R2: 0.9549), PNorm: 189.4628, GNorm: 0.4828
Epoch 00282: reducing learning rate of group 0 to 1.4224e-05.
[281/299] timecost: 62.49, lr: 0.000014, Train: (LOSS: 0.0055, MAE: 0.0055, RMSE: 0.0149, R2: 0.9929), Valid: (LOSS: 0.0338, MAE: 0.0338, RMSE: 0.0510, R2: 0.9551), PNorm: 189.4633, GNorm: 0.4884
[282/299] timecost: 62.30, lr: 0.000014, Train: (LOSS: 0.0054, MAE: 0.0054, RMSE: 0.0148, R2: 0.9928), Valid: (LOSS: 0.0333, MAE: 0.0333, RMSE: 0.0505, R2: 0.9558), PNorm: 189.4639, GNorm: 0.4041
[283/299] timecost: 62.31, lr: 0.000014, Train: (LOSS: 0.0053, MAE: 0.0053, RMSE: 0.0149, R2: 0.9932), Valid: (LOSS: 0.0337, MAE: 0.0337, RMSE: 0.0510, R2: 0.9550), PNorm: 189.4643, GNorm: 0.4196
[284/299] timecost: 62.32, lr: 0.000014, Train: (LOSS: 0.0053, MAE: 0.0053, RMSE: 0.0143, R2: 0.9927), Valid: (LOSS: 0.0337, MAE: 0.0337, RMSE: 0.0507, R2: 0.9556), PNorm: 189.4644, GNorm: 0.3685
[285/299] timecost: 62.39, lr: 0.000014, Train: (LOSS: 0.0053, MAE: 0.0053, RMSE: 0.0149, R2: 0.9937), Valid: (LOSS: 0.0336, MAE: 0.0336, RMSE: 0.0508, R2: 0.9552), PNorm: 189.4648, GNorm: 0.4526
[286/299] timecost: 60.18, lr: 0.000014, Train: (LOSS: 0.0055, MAE: 0.0055, RMSE: 0.0140, R2: 0.9927), Valid: (LOSS: 0.0336, MAE: 0.0336, RMSE: 0.0511, R2: 0.9547), PNorm: 189.4651, GNorm: 0.5000
[287/299] timecost: 59.03, lr: 0.000014, Train: (LOSS: 0.0053, MAE: 0.0053, RMSE: 0.0146, R2: 0.9925), Valid: (LOSS: 0.0339, MAE: 0.0339, RMSE: 0.0514, R2: 0.9541), PNorm: 189.4658, GNorm: 0.4071
[288/299] timecost: 59.68, lr: 0.000014, Train: (LOSS: 0.0053, MAE: 0.0053, RMSE: 0.0148, R2: 0.9929), Valid: (LOSS: 0.0338, MAE: 0.0338, RMSE: 0.0512, R2: 0.9545), PNorm: 189.4663, GNorm: 0.5000
[289/299] timecost: 59.83, lr: 0.000014, Train: (LOSS: 0.0054, MAE: 0.0054, RMSE: 0.0150, R2: 0.9932), Valid: (LOSS: 0.0340, MAE: 0.0340, RMSE: 0.0514, R2: 0.9543), PNorm: 189.4671, GNorm: 0.3879
[290/299] timecost: 60.34, lr: 0.000014, Train: (LOSS: 0.0053, MAE: 0.0053, RMSE: 0.0147, R2: 0.9931), Valid: (LOSS: 0.0335, MAE: 0.0335, RMSE: 0.0507, R2: 0.9553), PNorm: 189.4674, GNorm: 0.5000
[291/299] timecost: 60.22, lr: 0.000014, Train: (LOSS: 0.0053, MAE: 0.0053, RMSE: 0.0145, R2: 0.9935), Valid: (LOSS: 0.0337, MAE: 0.0337, RMSE: 0.0508, R2: 0.9554), PNorm: 189.4675, GNorm: 0.3857
[292/299] timecost: 59.74, lr: 0.000014, Train: (LOSS: 0.0052, MAE: 0.0052, RMSE: 0.0149, R2: 0.9926), Valid: (LOSS: 0.0338, MAE: 0.0338, RMSE: 0.0509, R2: 0.9551), PNorm: 189.4685, GNorm: 0.3124
[293/299] timecost: 59.45, lr: 0.000014, Train: (LOSS: 0.0052, MAE: 0.0052, RMSE: 0.0149, R2: 0.9936), Valid: (LOSS: 0.0336, MAE: 0.0336, RMSE: 0.0512, R2: 0.9546), PNorm: 189.4688, GNorm: 0.4515
[294/299] timecost: 58.28, lr: 0.000014, Train: (LOSS: 0.0054, MAE: 0.0054, RMSE: 0.0148, R2: 0.9931), Valid: (LOSS: 0.0339, MAE: 0.0339, RMSE: 0.0512, R2: 0.9544), PNorm: 189.4697, GNorm: 0.5000
[295/299] timecost: 58.08, lr: 0.000014, Train: (LOSS: 0.0053, MAE: 0.0053, RMSE: 0.0148, R2: 0.9933), Valid: (LOSS: 0.0338, MAE: 0.0338, RMSE: 0.0510, R2: 0.9551), PNorm: 189.4702, GNorm: 0.4086
[296/299] timecost: 58.36, lr: 0.000014, Train: (LOSS: 0.0053, MAE: 0.0053, RMSE: 0.0145, R2: 0.9937), Valid: (LOSS: 0.0336, MAE: 0.0336, RMSE: 0.0508, R2: 0.9554), PNorm: 189.4706, GNorm: 0.4547
Epoch 00298: reducing learning rate of group 0 to 1.2091e-05.
[297/299] timecost: 58.42, lr: 0.000012, Train: (LOSS: 0.0052, MAE: 0.0052, RMSE: 0.0144, R2: 0.9934), Valid: (LOSS: 0.0339, MAE: 0.0339, RMSE: 0.0513, R2: 0.9546), PNorm: 189.4712, GNorm: 0.3774
[298/299] timecost: 60.84, lr: 0.000012, Train: (LOSS: 0.0052, MAE: 0.0052, RMSE: 0.0144, R2: 0.9936), Valid: (LOSS: 0.0340, MAE: 0.0340, RMSE: 0.0512, R2: 0.9545), PNorm: 189.4713, GNorm: 0.4439
[299/299] timecost: 62.39, lr: 0.000012, Train: (LOSS: 0.0051, MAE: 0.0051, RMSE: 0.0146, R2: 0.9928), Valid: (LOSS: 0.0335, MAE: 0.0335, RMSE: 0.0506, R2: 0.9555), PNorm: 189.4723, GNorm: 0.4020
==========Training End==========
==========Test Best Model==========
================Final Results=======================
mse: 0.0352 +- 0.0000:
rmse: 0.0558 +- 0.0000:
mae: 0.0352 +- 0.0000:
r2: 0.9531 +- 0.0000:
tensor([[0.0000, 0.0000],
        [0.0000, 0.0000],
        [0.1542, 0.1828],
        ...,
        [0.0125, 0.0144],
        [0.0000, 0.0000],
        [0.0000, 0.0000]], device='cuda:0')
