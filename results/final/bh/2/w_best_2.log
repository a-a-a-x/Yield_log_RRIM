cuda available with GPU: Tesla V100-PCIE-16GB
==========Load Seed==========
set_random_seed
0
==========Training Start==========
Training Graphs:  2491
Valid Graphs:  277
Test Graphs:  1187
============Loading pretrained weights to generate initialization============
============Creating new layers============
============Creating Model============
Training Graphs Batches:  78
Valid Graphs Batches:  9
Test Graphs Batches:  37
[0/299] timecost: 71.00, lr: 0.000030, Train: (LOSS: 0.2341, MAE: 0.2341, RMSE: 0.2767, R2: -0.0768), Valid: (LOSS: 0.2329, MAE: 0.2329, RMSE: 0.2690, R2: 0.0252), PNorm: 174.3309, GNorm: 0.1828
[1/299] timecost: 64.90, lr: 0.000030, Train: (LOSS: 0.2182, MAE: 0.2182, RMSE: 0.2617, R2: 0.0257), Valid: (LOSS: 0.2237, MAE: 0.2237, RMSE: 0.2591, R2: 0.0946), PNorm: 173.6582, GNorm: 0.9840
[2/299] timecost: 64.74, lr: 0.000030, Train: (LOSS: 0.2064, MAE: 0.2064, RMSE: 0.2489, R2: 0.1272), Valid: (LOSS: 0.2257, MAE: 0.2257, RMSE: 0.2579, R2: 0.1019), PNorm: 173.1138, GNorm: 1.0398
[3/299] timecost: 65.23, lr: 0.000030, Train: (LOSS: 0.1996, MAE: 0.1996, RMSE: 0.2442, R2: 0.1389), Valid: (LOSS: 0.1921, MAE: 0.1921, RMSE: 0.2372, R2: 0.2389), PNorm: 172.6792, GNorm: 2.0473
[4/299] timecost: 65.37, lr: 0.000030, Train: (LOSS: 0.1756, MAE: 0.1756, RMSE: 0.2230, R2: 0.2866), Valid: (LOSS: 0.1722, MAE: 0.1722, RMSE: 0.2084, R2: 0.4080), PNorm: 172.3272, GNorm: 1.4630
[5/299] timecost: 65.80, lr: 0.000030, Train: (LOSS: 0.1473, MAE: 0.1473, RMSE: 0.1948, R2: 0.4492), Valid: (LOSS: 0.1435, MAE: 0.1435, RMSE: 0.1865, R2: 0.5238), PNorm: 172.0175, GNorm: 1.7086
[6/299] timecost: 65.94, lr: 0.000030, Train: (LOSS: 0.1451, MAE: 0.1451, RMSE: 0.1931, R2: 0.4602), Valid: (LOSS: 0.1328, MAE: 0.1328, RMSE: 0.1774, R2: 0.5581), PNorm: 171.7306, GNorm: 2.5685
[7/299] timecost: 65.56, lr: 0.000030, Train: (LOSS: 0.1342, MAE: 0.1342, RMSE: 0.1809, R2: 0.5243), Valid: (LOSS: 0.1377, MAE: 0.1377, RMSE: 0.1844, R2: 0.5252), PNorm: 171.4562, GNorm: 1.1429
[8/299] timecost: 64.87, lr: 0.000030, Train: (LOSS: 0.1307, MAE: 0.1307, RMSE: 0.1796, R2: 0.5306), Valid: (LOSS: 0.1230, MAE: 0.1230, RMSE: 0.1636, R2: 0.6312), PNorm: 171.1958, GNorm: 0.9678
[9/299] timecost: 64.80, lr: 0.000030, Train: (LOSS: 0.1310, MAE: 0.1310, RMSE: 0.1766, R2: 0.5344), Valid: (LOSS: 0.1212, MAE: 0.1212, RMSE: 0.1668, R2: 0.6137), PNorm: 170.9472, GNorm: 1.4885
[10/299] timecost: 65.12, lr: 0.000030, Train: (LOSS: 0.1212, MAE: 0.1212, RMSE: 0.1687, R2: 0.5803), Valid: (LOSS: 0.1246, MAE: 0.1246, RMSE: 0.1670, R2: 0.6196), PNorm: 170.7094, GNorm: 1.9575
[11/299] timecost: 64.14, lr: 0.000030, Train: (LOSS: 0.1198, MAE: 0.1198, RMSE: 0.1666, R2: 0.5982), Valid: (LOSS: 0.1290, MAE: 0.1290, RMSE: 0.1887, R2: 0.4909), PNorm: 170.4830, GNorm: 2.0713
[12/299] timecost: 64.13, lr: 0.000030, Train: (LOSS: 0.1157, MAE: 0.1157, RMSE: 0.1642, R2: 0.6098), Valid: (LOSS: 0.1112, MAE: 0.1112, RMSE: 0.1519, R2: 0.6764), PNorm: 170.2630, GNorm: 1.1382
[13/299] timecost: 64.60, lr: 0.000030, Train: (LOSS: 0.1076, MAE: 0.1076, RMSE: 0.1542, R2: 0.6402), Valid: (LOSS: 0.1272, MAE: 0.1272, RMSE: 0.1766, R2: 0.5540), PNorm: 170.0484, GNorm: 1.1775
[14/299] timecost: 64.49, lr: 0.000030, Train: (LOSS: 0.1142, MAE: 0.1142, RMSE: 0.1614, R2: 0.6178), Valid: (LOSS: 0.1324, MAE: 0.1324, RMSE: 0.1846, R2: 0.5338), PNorm: 169.8440, GNorm: 1.5202
[15/299] timecost: 64.22, lr: 0.000030, Train: (LOSS: 0.1094, MAE: 0.1094, RMSE: 0.1551, R2: 0.6430), Valid: (LOSS: 0.1125, MAE: 0.1125, RMSE: 0.1592, R2: 0.6524), PNorm: 169.6459, GNorm: 1.5412
[16/299] timecost: 64.72, lr: 0.000030, Train: (LOSS: 0.1020, MAE: 0.1020, RMSE: 0.1479, R2: 0.6696), Valid: (LOSS: 0.1086, MAE: 0.1086, RMSE: 0.1583, R2: 0.6316), PNorm: 169.4547, GNorm: 1.7286
[17/299] timecost: 64.22, lr: 0.000030, Train: (LOSS: 0.1036, MAE: 0.1036, RMSE: 0.1483, R2: 0.6687), Valid: (LOSS: 0.0961, MAE: 0.0961, RMSE: 0.1349, R2: 0.7456), PNorm: 169.2712, GNorm: 1.0750
[18/299] timecost: 64.76, lr: 0.000030, Train: (LOSS: 0.0979, MAE: 0.0979, RMSE: 0.1423, R2: 0.7005), Valid: (LOSS: 0.0914, MAE: 0.0914, RMSE: 0.1308, R2: 0.7583), PNorm: 169.0921, GNorm: 1.7994
[19/299] timecost: 67.85, lr: 0.000030, Train: (LOSS: 0.0957, MAE: 0.0957, RMSE: 0.1393, R2: 0.7068), Valid: (LOSS: 0.1007, MAE: 0.1007, RMSE: 0.1445, R2: 0.7108), PNorm: 168.9189, GNorm: 1.9515
[20/299] timecost: 67.87, lr: 0.000030, Train: (LOSS: 0.0983, MAE: 0.0983, RMSE: 0.1440, R2: 0.6859), Valid: (LOSS: 0.0915, MAE: 0.0915, RMSE: 0.1278, R2: 0.7675), PNorm: 168.7505, GNorm: 2.4658
[21/299] timecost: 67.86, lr: 0.000030, Train: (LOSS: 0.0993, MAE: 0.0993, RMSE: 0.1446, R2: 0.6834), Valid: (LOSS: 0.0891, MAE: 0.0891, RMSE: 0.1290, R2: 0.7657), PNorm: 168.5870, GNorm: 1.7822
[22/299] timecost: 67.87, lr: 0.000030, Train: (LOSS: 0.0884, MAE: 0.0884, RMSE: 0.1302, R2: 0.7355), Valid: (LOSS: 0.0991, MAE: 0.0991, RMSE: 0.1374, R2: 0.7322), PNorm: 168.4243, GNorm: 1.6784
[23/299] timecost: 66.69, lr: 0.000030, Train: (LOSS: 0.0865, MAE: 0.0865, RMSE: 0.1256, R2: 0.7606), Valid: (LOSS: 0.0927, MAE: 0.0927, RMSE: 0.1388, R2: 0.7255), PNorm: 168.2658, GNorm: 1.2636
[24/299] timecost: 66.16, lr: 0.000030, Train: (LOSS: 0.0866, MAE: 0.0866, RMSE: 0.1305, R2: 0.7455), Valid: (LOSS: 0.0824, MAE: 0.0824, RMSE: 0.1217, R2: 0.7939), PNorm: 168.1100, GNorm: 2.0684
[25/299] timecost: 66.35, lr: 0.000030, Train: (LOSS: 0.0808, MAE: 0.0808, RMSE: 0.1210, R2: 0.7808), Valid: (LOSS: 0.0849, MAE: 0.0849, RMSE: 0.1244, R2: 0.7811), PNorm: 167.9547, GNorm: 1.8933
[26/299] timecost: 65.57, lr: 0.000030, Train: (LOSS: 0.0803, MAE: 0.0803, RMSE: 0.1208, R2: 0.7810), Valid: (LOSS: 0.0952, MAE: 0.0952, RMSE: 0.1450, R2: 0.7028), PNorm: 167.8045, GNorm: 5.6271
[27/299] timecost: 65.16, lr: 0.000030, Train: (LOSS: 0.0804, MAE: 0.0804, RMSE: 0.1222, R2: 0.7728), Valid: (LOSS: 0.0889, MAE: 0.0889, RMSE: 0.1334, R2: 0.7497), PNorm: 167.6549, GNorm: 0.9743
[28/299] timecost: 65.21, lr: 0.000030, Train: (LOSS: 0.0793, MAE: 0.0793, RMSE: 0.1216, R2: 0.7760), Valid: (LOSS: 0.0914, MAE: 0.0914, RMSE: 0.1387, R2: 0.7326), PNorm: 167.5096, GNorm: 2.1899
[29/299] timecost: 64.87, lr: 0.000030, Train: (LOSS: 0.0828, MAE: 0.0828, RMSE: 0.1254, R2: 0.7607), Valid: (LOSS: 0.0778, MAE: 0.0778, RMSE: 0.1168, R2: 0.8047), PNorm: 167.3668, GNorm: 1.5663
[30/299] timecost: 64.59, lr: 0.000030, Train: (LOSS: 0.0773, MAE: 0.0773, RMSE: 0.1194, R2: 0.7834), Valid: (LOSS: 0.0955, MAE: 0.0955, RMSE: 0.1371, R2: 0.7338), PNorm: 167.2260, GNorm: 1.2369
[31/299] timecost: 64.65, lr: 0.000030, Train: (LOSS: 0.0741, MAE: 0.0741, RMSE: 0.1146, R2: 0.8022), Valid: (LOSS: 0.0813, MAE: 0.0813, RMSE: 0.1252, R2: 0.7788), PNorm: 167.0856, GNorm: 1.8874
[32/299] timecost: 64.79, lr: 0.000030, Train: (LOSS: 0.0737, MAE: 0.0737, RMSE: 0.1139, R2: 0.8046), Valid: (LOSS: 0.0770, MAE: 0.0770, RMSE: 0.1145, R2: 0.8106), PNorm: 166.9446, GNorm: 1.8108
[33/299] timecost: 64.51, lr: 0.000030, Train: (LOSS: 0.0706, MAE: 0.0706, RMSE: 0.1111, R2: 0.8088), Valid: (LOSS: 0.0768, MAE: 0.0768, RMSE: 0.1147, R2: 0.8139), PNorm: 166.8041, GNorm: 1.1644
[34/299] timecost: 64.55, lr: 0.000030, Train: (LOSS: 0.0722, MAE: 0.0722, RMSE: 0.1128, R2: 0.8084), Valid: (LOSS: 0.0856, MAE: 0.0856, RMSE: 0.1278, R2: 0.7708), PNorm: 166.6668, GNorm: 1.7249
[35/299] timecost: 64.14, lr: 0.000030, Train: (LOSS: 0.0711, MAE: 0.0711, RMSE: 0.1119, R2: 0.8158), Valid: (LOSS: 0.0759, MAE: 0.0759, RMSE: 0.1162, R2: 0.8099), PNorm: 166.5292, GNorm: 1.8714
[36/299] timecost: 64.73, lr: 0.000030, Train: (LOSS: 0.0698, MAE: 0.0698, RMSE: 0.1100, R2: 0.8188), Valid: (LOSS: 0.0746, MAE: 0.0746, RMSE: 0.1102, R2: 0.8255), PNorm: 166.3933, GNorm: 3.6529
[37/299] timecost: 64.55, lr: 0.000030, Train: (LOSS: 0.0679, MAE: 0.0679, RMSE: 0.1072, R2: 0.8212), Valid: (LOSS: 0.0748, MAE: 0.0748, RMSE: 0.1106, R2: 0.8233), PNorm: 166.2576, GNorm: 1.8423
[38/299] timecost: 64.76, lr: 0.000030, Train: (LOSS: 0.0679, MAE: 0.0679, RMSE: 0.1086, R2: 0.8194), Valid: (LOSS: 0.0772, MAE: 0.0772, RMSE: 0.1196, R2: 0.7967), PNorm: 166.1250, GNorm: 0.9140
[39/299] timecost: 64.29, lr: 0.000030, Train: (LOSS: 0.0691, MAE: 0.0691, RMSE: 0.1092, R2: 0.8211), Valid: (LOSS: 0.0728, MAE: 0.0728, RMSE: 0.1067, R2: 0.8379), PNorm: 165.9935, GNorm: 2.0381
[40/299] timecost: 64.99, lr: 0.000030, Train: (LOSS: 0.0704, MAE: 0.0704, RMSE: 0.1106, R2: 0.8143), Valid: (LOSS: 0.0741, MAE: 0.0741, RMSE: 0.1089, R2: 0.8320), PNorm: 165.8635, GNorm: 2.4677
[41/299] timecost: 65.12, lr: 0.000030, Train: (LOSS: 0.0647, MAE: 0.0647, RMSE: 0.1009, R2: 0.8446), Valid: (LOSS: 0.0748, MAE: 0.0748, RMSE: 0.1167, R2: 0.8076), PNorm: 165.7356, GNorm: 1.6042
[42/299] timecost: 64.79, lr: 0.000030, Train: (LOSS: 0.0623, MAE: 0.0623, RMSE: 0.0999, R2: 0.8479), Valid: (LOSS: 0.0702, MAE: 0.0702, RMSE: 0.1063, R2: 0.8425), PNorm: 165.6064, GNorm: 2.2306
[43/299] timecost: 65.01, lr: 0.000030, Train: (LOSS: 0.0629, MAE: 0.0629, RMSE: 0.0986, R2: 0.8471), Valid: (LOSS: 0.0734, MAE: 0.0734, RMSE: 0.1063, R2: 0.8401), PNorm: 165.4800, GNorm: 2.1851
[44/299] timecost: 64.57, lr: 0.000030, Train: (LOSS: 0.0622, MAE: 0.0622, RMSE: 0.0981, R2: 0.8460), Valid: (LOSS: 0.0651, MAE: 0.0651, RMSE: 0.0966, R2: 0.8692), PNorm: 165.3551, GNorm: 2.2301
[45/299] timecost: 64.44, lr: 0.000030, Train: (LOSS: 0.0671, MAE: 0.0671, RMSE: 0.1032, R2: 0.8356), Valid: (LOSS: 0.0658, MAE: 0.0658, RMSE: 0.0968, R2: 0.8685), PNorm: 165.2325, GNorm: 1.5801
[46/299] timecost: 64.88, lr: 0.000030, Train: (LOSS: 0.0602, MAE: 0.0602, RMSE: 0.0957, R2: 0.8569), Valid: (LOSS: 0.0698, MAE: 0.0698, RMSE: 0.1061, R2: 0.8394), PNorm: 165.1100, GNorm: 1.2339
[47/299] timecost: 64.40, lr: 0.000030, Train: (LOSS: 0.0538, MAE: 0.0538, RMSE: 0.0870, R2: 0.8797), Valid: (LOSS: 0.0562, MAE: 0.0562, RMSE: 0.0862, R2: 0.8953), PNorm: 164.9879, GNorm: 1.3581
[48/299] timecost: 64.31, lr: 0.000030, Train: (LOSS: 0.0526, MAE: 0.0526, RMSE: 0.0836, R2: 0.8874), Valid: (LOSS: 0.0607, MAE: 0.0607, RMSE: 0.0947, R2: 0.8733), PNorm: 164.8649, GNorm: 1.2666
[49/299] timecost: 64.73, lr: 0.000030, Train: (LOSS: 0.0530, MAE: 0.0530, RMSE: 0.0833, R2: 0.8924), Valid: (LOSS: 0.0601, MAE: 0.0601, RMSE: 0.0940, R2: 0.8769), PNorm: 164.7440, GNorm: 1.8389
[50/299] timecost: 64.76, lr: 0.000030, Train: (LOSS: 0.0540, MAE: 0.0540, RMSE: 0.0841, R2: 0.8912), Valid: (LOSS: 0.0554, MAE: 0.0554, RMSE: 0.0876, R2: 0.8928), PNorm: 164.6245, GNorm: 0.9979
[51/299] timecost: 64.86, lr: 0.000030, Train: (LOSS: 0.0508, MAE: 0.0508, RMSE: 0.0820, R2: 0.8954), Valid: (LOSS: 0.0645, MAE: 0.0645, RMSE: 0.0934, R2: 0.8730), PNorm: 164.5046, GNorm: 1.9181
[52/299] timecost: 66.00, lr: 0.000030, Train: (LOSS: 0.0500, MAE: 0.0500, RMSE: 0.0801, R2: 0.8996), Valid: (LOSS: 0.0541, MAE: 0.0541, RMSE: 0.0822, R2: 0.9026), PNorm: 164.3850, GNorm: 2.0022
[53/299] timecost: 65.69, lr: 0.000030, Train: (LOSS: 0.0499, MAE: 0.0499, RMSE: 0.0788, R2: 0.9026), Valid: (LOSS: 0.0571, MAE: 0.0571, RMSE: 0.0834, R2: 0.9004), PNorm: 164.2660, GNorm: 1.4121
[54/299] timecost: 64.65, lr: 0.000030, Train: (LOSS: 0.0489, MAE: 0.0489, RMSE: 0.0784, R2: 0.8994), Valid: (LOSS: 0.0564, MAE: 0.0564, RMSE: 0.0832, R2: 0.9001), PNorm: 164.1496, GNorm: 2.6646
[55/299] timecost: 65.46, lr: 0.000030, Train: (LOSS: 0.0481, MAE: 0.0481, RMSE: 0.0777, R2: 0.9079), Valid: (LOSS: 0.0522, MAE: 0.0522, RMSE: 0.0771, R2: 0.9145), PNorm: 164.0323, GNorm: 1.1258
[56/299] timecost: 65.62, lr: 0.000030, Train: (LOSS: 0.0476, MAE: 0.0476, RMSE: 0.0770, R2: 0.9049), Valid: (LOSS: 0.0598, MAE: 0.0598, RMSE: 0.0943, R2: 0.8771), PNorm: 163.9157, GNorm: 2.5712
[57/299] timecost: 64.89, lr: 0.000030, Train: (LOSS: 0.0475, MAE: 0.0475, RMSE: 0.0766, R2: 0.9036), Valid: (LOSS: 0.0611, MAE: 0.0611, RMSE: 0.0934, R2: 0.8785), PNorm: 163.7991, GNorm: 1.3703
[58/299] timecost: 65.04, lr: 0.000030, Train: (LOSS: 0.0473, MAE: 0.0473, RMSE: 0.0760, R2: 0.9068), Valid: (LOSS: 0.0521, MAE: 0.0521, RMSE: 0.0834, R2: 0.9028), PNorm: 163.6834, GNorm: 1.1275
[59/299] timecost: 65.17, lr: 0.000030, Train: (LOSS: 0.0435, MAE: 0.0435, RMSE: 0.0710, R2: 0.9191), Valid: (LOSS: 0.0503, MAE: 0.0503, RMSE: 0.0780, R2: 0.9135), PNorm: 163.5669, GNorm: 1.2866
[60/299] timecost: 65.19, lr: 0.000030, Train: (LOSS: 0.0443, MAE: 0.0443, RMSE: 0.0720, R2: 0.9139), Valid: (LOSS: 0.0498, MAE: 0.0498, RMSE: 0.0785, R2: 0.9121), PNorm: 163.4509, GNorm: 2.4855
[61/299] timecost: 65.23, lr: 0.000030, Train: (LOSS: 0.0443, MAE: 0.0443, RMSE: 0.0722, R2: 0.9135), Valid: (LOSS: 0.0537, MAE: 0.0537, RMSE: 0.0810, R2: 0.9038), PNorm: 163.3360, GNorm: 1.1393
[62/299] timecost: 65.01, lr: 0.000030, Train: (LOSS: 0.0435, MAE: 0.0435, RMSE: 0.0712, R2: 0.9191), Valid: (LOSS: 0.0513, MAE: 0.0513, RMSE: 0.0774, R2: 0.9146), PNorm: 163.2215, GNorm: 1.2433
[63/299] timecost: 65.27, lr: 0.000030, Train: (LOSS: 0.0435, MAE: 0.0435, RMSE: 0.0689, R2: 0.9230), Valid: (LOSS: 0.0521, MAE: 0.0521, RMSE: 0.0822, R2: 0.9057), PNorm: 163.1061, GNorm: 0.9120
[64/299] timecost: 64.90, lr: 0.000030, Train: (LOSS: 0.0436, MAE: 0.0436, RMSE: 0.0698, R2: 0.9235), Valid: (LOSS: 0.0518, MAE: 0.0518, RMSE: 0.0809, R2: 0.9075), PNorm: 162.9917, GNorm: 1.1227
[65/299] timecost: 65.19, lr: 0.000030, Train: (LOSS: 0.0406, MAE: 0.0406, RMSE: 0.0659, R2: 0.9311), Valid: (LOSS: 0.0468, MAE: 0.0468, RMSE: 0.0722, R2: 0.9236), PNorm: 162.8770, GNorm: 0.9202
[66/299] timecost: 65.35, lr: 0.000030, Train: (LOSS: 0.0408, MAE: 0.0408, RMSE: 0.0671, R2: 0.9286), Valid: (LOSS: 0.0514, MAE: 0.0514, RMSE: 0.0773, R2: 0.9143), PNorm: 162.7616, GNorm: 1.8964
[67/299] timecost: 65.50, lr: 0.000030, Train: (LOSS: 0.0423, MAE: 0.0423, RMSE: 0.0685, R2: 0.9257), Valid: (LOSS: 0.0499, MAE: 0.0499, RMSE: 0.0779, R2: 0.9125), PNorm: 162.6478, GNorm: 1.9972
[68/299] timecost: 65.30, lr: 0.000030, Train: (LOSS: 0.0402, MAE: 0.0402, RMSE: 0.0661, R2: 0.9296), Valid: (LOSS: 0.0459, MAE: 0.0459, RMSE: 0.0702, R2: 0.9287), PNorm: 162.5343, GNorm: 1.3919
[69/299] timecost: 64.86, lr: 0.000030, Train: (LOSS: 0.0382, MAE: 0.0382, RMSE: 0.0616, R2: 0.9357), Valid: (LOSS: 0.0502, MAE: 0.0502, RMSE: 0.0744, R2: 0.9192), PNorm: 162.4206, GNorm: 1.1575
[70/299] timecost: 64.87, lr: 0.000030, Train: (LOSS: 0.0381, MAE: 0.0381, RMSE: 0.0619, R2: 0.9349), Valid: (LOSS: 0.0506, MAE: 0.0506, RMSE: 0.0763, R2: 0.9161), PNorm: 162.3058, GNorm: 1.7114
[71/299] timecost: 64.86, lr: 0.000030, Train: (LOSS: 0.0391, MAE: 0.0391, RMSE: 0.0620, R2: 0.9369), Valid: (LOSS: 0.0485, MAE: 0.0485, RMSE: 0.0741, R2: 0.9201), PNorm: 162.1919, GNorm: 1.1126
[72/299] timecost: 64.88, lr: 0.000030, Train: (LOSS: 0.0369, MAE: 0.0369, RMSE: 0.0603, R2: 0.9421), Valid: (LOSS: 0.0467, MAE: 0.0467, RMSE: 0.0715, R2: 0.9271), PNorm: 162.0770, GNorm: 1.1036
[73/299] timecost: 64.44, lr: 0.000030, Train: (LOSS: 0.0359, MAE: 0.0359, RMSE: 0.0584, R2: 0.9413), Valid: (LOSS: 0.0501, MAE: 0.0501, RMSE: 0.0741, R2: 0.9192), PNorm: 161.9632, GNorm: 1.2969
[74/299] timecost: 64.56, lr: 0.000030, Train: (LOSS: 0.0371, MAE: 0.0371, RMSE: 0.0590, R2: 0.9446), Valid: (LOSS: 0.0478, MAE: 0.0478, RMSE: 0.0763, R2: 0.9172), PNorm: 161.8495, GNorm: 1.7996
[75/299] timecost: 65.09, lr: 0.000030, Train: (LOSS: 0.0364, MAE: 0.0364, RMSE: 0.0583, R2: 0.9434), Valid: (LOSS: 0.0460, MAE: 0.0460, RMSE: 0.0670, R2: 0.9349), PNorm: 161.7359, GNorm: 2.7029
[76/299] timecost: 65.00, lr: 0.000030, Train: (LOSS: 0.0362, MAE: 0.0362, RMSE: 0.0592, R2: 0.9447), Valid: (LOSS: 0.0462, MAE: 0.0462, RMSE: 0.0693, R2: 0.9289), PNorm: 161.6220, GNorm: 1.5615
[77/299] timecost: 64.27, lr: 0.000030, Train: (LOSS: 0.0367, MAE: 0.0367, RMSE: 0.0579, R2: 0.9441), Valid: (LOSS: 0.0492, MAE: 0.0492, RMSE: 0.0725, R2: 0.9221), PNorm: 161.5092, GNorm: 1.5905
[78/299] timecost: 63.67, lr: 0.000030, Train: (LOSS: 0.0379, MAE: 0.0379, RMSE: 0.0624, R2: 0.9353), Valid: (LOSS: 0.0458, MAE: 0.0458, RMSE: 0.0694, R2: 0.9311), PNorm: 161.3970, GNorm: 1.0857
[79/299] timecost: 64.78, lr: 0.000030, Train: (LOSS: 0.0347, MAE: 0.0347, RMSE: 0.0564, R2: 0.9482), Valid: (LOSS: 0.0471, MAE: 0.0471, RMSE: 0.0727, R2: 0.9252), PNorm: 161.2837, GNorm: 1.2231
[80/299] timecost: 64.58, lr: 0.000030, Train: (LOSS: 0.0350, MAE: 0.0350, RMSE: 0.0564, R2: 0.9469), Valid: (LOSS: 0.0434, MAE: 0.0434, RMSE: 0.0688, R2: 0.9328), PNorm: 161.1710, GNorm: 1.5508
[81/299] timecost: 64.30, lr: 0.000030, Train: (LOSS: 0.0340, MAE: 0.0340, RMSE: 0.0560, R2: 0.9467), Valid: (LOSS: 0.0543, MAE: 0.0543, RMSE: 0.0840, R2: 0.9001), PNorm: 161.0582, GNorm: 1.6629
[82/299] timecost: 63.93, lr: 0.000030, Train: (LOSS: 0.0352, MAE: 0.0352, RMSE: 0.0568, R2: 0.9481), Valid: (LOSS: 0.0458, MAE: 0.0458, RMSE: 0.0665, R2: 0.9367), PNorm: 160.9460, GNorm: 2.2850
[83/299] timecost: 67.04, lr: 0.000030, Train: (LOSS: 0.0336, MAE: 0.0336, RMSE: 0.0544, R2: 0.9516), Valid: (LOSS: 0.0475, MAE: 0.0475, RMSE: 0.0672, R2: 0.9356), PNorm: 160.8325, GNorm: 1.2256
[84/299] timecost: 67.59, lr: 0.000030, Train: (LOSS: 0.0333, MAE: 0.0333, RMSE: 0.0536, R2: 0.9524), Valid: (LOSS: 0.0442, MAE: 0.0442, RMSE: 0.0691, R2: 0.9328), PNorm: 160.7196, GNorm: 1.6023
[85/299] timecost: 67.77, lr: 0.000030, Train: (LOSS: 0.0329, MAE: 0.0329, RMSE: 0.0544, R2: 0.9522), Valid: (LOSS: 0.0483, MAE: 0.0483, RMSE: 0.0741, R2: 0.9217), PNorm: 160.6082, GNorm: 1.4012
[86/299] timecost: 65.59, lr: 0.000030, Train: (LOSS: 0.0329, MAE: 0.0329, RMSE: 0.0541, R2: 0.9533), Valid: (LOSS: 0.0435, MAE: 0.0435, RMSE: 0.0646, R2: 0.9397), PNorm: 160.4951, GNorm: 1.2694
[87/299] timecost: 64.94, lr: 0.000030, Train: (LOSS: 0.0323, MAE: 0.0323, RMSE: 0.0539, R2: 0.9540), Valid: (LOSS: 0.0416, MAE: 0.0416, RMSE: 0.0617, R2: 0.9452), PNorm: 160.3834, GNorm: 1.1325
[88/299] timecost: 65.26, lr: 0.000030, Train: (LOSS: 0.0318, MAE: 0.0318, RMSE: 0.0535, R2: 0.9529), Valid: (LOSS: 0.0483, MAE: 0.0483, RMSE: 0.0747, R2: 0.9228), PNorm: 160.2701, GNorm: 1.2953
[89/299] timecost: 64.86, lr: 0.000030, Train: (LOSS: 0.0331, MAE: 0.0331, RMSE: 0.0529, R2: 0.9548), Valid: (LOSS: 0.0497, MAE: 0.0497, RMSE: 0.0718, R2: 0.9268), PNorm: 160.1587, GNorm: 1.2651
[90/299] timecost: 64.18, lr: 0.000030, Train: (LOSS: 0.0330, MAE: 0.0330, RMSE: 0.0535, R2: 0.9522), Valid: (LOSS: 0.0429, MAE: 0.0429, RMSE: 0.0672, R2: 0.9337), PNorm: 160.0461, GNorm: 0.8645
[91/299] timecost: 63.99, lr: 0.000030, Train: (LOSS: 0.0318, MAE: 0.0318, RMSE: 0.0522, R2: 0.9548), Valid: (LOSS: 0.0515, MAE: 0.0515, RMSE: 0.0792, R2: 0.9115), PNorm: 159.9344, GNorm: 1.3530
[92/299] timecost: 63.80, lr: 0.000030, Train: (LOSS: 0.0314, MAE: 0.0314, RMSE: 0.0512, R2: 0.9570), Valid: (LOSS: 0.0441, MAE: 0.0441, RMSE: 0.0646, R2: 0.9394), PNorm: 159.8214, GNorm: 0.9989
[93/299] timecost: 65.59, lr: 0.000030, Train: (LOSS: 0.0299, MAE: 0.0299, RMSE: 0.0491, R2: 0.9586), Valid: (LOSS: 0.0439, MAE: 0.0439, RMSE: 0.0669, R2: 0.9380), PNorm: 159.7087, GNorm: 1.4002
[94/299] timecost: 65.61, lr: 0.000030, Train: (LOSS: 0.0309, MAE: 0.0309, RMSE: 0.0501, R2: 0.9562), Valid: (LOSS: 0.0427, MAE: 0.0427, RMSE: 0.0626, R2: 0.9456), PNorm: 159.5967, GNorm: 1.4671
[95/299] timecost: 65.35, lr: 0.000030, Train: (LOSS: 0.0298, MAE: 0.0298, RMSE: 0.0488, R2: 0.9604), Valid: (LOSS: 0.0436, MAE: 0.0436, RMSE: 0.0649, R2: 0.9416), PNorm: 159.4849, GNorm: 1.5041
[96/299] timecost: 65.71, lr: 0.000030, Train: (LOSS: 0.0293, MAE: 0.0293, RMSE: 0.0483, R2: 0.9598), Valid: (LOSS: 0.0521, MAE: 0.0521, RMSE: 0.0779, R2: 0.9171), PNorm: 159.3712, GNorm: 1.0597
[97/299] timecost: 65.69, lr: 0.000030, Train: (LOSS: 0.0302, MAE: 0.0302, RMSE: 0.0477, R2: 0.9623), Valid: (LOSS: 0.0407, MAE: 0.0407, RMSE: 0.0592, R2: 0.9519), PNorm: 159.2607, GNorm: 1.0116
[98/299] timecost: 64.86, lr: 0.000030, Train: (LOSS: 0.0284, MAE: 0.0284, RMSE: 0.0461, R2: 0.9638), Valid: (LOSS: 0.0437, MAE: 0.0437, RMSE: 0.0658, R2: 0.9416), PNorm: 159.1487, GNorm: 1.1685
[99/299] timecost: 65.48, lr: 0.000030, Train: (LOSS: 0.0296, MAE: 0.0296, RMSE: 0.0475, R2: 0.9610), Valid: (LOSS: 0.0427, MAE: 0.0427, RMSE: 0.0648, R2: 0.9428), PNorm: 159.0370, GNorm: 0.7946
[100/299] timecost: 65.61, lr: 0.000030, Train: (LOSS: 0.0279, MAE: 0.0279, RMSE: 0.0457, R2: 0.9664), Valid: (LOSS: 0.0396, MAE: 0.0396, RMSE: 0.0581, R2: 0.9524), PNorm: 158.9249, GNorm: 1.8601
[101/299] timecost: 65.30, lr: 0.000030, Train: (LOSS: 0.0285, MAE: 0.0285, RMSE: 0.0455, R2: 0.9660), Valid: (LOSS: 0.0428, MAE: 0.0428, RMSE: 0.0626, R2: 0.9463), PNorm: 158.8129, GNorm: 1.1666
[102/299] timecost: 65.30, lr: 0.000030, Train: (LOSS: 0.0277, MAE: 0.0277, RMSE: 0.0450, R2: 0.9637), Valid: (LOSS: 0.0434, MAE: 0.0434, RMSE: 0.0656, R2: 0.9413), PNorm: 158.7011, GNorm: 0.9737
[103/299] timecost: 65.00, lr: 0.000030, Train: (LOSS: 0.0288, MAE: 0.0288, RMSE: 0.0470, R2: 0.9629), Valid: (LOSS: 0.0405, MAE: 0.0405, RMSE: 0.0611, R2: 0.9489), PNorm: 158.5900, GNorm: 1.1688
[104/299] timecost: 65.31, lr: 0.000030, Train: (LOSS: 0.0279, MAE: 0.0279, RMSE: 0.0452, R2: 0.9648), Valid: (LOSS: 0.0403, MAE: 0.0403, RMSE: 0.0615, R2: 0.9478), PNorm: 158.4779, GNorm: 1.3345
[105/299] timecost: 65.43, lr: 0.000030, Train: (LOSS: 0.0270, MAE: 0.0270, RMSE: 0.0438, R2: 0.9666), Valid: (LOSS: 0.0413, MAE: 0.0413, RMSE: 0.0612, R2: 0.9486), PNorm: 158.3668, GNorm: 1.0350
[106/299] timecost: 65.43, lr: 0.000030, Train: (LOSS: 0.0284, MAE: 0.0284, RMSE: 0.0450, R2: 0.9653), Valid: (LOSS: 0.0428, MAE: 0.0428, RMSE: 0.0618, R2: 0.9473), PNorm: 158.2549, GNorm: 1.2604
[107/299] timecost: 64.25, lr: 0.000030, Train: (LOSS: 0.0267, MAE: 0.0267, RMSE: 0.0429, R2: 0.9679), Valid: (LOSS: 0.0429, MAE: 0.0429, RMSE: 0.0634, R2: 0.9452), PNorm: 158.1439, GNorm: 0.9857
[108/299] timecost: 64.90, lr: 0.000030, Train: (LOSS: 0.0262, MAE: 0.0262, RMSE: 0.0425, R2: 0.9685), Valid: (LOSS: 0.0406, MAE: 0.0406, RMSE: 0.0611, R2: 0.9476), PNorm: 158.0322, GNorm: 1.5679
[109/299] timecost: 64.71, lr: 0.000030, Train: (LOSS: 0.0266, MAE: 0.0266, RMSE: 0.0433, R2: 0.9684), Valid: (LOSS: 0.0419, MAE: 0.0419, RMSE: 0.0655, R2: 0.9417), PNorm: 157.9211, GNorm: 0.9684
[110/299] timecost: 64.63, lr: 0.000030, Train: (LOSS: 0.0259, MAE: 0.0259, RMSE: 0.0413, R2: 0.9696), Valid: (LOSS: 0.0421, MAE: 0.0421, RMSE: 0.0611, R2: 0.9493), PNorm: 157.8092, GNorm: 0.9769
[111/299] timecost: 64.60, lr: 0.000030, Train: (LOSS: 0.0263, MAE: 0.0263, RMSE: 0.0421, R2: 0.9696), Valid: (LOSS: 0.0400, MAE: 0.0400, RMSE: 0.0606, R2: 0.9490), PNorm: 157.6975, GNorm: 0.8059
[112/299] timecost: 64.18, lr: 0.000030, Train: (LOSS: 0.0269, MAE: 0.0269, RMSE: 0.0426, R2: 0.9686), Valid: (LOSS: 0.0411, MAE: 0.0411, RMSE: 0.0610, R2: 0.9493), PNorm: 157.5873, GNorm: 0.9642
[113/299] timecost: 64.68, lr: 0.000030, Train: (LOSS: 0.0263, MAE: 0.0263, RMSE: 0.0420, R2: 0.9692), Valid: (LOSS: 0.0419, MAE: 0.0419, RMSE: 0.0623, R2: 0.9463), PNorm: 157.4772, GNorm: 1.1151
[114/299] timecost: 64.71, lr: 0.000030, Train: (LOSS: 0.0256, MAE: 0.0256, RMSE: 0.0409, R2: 0.9712), Valid: (LOSS: 0.0393, MAE: 0.0393, RMSE: 0.0574, R2: 0.9543), PNorm: 157.3660, GNorm: 1.2686
[115/299] timecost: 65.25, lr: 0.000030, Train: (LOSS: 0.0251, MAE: 0.0251, RMSE: 0.0398, R2: 0.9718), Valid: (LOSS: 0.0401, MAE: 0.0401, RMSE: 0.0605, R2: 0.9497), PNorm: 157.2557, GNorm: 0.9601
[116/299] timecost: 65.24, lr: 0.000030, Train: (LOSS: 0.0256, MAE: 0.0256, RMSE: 0.0409, R2: 0.9713), Valid: (LOSS: 0.0425, MAE: 0.0425, RMSE: 0.0607, R2: 0.9492), PNorm: 157.1450, GNorm: 1.0697
[117/299] timecost: 64.10, lr: 0.000030, Train: (LOSS: 0.0244, MAE: 0.0244, RMSE: 0.0390, R2: 0.9743), Valid: (LOSS: 0.0391, MAE: 0.0391, RMSE: 0.0578, R2: 0.9541), PNorm: 157.0340, GNorm: 1.5862
[118/299] timecost: 64.87, lr: 0.000030, Train: (LOSS: 0.0254, MAE: 0.0254, RMSE: 0.0407, R2: 0.9719), Valid: (LOSS: 0.0424, MAE: 0.0424, RMSE: 0.0630, R2: 0.9462), PNorm: 156.9239, GNorm: 1.4418
[119/299] timecost: 64.90, lr: 0.000030, Train: (LOSS: 0.0256, MAE: 0.0256, RMSE: 0.0408, R2: 0.9721), Valid: (LOSS: 0.0408, MAE: 0.0408, RMSE: 0.0583, R2: 0.9540), PNorm: 156.8132, GNorm: 1.2026
[120/299] timecost: 64.96, lr: 0.000030, Train: (LOSS: 0.0241, MAE: 0.0241, RMSE: 0.0391, R2: 0.9745), Valid: (LOSS: 0.0400, MAE: 0.0400, RMSE: 0.0584, R2: 0.9537), PNorm: 156.7014, GNorm: 0.9227
[121/299] timecost: 65.22, lr: 0.000030, Train: (LOSS: 0.0250, MAE: 0.0250, RMSE: 0.0392, R2: 0.9748), Valid: (LOSS: 0.0400, MAE: 0.0400, RMSE: 0.0592, R2: 0.9520), PNorm: 156.5918, GNorm: 1.6632
[122/299] timecost: 64.82, lr: 0.000030, Train: (LOSS: 0.0246, MAE: 0.0246, RMSE: 0.0395, R2: 0.9748), Valid: (LOSS: 0.0427, MAE: 0.0427, RMSE: 0.0646, R2: 0.9427), PNorm: 156.4824, GNorm: 1.0957
[123/299] timecost: 65.03, lr: 0.000030, Train: (LOSS: 0.0237, MAE: 0.0237, RMSE: 0.0377, R2: 0.9752), Valid: (LOSS: 0.0398, MAE: 0.0398, RMSE: 0.0584, R2: 0.9529), PNorm: 156.3721, GNorm: 0.8215
[124/299] timecost: 63.99, lr: 0.000030, Train: (LOSS: 0.0233, MAE: 0.0233, RMSE: 0.0371, R2: 0.9767), Valid: (LOSS: 0.0405, MAE: 0.0405, RMSE: 0.0599, R2: 0.9504), PNorm: 156.2627, GNorm: 0.8330
[125/299] timecost: 63.79, lr: 0.000030, Train: (LOSS: 0.0222, MAE: 0.0222, RMSE: 0.0356, R2: 0.9788), Valid: (LOSS: 0.0391, MAE: 0.0391, RMSE: 0.0594, R2: 0.9519), PNorm: 156.1525, GNorm: 1.3013
[126/299] timecost: 63.59, lr: 0.000030, Train: (LOSS: 0.0230, MAE: 0.0230, RMSE: 0.0356, R2: 0.9774), Valid: (LOSS: 0.0372, MAE: 0.0372, RMSE: 0.0560, R2: 0.9563), PNorm: 156.0419, GNorm: 0.9834
[127/299] timecost: 63.95, lr: 0.000030, Train: (LOSS: 0.0221, MAE: 0.0221, RMSE: 0.0350, R2: 0.9781), Valid: (LOSS: 0.0394, MAE: 0.0394, RMSE: 0.0582, R2: 0.9537), PNorm: 155.9328, GNorm: 0.9370
[128/299] timecost: 63.60, lr: 0.000030, Train: (LOSS: 0.0222, MAE: 0.0222, RMSE: 0.0352, R2: 0.9786), Valid: (LOSS: 0.0386, MAE: 0.0386, RMSE: 0.0558, R2: 0.9567), PNorm: 155.8226, GNorm: 0.8542
[129/299] timecost: 66.73, lr: 0.000030, Train: (LOSS: 0.0234, MAE: 0.0234, RMSE: 0.0366, R2: 0.9760), Valid: (LOSS: 0.0423, MAE: 0.0423, RMSE: 0.0642, R2: 0.9435), PNorm: 155.7135, GNorm: 1.4002
[130/299] timecost: 67.35, lr: 0.000030, Train: (LOSS: 0.0239, MAE: 0.0239, RMSE: 0.0374, R2: 0.9750), Valid: (LOSS: 0.0386, MAE: 0.0386, RMSE: 0.0568, R2: 0.9557), PNorm: 155.6067, GNorm: 1.0188
[131/299] timecost: 67.30, lr: 0.000030, Train: (LOSS: 0.0226, MAE: 0.0226, RMSE: 0.0347, R2: 0.9795), Valid: (LOSS: 0.0420, MAE: 0.0420, RMSE: 0.0625, R2: 0.9470), PNorm: 155.4975, GNorm: 2.2573
[132/299] timecost: 66.60, lr: 0.000030, Train: (LOSS: 0.0224, MAE: 0.0224, RMSE: 0.0345, R2: 0.9803), Valid: (LOSS: 0.0386, MAE: 0.0386, RMSE: 0.0564, R2: 0.9566), PNorm: 155.3884, GNorm: 1.1109
[133/299] timecost: 64.83, lr: 0.000030, Train: (LOSS: 0.0215, MAE: 0.0215, RMSE: 0.0336, R2: 0.9810), Valid: (LOSS: 0.0381, MAE: 0.0381, RMSE: 0.0553, R2: 0.9583), PNorm: 155.2805, GNorm: 0.7695
[134/299] timecost: 64.15, lr: 0.000030, Train: (LOSS: 0.0211, MAE: 0.0211, RMSE: 0.0335, R2: 0.9803), Valid: (LOSS: 0.0387, MAE: 0.0387, RMSE: 0.0554, R2: 0.9582), PNorm: 155.1715, GNorm: 0.7200
[135/299] timecost: 64.05, lr: 0.000030, Train: (LOSS: 0.0227, MAE: 0.0227, RMSE: 0.0356, R2: 0.9777), Valid: (LOSS: 0.0416, MAE: 0.0416, RMSE: 0.0591, R2: 0.9523), PNorm: 155.0628, GNorm: 1.0426
[136/299] timecost: 64.58, lr: 0.000030, Train: (LOSS: 0.0224, MAE: 0.0224, RMSE: 0.0344, R2: 0.9793), Valid: (LOSS: 0.0385, MAE: 0.0385, RMSE: 0.0558, R2: 0.9573), PNorm: 154.9555, GNorm: 0.9099
[137/299] timecost: 64.93, lr: 0.000030, Train: (LOSS: 0.0199, MAE: 0.0199, RMSE: 0.0310, R2: 0.9821), Valid: (LOSS: 0.0387, MAE: 0.0387, RMSE: 0.0559, R2: 0.9566), PNorm: 154.8468, GNorm: 1.0201
[138/299] timecost: 64.46, lr: 0.000030, Train: (LOSS: 0.0219, MAE: 0.0219, RMSE: 0.0337, R2: 0.9804), Valid: (LOSS: 0.0400, MAE: 0.0400, RMSE: 0.0570, R2: 0.9559), PNorm: 154.7395, GNorm: 0.7947
[139/299] timecost: 64.98, lr: 0.000030, Train: (LOSS: 0.0203, MAE: 0.0203, RMSE: 0.0316, R2: 0.9819), Valid: (LOSS: 0.0391, MAE: 0.0391, RMSE: 0.0576, R2: 0.9548), PNorm: 154.6301, GNorm: 1.1830
[140/299] timecost: 65.29, lr: 0.000030, Train: (LOSS: 0.0213, MAE: 0.0213, RMSE: 0.0328, R2: 0.9820), Valid: (LOSS: 0.0379, MAE: 0.0379, RMSE: 0.0540, R2: 0.9600), PNorm: 154.5217, GNorm: 0.8713
[141/299] timecost: 65.01, lr: 0.000030, Train: (LOSS: 0.0210, MAE: 0.0210, RMSE: 0.0322, R2: 0.9816), Valid: (LOSS: 0.0396, MAE: 0.0396, RMSE: 0.0579, R2: 0.9544), PNorm: 154.4135, GNorm: 1.1475
[142/299] timecost: 63.91, lr: 0.000030, Train: (LOSS: 0.0201, MAE: 0.0201, RMSE: 0.0309, R2: 0.9821), Valid: (LOSS: 0.0372, MAE: 0.0372, RMSE: 0.0535, R2: 0.9610), PNorm: 154.3046, GNorm: 1.0249
[143/299] timecost: 63.87, lr: 0.000030, Train: (LOSS: 0.0198, MAE: 0.0198, RMSE: 0.0313, R2: 0.9835), Valid: (LOSS: 0.0394, MAE: 0.0394, RMSE: 0.0571, R2: 0.9560), PNorm: 154.1957, GNorm: 1.2753
[144/299] timecost: 63.58, lr: 0.000030, Train: (LOSS: 0.0196, MAE: 0.0196, RMSE: 0.0308, R2: 0.9830), Valid: (LOSS: 0.0379, MAE: 0.0379, RMSE: 0.0543, R2: 0.9595), PNorm: 154.0865, GNorm: 1.4243
[145/299] timecost: 63.94, lr: 0.000030, Train: (LOSS: 0.0197, MAE: 0.0197, RMSE: 0.0305, R2: 0.9844), Valid: (LOSS: 0.0369, MAE: 0.0369, RMSE: 0.0541, R2: 0.9596), PNorm: 153.9782, GNorm: 0.7221
[146/299] timecost: 64.33, lr: 0.000030, Train: (LOSS: 0.0192, MAE: 0.0192, RMSE: 0.0305, R2: 0.9828), Valid: (LOSS: 0.0376, MAE: 0.0376, RMSE: 0.0545, R2: 0.9593), PNorm: 153.8695, GNorm: 1.0081
Epoch 00148: reducing learning rate of group 0 to 2.7000e-05.
[147/299] timecost: 64.23, lr: 0.000027, Train: (LOSS: 0.0201, MAE: 0.0201, RMSE: 0.0312, R2: 0.9827), Valid: (LOSS: 0.0390, MAE: 0.0390, RMSE: 0.0571, R2: 0.9550), PNorm: 153.7619, GNorm: 1.0148
[148/299] timecost: 63.89, lr: 0.000027, Train: (LOSS: 0.0193, MAE: 0.0193, RMSE: 0.0302, R2: 0.9829), Valid: (LOSS: 0.0386, MAE: 0.0386, RMSE: 0.0561, R2: 0.9566), PNorm: 153.6638, GNorm: 0.9483
[149/299] timecost: 63.85, lr: 0.000027, Train: (LOSS: 0.0184, MAE: 0.0184, RMSE: 0.0286, R2: 0.9862), Valid: (LOSS: 0.0391, MAE: 0.0391, RMSE: 0.0580, R2: 0.9537), PNorm: 153.5651, GNorm: 0.8372
[150/299] timecost: 63.85, lr: 0.000027, Train: (LOSS: 0.0195, MAE: 0.0195, RMSE: 0.0301, R2: 0.9849), Valid: (LOSS: 0.0381, MAE: 0.0381, RMSE: 0.0544, R2: 0.9596), PNorm: 153.4679, GNorm: 2.2907
[151/299] timecost: 64.19, lr: 0.000027, Train: (LOSS: 0.0188, MAE: 0.0188, RMSE: 0.0292, R2: 0.9849), Valid: (LOSS: 0.0381, MAE: 0.0381, RMSE: 0.0560, R2: 0.9573), PNorm: 153.3704, GNorm: 1.9181
[152/299] timecost: 63.86, lr: 0.000027, Train: (LOSS: 0.0185, MAE: 0.0185, RMSE: 0.0286, R2: 0.9852), Valid: (LOSS: 0.0383, MAE: 0.0383, RMSE: 0.0551, R2: 0.9586), PNorm: 153.2729, GNorm: 1.0095
[153/299] timecost: 64.20, lr: 0.000027, Train: (LOSS: 0.0175, MAE: 0.0175, RMSE: 0.0271, R2: 0.9865), Valid: (LOSS: 0.0371, MAE: 0.0371, RMSE: 0.0514, R2: 0.9633), PNorm: 153.1752, GNorm: 1.1662
[154/299] timecost: 63.59, lr: 0.000027, Train: (LOSS: 0.0181, MAE: 0.0181, RMSE: 0.0280, R2: 0.9866), Valid: (LOSS: 0.0367, MAE: 0.0367, RMSE: 0.0518, R2: 0.9634), PNorm: 153.0769, GNorm: 1.2360
[155/299] timecost: 64.45, lr: 0.000027, Train: (LOSS: 0.0179, MAE: 0.0179, RMSE: 0.0278, R2: 0.9865), Valid: (LOSS: 0.0367, MAE: 0.0367, RMSE: 0.0532, R2: 0.9611), PNorm: 152.9796, GNorm: 1.8559
[156/299] timecost: 63.80, lr: 0.000027, Train: (LOSS: 0.0178, MAE: 0.0178, RMSE: 0.0276, R2: 0.9863), Valid: (LOSS: 0.0363, MAE: 0.0363, RMSE: 0.0510, R2: 0.9635), PNorm: 152.8821, GNorm: 0.7610
[157/299] timecost: 64.21, lr: 0.000027, Train: (LOSS: 0.0174, MAE: 0.0174, RMSE: 0.0271, R2: 0.9871), Valid: (LOSS: 0.0371, MAE: 0.0371, RMSE: 0.0533, R2: 0.9604), PNorm: 152.7852, GNorm: 0.8555
[158/299] timecost: 63.78, lr: 0.000027, Train: (LOSS: 0.0180, MAE: 0.0180, RMSE: 0.0280, R2: 0.9867), Valid: (LOSS: 0.0385, MAE: 0.0385, RMSE: 0.0558, R2: 0.9570), PNorm: 152.6879, GNorm: 0.9847
[159/299] timecost: 63.95, lr: 0.000027, Train: (LOSS: 0.0180, MAE: 0.0180, RMSE: 0.0279, R2: 0.9868), Valid: (LOSS: 0.0357, MAE: 0.0357, RMSE: 0.0501, R2: 0.9649), PNorm: 152.5905, GNorm: 0.7879
[160/299] timecost: 64.07, lr: 0.000027, Train: (LOSS: 0.0171, MAE: 0.0171, RMSE: 0.0263, R2: 0.9873), Valid: (LOSS: 0.0375, MAE: 0.0375, RMSE: 0.0546, R2: 0.9593), PNorm: 152.4928, GNorm: 0.9411
[161/299] timecost: 64.05, lr: 0.000027, Train: (LOSS: 0.0168, MAE: 0.0168, RMSE: 0.0258, R2: 0.9885), Valid: (LOSS: 0.0370, MAE: 0.0370, RMSE: 0.0525, R2: 0.9621), PNorm: 152.3949, GNorm: 1.0631
[162/299] timecost: 64.84, lr: 0.000027, Train: (LOSS: 0.0168, MAE: 0.0168, RMSE: 0.0260, R2: 0.9881), Valid: (LOSS: 0.0371, MAE: 0.0371, RMSE: 0.0539, R2: 0.9595), PNorm: 152.2984, GNorm: 1.0550
[163/299] timecost: 63.71, lr: 0.000027, Train: (LOSS: 0.0176, MAE: 0.0176, RMSE: 0.0273, R2: 0.9873), Valid: (LOSS: 0.0367, MAE: 0.0367, RMSE: 0.0526, R2: 0.9612), PNorm: 152.2008, GNorm: 0.9182
[164/299] timecost: 63.63, lr: 0.000027, Train: (LOSS: 0.0176, MAE: 0.0176, RMSE: 0.0273, R2: 0.9874), Valid: (LOSS: 0.0365, MAE: 0.0365, RMSE: 0.0527, R2: 0.9617), PNorm: 152.1035, GNorm: 1.2615
[165/299] timecost: 63.53, lr: 0.000027, Train: (LOSS: 0.0178, MAE: 0.0178, RMSE: 0.0275, R2: 0.9869), Valid: (LOSS: 0.0373, MAE: 0.0373, RMSE: 0.0525, R2: 0.9614), PNorm: 152.0073, GNorm: 0.9565
[166/299] timecost: 63.23, lr: 0.000027, Train: (LOSS: 0.0164, MAE: 0.0164, RMSE: 0.0250, R2: 0.9886), Valid: (LOSS: 0.0373, MAE: 0.0373, RMSE: 0.0531, R2: 0.9611), PNorm: 151.9103, GNorm: 1.5734
[167/299] timecost: 63.68, lr: 0.000027, Train: (LOSS: 0.0168, MAE: 0.0168, RMSE: 0.0255, R2: 0.9886), Valid: (LOSS: 0.0350, MAE: 0.0350, RMSE: 0.0494, R2: 0.9654), PNorm: 151.8129, GNorm: 1.3466
[168/299] timecost: 63.95, lr: 0.000027, Train: (LOSS: 0.0169, MAE: 0.0169, RMSE: 0.0262, R2: 0.9879), Valid: (LOSS: 0.0381, MAE: 0.0381, RMSE: 0.0548, R2: 0.9586), PNorm: 151.7157, GNorm: 1.4684
[169/299] timecost: 65.17, lr: 0.000027, Train: (LOSS: 0.0160, MAE: 0.0160, RMSE: 0.0249, R2: 0.9888), Valid: (LOSS: 0.0365, MAE: 0.0365, RMSE: 0.0513, R2: 0.9630), PNorm: 151.6188, GNorm: 0.8575
[170/299] timecost: 65.17, lr: 0.000027, Train: (LOSS: 0.0164, MAE: 0.0164, RMSE: 0.0252, R2: 0.9888), Valid: (LOSS: 0.0353, MAE: 0.0353, RMSE: 0.0496, R2: 0.9654), PNorm: 151.5218, GNorm: 0.7876
[171/299] timecost: 63.86, lr: 0.000027, Train: (LOSS: 0.0156, MAE: 0.0156, RMSE: 0.0240, R2: 0.9897), Valid: (LOSS: 0.0380, MAE: 0.0380, RMSE: 0.0521, R2: 0.9610), PNorm: 151.4235, GNorm: 1.0913
[172/299] timecost: 63.77, lr: 0.000027, Train: (LOSS: 0.0177, MAE: 0.0177, RMSE: 0.0264, R2: 0.9885), Valid: (LOSS: 0.0373, MAE: 0.0373, RMSE: 0.0543, R2: 0.9598), PNorm: 151.3276, GNorm: 0.9679
[173/299] timecost: 66.15, lr: 0.000027, Train: (LOSS: 0.0156, MAE: 0.0156, RMSE: 0.0241, R2: 0.9898), Valid: (LOSS: 0.0367, MAE: 0.0367, RMSE: 0.0523, R2: 0.9619), PNorm: 151.2300, GNorm: 1.4300
[174/299] timecost: 67.05, lr: 0.000027, Train: (LOSS: 0.0157, MAE: 0.0157, RMSE: 0.0243, R2: 0.9899), Valid: (LOSS: 0.0369, MAE: 0.0369, RMSE: 0.0550, R2: 0.9573), PNorm: 151.1328, GNorm: 2.6350
[175/299] timecost: 64.41, lr: 0.000027, Train: (LOSS: 0.0163, MAE: 0.0163, RMSE: 0.0252, R2: 0.9889), Valid: (LOSS: 0.0369, MAE: 0.0369, RMSE: 0.0546, R2: 0.9589), PNorm: 151.0359, GNorm: 0.9479
[176/299] timecost: 64.58, lr: 0.000027, Train: (LOSS: 0.0150, MAE: 0.0150, RMSE: 0.0225, R2: 0.9907), Valid: (LOSS: 0.0352, MAE: 0.0352, RMSE: 0.0489, R2: 0.9667), PNorm: 150.9389, GNorm: 0.9762
[177/299] timecost: 64.00, lr: 0.000027, Train: (LOSS: 0.0163, MAE: 0.0163, RMSE: 0.0245, R2: 0.9902), Valid: (LOSS: 0.0361, MAE: 0.0361, RMSE: 0.0503, R2: 0.9645), PNorm: 150.8419, GNorm: 1.3361
[178/299] timecost: 63.96, lr: 0.000027, Train: (LOSS: 0.0163, MAE: 0.0163, RMSE: 0.0246, R2: 0.9897), Valid: (LOSS: 0.0374, MAE: 0.0374, RMSE: 0.0530, R2: 0.9615), PNorm: 150.7450, GNorm: 1.1609
[179/299] timecost: 64.14, lr: 0.000027, Train: (LOSS: 0.0159, MAE: 0.0159, RMSE: 0.0236, R2: 0.9892), Valid: (LOSS: 0.0367, MAE: 0.0367, RMSE: 0.0533, R2: 0.9606), PNorm: 150.6484, GNorm: 1.3566
[180/299] timecost: 63.84, lr: 0.000027, Train: (LOSS: 0.0156, MAE: 0.0156, RMSE: 0.0240, R2: 0.9893), Valid: (LOSS: 0.0381, MAE: 0.0381, RMSE: 0.0553, R2: 0.9573), PNorm: 150.5518, GNorm: 0.8011
[181/299] timecost: 63.85, lr: 0.000027, Train: (LOSS: 0.0149, MAE: 0.0149, RMSE: 0.0231, R2: 0.9911), Valid: (LOSS: 0.0385, MAE: 0.0385, RMSE: 0.0566, R2: 0.9561), PNorm: 150.4546, GNorm: 0.9918
[182/299] timecost: 63.83, lr: 0.000027, Train: (LOSS: 0.0155, MAE: 0.0155, RMSE: 0.0230, R2: 0.9912), Valid: (LOSS: 0.0399, MAE: 0.0399, RMSE: 0.0585, R2: 0.9529), PNorm: 150.3570, GNorm: 1.0091
[183/299] timecost: 64.12, lr: 0.000027, Train: (LOSS: 0.0154, MAE: 0.0154, RMSE: 0.0234, R2: 0.9909), Valid: (LOSS: 0.0374, MAE: 0.0374, RMSE: 0.0533, R2: 0.9613), PNorm: 150.2610, GNorm: 0.7112
[184/299] timecost: 64.34, lr: 0.000027, Train: (LOSS: 0.0151, MAE: 0.0151, RMSE: 0.0235, R2: 0.9905), Valid: (LOSS: 0.0378, MAE: 0.0378, RMSE: 0.0531, R2: 0.9609), PNorm: 150.1640, GNorm: 0.6806
[185/299] timecost: 63.95, lr: 0.000027, Train: (LOSS: 0.0149, MAE: 0.0149, RMSE: 0.0228, R2: 0.9911), Valid: (LOSS: 0.0364, MAE: 0.0364, RMSE: 0.0503, R2: 0.9642), PNorm: 150.0668, GNorm: 1.3788
[186/299] timecost: 63.97, lr: 0.000027, Train: (LOSS: 0.0147, MAE: 0.0147, RMSE: 0.0221, R2: 0.9919), Valid: (LOSS: 0.0372, MAE: 0.0372, RMSE: 0.0548, R2: 0.9589), PNorm: 149.9693, GNorm: 1.1011
[187/299] timecost: 64.61, lr: 0.000027, Train: (LOSS: 0.0149, MAE: 0.0149, RMSE: 0.0224, R2: 0.9914), Valid: (LOSS: 0.0369, MAE: 0.0369, RMSE: 0.0548, R2: 0.9583), PNorm: 149.8718, GNorm: 0.9259
Epoch 00189: reducing learning rate of group 0 to 2.4300e-05.
[188/299] timecost: 64.86, lr: 0.000024, Train: (LOSS: 0.0153, MAE: 0.0153, RMSE: 0.0228, R2: 0.9917), Valid: (LOSS: 0.0368, MAE: 0.0368, RMSE: 0.0541, R2: 0.9598), PNorm: 149.7759, GNorm: 0.8922
[189/299] timecost: 64.42, lr: 0.000024, Train: (LOSS: 0.0141, MAE: 0.0141, RMSE: 0.0215, R2: 0.9921), Valid: (LOSS: 0.0352, MAE: 0.0352, RMSE: 0.0490, R2: 0.9666), PNorm: 149.6888, GNorm: 1.5178
[190/299] timecost: 64.01, lr: 0.000024, Train: (LOSS: 0.0139, MAE: 0.0139, RMSE: 0.0211, R2: 0.9924), Valid: (LOSS: 0.0353, MAE: 0.0353, RMSE: 0.0503, R2: 0.9651), PNorm: 149.6007, GNorm: 1.1448
[191/299] timecost: 64.18, lr: 0.000024, Train: (LOSS: 0.0156, MAE: 0.0156, RMSE: 0.0233, R2: 0.9907), Valid: (LOSS: 0.0365, MAE: 0.0365, RMSE: 0.0514, R2: 0.9632), PNorm: 149.5130, GNorm: 1.2144
[192/299] timecost: 63.79, lr: 0.000024, Train: (LOSS: 0.0136, MAE: 0.0136, RMSE: 0.0209, R2: 0.9922), Valid: (LOSS: 0.0354, MAE: 0.0354, RMSE: 0.0488, R2: 0.9672), PNorm: 149.4261, GNorm: 1.0631
[193/299] timecost: 64.06, lr: 0.000024, Train: (LOSS: 0.0136, MAE: 0.0136, RMSE: 0.0206, R2: 0.9928), Valid: (LOSS: 0.0373, MAE: 0.0373, RMSE: 0.0524, R2: 0.9614), PNorm: 149.3393, GNorm: 1.0933
[194/299] timecost: 65.16, lr: 0.000024, Train: (LOSS: 0.0141, MAE: 0.0141, RMSE: 0.0215, R2: 0.9921), Valid: (LOSS: 0.0352, MAE: 0.0352, RMSE: 0.0494, R2: 0.9663), PNorm: 149.2524, GNorm: 0.7589
[195/299] timecost: 63.40, lr: 0.000024, Train: (LOSS: 0.0133, MAE: 0.0133, RMSE: 0.0198, R2: 0.9934), Valid: (LOSS: 0.0360, MAE: 0.0360, RMSE: 0.0503, R2: 0.9648), PNorm: 149.1648, GNorm: 1.1173
[196/299] timecost: 63.89, lr: 0.000024, Train: (LOSS: 0.0140, MAE: 0.0140, RMSE: 0.0208, R2: 0.9926), Valid: (LOSS: 0.0373, MAE: 0.0373, RMSE: 0.0556, R2: 0.9570), PNorm: 149.0777, GNorm: 0.9237
[197/299] timecost: 63.98, lr: 0.000024, Train: (LOSS: 0.0133, MAE: 0.0133, RMSE: 0.0201, R2: 0.9931), Valid: (LOSS: 0.0356, MAE: 0.0356, RMSE: 0.0497, R2: 0.9656), PNorm: 148.9911, GNorm: 0.8933
[198/299] timecost: 63.78, lr: 0.000024, Train: (LOSS: 0.0135, MAE: 0.0135, RMSE: 0.0205, R2: 0.9932), Valid: (LOSS: 0.0356, MAE: 0.0356, RMSE: 0.0505, R2: 0.9646), PNorm: 148.9031, GNorm: 1.1064
[199/299] timecost: 63.52, lr: 0.000024, Train: (LOSS: 0.0134, MAE: 0.0134, RMSE: 0.0201, R2: 0.9931), Valid: (LOSS: 0.0368, MAE: 0.0368, RMSE: 0.0523, R2: 0.9621), PNorm: 148.8154, GNorm: 0.5328
[200/299] timecost: 63.60, lr: 0.000024, Train: (LOSS: 0.0130, MAE: 0.0130, RMSE: 0.0196, R2: 0.9933), Valid: (LOSS: 0.0379, MAE: 0.0379, RMSE: 0.0531, R2: 0.9602), PNorm: 148.7276, GNorm: 1.3373
[201/299] timecost: 63.64, lr: 0.000024, Train: (LOSS: 0.0129, MAE: 0.0129, RMSE: 0.0195, R2: 0.9932), Valid: (LOSS: 0.0348, MAE: 0.0348, RMSE: 0.0493, R2: 0.9660), PNorm: 148.6393, GNorm: 1.7837
[202/299] timecost: 63.50, lr: 0.000024, Train: (LOSS: 0.0140, MAE: 0.0140, RMSE: 0.0210, R2: 0.9925), Valid: (LOSS: 0.0361, MAE: 0.0361, RMSE: 0.0523, R2: 0.9621), PNorm: 148.5522, GNorm: 0.7087
[203/299] timecost: 63.76, lr: 0.000024, Train: (LOSS: 0.0124, MAE: 0.0124, RMSE: 0.0185, R2: 0.9942), Valid: (LOSS: 0.0360, MAE: 0.0360, RMSE: 0.0509, R2: 0.9637), PNorm: 148.4643, GNorm: 1.2891
[204/299] timecost: 63.54, lr: 0.000024, Train: (LOSS: 0.0130, MAE: 0.0130, RMSE: 0.0196, R2: 0.9936), Valid: (LOSS: 0.0358, MAE: 0.0358, RMSE: 0.0515, R2: 0.9627), PNorm: 148.3763, GNorm: 0.8425
[205/299] timecost: 63.33, lr: 0.000024, Train: (LOSS: 0.0126, MAE: 0.0126, RMSE: 0.0189, R2: 0.9940), Valid: (LOSS: 0.0363, MAE: 0.0363, RMSE: 0.0532, R2: 0.9607), PNorm: 148.2884, GNorm: 0.8817
[206/299] timecost: 63.70, lr: 0.000024, Train: (LOSS: 0.0126, MAE: 0.0126, RMSE: 0.0188, R2: 0.9943), Valid: (LOSS: 0.0356, MAE: 0.0356, RMSE: 0.0500, R2: 0.9654), PNorm: 148.2006, GNorm: 1.3463
[207/299] timecost: 63.84, lr: 0.000024, Train: (LOSS: 0.0124, MAE: 0.0124, RMSE: 0.0185, R2: 0.9942), Valid: (LOSS: 0.0349, MAE: 0.0349, RMSE: 0.0495, R2: 0.9657), PNorm: 148.1119, GNorm: 0.7684
[208/299] timecost: 63.52, lr: 0.000024, Train: (LOSS: 0.0119, MAE: 0.0119, RMSE: 0.0178, R2: 0.9945), Valid: (LOSS: 0.0363, MAE: 0.0363, RMSE: 0.0530, R2: 0.9612), PNorm: 148.0239, GNorm: 0.7612
Epoch 00210: reducing learning rate of group 0 to 2.1870e-05.
[209/299] timecost: 63.74, lr: 0.000022, Train: (LOSS: 0.0120, MAE: 0.0120, RMSE: 0.0182, R2: 0.9947), Valid: (LOSS: 0.0353, MAE: 0.0353, RMSE: 0.0502, R2: 0.9651), PNorm: 147.9361, GNorm: 1.0775
[210/299] timecost: 63.73, lr: 0.000022, Train: (LOSS: 0.0121, MAE: 0.0121, RMSE: 0.0178, R2: 0.9947), Valid: (LOSS: 0.0365, MAE: 0.0365, RMSE: 0.0523, R2: 0.9622), PNorm: 147.8558, GNorm: 1.0176
[211/299] timecost: 63.72, lr: 0.000022, Train: (LOSS: 0.0119, MAE: 0.0119, RMSE: 0.0180, R2: 0.9947), Valid: (LOSS: 0.0368, MAE: 0.0368, RMSE: 0.0536, R2: 0.9602), PNorm: 147.7765, GNorm: 0.7758
[212/299] timecost: 63.53, lr: 0.000022, Train: (LOSS: 0.0119, MAE: 0.0119, RMSE: 0.0177, R2: 0.9948), Valid: (LOSS: 0.0355, MAE: 0.0355, RMSE: 0.0500, R2: 0.9646), PNorm: 147.6972, GNorm: 0.9106
[213/299] timecost: 63.71, lr: 0.000022, Train: (LOSS: 0.0111, MAE: 0.0111, RMSE: 0.0168, R2: 0.9952), Valid: (LOSS: 0.0356, MAE: 0.0356, RMSE: 0.0506, R2: 0.9632), PNorm: 147.6181, GNorm: 1.0785
[214/299] timecost: 63.70, lr: 0.000022, Train: (LOSS: 0.0116, MAE: 0.0116, RMSE: 0.0174, R2: 0.9948), Valid: (LOSS: 0.0354, MAE: 0.0354, RMSE: 0.0510, R2: 0.9636), PNorm: 147.5384, GNorm: 0.8119
[215/299] timecost: 63.60, lr: 0.000022, Train: (LOSS: 0.0115, MAE: 0.0115, RMSE: 0.0173, R2: 0.9952), Valid: (LOSS: 0.0355, MAE: 0.0355, RMSE: 0.0507, R2: 0.9643), PNorm: 147.4588, GNorm: 0.6586
[216/299] timecost: 63.86, lr: 0.000022, Train: (LOSS: 0.0112, MAE: 0.0112, RMSE: 0.0169, R2: 0.9952), Valid: (LOSS: 0.0350, MAE: 0.0350, RMSE: 0.0502, R2: 0.9647), PNorm: 147.3786, GNorm: 0.8739
[217/299] timecost: 63.61, lr: 0.000022, Train: (LOSS: 0.0112, MAE: 0.0112, RMSE: 0.0168, R2: 0.9954), Valid: (LOSS: 0.0345, MAE: 0.0345, RMSE: 0.0491, R2: 0.9662), PNorm: 147.2987, GNorm: 0.8110
[218/299] timecost: 63.40, lr: 0.000022, Train: (LOSS: 0.0108, MAE: 0.0108, RMSE: 0.0163, R2: 0.9951), Valid: (LOSS: 0.0356, MAE: 0.0356, RMSE: 0.0486, R2: 0.9669), PNorm: 147.2197, GNorm: 0.8349
[219/299] timecost: 63.69, lr: 0.000022, Train: (LOSS: 0.0112, MAE: 0.0112, RMSE: 0.0167, R2: 0.9956), Valid: (LOSS: 0.0364, MAE: 0.0364, RMSE: 0.0533, R2: 0.9607), PNorm: 147.1392, GNorm: 0.9106
[220/299] timecost: 63.37, lr: 0.000022, Train: (LOSS: 0.0112, MAE: 0.0112, RMSE: 0.0166, R2: 0.9953), Valid: (LOSS: 0.0358, MAE: 0.0358, RMSE: 0.0503, R2: 0.9641), PNorm: 147.0594, GNorm: 1.3547
[221/299] timecost: 63.46, lr: 0.000022, Train: (LOSS: 0.0112, MAE: 0.0112, RMSE: 0.0168, R2: 0.9954), Valid: (LOSS: 0.0359, MAE: 0.0359, RMSE: 0.0511, R2: 0.9641), PNorm: 146.9808, GNorm: 0.8878
[222/299] timecost: 63.31, lr: 0.000022, Train: (LOSS: 0.0113, MAE: 0.0113, RMSE: 0.0170, R2: 0.9953), Valid: (LOSS: 0.0360, MAE: 0.0360, RMSE: 0.0522, R2: 0.9622), PNorm: 146.9014, GNorm: 0.7714
[223/299] timecost: 63.22, lr: 0.000022, Train: (LOSS: 0.0104, MAE: 0.0104, RMSE: 0.0156, R2: 0.9962), Valid: (LOSS: 0.0357, MAE: 0.0357, RMSE: 0.0515, R2: 0.9634), PNorm: 146.8220, GNorm: 0.9196
[224/299] timecost: 63.17, lr: 0.000022, Train: (LOSS: 0.0120, MAE: 0.0120, RMSE: 0.0178, R2: 0.9948), Valid: (LOSS: 0.0363, MAE: 0.0363, RMSE: 0.0534, R2: 0.9607), PNorm: 146.7424, GNorm: 1.0278
[225/299] timecost: 63.13, lr: 0.000022, Train: (LOSS: 0.0115, MAE: 0.0115, RMSE: 0.0171, R2: 0.9950), Valid: (LOSS: 0.0366, MAE: 0.0366, RMSE: 0.0532, R2: 0.9612), PNorm: 146.6634, GNorm: 1.6561
[226/299] timecost: 63.66, lr: 0.000022, Train: (LOSS: 0.0109, MAE: 0.0109, RMSE: 0.0162, R2: 0.9954), Valid: (LOSS: 0.0351, MAE: 0.0351, RMSE: 0.0497, R2: 0.9654), PNorm: 146.5849, GNorm: 1.4248
[227/299] timecost: 63.79, lr: 0.000022, Train: (LOSS: 0.0111, MAE: 0.0111, RMSE: 0.0165, R2: 0.9956), Valid: (LOSS: 0.0370, MAE: 0.0370, RMSE: 0.0523, R2: 0.9621), PNorm: 146.5062, GNorm: 0.9242
[228/299] timecost: 64.15, lr: 0.000022, Train: (LOSS: 0.0115, MAE: 0.0115, RMSE: 0.0169, R2: 0.9956), Valid: (LOSS: 0.0370, MAE: 0.0370, RMSE: 0.0528, R2: 0.9615), PNorm: 146.4287, GNorm: 0.7924
[229/299] timecost: 63.73, lr: 0.000022, Train: (LOSS: 0.0112, MAE: 0.0112, RMSE: 0.0168, R2: 0.9955), Valid: (LOSS: 0.0355, MAE: 0.0355, RMSE: 0.0501, R2: 0.9652), PNorm: 146.3505, GNorm: 1.1328
[230/299] timecost: 63.43, lr: 0.000022, Train: (LOSS: 0.0109, MAE: 0.0109, RMSE: 0.0164, R2: 0.9955), Valid: (LOSS: 0.0365, MAE: 0.0365, RMSE: 0.0525, R2: 0.9619), PNorm: 146.2722, GNorm: 1.0933
[231/299] timecost: 63.36, lr: 0.000022, Train: (LOSS: 0.0108, MAE: 0.0108, RMSE: 0.0162, R2: 0.9956), Valid: (LOSS: 0.0350, MAE: 0.0350, RMSE: 0.0505, R2: 0.9645), PNorm: 146.1944, GNorm: 1.2079
[232/299] timecost: 63.45, lr: 0.000022, Train: (LOSS: 0.0106, MAE: 0.0106, RMSE: 0.0159, R2: 0.9960), Valid: (LOSS: 0.0356, MAE: 0.0356, RMSE: 0.0520, R2: 0.9629), PNorm: 146.1157, GNorm: 1.1043
[233/299] timecost: 63.20, lr: 0.000022, Train: (LOSS: 0.0096, MAE: 0.0096, RMSE: 0.0147, R2: 0.9964), Valid: (LOSS: 0.0361, MAE: 0.0361, RMSE: 0.0516, R2: 0.9623), PNorm: 146.0372, GNorm: 1.5266
[234/299] timecost: 63.35, lr: 0.000022, Train: (LOSS: 0.0105, MAE: 0.0105, RMSE: 0.0156, R2: 0.9961), Valid: (LOSS: 0.0355, MAE: 0.0355, RMSE: 0.0496, R2: 0.9654), PNorm: 145.9590, GNorm: 0.9025
[235/299] timecost: 63.98, lr: 0.000022, Train: (LOSS: 0.0111, MAE: 0.0111, RMSE: 0.0165, R2: 0.9956), Valid: (LOSS: 0.0348, MAE: 0.0348, RMSE: 0.0495, R2: 0.9659), PNorm: 145.8810, GNorm: 0.8870
[236/299] timecost: 63.51, lr: 0.000022, Train: (LOSS: 0.0110, MAE: 0.0110, RMSE: 0.0162, R2: 0.9958), Valid: (LOSS: 0.0351, MAE: 0.0351, RMSE: 0.0501, R2: 0.9648), PNorm: 145.8035, GNorm: 1.6846
[237/299] timecost: 63.23, lr: 0.000022, Train: (LOSS: 0.0099, MAE: 0.0099, RMSE: 0.0150, R2: 0.9964), Valid: (LOSS: 0.0356, MAE: 0.0356, RMSE: 0.0517, R2: 0.9631), PNorm: 145.7256, GNorm: 0.9271
Epoch 00239: reducing learning rate of group 0 to 1.9683e-05.
[238/299] timecost: 63.35, lr: 0.000020, Train: (LOSS: 0.0101, MAE: 0.0101, RMSE: 0.0154, R2: 0.9960), Valid: (LOSS: 0.0351, MAE: 0.0351, RMSE: 0.0507, R2: 0.9644), PNorm: 145.6470, GNorm: 0.7406
[239/299] timecost: 63.55, lr: 0.000020, Train: (LOSS: 0.0101, MAE: 0.0101, RMSE: 0.0152, R2: 0.9963), Valid: (LOSS: 0.0362, MAE: 0.0362, RMSE: 0.0517, R2: 0.9626), PNorm: 145.5772, GNorm: 0.7273
[240/299] timecost: 63.26, lr: 0.000020, Train: (LOSS: 0.0105, MAE: 0.0105, RMSE: 0.0157, R2: 0.9961), Valid: (LOSS: 0.0352, MAE: 0.0352, RMSE: 0.0500, R2: 0.9652), PNorm: 145.5066, GNorm: 0.9576
[241/299] timecost: 63.06, lr: 0.000020, Train: (LOSS: 0.0102, MAE: 0.0102, RMSE: 0.0152, R2: 0.9959), Valid: (LOSS: 0.0357, MAE: 0.0357, RMSE: 0.0510, R2: 0.9640), PNorm: 145.4371, GNorm: 1.3067
[242/299] timecost: 63.13, lr: 0.000020, Train: (LOSS: 0.0098, MAE: 0.0098, RMSE: 0.0146, R2: 0.9966), Valid: (LOSS: 0.0357, MAE: 0.0357, RMSE: 0.0516, R2: 0.9628), PNorm: 145.3674, GNorm: 0.9985
[243/299] timecost: 63.10, lr: 0.000020, Train: (LOSS: 0.0096, MAE: 0.0096, RMSE: 0.0144, R2: 0.9966), Valid: (LOSS: 0.0363, MAE: 0.0363, RMSE: 0.0532, R2: 0.9610), PNorm: 145.2980, GNorm: 0.6553
[244/299] timecost: 63.19, lr: 0.000020, Train: (LOSS: 0.0101, MAE: 0.0101, RMSE: 0.0149, R2: 0.9964), Valid: (LOSS: 0.0349, MAE: 0.0349, RMSE: 0.0494, R2: 0.9658), PNorm: 145.2283, GNorm: 0.9503
[245/299] timecost: 63.79, lr: 0.000020, Train: (LOSS: 0.0096, MAE: 0.0096, RMSE: 0.0143, R2: 0.9968), Valid: (LOSS: 0.0354, MAE: 0.0354, RMSE: 0.0515, R2: 0.9635), PNorm: 145.1587, GNorm: 1.0494
[246/299] timecost: 63.07, lr: 0.000020, Train: (LOSS: 0.0089, MAE: 0.0089, RMSE: 0.0135, R2: 0.9968), Valid: (LOSS: 0.0355, MAE: 0.0355, RMSE: 0.0515, R2: 0.9629), PNorm: 145.0884, GNorm: 1.0714
[247/299] timecost: 63.30, lr: 0.000020, Train: (LOSS: 0.0096, MAE: 0.0096, RMSE: 0.0145, R2: 0.9963), Valid: (LOSS: 0.0362, MAE: 0.0362, RMSE: 0.0512, R2: 0.9629), PNorm: 145.0185, GNorm: 1.6131
[248/299] timecost: 63.39, lr: 0.000020, Train: (LOSS: 0.0090, MAE: 0.0090, RMSE: 0.0137, R2: 0.9968), Valid: (LOSS: 0.0356, MAE: 0.0356, RMSE: 0.0510, R2: 0.9635), PNorm: 144.9491, GNorm: 1.0649
[249/299] timecost: 63.43, lr: 0.000020, Train: (LOSS: 0.0095, MAE: 0.0095, RMSE: 0.0143, R2: 0.9966), Valid: (LOSS: 0.0359, MAE: 0.0359, RMSE: 0.0510, R2: 0.9641), PNorm: 144.8796, GNorm: 1.0049
[250/299] timecost: 63.30, lr: 0.000020, Train: (LOSS: 0.0103, MAE: 0.0103, RMSE: 0.0153, R2: 0.9962), Valid: (LOSS: 0.0358, MAE: 0.0358, RMSE: 0.0517, R2: 0.9629), PNorm: 144.8100, GNorm: 1.0885
[251/299] timecost: 63.14, lr: 0.000020, Train: (LOSS: 0.0106, MAE: 0.0106, RMSE: 0.0155, R2: 0.9960), Valid: (LOSS: 0.0356, MAE: 0.0356, RMSE: 0.0515, R2: 0.9632), PNorm: 144.7409, GNorm: 0.7642
[252/299] timecost: 63.50, lr: 0.000020, Train: (LOSS: 0.0098, MAE: 0.0098, RMSE: 0.0146, R2: 0.9965), Valid: (LOSS: 0.0350, MAE: 0.0350, RMSE: 0.0505, R2: 0.9642), PNorm: 144.6717, GNorm: 1.0953
[253/299] timecost: 63.46, lr: 0.000020, Train: (LOSS: 0.0096, MAE: 0.0096, RMSE: 0.0145, R2: 0.9964), Valid: (LOSS: 0.0370, MAE: 0.0370, RMSE: 0.0531, R2: 0.9607), PNorm: 144.6028, GNorm: 0.8400
[254/299] timecost: 63.80, lr: 0.000020, Train: (LOSS: 0.0096, MAE: 0.0096, RMSE: 0.0140, R2: 0.9965), Valid: (LOSS: 0.0365, MAE: 0.0365, RMSE: 0.0515, R2: 0.9624), PNorm: 144.5336, GNorm: 0.7216
[255/299] timecost: 63.79, lr: 0.000020, Train: (LOSS: 0.0094, MAE: 0.0094, RMSE: 0.0140, R2: 0.9967), Valid: (LOSS: 0.0348, MAE: 0.0348, RMSE: 0.0507, R2: 0.9644), PNorm: 144.4650, GNorm: 0.7118
[256/299] timecost: 63.99, lr: 0.000020, Train: (LOSS: 0.0090, MAE: 0.0090, RMSE: 0.0133, R2: 0.9971), Valid: (LOSS: 0.0356, MAE: 0.0356, RMSE: 0.0510, R2: 0.9639), PNorm: 144.3958, GNorm: 1.4839
[257/299] timecost: 64.21, lr: 0.000020, Train: (LOSS: 0.0093, MAE: 0.0093, RMSE: 0.0140, R2: 0.9968), Valid: (LOSS: 0.0358, MAE: 0.0358, RMSE: 0.0517, R2: 0.9631), PNorm: 144.3269, GNorm: 1.0715
[258/299] timecost: 64.07, lr: 0.000020, Train: (LOSS: 0.0091, MAE: 0.0091, RMSE: 0.0137, R2: 0.9965), Valid: (LOSS: 0.0360, MAE: 0.0360, RMSE: 0.0522, R2: 0.9626), PNorm: 144.2588, GNorm: 1.2690
Epoch 00260: reducing learning rate of group 0 to 1.7715e-05.
[259/299] timecost: 64.72, lr: 0.000018, Train: (LOSS: 0.0097, MAE: 0.0097, RMSE: 0.0142, R2: 0.9967), Valid: (LOSS: 0.0359, MAE: 0.0359, RMSE: 0.0521, R2: 0.9623), PNorm: 144.1905, GNorm: 1.5254
[260/299] timecost: 64.11, lr: 0.000018, Train: (LOSS: 0.0097, MAE: 0.0097, RMSE: 0.0144, R2: 0.9966), Valid: (LOSS: 0.0362, MAE: 0.0362, RMSE: 0.0518, R2: 0.9626), PNorm: 144.1298, GNorm: 0.8362
[261/299] timecost: 64.36, lr: 0.000018, Train: (LOSS: 0.0088, MAE: 0.0088, RMSE: 0.0133, R2: 0.9970), Valid: (LOSS: 0.0370, MAE: 0.0370, RMSE: 0.0515, R2: 0.9629), PNorm: 144.0684, GNorm: 0.8646
[262/299] timecost: 64.14, lr: 0.000018, Train: (LOSS: 0.0087, MAE: 0.0087, RMSE: 0.0130, R2: 0.9972), Valid: (LOSS: 0.0366, MAE: 0.0366, RMSE: 0.0531, R2: 0.9610), PNorm: 144.0063, GNorm: 0.8369
[263/299] timecost: 63.78, lr: 0.000018, Train: (LOSS: 0.0090, MAE: 0.0090, RMSE: 0.0134, R2: 0.9970), Valid: (LOSS: 0.0362, MAE: 0.0362, RMSE: 0.0518, R2: 0.9628), PNorm: 143.9448, GNorm: 1.4694
[264/299] timecost: 64.09, lr: 0.000018, Train: (LOSS: 0.0081, MAE: 0.0081, RMSE: 0.0126, R2: 0.9973), Valid: (LOSS: 0.0375, MAE: 0.0375, RMSE: 0.0544, R2: 0.9595), PNorm: 143.8831, GNorm: 0.6841
[265/299] timecost: 63.76, lr: 0.000018, Train: (LOSS: 0.0084, MAE: 0.0084, RMSE: 0.0128, R2: 0.9971), Valid: (LOSS: 0.0351, MAE: 0.0351, RMSE: 0.0497, R2: 0.9657), PNorm: 143.8216, GNorm: 0.6101
[266/299] timecost: 63.59, lr: 0.000018, Train: (LOSS: 0.0084, MAE: 0.0084, RMSE: 0.0128, R2: 0.9971), Valid: (LOSS: 0.0353, MAE: 0.0353, RMSE: 0.0510, R2: 0.9638), PNorm: 143.7594, GNorm: 1.4388
[267/299] timecost: 63.46, lr: 0.000018, Train: (LOSS: 0.0086, MAE: 0.0086, RMSE: 0.0129, R2: 0.9972), Valid: (LOSS: 0.0360, MAE: 0.0360, RMSE: 0.0513, R2: 0.9638), PNorm: 143.6979, GNorm: 0.5452
[268/299] timecost: 63.68, lr: 0.000018, Train: (LOSS: 0.0083, MAE: 0.0083, RMSE: 0.0126, R2: 0.9974), Valid: (LOSS: 0.0356, MAE: 0.0356, RMSE: 0.0500, R2: 0.9652), PNorm: 143.6359, GNorm: 1.1907
[269/299] timecost: 63.78, lr: 0.000018, Train: (LOSS: 0.0084, MAE: 0.0084, RMSE: 0.0129, R2: 0.9971), Valid: (LOSS: 0.0353, MAE: 0.0353, RMSE: 0.0508, R2: 0.9645), PNorm: 143.5743, GNorm: 0.7721
[270/299] timecost: 64.03, lr: 0.000018, Train: (LOSS: 0.0087, MAE: 0.0087, RMSE: 0.0129, R2: 0.9972), Valid: (LOSS: 0.0354, MAE: 0.0354, RMSE: 0.0505, R2: 0.9644), PNorm: 143.5130, GNorm: 0.7595
[271/299] timecost: 64.06, lr: 0.000018, Train: (LOSS: 0.0085, MAE: 0.0085, RMSE: 0.0127, R2: 0.9972), Valid: (LOSS: 0.0360, MAE: 0.0360, RMSE: 0.0511, R2: 0.9639), PNorm: 143.4509, GNorm: 1.3225
[272/299] timecost: 63.83, lr: 0.000018, Train: (LOSS: 0.0086, MAE: 0.0086, RMSE: 0.0131, R2: 0.9971), Valid: (LOSS: 0.0357, MAE: 0.0357, RMSE: 0.0505, R2: 0.9646), PNorm: 143.3898, GNorm: 0.7116
[273/299] timecost: 64.08, lr: 0.000018, Train: (LOSS: 0.0080, MAE: 0.0080, RMSE: 0.0122, R2: 0.9974), Valid: (LOSS: 0.0355, MAE: 0.0355, RMSE: 0.0504, R2: 0.9643), PNorm: 143.3292, GNorm: 0.7592
[274/299] timecost: 63.53, lr: 0.000018, Train: (LOSS: 0.0083, MAE: 0.0083, RMSE: 0.0123, R2: 0.9973), Valid: (LOSS: 0.0364, MAE: 0.0364, RMSE: 0.0510, R2: 0.9636), PNorm: 143.2676, GNorm: 1.2664
[275/299] timecost: 64.26, lr: 0.000018, Train: (LOSS: 0.0083, MAE: 0.0083, RMSE: 0.0125, R2: 0.9972), Valid: (LOSS: 0.0356, MAE: 0.0356, RMSE: 0.0507, R2: 0.9640), PNorm: 143.2064, GNorm: 0.8063
[276/299] timecost: 64.36, lr: 0.000018, Train: (LOSS: 0.0080, MAE: 0.0080, RMSE: 0.0121, R2: 0.9974), Valid: (LOSS: 0.0351, MAE: 0.0351, RMSE: 0.0507, R2: 0.9641), PNorm: 143.1450, GNorm: 0.6432
[277/299] timecost: 64.36, lr: 0.000018, Train: (LOSS: 0.0081, MAE: 0.0081, RMSE: 0.0123, R2: 0.9974), Valid: (LOSS: 0.0353, MAE: 0.0353, RMSE: 0.0502, R2: 0.9649), PNorm: 143.0833, GNorm: 0.6488
[278/299] timecost: 63.76, lr: 0.000018, Train: (LOSS: 0.0083, MAE: 0.0083, RMSE: 0.0127, R2: 0.9971), Valid: (LOSS: 0.0358, MAE: 0.0358, RMSE: 0.0504, R2: 0.9649), PNorm: 143.0215, GNorm: 0.9048
[279/299] timecost: 63.43, lr: 0.000018, Train: (LOSS: 0.0083, MAE: 0.0083, RMSE: 0.0126, R2: 0.9973), Valid: (LOSS: 0.0348, MAE: 0.0348, RMSE: 0.0486, R2: 0.9670), PNorm: 142.9605, GNorm: 0.7105
Epoch 00281: reducing learning rate of group 0 to 1.5943e-05.
[280/299] timecost: 63.72, lr: 0.000016, Train: (LOSS: 0.0085, MAE: 0.0085, RMSE: 0.0127, R2: 0.9972), Valid: (LOSS: 0.0354, MAE: 0.0354, RMSE: 0.0504, R2: 0.9645), PNorm: 142.8996, GNorm: 0.8904
[281/299] timecost: 63.48, lr: 0.000016, Train: (LOSS: 0.0077, MAE: 0.0077, RMSE: 0.0117, R2: 0.9977), Valid: (LOSS: 0.0353, MAE: 0.0353, RMSE: 0.0504, R2: 0.9649), PNorm: 142.8443, GNorm: 0.7551
[282/299] timecost: 64.48, lr: 0.000016, Train: (LOSS: 0.0075, MAE: 0.0075, RMSE: 0.0117, R2: 0.9977), Valid: (LOSS: 0.0360, MAE: 0.0360, RMSE: 0.0520, R2: 0.9623), PNorm: 142.7890, GNorm: 1.3510
[283/299] timecost: 64.36, lr: 0.000016, Train: (LOSS: 0.0077, MAE: 0.0077, RMSE: 0.0116, R2: 0.9977), Valid: (LOSS: 0.0355, MAE: 0.0355, RMSE: 0.0514, R2: 0.9632), PNorm: 142.7337, GNorm: 0.8153
[284/299] timecost: 64.43, lr: 0.000016, Train: (LOSS: 0.0072, MAE: 0.0072, RMSE: 0.0111, R2: 0.9978), Valid: (LOSS: 0.0360, MAE: 0.0360, RMSE: 0.0512, R2: 0.9638), PNorm: 142.6782, GNorm: 1.0965
[285/299] timecost: 64.48, lr: 0.000016, Train: (LOSS: 0.0079, MAE: 0.0079, RMSE: 0.0119, R2: 0.9975), Valid: (LOSS: 0.0350, MAE: 0.0350, RMSE: 0.0506, R2: 0.9644), PNorm: 142.6233, GNorm: 1.3130
[286/299] timecost: 64.87, lr: 0.000016, Train: (LOSS: 0.0077, MAE: 0.0077, RMSE: 0.0116, R2: 0.9975), Valid: (LOSS: 0.0353, MAE: 0.0353, RMSE: 0.0508, R2: 0.9636), PNorm: 142.5678, GNorm: 1.1128
[287/299] timecost: 64.87, lr: 0.000016, Train: (LOSS: 0.0075, MAE: 0.0075, RMSE: 0.0115, R2: 0.9978), Valid: (LOSS: 0.0349, MAE: 0.0349, RMSE: 0.0500, R2: 0.9652), PNorm: 142.5124, GNorm: 0.6494
[288/299] timecost: 64.26, lr: 0.000016, Train: (LOSS: 0.0071, MAE: 0.0071, RMSE: 0.0111, R2: 0.9979), Valid: (LOSS: 0.0352, MAE: 0.0352, RMSE: 0.0511, R2: 0.9635), PNorm: 142.4572, GNorm: 0.6382
[289/299] timecost: 64.16, lr: 0.000016, Train: (LOSS: 0.0075, MAE: 0.0075, RMSE: 0.0116, R2: 0.9977), Valid: (LOSS: 0.0353, MAE: 0.0353, RMSE: 0.0498, R2: 0.9655), PNorm: 142.4027, GNorm: 1.0513
[290/299] timecost: 64.46, lr: 0.000016, Train: (LOSS: 0.0072, MAE: 0.0072, RMSE: 0.0110, R2: 0.9979), Valid: (LOSS: 0.0363, MAE: 0.0363, RMSE: 0.0514, R2: 0.9632), PNorm: 142.3476, GNorm: 0.8119
[291/299] timecost: 64.04, lr: 0.000016, Train: (LOSS: 0.0075, MAE: 0.0075, RMSE: 0.0113, R2: 0.9977), Valid: (LOSS: 0.0344, MAE: 0.0344, RMSE: 0.0487, R2: 0.9671), PNorm: 142.2924, GNorm: 0.9382
[292/299] timecost: 64.18, lr: 0.000016, Train: (LOSS: 0.0069, MAE: 0.0069, RMSE: 0.0106, R2: 0.9981), Valid: (LOSS: 0.0353, MAE: 0.0353, RMSE: 0.0510, R2: 0.9637), PNorm: 142.2374, GNorm: 1.6866
[293/299] timecost: 64.18, lr: 0.000016, Train: (LOSS: 0.0071, MAE: 0.0071, RMSE: 0.0109, R2: 0.9979), Valid: (LOSS: 0.0354, MAE: 0.0354, RMSE: 0.0508, R2: 0.9643), PNorm: 142.1820, GNorm: 0.7630
[294/299] timecost: 64.07, lr: 0.000016, Train: (LOSS: 0.0071, MAE: 0.0071, RMSE: 0.0107, R2: 0.9980), Valid: (LOSS: 0.0357, MAE: 0.0357, RMSE: 0.0515, R2: 0.9631), PNorm: 142.1261, GNorm: 1.0954
[295/299] timecost: 63.90, lr: 0.000016, Train: (LOSS: 0.0070, MAE: 0.0070, RMSE: 0.0109, R2: 0.9979), Valid: (LOSS: 0.0346, MAE: 0.0346, RMSE: 0.0490, R2: 0.9668), PNorm: 142.0713, GNorm: 0.7437
[296/299] timecost: 63.50, lr: 0.000016, Train: (LOSS: 0.0070, MAE: 0.0070, RMSE: 0.0108, R2: 0.9978), Valid: (LOSS: 0.0354, MAE: 0.0354, RMSE: 0.0507, R2: 0.9643), PNorm: 142.0162, GNorm: 0.8747
[297/299] timecost: 63.62, lr: 0.000016, Train: (LOSS: 0.0069, MAE: 0.0069, RMSE: 0.0106, R2: 0.9978), Valid: (LOSS: 0.0348, MAE: 0.0348, RMSE: 0.0498, R2: 0.9655), PNorm: 141.9607, GNorm: 0.8983
[298/299] timecost: 64.40, lr: 0.000016, Train: (LOSS: 0.0070, MAE: 0.0070, RMSE: 0.0105, R2: 0.9981), Valid: (LOSS: 0.0352, MAE: 0.0352, RMSE: 0.0508, R2: 0.9644), PNorm: 141.9060, GNorm: 0.7892
[299/299] timecost: 64.24, lr: 0.000016, Train: (LOSS: 0.0076, MAE: 0.0076, RMSE: 0.0115, R2: 0.9978), Valid: (LOSS: 0.0352, MAE: 0.0352, RMSE: 0.0504, R2: 0.9644), PNorm: 141.8513, GNorm: 0.6154
==========Training End==========
==========Test Best Model==========
================Final Results=======================
mse: 0.0342 +- 0.0000:
rmse: 0.0525 +- 0.0000:
mae: 0.0342 +- 0.0000:
r2: 0.9555 +- 0.0000:
tensor([[0.0000, 0.0000],
        [0.0000, 0.0000],
        [0.0000, 0.0000],
        ...,
        [0.0000, 0.0000],
        [0.5015, 0.4354],
        [0.7284, 0.6980]], device='cuda:0')
