cuda available with GPU: Tesla V100-PCIE-16GB
==========Load Seed==========
set_random_seed
0
==========Training Start==========
Training Graphs:  2491
Valid Graphs:  277
Test Graphs:  1187
============Not pretrained weights used============
============Creating new layers============
============Creating Model============
Training Graphs Batches:  78
Valid Graphs Batches:  9
Test Graphs Batches:  37
[0/299] timecost: 64.06, lr: 0.000100, Train: (LOSS: 0.2109, MAE: 0.2109, RMSE: 0.2609, R2: 0.0188), Valid: (LOSS: 0.1814, MAE: 0.1814, RMSE: 0.2297, R2: 0.2661), PNorm: 187.0380, GNorm: 0.3614
[1/299] timecost: 62.05, lr: 0.000100, Train: (LOSS: 0.1679, MAE: 0.1679, RMSE: 0.2185, R2: 0.3251), Valid: (LOSS: 0.1476, MAE: 0.1476, RMSE: 0.1920, R2: 0.4804), PNorm: 187.0756, GNorm: 0.5000
[2/299] timecost: 62.05, lr: 0.000100, Train: (LOSS: 0.1460, MAE: 0.1460, RMSE: 0.1943, R2: 0.4485), Valid: (LOSS: 0.1322, MAE: 0.1322, RMSE: 0.1831, R2: 0.5180), PNorm: 187.1052, GNorm: 0.5000
[3/299] timecost: 61.81, lr: 0.000100, Train: (LOSS: 0.1389, MAE: 0.1389, RMSE: 0.1890, R2: 0.4735), Valid: (LOSS: 0.1534, MAE: 0.1534, RMSE: 0.1983, R2: 0.4557), PNorm: 187.1259, GNorm: 0.5000
[4/299] timecost: 63.24, lr: 0.000100, Train: (LOSS: 0.1393, MAE: 0.1393, RMSE: 0.1890, R2: 0.4653), Valid: (LOSS: 0.1220, MAE: 0.1220, RMSE: 0.1625, R2: 0.6287), PNorm: 187.1450, GNorm: 0.5000
[5/299] timecost: 62.24, lr: 0.000100, Train: (LOSS: 0.1339, MAE: 0.1339, RMSE: 0.1825, R2: 0.5177), Valid: (LOSS: 0.1248, MAE: 0.1248, RMSE: 0.1701, R2: 0.5942), PNorm: 187.1602, GNorm: 0.4124
[6/299] timecost: 61.82, lr: 0.000100, Train: (LOSS: 0.1269, MAE: 0.1269, RMSE: 0.1759, R2: 0.5379), Valid: (LOSS: 0.1184, MAE: 0.1184, RMSE: 0.1648, R2: 0.6177), PNorm: 187.1828, GNorm: 0.5000
[7/299] timecost: 61.97, lr: 0.000100, Train: (LOSS: 0.1250, MAE: 0.1250, RMSE: 0.1712, R2: 0.5650), Valid: (LOSS: 0.1146, MAE: 0.1146, RMSE: 0.1636, R2: 0.6236), PNorm: 187.2098, GNorm: 0.5000
[8/299] timecost: 59.31, lr: 0.000100, Train: (LOSS: 0.1200, MAE: 0.1200, RMSE: 0.1676, R2: 0.5848), Valid: (LOSS: 0.1332, MAE: 0.1332, RMSE: 0.1889, R2: 0.5045), PNorm: 187.2468, GNorm: 0.5000
[9/299] timecost: 58.71, lr: 0.000100, Train: (LOSS: 0.1188, MAE: 0.1188, RMSE: 0.1662, R2: 0.5934), Valid: (LOSS: 0.1082, MAE: 0.1082, RMSE: 0.1530, R2: 0.6756), PNorm: 187.2733, GNorm: 0.5000
[10/299] timecost: 58.78, lr: 0.000100, Train: (LOSS: 0.1119, MAE: 0.1119, RMSE: 0.1587, R2: 0.6309), Valid: (LOSS: 0.0988, MAE: 0.0988, RMSE: 0.1358, R2: 0.7425), PNorm: 187.3065, GNorm: 0.5000
[11/299] timecost: 59.53, lr: 0.000100, Train: (LOSS: 0.0985, MAE: 0.0985, RMSE: 0.1429, R2: 0.6954), Valid: (LOSS: 0.0941, MAE: 0.0941, RMSE: 0.1305, R2: 0.7595), PNorm: 187.3335, GNorm: 0.5000
[12/299] timecost: 63.63, lr: 0.000100, Train: (LOSS: 0.0927, MAE: 0.0927, RMSE: 0.1353, R2: 0.7193), Valid: (LOSS: 0.0881, MAE: 0.0881, RMSE: 0.1367, R2: 0.7323), PNorm: 187.3619, GNorm: 0.5000
[13/299] timecost: 63.69, lr: 0.000100, Train: (LOSS: 0.0894, MAE: 0.0894, RMSE: 0.1311, R2: 0.7369), Valid: (LOSS: 0.0812, MAE: 0.0812, RMSE: 0.1182, R2: 0.7946), PNorm: 187.3878, GNorm: 0.5000
[14/299] timecost: 64.05, lr: 0.000100, Train: (LOSS: 0.0837, MAE: 0.0837, RMSE: 0.1228, R2: 0.7662), Valid: (LOSS: 0.0803, MAE: 0.0803, RMSE: 0.1153, R2: 0.8118), PNorm: 187.4097, GNorm: 0.5000
[15/299] timecost: 64.04, lr: 0.000100, Train: (LOSS: 0.0816, MAE: 0.0816, RMSE: 0.1208, R2: 0.7799), Valid: (LOSS: 0.0847, MAE: 0.0847, RMSE: 0.1262, R2: 0.7686), PNorm: 187.4324, GNorm: 0.5000
[16/299] timecost: 64.11, lr: 0.000100, Train: (LOSS: 0.0804, MAE: 0.0804, RMSE: 0.1208, R2: 0.7790), Valid: (LOSS: 0.0753, MAE: 0.0753, RMSE: 0.1067, R2: 0.8323), PNorm: 187.4545, GNorm: 0.4440
[17/299] timecost: 64.47, lr: 0.000100, Train: (LOSS: 0.0734, MAE: 0.0734, RMSE: 0.1130, R2: 0.8037), Valid: (LOSS: 0.0739, MAE: 0.0739, RMSE: 0.1017, R2: 0.8521), PNorm: 187.4747, GNorm: 0.5000
[18/299] timecost: 63.61, lr: 0.000100, Train: (LOSS: 0.0675, MAE: 0.0675, RMSE: 0.1015, R2: 0.8419), Valid: (LOSS: 0.0695, MAE: 0.0695, RMSE: 0.1004, R2: 0.8544), PNorm: 187.4926, GNorm: 0.4562
[19/299] timecost: 63.91, lr: 0.000100, Train: (LOSS: 0.0625, MAE: 0.0625, RMSE: 0.0942, R2: 0.8562), Valid: (LOSS: 0.0641, MAE: 0.0641, RMSE: 0.0919, R2: 0.8788), PNorm: 187.5089, GNorm: 0.5000
[20/299] timecost: 63.84, lr: 0.000100, Train: (LOSS: 0.0606, MAE: 0.0606, RMSE: 0.0923, R2: 0.8679), Valid: (LOSS: 0.0572, MAE: 0.0572, RMSE: 0.0855, R2: 0.8918), PNorm: 187.5291, GNorm: 0.5000
[21/299] timecost: 64.23, lr: 0.000100, Train: (LOSS: 0.0566, MAE: 0.0566, RMSE: 0.0871, R2: 0.8764), Valid: (LOSS: 0.0582, MAE: 0.0582, RMSE: 0.0846, R2: 0.8955), PNorm: 187.5449, GNorm: 0.5000
[22/299] timecost: 64.05, lr: 0.000100, Train: (LOSS: 0.0551, MAE: 0.0551, RMSE: 0.0845, R2: 0.8889), Valid: (LOSS: 0.0526, MAE: 0.0526, RMSE: 0.0753, R2: 0.9157), PNorm: 187.5636, GNorm: 0.4340
[23/299] timecost: 63.46, lr: 0.000100, Train: (LOSS: 0.0553, MAE: 0.0553, RMSE: 0.0855, R2: 0.8821), Valid: (LOSS: 0.0533, MAE: 0.0533, RMSE: 0.0821, R2: 0.9018), PNorm: 187.5784, GNorm: 0.5000
[24/299] timecost: 61.09, lr: 0.000100, Train: (LOSS: 0.0517, MAE: 0.0517, RMSE: 0.0813, R2: 0.8944), Valid: (LOSS: 0.0554, MAE: 0.0554, RMSE: 0.0868, R2: 0.8920), PNorm: 187.5956, GNorm: 0.5000
[25/299] timecost: 63.14, lr: 0.000100, Train: (LOSS: 0.0502, MAE: 0.0502, RMSE: 0.0793, R2: 0.9000), Valid: (LOSS: 0.0506, MAE: 0.0506, RMSE: 0.0770, R2: 0.9133), PNorm: 187.6104, GNorm: 0.5000
[26/299] timecost: 60.93, lr: 0.000100, Train: (LOSS: 0.0495, MAE: 0.0495, RMSE: 0.0799, R2: 0.8976), Valid: (LOSS: 0.0551, MAE: 0.0551, RMSE: 0.0811, R2: 0.9065), PNorm: 187.6275, GNorm: 0.5000
[27/299] timecost: 63.15, lr: 0.000100, Train: (LOSS: 0.0493, MAE: 0.0493, RMSE: 0.0784, R2: 0.8987), Valid: (LOSS: 0.0502, MAE: 0.0502, RMSE: 0.0749, R2: 0.9178), PNorm: 187.6473, GNorm: 0.5000
[28/299] timecost: 63.57, lr: 0.000100, Train: (LOSS: 0.0453, MAE: 0.0453, RMSE: 0.0734, R2: 0.9131), Valid: (LOSS: 0.0483, MAE: 0.0483, RMSE: 0.0767, R2: 0.9146), PNorm: 187.6623, GNorm: 0.4170
[29/299] timecost: 63.95, lr: 0.000100, Train: (LOSS: 0.0442, MAE: 0.0442, RMSE: 0.0710, R2: 0.9175), Valid: (LOSS: 0.0460, MAE: 0.0460, RMSE: 0.0672, R2: 0.9319), PNorm: 187.6740, GNorm: 0.5000
[30/299] timecost: 64.06, lr: 0.000100, Train: (LOSS: 0.0447, MAE: 0.0447, RMSE: 0.0721, R2: 0.9096), Valid: (LOSS: 0.0478, MAE: 0.0478, RMSE: 0.0714, R2: 0.9254), PNorm: 187.6922, GNorm: 0.4280
[31/299] timecost: 64.02, lr: 0.000100, Train: (LOSS: 0.0440, MAE: 0.0440, RMSE: 0.0711, R2: 0.9175), Valid: (LOSS: 0.0536, MAE: 0.0536, RMSE: 0.0828, R2: 0.9036), PNorm: 187.7089, GNorm: 0.5000
[32/299] timecost: 61.23, lr: 0.000100, Train: (LOSS: 0.0438, MAE: 0.0438, RMSE: 0.0701, R2: 0.9161), Valid: (LOSS: 0.0486, MAE: 0.0486, RMSE: 0.0734, R2: 0.9209), PNorm: 187.7269, GNorm: 0.5000
[33/299] timecost: 60.93, lr: 0.000100, Train: (LOSS: 0.0407, MAE: 0.0407, RMSE: 0.0658, R2: 0.9271), Valid: (LOSS: 0.0494, MAE: 0.0494, RMSE: 0.0783, R2: 0.9149), PNorm: 187.7412, GNorm: 0.5000
[34/299] timecost: 60.57, lr: 0.000100, Train: (LOSS: 0.0401, MAE: 0.0401, RMSE: 0.0647, R2: 0.9307), Valid: (LOSS: 0.0457, MAE: 0.0457, RMSE: 0.0674, R2: 0.9333), PNorm: 187.7540, GNorm: 0.5000
[35/299] timecost: 61.03, lr: 0.000100, Train: (LOSS: 0.0391, MAE: 0.0391, RMSE: 0.0626, R2: 0.9340), Valid: (LOSS: 0.0546, MAE: 0.0546, RMSE: 0.0815, R2: 0.9049), PNorm: 187.7654, GNorm: 0.5000
[36/299] timecost: 63.23, lr: 0.000100, Train: (LOSS: 0.0407, MAE: 0.0407, RMSE: 0.0642, R2: 0.9318), Valid: (LOSS: 0.0464, MAE: 0.0464, RMSE: 0.0715, R2: 0.9276), PNorm: 187.7824, GNorm: 0.5000
[37/299] timecost: 63.92, lr: 0.000100, Train: (LOSS: 0.0386, MAE: 0.0386, RMSE: 0.0631, R2: 0.9340), Valid: (LOSS: 0.0466, MAE: 0.0466, RMSE: 0.0649, R2: 0.9386), PNorm: 187.7997, GNorm: 0.4517
[38/299] timecost: 63.81, lr: 0.000100, Train: (LOSS: 0.0399, MAE: 0.0399, RMSE: 0.0638, R2: 0.9342), Valid: (LOSS: 0.0460, MAE: 0.0460, RMSE: 0.0674, R2: 0.9338), PNorm: 187.8141, GNorm: 0.5000
[39/299] timecost: 64.04, lr: 0.000100, Train: (LOSS: 0.0368, MAE: 0.0368, RMSE: 0.0600, R2: 0.9400), Valid: (LOSS: 0.0431, MAE: 0.0431, RMSE: 0.0652, R2: 0.9383), PNorm: 187.8289, GNorm: 0.2938
[40/299] timecost: 63.62, lr: 0.000100, Train: (LOSS: 0.0358, MAE: 0.0358, RMSE: 0.0590, R2: 0.9438), Valid: (LOSS: 0.0436, MAE: 0.0436, RMSE: 0.0655, R2: 0.9373), PNorm: 187.8390, GNorm: 0.5000
[41/299] timecost: 64.15, lr: 0.000100, Train: (LOSS: 0.0360, MAE: 0.0360, RMSE: 0.0582, R2: 0.9440), Valid: (LOSS: 0.0440, MAE: 0.0440, RMSE: 0.0658, R2: 0.9376), PNorm: 187.8510, GNorm: 0.5000
[42/299] timecost: 63.78, lr: 0.000100, Train: (LOSS: 0.0348, MAE: 0.0348, RMSE: 0.0565, R2: 0.9443), Valid: (LOSS: 0.0427, MAE: 0.0427, RMSE: 0.0655, R2: 0.9384), PNorm: 187.8655, GNorm: 0.4218
[43/299] timecost: 63.52, lr: 0.000100, Train: (LOSS: 0.0340, MAE: 0.0340, RMSE: 0.0560, R2: 0.9490), Valid: (LOSS: 0.0473, MAE: 0.0473, RMSE: 0.0721, R2: 0.9274), PNorm: 187.8756, GNorm: 0.5000
[44/299] timecost: 63.84, lr: 0.000100, Train: (LOSS: 0.0338, MAE: 0.0338, RMSE: 0.0555, R2: 0.9477), Valid: (LOSS: 0.0484, MAE: 0.0484, RMSE: 0.0732, R2: 0.9250), PNorm: 187.8887, GNorm: 0.4253
[45/299] timecost: 63.96, lr: 0.000100, Train: (LOSS: 0.0339, MAE: 0.0339, RMSE: 0.0542, R2: 0.9458), Valid: (LOSS: 0.0451, MAE: 0.0451, RMSE: 0.0699, R2: 0.9303), PNorm: 187.8977, GNorm: 0.5000
[46/299] timecost: 63.77, lr: 0.000100, Train: (LOSS: 0.0310, MAE: 0.0310, RMSE: 0.0514, R2: 0.9554), Valid: (LOSS: 0.0463, MAE: 0.0463, RMSE: 0.0713, R2: 0.9249), PNorm: 187.9096, GNorm: 0.5000
[47/299] timecost: 63.52, lr: 0.000100, Train: (LOSS: 0.0315, MAE: 0.0315, RMSE: 0.0511, R2: 0.9564), Valid: (LOSS: 0.0435, MAE: 0.0435, RMSE: 0.0674, R2: 0.9342), PNorm: 187.9230, GNorm: 0.4226
[48/299] timecost: 63.58, lr: 0.000100, Train: (LOSS: 0.0315, MAE: 0.0315, RMSE: 0.0505, R2: 0.9539), Valid: (LOSS: 0.0435, MAE: 0.0435, RMSE: 0.0668, R2: 0.9357), PNorm: 187.9343, GNorm: 0.5000
[49/299] timecost: 63.66, lr: 0.000100, Train: (LOSS: 0.0307, MAE: 0.0307, RMSE: 0.0507, R2: 0.9555), Valid: (LOSS: 0.0461, MAE: 0.0461, RMSE: 0.0682, R2: 0.9332), PNorm: 187.9447, GNorm: 0.5000
[50/299] timecost: 61.73, lr: 0.000100, Train: (LOSS: 0.0315, MAE: 0.0315, RMSE: 0.0518, R2: 0.9531), Valid: (LOSS: 0.0437, MAE: 0.0437, RMSE: 0.0673, R2: 0.9349), PNorm: 187.9606, GNorm: 0.4942
[51/299] timecost: 58.65, lr: 0.000100, Train: (LOSS: 0.0310, MAE: 0.0310, RMSE: 0.0503, R2: 0.9565), Valid: (LOSS: 0.0426, MAE: 0.0426, RMSE: 0.0667, R2: 0.9351), PNorm: 187.9744, GNorm: 0.4039
[52/299] timecost: 59.39, lr: 0.000100, Train: (LOSS: 0.0297, MAE: 0.0297, RMSE: 0.0487, R2: 0.9608), Valid: (LOSS: 0.0413, MAE: 0.0413, RMSE: 0.0610, R2: 0.9447), PNorm: 187.9857, GNorm: 0.5000
[53/299] timecost: 59.66, lr: 0.000100, Train: (LOSS: 0.0292, MAE: 0.0292, RMSE: 0.0476, R2: 0.9618), Valid: (LOSS: 0.0416, MAE: 0.0416, RMSE: 0.0631, R2: 0.9424), PNorm: 187.9973, GNorm: 0.5000
[54/299] timecost: 59.45, lr: 0.000100, Train: (LOSS: 0.0287, MAE: 0.0287, RMSE: 0.0467, R2: 0.9614), Valid: (LOSS: 0.0416, MAE: 0.0416, RMSE: 0.0611, R2: 0.9447), PNorm: 188.0098, GNorm: 0.5000
[55/299] timecost: 60.05, lr: 0.000100, Train: (LOSS: 0.0289, MAE: 0.0289, RMSE: 0.0477, R2: 0.9591), Valid: (LOSS: 0.0422, MAE: 0.0422, RMSE: 0.0627, R2: 0.9423), PNorm: 188.0225, GNorm: 0.5000
[56/299] timecost: 59.50, lr: 0.000100, Train: (LOSS: 0.0295, MAE: 0.0295, RMSE: 0.0480, R2: 0.9593), Valid: (LOSS: 0.0441, MAE: 0.0441, RMSE: 0.0631, R2: 0.9427), PNorm: 188.0347, GNorm: 0.5000
[57/299] timecost: 59.92, lr: 0.000100, Train: (LOSS: 0.0287, MAE: 0.0287, RMSE: 0.0470, R2: 0.9632), Valid: (LOSS: 0.0439, MAE: 0.0439, RMSE: 0.0640, R2: 0.9391), PNorm: 188.0469, GNorm: 0.5000
[58/299] timecost: 59.17, lr: 0.000100, Train: (LOSS: 0.0280, MAE: 0.0280, RMSE: 0.0450, R2: 0.9656), Valid: (LOSS: 0.0402, MAE: 0.0402, RMSE: 0.0626, R2: 0.9427), PNorm: 188.0585, GNorm: 0.4224
[59/299] timecost: 59.71, lr: 0.000100, Train: (LOSS: 0.0281, MAE: 0.0281, RMSE: 0.0448, R2: 0.9641), Valid: (LOSS: 0.0425, MAE: 0.0425, RMSE: 0.0625, R2: 0.9440), PNorm: 188.0729, GNorm: 0.4325
[60/299] timecost: 60.00, lr: 0.000100, Train: (LOSS: 0.0266, MAE: 0.0266, RMSE: 0.0438, R2: 0.9670), Valid: (LOSS: 0.0402, MAE: 0.0402, RMSE: 0.0597, R2: 0.9483), PNorm: 188.0838, GNorm: 0.4518
[61/299] timecost: 61.03, lr: 0.000100, Train: (LOSS: 0.0270, MAE: 0.0270, RMSE: 0.0437, R2: 0.9667), Valid: (LOSS: 0.0392, MAE: 0.0392, RMSE: 0.0584, R2: 0.9475), PNorm: 188.0985, GNorm: 0.4065
[62/299] timecost: 60.95, lr: 0.000100, Train: (LOSS: 0.0269, MAE: 0.0269, RMSE: 0.0434, R2: 0.9690), Valid: (LOSS: 0.0416, MAE: 0.0416, RMSE: 0.0632, R2: 0.9423), PNorm: 188.1107, GNorm: 0.5000
[63/299] timecost: 61.07, lr: 0.000100, Train: (LOSS: 0.0252, MAE: 0.0252, RMSE: 0.0402, R2: 0.9725), Valid: (LOSS: 0.0424, MAE: 0.0424, RMSE: 0.0622, R2: 0.9445), PNorm: 188.1236, GNorm: 0.5000
[64/299] timecost: 61.06, lr: 0.000100, Train: (LOSS: 0.0262, MAE: 0.0262, RMSE: 0.0424, R2: 0.9688), Valid: (LOSS: 0.0421, MAE: 0.0421, RMSE: 0.0598, R2: 0.9486), PNorm: 188.1344, GNorm: 0.4815
[65/299] timecost: 61.23, lr: 0.000100, Train: (LOSS: 0.0247, MAE: 0.0247, RMSE: 0.0394, R2: 0.9734), Valid: (LOSS: 0.0407, MAE: 0.0407, RMSE: 0.0573, R2: 0.9524), PNorm: 188.1476, GNorm: 0.5000
[66/299] timecost: 61.09, lr: 0.000100, Train: (LOSS: 0.0251, MAE: 0.0251, RMSE: 0.0404, R2: 0.9728), Valid: (LOSS: 0.0393, MAE: 0.0393, RMSE: 0.0585, R2: 0.9493), PNorm: 188.1625, GNorm: 0.5000
[67/299] timecost: 60.97, lr: 0.000100, Train: (LOSS: 0.0243, MAE: 0.0243, RMSE: 0.0397, R2: 0.9725), Valid: (LOSS: 0.0391, MAE: 0.0391, RMSE: 0.0578, R2: 0.9522), PNorm: 188.1741, GNorm: 0.5000
[68/299] timecost: 61.31, lr: 0.000100, Train: (LOSS: 0.0250, MAE: 0.0250, RMSE: 0.0399, R2: 0.9692), Valid: (LOSS: 0.0413, MAE: 0.0413, RMSE: 0.0612, R2: 0.9465), PNorm: 188.1864, GNorm: 0.5000
[69/299] timecost: 61.81, lr: 0.000100, Train: (LOSS: 0.0241, MAE: 0.0241, RMSE: 0.0374, R2: 0.9758), Valid: (LOSS: 0.0402, MAE: 0.0402, RMSE: 0.0597, R2: 0.9499), PNorm: 188.1993, GNorm: 0.5000
[70/299] timecost: 62.11, lr: 0.000100, Train: (LOSS: 0.0243, MAE: 0.0243, RMSE: 0.0379, R2: 0.9751), Valid: (LOSS: 0.0395, MAE: 0.0395, RMSE: 0.0570, R2: 0.9521), PNorm: 188.2113, GNorm: 0.5000
[71/299] timecost: 61.75, lr: 0.000100, Train: (LOSS: 0.0229, MAE: 0.0229, RMSE: 0.0363, R2: 0.9772), Valid: (LOSS: 0.0378, MAE: 0.0378, RMSE: 0.0563, R2: 0.9542), PNorm: 188.2205, GNorm: 0.4914
[72/299] timecost: 61.81, lr: 0.000100, Train: (LOSS: 0.0236, MAE: 0.0236, RMSE: 0.0373, R2: 0.9752), Valid: (LOSS: 0.0397, MAE: 0.0397, RMSE: 0.0573, R2: 0.9525), PNorm: 188.2328, GNorm: 0.4289
[73/299] timecost: 61.23, lr: 0.000100, Train: (LOSS: 0.0234, MAE: 0.0234, RMSE: 0.0369, R2: 0.9753), Valid: (LOSS: 0.0383, MAE: 0.0383, RMSE: 0.0580, R2: 0.9527), PNorm: 188.2432, GNorm: 0.5000
[74/299] timecost: 60.47, lr: 0.000100, Train: (LOSS: 0.0223, MAE: 0.0223, RMSE: 0.0356, R2: 0.9762), Valid: (LOSS: 0.0372, MAE: 0.0372, RMSE: 0.0556, R2: 0.9559), PNorm: 188.2536, GNorm: 0.4784
[75/299] timecost: 60.05, lr: 0.000100, Train: (LOSS: 0.0227, MAE: 0.0227, RMSE: 0.0362, R2: 0.9762), Valid: (LOSS: 0.0414, MAE: 0.0414, RMSE: 0.0625, R2: 0.9433), PNorm: 188.2647, GNorm: 0.5000
[76/299] timecost: 59.70, lr: 0.000100, Train: (LOSS: 0.0230, MAE: 0.0230, RMSE: 0.0370, R2: 0.9765), Valid: (LOSS: 0.0396, MAE: 0.0396, RMSE: 0.0563, R2: 0.9554), PNorm: 188.2791, GNorm: 0.4736
[77/299] timecost: 61.28, lr: 0.000100, Train: (LOSS: 0.0225, MAE: 0.0225, RMSE: 0.0355, R2: 0.9783), Valid: (LOSS: 0.0404, MAE: 0.0404, RMSE: 0.0611, R2: 0.9459), PNorm: 188.2911, GNorm: 0.5000
[78/299] timecost: 62.05, lr: 0.000100, Train: (LOSS: 0.0219, MAE: 0.0219, RMSE: 0.0357, R2: 0.9773), Valid: (LOSS: 0.0388, MAE: 0.0388, RMSE: 0.0567, R2: 0.9532), PNorm: 188.3040, GNorm: 0.4507
[79/299] timecost: 62.06, lr: 0.000100, Train: (LOSS: 0.0212, MAE: 0.0212, RMSE: 0.0338, R2: 0.9795), Valid: (LOSS: 0.0401, MAE: 0.0401, RMSE: 0.0585, R2: 0.9513), PNorm: 188.3144, GNorm: 0.4282
[80/299] timecost: 62.01, lr: 0.000100, Train: (LOSS: 0.0217, MAE: 0.0217, RMSE: 0.0344, R2: 0.9790), Valid: (LOSS: 0.0409, MAE: 0.0409, RMSE: 0.0581, R2: 0.9512), PNorm: 188.3272, GNorm: 0.4816
[81/299] timecost: 61.86, lr: 0.000100, Train: (LOSS: 0.0226, MAE: 0.0226, RMSE: 0.0358, R2: 0.9773), Valid: (LOSS: 0.0396, MAE: 0.0396, RMSE: 0.0588, R2: 0.9507), PNorm: 188.3391, GNorm: 0.4854
[82/299] timecost: 61.92, lr: 0.000100, Train: (LOSS: 0.0219, MAE: 0.0219, RMSE: 0.0341, R2: 0.9789), Valid: (LOSS: 0.0418, MAE: 0.0418, RMSE: 0.0591, R2: 0.9507), PNorm: 188.3537, GNorm: 0.5000
[83/299] timecost: 61.64, lr: 0.000100, Train: (LOSS: 0.0212, MAE: 0.0212, RMSE: 0.0334, R2: 0.9795), Valid: (LOSS: 0.0395, MAE: 0.0395, RMSE: 0.0598, R2: 0.9498), PNorm: 188.3703, GNorm: 0.5000
[84/299] timecost: 61.76, lr: 0.000100, Train: (LOSS: 0.0213, MAE: 0.0213, RMSE: 0.0339, R2: 0.9798), Valid: (LOSS: 0.0400, MAE: 0.0400, RMSE: 0.0612, R2: 0.9452), PNorm: 188.3850, GNorm: 0.4968
[85/299] timecost: 62.16, lr: 0.000100, Train: (LOSS: 0.0216, MAE: 0.0216, RMSE: 0.0340, R2: 0.9799), Valid: (LOSS: 0.0423, MAE: 0.0423, RMSE: 0.0642, R2: 0.9420), PNorm: 188.3994, GNorm: 0.5000
[86/299] timecost: 63.61, lr: 0.000100, Train: (LOSS: 0.0215, MAE: 0.0215, RMSE: 0.0334, R2: 0.9801), Valid: (LOSS: 0.0385, MAE: 0.0385, RMSE: 0.0570, R2: 0.9532), PNorm: 188.4124, GNorm: 0.4136
[87/299] timecost: 63.52, lr: 0.000100, Train: (LOSS: 0.0216, MAE: 0.0216, RMSE: 0.0343, R2: 0.9799), Valid: (LOSS: 0.0393, MAE: 0.0393, RMSE: 0.0582, R2: 0.9514), PNorm: 188.4262, GNorm: 0.4074
[88/299] timecost: 63.79, lr: 0.000100, Train: (LOSS: 0.0214, MAE: 0.0214, RMSE: 0.0331, R2: 0.9818), Valid: (LOSS: 0.0399, MAE: 0.0399, RMSE: 0.0608, R2: 0.9472), PNorm: 188.4405, GNorm: 0.4542
[89/299] timecost: 63.89, lr: 0.000100, Train: (LOSS: 0.0208, MAE: 0.0208, RMSE: 0.0325, R2: 0.9811), Valid: (LOSS: 0.0391, MAE: 0.0391, RMSE: 0.0579, R2: 0.9515), PNorm: 188.4511, GNorm: 0.5000
Epoch 00091: reducing learning rate of group 0 to 8.5000e-05.
[90/299] timecost: 63.92, lr: 0.000085, Train: (LOSS: 0.0211, MAE: 0.0211, RMSE: 0.0327, R2: 0.9821), Valid: (LOSS: 0.0370, MAE: 0.0370, RMSE: 0.0551, R2: 0.9554), PNorm: 188.4628, GNorm: 0.4673
[91/299] timecost: 63.29, lr: 0.000085, Train: (LOSS: 0.0186, MAE: 0.0186, RMSE: 0.0297, R2: 0.9845), Valid: (LOSS: 0.0374, MAE: 0.0374, RMSE: 0.0571, R2: 0.9532), PNorm: 188.4708, GNorm: 0.5000
[92/299] timecost: 63.61, lr: 0.000085, Train: (LOSS: 0.0178, MAE: 0.0178, RMSE: 0.0278, R2: 0.9859), Valid: (LOSS: 0.0366, MAE: 0.0366, RMSE: 0.0543, R2: 0.9570), PNorm: 188.4799, GNorm: 0.4832
[93/299] timecost: 63.62, lr: 0.000085, Train: (LOSS: 0.0173, MAE: 0.0173, RMSE: 0.0270, R2: 0.9865), Valid: (LOSS: 0.0384, MAE: 0.0384, RMSE: 0.0575, R2: 0.9513), PNorm: 188.4871, GNorm: 0.4942
[94/299] timecost: 63.72, lr: 0.000085, Train: (LOSS: 0.0178, MAE: 0.0178, RMSE: 0.0277, R2: 0.9850), Valid: (LOSS: 0.0381, MAE: 0.0381, RMSE: 0.0555, R2: 0.9544), PNorm: 188.4942, GNorm: 0.4622
[95/299] timecost: 63.63, lr: 0.000085, Train: (LOSS: 0.0177, MAE: 0.0177, RMSE: 0.0275, R2: 0.9861), Valid: (LOSS: 0.0377, MAE: 0.0377, RMSE: 0.0552, R2: 0.9558), PNorm: 188.5040, GNorm: 0.4080
[96/299] timecost: 63.81, lr: 0.000085, Train: (LOSS: 0.0172, MAE: 0.0172, RMSE: 0.0270, R2: 0.9874), Valid: (LOSS: 0.0391, MAE: 0.0391, RMSE: 0.0587, R2: 0.9499), PNorm: 188.5109, GNorm: 0.4075
[97/299] timecost: 62.60, lr: 0.000085, Train: (LOSS: 0.0166, MAE: 0.0166, RMSE: 0.0262, R2: 0.9873), Valid: (LOSS: 0.0380, MAE: 0.0380, RMSE: 0.0551, R2: 0.9551), PNorm: 188.5207, GNorm: 0.4085
[98/299] timecost: 60.16, lr: 0.000085, Train: (LOSS: 0.0174, MAE: 0.0174, RMSE: 0.0275, R2: 0.9865), Valid: (LOSS: 0.0358, MAE: 0.0358, RMSE: 0.0547, R2: 0.9562), PNorm: 188.5299, GNorm: 0.4586
[99/299] timecost: 58.83, lr: 0.000085, Train: (LOSS: 0.0163, MAE: 0.0163, RMSE: 0.0259, R2: 0.9877), Valid: (LOSS: 0.0366, MAE: 0.0366, RMSE: 0.0554, R2: 0.9546), PNorm: 188.5388, GNorm: 0.3890
[100/299] timecost: 59.13, lr: 0.000085, Train: (LOSS: 0.0165, MAE: 0.0165, RMSE: 0.0261, R2: 0.9871), Valid: (LOSS: 0.0379, MAE: 0.0379, RMSE: 0.0548, R2: 0.9560), PNorm: 188.5454, GNorm: 0.5000
[101/299] timecost: 58.93, lr: 0.000085, Train: (LOSS: 0.0157, MAE: 0.0157, RMSE: 0.0252, R2: 0.9885), Valid: (LOSS: 0.0383, MAE: 0.0383, RMSE: 0.0572, R2: 0.9529), PNorm: 188.5534, GNorm: 0.5000
[102/299] timecost: 59.05, lr: 0.000085, Train: (LOSS: 0.0162, MAE: 0.0162, RMSE: 0.0251, R2: 0.9886), Valid: (LOSS: 0.0400, MAE: 0.0400, RMSE: 0.0579, R2: 0.9502), PNorm: 188.5628, GNorm: 0.5000
[103/299] timecost: 58.93, lr: 0.000085, Train: (LOSS: 0.0159, MAE: 0.0159, RMSE: 0.0249, R2: 0.9889), Valid: (LOSS: 0.0394, MAE: 0.0394, RMSE: 0.0595, R2: 0.9493), PNorm: 188.5709, GNorm: 0.5000
[104/299] timecost: 61.07, lr: 0.000085, Train: (LOSS: 0.0158, MAE: 0.0158, RMSE: 0.0255, R2: 0.9876), Valid: (LOSS: 0.0376, MAE: 0.0376, RMSE: 0.0562, R2: 0.9544), PNorm: 188.5777, GNorm: 0.5000
[105/299] timecost: 61.31, lr: 0.000085, Train: (LOSS: 0.0154, MAE: 0.0154, RMSE: 0.0245, R2: 0.9892), Valid: (LOSS: 0.0372, MAE: 0.0372, RMSE: 0.0555, R2: 0.9544), PNorm: 188.5845, GNorm: 0.4358
[106/299] timecost: 61.91, lr: 0.000085, Train: (LOSS: 0.0169, MAE: 0.0169, RMSE: 0.0264, R2: 0.9878), Valid: (LOSS: 0.0363, MAE: 0.0363, RMSE: 0.0554, R2: 0.9556), PNorm: 188.5950, GNorm: 0.3920
[107/299] timecost: 61.69, lr: 0.000085, Train: (LOSS: 0.0157, MAE: 0.0157, RMSE: 0.0253, R2: 0.9883), Valid: (LOSS: 0.0370, MAE: 0.0370, RMSE: 0.0530, R2: 0.9590), PNorm: 188.6033, GNorm: 0.4995
[108/299] timecost: 61.32, lr: 0.000085, Train: (LOSS: 0.0152, MAE: 0.0152, RMSE: 0.0242, R2: 0.9893), Valid: (LOSS: 0.0376, MAE: 0.0376, RMSE: 0.0550, R2: 0.9546), PNorm: 188.6126, GNorm: 0.4968
[109/299] timecost: 63.15, lr: 0.000085, Train: (LOSS: 0.0155, MAE: 0.0155, RMSE: 0.0249, R2: 0.9874), Valid: (LOSS: 0.0405, MAE: 0.0405, RMSE: 0.0639, R2: 0.9399), PNorm: 188.6236, GNorm: 0.5000
[110/299] timecost: 63.56, lr: 0.000085, Train: (LOSS: 0.0159, MAE: 0.0159, RMSE: 0.0256, R2: 0.9872), Valid: (LOSS: 0.0376, MAE: 0.0376, RMSE: 0.0552, R2: 0.9552), PNorm: 188.6318, GNorm: 0.5000
[111/299] timecost: 63.38, lr: 0.000085, Train: (LOSS: 0.0159, MAE: 0.0159, RMSE: 0.0253, R2: 0.9875), Valid: (LOSS: 0.0404, MAE: 0.0404, RMSE: 0.0588, R2: 0.9497), PNorm: 188.6458, GNorm: 0.4820
[112/299] timecost: 63.08, lr: 0.000085, Train: (LOSS: 0.0154, MAE: 0.0154, RMSE: 0.0245, R2: 0.9890), Valid: (LOSS: 0.0374, MAE: 0.0374, RMSE: 0.0547, R2: 0.9549), PNorm: 188.6564, GNorm: 0.4616
[113/299] timecost: 61.39, lr: 0.000085, Train: (LOSS: 0.0158, MAE: 0.0158, RMSE: 0.0248, R2: 0.9886), Valid: (LOSS: 0.0377, MAE: 0.0377, RMSE: 0.0540, R2: 0.9564), PNorm: 188.6642, GNorm: 0.3806
Epoch 00115: reducing learning rate of group 0 to 7.2250e-05.
[114/299] timecost: 63.37, lr: 0.000072, Train: (LOSS: 0.0154, MAE: 0.0154, RMSE: 0.0243, R2: 0.9890), Valid: (LOSS: 0.0370, MAE: 0.0370, RMSE: 0.0543, R2: 0.9565), PNorm: 188.6751, GNorm: 0.4266
[115/299] timecost: 63.37, lr: 0.000072, Train: (LOSS: 0.0147, MAE: 0.0147, RMSE: 0.0233, R2: 0.9900), Valid: (LOSS: 0.0369, MAE: 0.0369, RMSE: 0.0549, R2: 0.9565), PNorm: 188.6819, GNorm: 0.4402
[116/299] timecost: 63.37, lr: 0.000072, Train: (LOSS: 0.0138, MAE: 0.0138, RMSE: 0.0223, R2: 0.9901), Valid: (LOSS: 0.0380, MAE: 0.0380, RMSE: 0.0574, R2: 0.9509), PNorm: 188.6901, GNorm: 0.3930
[117/299] timecost: 63.60, lr: 0.000072, Train: (LOSS: 0.0138, MAE: 0.0138, RMSE: 0.0225, R2: 0.9909), Valid: (LOSS: 0.0375, MAE: 0.0375, RMSE: 0.0552, R2: 0.9552), PNorm: 188.6978, GNorm: 0.4893
[118/299] timecost: 63.71, lr: 0.000072, Train: (LOSS: 0.0134, MAE: 0.0134, RMSE: 0.0220, R2: 0.9906), Valid: (LOSS: 0.0387, MAE: 0.0387, RMSE: 0.0578, R2: 0.9507), PNorm: 188.7055, GNorm: 0.3770
[119/299] timecost: 63.49, lr: 0.000072, Train: (LOSS: 0.0132, MAE: 0.0132, RMSE: 0.0218, R2: 0.9910), Valid: (LOSS: 0.0378, MAE: 0.0378, RMSE: 0.0553, R2: 0.9553), PNorm: 188.7124, GNorm: 0.5000
[120/299] timecost: 63.34, lr: 0.000072, Train: (LOSS: 0.0138, MAE: 0.0138, RMSE: 0.0219, R2: 0.9900), Valid: (LOSS: 0.0374, MAE: 0.0374, RMSE: 0.0544, R2: 0.9554), PNorm: 188.7197, GNorm: 0.4237
[121/299] timecost: 63.34, lr: 0.000072, Train: (LOSS: 0.0134, MAE: 0.0134, RMSE: 0.0219, R2: 0.9903), Valid: (LOSS: 0.0381, MAE: 0.0381, RMSE: 0.0560, R2: 0.9535), PNorm: 188.7283, GNorm: 0.3041
[122/299] timecost: 63.51, lr: 0.000072, Train: (LOSS: 0.0127, MAE: 0.0127, RMSE: 0.0209, R2: 0.9913), Valid: (LOSS: 0.0372, MAE: 0.0372, RMSE: 0.0550, R2: 0.9552), PNorm: 188.7356, GNorm: 0.4997
[123/299] timecost: 62.68, lr: 0.000072, Train: (LOSS: 0.0126, MAE: 0.0126, RMSE: 0.0205, R2: 0.9918), Valid: (LOSS: 0.0382, MAE: 0.0382, RMSE: 0.0562, R2: 0.9542), PNorm: 188.7426, GNorm: 0.4464
[124/299] timecost: 60.45, lr: 0.000072, Train: (LOSS: 0.0134, MAE: 0.0134, RMSE: 0.0215, R2: 0.9913), Valid: (LOSS: 0.0383, MAE: 0.0383, RMSE: 0.0564, R2: 0.9524), PNorm: 188.7507, GNorm: 0.4930
[125/299] timecost: 60.36, lr: 0.000072, Train: (LOSS: 0.0128, MAE: 0.0128, RMSE: 0.0210, R2: 0.9911), Valid: (LOSS: 0.0388, MAE: 0.0388, RMSE: 0.0575, R2: 0.9509), PNorm: 188.7570, GNorm: 0.3549
[126/299] timecost: 60.66, lr: 0.000072, Train: (LOSS: 0.0131, MAE: 0.0131, RMSE: 0.0209, R2: 0.9910), Valid: (LOSS: 0.0372, MAE: 0.0372, RMSE: 0.0529, R2: 0.9581), PNorm: 188.7657, GNorm: 0.4716
[127/299] timecost: 61.92, lr: 0.000072, Train: (LOSS: 0.0122, MAE: 0.0122, RMSE: 0.0198, R2: 0.9922), Valid: (LOSS: 0.0369, MAE: 0.0369, RMSE: 0.0554, R2: 0.9548), PNorm: 188.7738, GNorm: 0.5000
[128/299] timecost: 63.23, lr: 0.000072, Train: (LOSS: 0.0125, MAE: 0.0125, RMSE: 0.0206, R2: 0.9916), Valid: (LOSS: 0.0384, MAE: 0.0384, RMSE: 0.0560, R2: 0.9537), PNorm: 188.7828, GNorm: 0.5000
[129/299] timecost: 63.30, lr: 0.000072, Train: (LOSS: 0.0129, MAE: 0.0129, RMSE: 0.0209, R2: 0.9916), Valid: (LOSS: 0.0363, MAE: 0.0363, RMSE: 0.0529, R2: 0.9587), PNorm: 188.7913, GNorm: 0.4081
Epoch 00131: reducing learning rate of group 0 to 6.1413e-05.
[130/299] timecost: 63.69, lr: 0.000061, Train: (LOSS: 0.0128, MAE: 0.0128, RMSE: 0.0205, R2: 0.9925), Valid: (LOSS: 0.0372, MAE: 0.0372, RMSE: 0.0524, R2: 0.9604), PNorm: 188.8005, GNorm: 0.4252
[131/299] timecost: 63.41, lr: 0.000061, Train: (LOSS: 0.0117, MAE: 0.0117, RMSE: 0.0183, R2: 0.9932), Valid: (LOSS: 0.0361, MAE: 0.0361, RMSE: 0.0515, R2: 0.9621), PNorm: 188.8078, GNorm: 0.4977
[132/299] timecost: 63.97, lr: 0.000061, Train: (LOSS: 0.0117, MAE: 0.0117, RMSE: 0.0184, R2: 0.9928), Valid: (LOSS: 0.0355, MAE: 0.0355, RMSE: 0.0513, R2: 0.9619), PNorm: 188.8161, GNorm: 0.4405
[133/299] timecost: 63.64, lr: 0.000061, Train: (LOSS: 0.0107, MAE: 0.0107, RMSE: 0.0173, R2: 0.9939), Valid: (LOSS: 0.0359, MAE: 0.0359, RMSE: 0.0512, R2: 0.9622), PNorm: 188.8221, GNorm: 0.4872
[134/299] timecost: 63.65, lr: 0.000061, Train: (LOSS: 0.0112, MAE: 0.0112, RMSE: 0.0177, R2: 0.9940), Valid: (LOSS: 0.0353, MAE: 0.0353, RMSE: 0.0510, R2: 0.9629), PNorm: 188.8285, GNorm: 0.4813
[135/299] timecost: 63.54, lr: 0.000061, Train: (LOSS: 0.0106, MAE: 0.0106, RMSE: 0.0170, R2: 0.9941), Valid: (LOSS: 0.0366, MAE: 0.0366, RMSE: 0.0526, R2: 0.9602), PNorm: 188.8353, GNorm: 0.3739
[136/299] timecost: 63.66, lr: 0.000061, Train: (LOSS: 0.0105, MAE: 0.0105, RMSE: 0.0169, R2: 0.9943), Valid: (LOSS: 0.0354, MAE: 0.0354, RMSE: 0.0508, R2: 0.9627), PNorm: 188.8413, GNorm: 0.4251
[137/299] timecost: 63.66, lr: 0.000061, Train: (LOSS: 0.0101, MAE: 0.0101, RMSE: 0.0159, R2: 0.9951), Valid: (LOSS: 0.0356, MAE: 0.0356, RMSE: 0.0497, R2: 0.9648), PNorm: 188.8464, GNorm: 0.4480
[138/299] timecost: 63.56, lr: 0.000061, Train: (LOSS: 0.0101, MAE: 0.0101, RMSE: 0.0159, R2: 0.9946), Valid: (LOSS: 0.0347, MAE: 0.0347, RMSE: 0.0499, R2: 0.9648), PNorm: 188.8523, GNorm: 0.3660
[139/299] timecost: 63.95, lr: 0.000061, Train: (LOSS: 0.0098, MAE: 0.0098, RMSE: 0.0154, R2: 0.9951), Valid: (LOSS: 0.0345, MAE: 0.0345, RMSE: 0.0492, R2: 0.9654), PNorm: 188.8574, GNorm: 0.3877
[140/299] timecost: 63.93, lr: 0.000061, Train: (LOSS: 0.0100, MAE: 0.0100, RMSE: 0.0157, R2: 0.9952), Valid: (LOSS: 0.0353, MAE: 0.0353, RMSE: 0.0501, R2: 0.9640), PNorm: 188.8641, GNorm: 0.4113
[141/299] timecost: 63.49, lr: 0.000061, Train: (LOSS: 0.0097, MAE: 0.0097, RMSE: 0.0153, R2: 0.9950), Valid: (LOSS: 0.0348, MAE: 0.0348, RMSE: 0.0509, R2: 0.9632), PNorm: 188.8702, GNorm: 0.5000
[142/299] timecost: 63.88, lr: 0.000061, Train: (LOSS: 0.0097, MAE: 0.0097, RMSE: 0.0152, R2: 0.9951), Valid: (LOSS: 0.0343, MAE: 0.0343, RMSE: 0.0485, R2: 0.9667), PNorm: 188.8741, GNorm: 0.3908
[143/299] timecost: 63.55, lr: 0.000061, Train: (LOSS: 0.0099, MAE: 0.0099, RMSE: 0.0151, R2: 0.9954), Valid: (LOSS: 0.0368, MAE: 0.0368, RMSE: 0.0529, R2: 0.9604), PNorm: 188.8811, GNorm: 0.3586
[144/299] timecost: 62.32, lr: 0.000061, Train: (LOSS: 0.0102, MAE: 0.0102, RMSE: 0.0156, R2: 0.9952), Valid: (LOSS: 0.0363, MAE: 0.0363, RMSE: 0.0508, R2: 0.9634), PNorm: 188.8875, GNorm: 0.5000
[145/299] timecost: 60.68, lr: 0.000061, Train: (LOSS: 0.0100, MAE: 0.0100, RMSE: 0.0155, R2: 0.9956), Valid: (LOSS: 0.0358, MAE: 0.0358, RMSE: 0.0513, R2: 0.9621), PNorm: 188.8921, GNorm: 0.5000
[146/299] timecost: 60.77, lr: 0.000061, Train: (LOSS: 0.0102, MAE: 0.0102, RMSE: 0.0155, R2: 0.9953), Valid: (LOSS: 0.0359, MAE: 0.0359, RMSE: 0.0510, R2: 0.9633), PNorm: 188.8979, GNorm: 0.4406
[147/299] timecost: 61.06, lr: 0.000061, Train: (LOSS: 0.0102, MAE: 0.0102, RMSE: 0.0155, R2: 0.9952), Valid: (LOSS: 0.0366, MAE: 0.0366, RMSE: 0.0519, R2: 0.9612), PNorm: 188.9038, GNorm: 0.4313
[148/299] timecost: 60.99, lr: 0.000061, Train: (LOSS: 0.0098, MAE: 0.0098, RMSE: 0.0152, R2: 0.9952), Valid: (LOSS: 0.0343, MAE: 0.0343, RMSE: 0.0500, R2: 0.9643), PNorm: 188.9107, GNorm: 0.4795
[149/299] timecost: 60.94, lr: 0.000061, Train: (LOSS: 0.0092, MAE: 0.0092, RMSE: 0.0143, R2: 0.9959), Valid: (LOSS: 0.0337, MAE: 0.0337, RMSE: 0.0477, R2: 0.9673), PNorm: 188.9165, GNorm: 0.4251
[150/299] timecost: 60.76, lr: 0.000061, Train: (LOSS: 0.0092, MAE: 0.0092, RMSE: 0.0144, R2: 0.9958), Valid: (LOSS: 0.0369, MAE: 0.0369, RMSE: 0.0512, R2: 0.9626), PNorm: 188.9201, GNorm: 0.4771
[151/299] timecost: 60.89, lr: 0.000061, Train: (LOSS: 0.0099, MAE: 0.0099, RMSE: 0.0155, R2: 0.9951), Valid: (LOSS: 0.0337, MAE: 0.0337, RMSE: 0.0475, R2: 0.9680), PNorm: 188.9285, GNorm: 0.4306
[152/299] timecost: 61.07, lr: 0.000061, Train: (LOSS: 0.0095, MAE: 0.0095, RMSE: 0.0147, R2: 0.9957), Valid: (LOSS: 0.0357, MAE: 0.0357, RMSE: 0.0505, R2: 0.9635), PNorm: 188.9344, GNorm: 0.4149
[153/299] timecost: 60.75, lr: 0.000061, Train: (LOSS: 0.0092, MAE: 0.0092, RMSE: 0.0141, R2: 0.9957), Valid: (LOSS: 0.0356, MAE: 0.0356, RMSE: 0.0508, R2: 0.9631), PNorm: 188.9391, GNorm: 0.5000
[154/299] timecost: 60.98, lr: 0.000061, Train: (LOSS: 0.0094, MAE: 0.0094, RMSE: 0.0143, R2: 0.9950), Valid: (LOSS: 0.0343, MAE: 0.0343, RMSE: 0.0487, R2: 0.9663), PNorm: 188.9452, GNorm: 0.3811
[155/299] timecost: 61.14, lr: 0.000061, Train: (LOSS: 0.0098, MAE: 0.0098, RMSE: 0.0147, R2: 0.9956), Valid: (LOSS: 0.0341, MAE: 0.0341, RMSE: 0.0486, R2: 0.9666), PNorm: 188.9509, GNorm: 0.4281
[156/299] timecost: 60.83, lr: 0.000061, Train: (LOSS: 0.0091, MAE: 0.0091, RMSE: 0.0140, R2: 0.9958), Valid: (LOSS: 0.0343, MAE: 0.0343, RMSE: 0.0489, R2: 0.9659), PNorm: 188.9572, GNorm: 0.3821
[157/299] timecost: 60.79, lr: 0.000061, Train: (LOSS: 0.0090, MAE: 0.0090, RMSE: 0.0139, R2: 0.9955), Valid: (LOSS: 0.0338, MAE: 0.0338, RMSE: 0.0484, R2: 0.9669), PNorm: 188.9617, GNorm: 0.4672
[158/299] timecost: 60.39, lr: 0.000061, Train: (LOSS: 0.0093, MAE: 0.0093, RMSE: 0.0143, R2: 0.9955), Valid: (LOSS: 0.0352, MAE: 0.0352, RMSE: 0.0498, R2: 0.9647), PNorm: 188.9660, GNorm: 0.4421
[159/299] timecost: 60.80, lr: 0.000061, Train: (LOSS: 0.0088, MAE: 0.0088, RMSE: 0.0134, R2: 0.9957), Valid: (LOSS: 0.0358, MAE: 0.0358, RMSE: 0.0504, R2: 0.9638), PNorm: 188.9721, GNorm: 0.5000
[160/299] timecost: 61.13, lr: 0.000061, Train: (LOSS: 0.0091, MAE: 0.0091, RMSE: 0.0140, R2: 0.9956), Valid: (LOSS: 0.0360, MAE: 0.0360, RMSE: 0.0512, R2: 0.9626), PNorm: 188.9794, GNorm: 0.5000
[161/299] timecost: 58.53, lr: 0.000061, Train: (LOSS: 0.0091, MAE: 0.0091, RMSE: 0.0140, R2: 0.9961), Valid: (LOSS: 0.0347, MAE: 0.0347, RMSE: 0.0485, R2: 0.9665), PNorm: 188.9841, GNorm: 0.4391
[162/299] timecost: 61.78, lr: 0.000061, Train: (LOSS: 0.0085, MAE: 0.0085, RMSE: 0.0129, R2: 0.9963), Valid: (LOSS: 0.0341, MAE: 0.0341, RMSE: 0.0483, R2: 0.9669), PNorm: 188.9904, GNorm: 0.4292
[163/299] timecost: 60.18, lr: 0.000061, Train: (LOSS: 0.0092, MAE: 0.0092, RMSE: 0.0139, R2: 0.9960), Valid: (LOSS: 0.0349, MAE: 0.0349, RMSE: 0.0500, R2: 0.9645), PNorm: 188.9962, GNorm: 0.3477
[164/299] timecost: 59.64, lr: 0.000061, Train: (LOSS: 0.0086, MAE: 0.0086, RMSE: 0.0131, R2: 0.9961), Valid: (LOSS: 0.0348, MAE: 0.0348, RMSE: 0.0500, R2: 0.9650), PNorm: 189.0012, GNorm: 0.4284
Epoch 00166: reducing learning rate of group 0 to 5.2201e-05.
[165/299] timecost: 60.54, lr: 0.000052, Train: (LOSS: 0.0089, MAE: 0.0089, RMSE: 0.0135, R2: 0.9965), Valid: (LOSS: 0.0356, MAE: 0.0356, RMSE: 0.0498, R2: 0.9651), PNorm: 189.0069, GNorm: 0.5000
[166/299] timecost: 60.82, lr: 0.000052, Train: (LOSS: 0.0080, MAE: 0.0080, RMSE: 0.0121, R2: 0.9969), Valid: (LOSS: 0.0350, MAE: 0.0350, RMSE: 0.0501, R2: 0.9647), PNorm: 189.0110, GNorm: 0.5000
[167/299] timecost: 60.89, lr: 0.000052, Train: (LOSS: 0.0078, MAE: 0.0078, RMSE: 0.0119, R2: 0.9964), Valid: (LOSS: 0.0331, MAE: 0.0331, RMSE: 0.0475, R2: 0.9682), PNorm: 189.0147, GNorm: 0.5000
[168/299] timecost: 60.41, lr: 0.000052, Train: (LOSS: 0.0075, MAE: 0.0075, RMSE: 0.0117, R2: 0.9969), Valid: (LOSS: 0.0351, MAE: 0.0351, RMSE: 0.0506, R2: 0.9638), PNorm: 189.0172, GNorm: 0.4769
[169/299] timecost: 59.85, lr: 0.000052, Train: (LOSS: 0.0078, MAE: 0.0078, RMSE: 0.0121, R2: 0.9967), Valid: (LOSS: 0.0349, MAE: 0.0349, RMSE: 0.0490, R2: 0.9660), PNorm: 189.0209, GNorm: 0.5000
[170/299] timecost: 59.98, lr: 0.000052, Train: (LOSS: 0.0075, MAE: 0.0075, RMSE: 0.0118, R2: 0.9971), Valid: (LOSS: 0.0339, MAE: 0.0339, RMSE: 0.0488, R2: 0.9661), PNorm: 189.0241, GNorm: 0.4709
[171/299] timecost: 60.22, lr: 0.000052, Train: (LOSS: 0.0071, MAE: 0.0071, RMSE: 0.0113, R2: 0.9970), Valid: (LOSS: 0.0339, MAE: 0.0339, RMSE: 0.0482, R2: 0.9669), PNorm: 189.0288, GNorm: 0.3581
[172/299] timecost: 60.23, lr: 0.000052, Train: (LOSS: 0.0077, MAE: 0.0077, RMSE: 0.0117, R2: 0.9969), Valid: (LOSS: 0.0346, MAE: 0.0346, RMSE: 0.0493, R2: 0.9659), PNorm: 189.0324, GNorm: 0.4790
[173/299] timecost: 59.60, lr: 0.000052, Train: (LOSS: 0.0077, MAE: 0.0077, RMSE: 0.0118, R2: 0.9972), Valid: (LOSS: 0.0340, MAE: 0.0340, RMSE: 0.0500, R2: 0.9644), PNorm: 189.0353, GNorm: 0.3222
[174/299] timecost: 60.65, lr: 0.000052, Train: (LOSS: 0.0074, MAE: 0.0074, RMSE: 0.0114, R2: 0.9971), Valid: (LOSS: 0.0342, MAE: 0.0342, RMSE: 0.0491, R2: 0.9658), PNorm: 189.0397, GNorm: 0.4688
[175/299] timecost: 60.19, lr: 0.000052, Train: (LOSS: 0.0072, MAE: 0.0072, RMSE: 0.0110, R2: 0.9973), Valid: (LOSS: 0.0346, MAE: 0.0346, RMSE: 0.0499, R2: 0.9645), PNorm: 189.0434, GNorm: 0.5000
[176/299] timecost: 60.07, lr: 0.000052, Train: (LOSS: 0.0077, MAE: 0.0077, RMSE: 0.0118, R2: 0.9969), Valid: (LOSS: 0.0348, MAE: 0.0348, RMSE: 0.0497, R2: 0.9650), PNorm: 189.0485, GNorm: 0.4341
[177/299] timecost: 61.67, lr: 0.000052, Train: (LOSS: 0.0074, MAE: 0.0074, RMSE: 0.0109, R2: 0.9972), Valid: (LOSS: 0.0342, MAE: 0.0342, RMSE: 0.0500, R2: 0.9644), PNorm: 189.0514, GNorm: 0.4902
[178/299] timecost: 61.18, lr: 0.000052, Train: (LOSS: 0.0074, MAE: 0.0074, RMSE: 0.0113, R2: 0.9973), Valid: (LOSS: 0.0341, MAE: 0.0341, RMSE: 0.0487, R2: 0.9665), PNorm: 189.0556, GNorm: 0.4129
[179/299] timecost: 61.27, lr: 0.000052, Train: (LOSS: 0.0075, MAE: 0.0075, RMSE: 0.0114, R2: 0.9971), Valid: (LOSS: 0.0348, MAE: 0.0348, RMSE: 0.0496, R2: 0.9651), PNorm: 189.0597, GNorm: 0.4316
[180/299] timecost: 60.97, lr: 0.000052, Train: (LOSS: 0.0074, MAE: 0.0074, RMSE: 0.0113, R2: 0.9974), Valid: (LOSS: 0.0355, MAE: 0.0355, RMSE: 0.0506, R2: 0.9635), PNorm: 189.0633, GNorm: 0.5000
[181/299] timecost: 61.26, lr: 0.000052, Train: (LOSS: 0.0073, MAE: 0.0073, RMSE: 0.0111, R2: 0.9974), Valid: (LOSS: 0.0330, MAE: 0.0330, RMSE: 0.0481, R2: 0.9671), PNorm: 189.0664, GNorm: 0.4597
[182/299] timecost: 61.13, lr: 0.000052, Train: (LOSS: 0.0070, MAE: 0.0070, RMSE: 0.0107, R2: 0.9972), Valid: (LOSS: 0.0341, MAE: 0.0341, RMSE: 0.0489, R2: 0.9659), PNorm: 189.0724, GNorm: 0.4005
Epoch 00184: reducing learning rate of group 0 to 4.4371e-05.
[183/299] timecost: 61.07, lr: 0.000044, Train: (LOSS: 0.0068, MAE: 0.0068, RMSE: 0.0104, R2: 0.9977), Valid: (LOSS: 0.0345, MAE: 0.0345, RMSE: 0.0491, R2: 0.9657), PNorm: 189.0757, GNorm: 0.5000
[184/299] timecost: 61.25, lr: 0.000044, Train: (LOSS: 0.0068, MAE: 0.0068, RMSE: 0.0109, R2: 0.9972), Valid: (LOSS: 0.0328, MAE: 0.0328, RMSE: 0.0470, R2: 0.9683), PNorm: 189.0775, GNorm: 0.4156
[185/299] timecost: 60.94, lr: 0.000044, Train: (LOSS: 0.0063, MAE: 0.0063, RMSE: 0.0097, R2: 0.9978), Valid: (LOSS: 0.0340, MAE: 0.0340, RMSE: 0.0497, R2: 0.9653), PNorm: 189.0801, GNorm: 0.3337
[186/299] timecost: 60.03, lr: 0.000044, Train: (LOSS: 0.0066, MAE: 0.0066, RMSE: 0.0103, R2: 0.9976), Valid: (LOSS: 0.0352, MAE: 0.0352, RMSE: 0.0497, R2: 0.9648), PNorm: 189.0833, GNorm: 0.3917
[187/299] timecost: 58.72, lr: 0.000044, Train: (LOSS: 0.0065, MAE: 0.0065, RMSE: 0.0101, R2: 0.9970), Valid: (LOSS: 0.0345, MAE: 0.0345, RMSE: 0.0501, R2: 0.9644), PNorm: 189.0852, GNorm: 0.4623
[188/299] timecost: 59.93, lr: 0.000044, Train: (LOSS: 0.0064, MAE: 0.0064, RMSE: 0.0098, R2: 0.9977), Valid: (LOSS: 0.0346, MAE: 0.0346, RMSE: 0.0500, R2: 0.9644), PNorm: 189.0898, GNorm: 0.4591
[189/299] timecost: 59.86, lr: 0.000044, Train: (LOSS: 0.0063, MAE: 0.0063, RMSE: 0.0098, R2: 0.9980), Valid: (LOSS: 0.0348, MAE: 0.0348, RMSE: 0.0499, R2: 0.9646), PNorm: 189.0923, GNorm: 0.5000
[190/299] timecost: 60.24, lr: 0.000044, Train: (LOSS: 0.0063, MAE: 0.0063, RMSE: 0.0095, R2: 0.9979), Valid: (LOSS: 0.0341, MAE: 0.0341, RMSE: 0.0486, R2: 0.9665), PNorm: 189.0944, GNorm: 0.3840
[191/299] timecost: 60.06, lr: 0.000044, Train: (LOSS: 0.0060, MAE: 0.0060, RMSE: 0.0094, R2: 0.9976), Valid: (LOSS: 0.0344, MAE: 0.0344, RMSE: 0.0487, R2: 0.9664), PNorm: 189.0977, GNorm: 0.3515
[192/299] timecost: 61.07, lr: 0.000044, Train: (LOSS: 0.0061, MAE: 0.0061, RMSE: 0.0094, R2: 0.9980), Valid: (LOSS: 0.0340, MAE: 0.0340, RMSE: 0.0491, R2: 0.9658), PNorm: 189.1008, GNorm: 0.5000
[193/299] timecost: 60.93, lr: 0.000044, Train: (LOSS: 0.0060, MAE: 0.0060, RMSE: 0.0094, R2: 0.9981), Valid: (LOSS: 0.0351, MAE: 0.0351, RMSE: 0.0495, R2: 0.9651), PNorm: 189.1047, GNorm: 0.4803
[194/299] timecost: 59.76, lr: 0.000044, Train: (LOSS: 0.0061, MAE: 0.0061, RMSE: 0.0091, R2: 0.9982), Valid: (LOSS: 0.0340, MAE: 0.0340, RMSE: 0.0488, R2: 0.9662), PNorm: 189.1056, GNorm: 0.4478
[195/299] timecost: 60.82, lr: 0.000044, Train: (LOSS: 0.0061, MAE: 0.0061, RMSE: 0.0092, R2: 0.9980), Valid: (LOSS: 0.0339, MAE: 0.0339, RMSE: 0.0491, R2: 0.9655), PNorm: 189.1099, GNorm: 0.5000
[196/299] timecost: 61.05, lr: 0.000044, Train: (LOSS: 0.0061, MAE: 0.0061, RMSE: 0.0095, R2: 0.9978), Valid: (LOSS: 0.0341, MAE: 0.0341, RMSE: 0.0484, R2: 0.9667), PNorm: 189.1121, GNorm: 0.3294
[197/299] timecost: 61.10, lr: 0.000044, Train: (LOSS: 0.0060, MAE: 0.0060, RMSE: 0.0093, R2: 0.9973), Valid: (LOSS: 0.0346, MAE: 0.0346, RMSE: 0.0488, R2: 0.9661), PNorm: 189.1147, GNorm: 0.3930
[198/299] timecost: 61.21, lr: 0.000044, Train: (LOSS: 0.0061, MAE: 0.0061, RMSE: 0.0092, R2: 0.9980), Valid: (LOSS: 0.0349, MAE: 0.0349, RMSE: 0.0500, R2: 0.9646), PNorm: 189.1189, GNorm: 0.3515
Epoch 00200: reducing learning rate of group 0 to 3.7715e-05.
[199/299] timecost: 60.82, lr: 0.000038, Train: (LOSS: 0.0060, MAE: 0.0060, RMSE: 0.0092, R2: 0.9983), Valid: (LOSS: 0.0344, MAE: 0.0344, RMSE: 0.0502, R2: 0.9642), PNorm: 189.1225, GNorm: 0.4701
[200/299] timecost: 60.84, lr: 0.000038, Train: (LOSS: 0.0056, MAE: 0.0056, RMSE: 0.0087, R2: 0.9982), Valid: (LOSS: 0.0348, MAE: 0.0348, RMSE: 0.0505, R2: 0.9638), PNorm: 189.1250, GNorm: 0.4052
[201/299] timecost: 60.40, lr: 0.000038, Train: (LOSS: 0.0055, MAE: 0.0055, RMSE: 0.0086, R2: 0.9984), Valid: (LOSS: 0.0340, MAE: 0.0340, RMSE: 0.0497, R2: 0.9646), PNorm: 189.1263, GNorm: 0.4551
[202/299] timecost: 61.09, lr: 0.000038, Train: (LOSS: 0.0055, MAE: 0.0055, RMSE: 0.0087, R2: 0.9982), Valid: (LOSS: 0.0339, MAE: 0.0339, RMSE: 0.0491, R2: 0.9659), PNorm: 189.1276, GNorm: 0.3411
[203/299] timecost: 58.63, lr: 0.000038, Train: (LOSS: 0.0052, MAE: 0.0052, RMSE: 0.0082, R2: 0.9984), Valid: (LOSS: 0.0341, MAE: 0.0341, RMSE: 0.0493, R2: 0.9655), PNorm: 189.1302, GNorm: 0.3370
[204/299] timecost: 58.39, lr: 0.000038, Train: (LOSS: 0.0051, MAE: 0.0051, RMSE: 0.0081, R2: 0.9984), Valid: (LOSS: 0.0336, MAE: 0.0336, RMSE: 0.0484, R2: 0.9668), PNorm: 189.1314, GNorm: 0.5000
[205/299] timecost: 58.37, lr: 0.000038, Train: (LOSS: 0.0053, MAE: 0.0053, RMSE: 0.0081, R2: 0.9986), Valid: (LOSS: 0.0344, MAE: 0.0344, RMSE: 0.0498, R2: 0.9648), PNorm: 189.1341, GNorm: 0.4110
[206/299] timecost: 59.53, lr: 0.000038, Train: (LOSS: 0.0052, MAE: 0.0052, RMSE: 0.0081, R2: 0.9983), Valid: (LOSS: 0.0338, MAE: 0.0338, RMSE: 0.0489, R2: 0.9659), PNorm: 189.1363, GNorm: 0.3294
[207/299] timecost: 60.83, lr: 0.000038, Train: (LOSS: 0.0053, MAE: 0.0053, RMSE: 0.0083, R2: 0.9984), Valid: (LOSS: 0.0335, MAE: 0.0335, RMSE: 0.0481, R2: 0.9672), PNorm: 189.1385, GNorm: 0.3687
[208/299] timecost: 60.83, lr: 0.000038, Train: (LOSS: 0.0052, MAE: 0.0052, RMSE: 0.0081, R2: 0.9984), Valid: (LOSS: 0.0335, MAE: 0.0335, RMSE: 0.0484, R2: 0.9667), PNorm: 189.1408, GNorm: 0.5000
[209/299] timecost: 60.96, lr: 0.000038, Train: (LOSS: 0.0052, MAE: 0.0052, RMSE: 0.0080, R2: 0.9984), Valid: (LOSS: 0.0346, MAE: 0.0346, RMSE: 0.0501, R2: 0.9646), PNorm: 189.1432, GNorm: 0.3794
[210/299] timecost: 60.66, lr: 0.000038, Train: (LOSS: 0.0050, MAE: 0.0050, RMSE: 0.0078, R2: 0.9984), Valid: (LOSS: 0.0340, MAE: 0.0340, RMSE: 0.0492, R2: 0.9658), PNorm: 189.1460, GNorm: 0.5000
[211/299] timecost: 60.04, lr: 0.000038, Train: (LOSS: 0.0052, MAE: 0.0052, RMSE: 0.0080, R2: 0.9986), Valid: (LOSS: 0.0340, MAE: 0.0340, RMSE: 0.0489, R2: 0.9662), PNorm: 189.1478, GNorm: 0.4227
[212/299] timecost: 60.07, lr: 0.000038, Train: (LOSS: 0.0051, MAE: 0.0051, RMSE: 0.0080, R2: 0.9983), Valid: (LOSS: 0.0346, MAE: 0.0346, RMSE: 0.0492, R2: 0.9658), PNorm: 189.1506, GNorm: 0.4145
[213/299] timecost: 60.27, lr: 0.000038, Train: (LOSS: 0.0050, MAE: 0.0050, RMSE: 0.0078, R2: 0.9986), Valid: (LOSS: 0.0345, MAE: 0.0345, RMSE: 0.0492, R2: 0.9656), PNorm: 189.1531, GNorm: 0.5000
[214/299] timecost: 60.68, lr: 0.000038, Train: (LOSS: 0.0051, MAE: 0.0051, RMSE: 0.0079, R2: 0.9983), Valid: (LOSS: 0.0349, MAE: 0.0349, RMSE: 0.0503, R2: 0.9645), PNorm: 189.1556, GNorm: 0.3931
Epoch 00216: reducing learning rate of group 0 to 3.2058e-05.
[215/299] timecost: 60.52, lr: 0.000032, Train: (LOSS: 0.0050, MAE: 0.0050, RMSE: 0.0077, R2: 0.9987), Valid: (LOSS: 0.0345, MAE: 0.0345, RMSE: 0.0494, R2: 0.9656), PNorm: 189.1577, GNorm: 0.5000
[216/299] timecost: 59.95, lr: 0.000032, Train: (LOSS: 0.0049, MAE: 0.0049, RMSE: 0.0076, R2: 0.9986), Valid: (LOSS: 0.0337, MAE: 0.0337, RMSE: 0.0483, R2: 0.9671), PNorm: 189.1588, GNorm: 0.4955
[217/299] timecost: 61.08, lr: 0.000032, Train: (LOSS: 0.0045, MAE: 0.0045, RMSE: 0.0072, R2: 0.9986), Valid: (LOSS: 0.0344, MAE: 0.0344, RMSE: 0.0496, R2: 0.9651), PNorm: 189.1601, GNorm: 0.4276
[218/299] timecost: 60.36, lr: 0.000032, Train: (LOSS: 0.0043, MAE: 0.0043, RMSE: 0.0069, R2: 0.9987), Valid: (LOSS: 0.0338, MAE: 0.0338, RMSE: 0.0482, R2: 0.9671), PNorm: 189.1620, GNorm: 0.4793
[219/299] timecost: 59.94, lr: 0.000032, Train: (LOSS: 0.0042, MAE: 0.0042, RMSE: 0.0068, R2: 0.9988), Valid: (LOSS: 0.0338, MAE: 0.0338, RMSE: 0.0489, R2: 0.9662), PNorm: 189.1640, GNorm: 0.4220
[220/299] timecost: 60.15, lr: 0.000032, Train: (LOSS: 0.0043, MAE: 0.0043, RMSE: 0.0068, R2: 0.9986), Valid: (LOSS: 0.0340, MAE: 0.0340, RMSE: 0.0494, R2: 0.9657), PNorm: 189.1646, GNorm: 0.4588
[221/299] timecost: 61.75, lr: 0.000032, Train: (LOSS: 0.0042, MAE: 0.0042, RMSE: 0.0067, R2: 0.9990), Valid: (LOSS: 0.0346, MAE: 0.0346, RMSE: 0.0496, R2: 0.9653), PNorm: 189.1665, GNorm: 0.5000
[222/299] timecost: 61.82, lr: 0.000032, Train: (LOSS: 0.0045, MAE: 0.0045, RMSE: 0.0071, R2: 0.9988), Valid: (LOSS: 0.0344, MAE: 0.0344, RMSE: 0.0490, R2: 0.9661), PNorm: 189.1678, GNorm: 0.5000
[223/299] timecost: 61.69, lr: 0.000032, Train: (LOSS: 0.0045, MAE: 0.0045, RMSE: 0.0070, R2: 0.9989), Valid: (LOSS: 0.0344, MAE: 0.0344, RMSE: 0.0493, R2: 0.9657), PNorm: 189.1700, GNorm: 0.3399
[224/299] timecost: 60.76, lr: 0.000032, Train: (LOSS: 0.0044, MAE: 0.0044, RMSE: 0.0069, R2: 0.9988), Valid: (LOSS: 0.0335, MAE: 0.0335, RMSE: 0.0484, R2: 0.9670), PNorm: 189.1716, GNorm: 0.4913
[225/299] timecost: 60.66, lr: 0.000032, Train: (LOSS: 0.0043, MAE: 0.0043, RMSE: 0.0068, R2: 0.9988), Valid: (LOSS: 0.0341, MAE: 0.0341, RMSE: 0.0497, R2: 0.9653), PNorm: 189.1736, GNorm: 0.4776
[226/299] timecost: 61.46, lr: 0.000032, Train: (LOSS: 0.0045, MAE: 0.0045, RMSE: 0.0071, R2: 0.9987), Valid: (LOSS: 0.0347, MAE: 0.0347, RMSE: 0.0487, R2: 0.9665), PNorm: 189.1753, GNorm: 0.4361
[227/299] timecost: 60.65, lr: 0.000032, Train: (LOSS: 0.0044, MAE: 0.0044, RMSE: 0.0070, R2: 0.9988), Valid: (LOSS: 0.0344, MAE: 0.0344, RMSE: 0.0493, R2: 0.9658), PNorm: 189.1767, GNorm: 0.4436
[228/299] timecost: 58.69, lr: 0.000032, Train: (LOSS: 0.0043, MAE: 0.0043, RMSE: 0.0068, R2: 0.9989), Valid: (LOSS: 0.0342, MAE: 0.0342, RMSE: 0.0488, R2: 0.9665), PNorm: 189.1775, GNorm: 0.4440
[229/299] timecost: 58.23, lr: 0.000032, Train: (LOSS: 0.0043, MAE: 0.0043, RMSE: 0.0068, R2: 0.9989), Valid: (LOSS: 0.0333, MAE: 0.0333, RMSE: 0.0483, R2: 0.9671), PNorm: 189.1796, GNorm: 0.4352
[230/299] timecost: 59.37, lr: 0.000032, Train: (LOSS: 0.0044, MAE: 0.0044, RMSE: 0.0068, R2: 0.9987), Valid: (LOSS: 0.0337, MAE: 0.0337, RMSE: 0.0489, R2: 0.9665), PNorm: 189.1819, GNorm: 0.4093
Epoch 00232: reducing learning rate of group 0 to 2.7249e-05.
[231/299] timecost: 60.32, lr: 0.000027, Train: (LOSS: 0.0041, MAE: 0.0041, RMSE: 0.0066, R2: 0.9989), Valid: (LOSS: 0.0347, MAE: 0.0347, RMSE: 0.0501, R2: 0.9648), PNorm: 189.1827, GNorm: 0.4126
[232/299] timecost: 59.94, lr: 0.000027, Train: (LOSS: 0.0040, MAE: 0.0040, RMSE: 0.0062, R2: 0.9991), Valid: (LOSS: 0.0340, MAE: 0.0340, RMSE: 0.0490, R2: 0.9664), PNorm: 189.1841, GNorm: 0.3292
[233/299] timecost: 60.09, lr: 0.000027, Train: (LOSS: 0.0039, MAE: 0.0039, RMSE: 0.0062, R2: 0.9988), Valid: (LOSS: 0.0342, MAE: 0.0342, RMSE: 0.0494, R2: 0.9657), PNorm: 189.1846, GNorm: 0.4137
[234/299] timecost: 59.08, lr: 0.000027, Train: (LOSS: 0.0039, MAE: 0.0039, RMSE: 0.0062, R2: 0.9990), Valid: (LOSS: 0.0342, MAE: 0.0342, RMSE: 0.0494, R2: 0.9653), PNorm: 189.1866, GNorm: 0.3932
[235/299] timecost: 61.52, lr: 0.000027, Train: (LOSS: 0.0039, MAE: 0.0039, RMSE: 0.0062, R2: 0.9990), Valid: (LOSS: 0.0343, MAE: 0.0343, RMSE: 0.0498, R2: 0.9651), PNorm: 189.1884, GNorm: 0.4091
[236/299] timecost: 63.16, lr: 0.000027, Train: (LOSS: 0.0039, MAE: 0.0039, RMSE: 0.0062, R2: 0.9990), Valid: (LOSS: 0.0341, MAE: 0.0341, RMSE: 0.0492, R2: 0.9657), PNorm: 189.1891, GNorm: 0.4254
[237/299] timecost: 62.67, lr: 0.000027, Train: (LOSS: 0.0039, MAE: 0.0039, RMSE: 0.0062, R2: 0.9988), Valid: (LOSS: 0.0340, MAE: 0.0340, RMSE: 0.0491, R2: 0.9660), PNorm: 189.1906, GNorm: 0.3292
[238/299] timecost: 61.45, lr: 0.000027, Train: (LOSS: 0.0036, MAE: 0.0036, RMSE: 0.0057, R2: 0.9991), Valid: (LOSS: 0.0339, MAE: 0.0339, RMSE: 0.0487, R2: 0.9666), PNorm: 189.1920, GNorm: 0.3968
[239/299] timecost: 63.36, lr: 0.000027, Train: (LOSS: 0.0036, MAE: 0.0036, RMSE: 0.0058, R2: 0.9990), Valid: (LOSS: 0.0336, MAE: 0.0336, RMSE: 0.0488, R2: 0.9665), PNorm: 189.1924, GNorm: 0.3100
[240/299] timecost: 63.23, lr: 0.000027, Train: (LOSS: 0.0037, MAE: 0.0037, RMSE: 0.0059, R2: 0.9991), Valid: (LOSS: 0.0347, MAE: 0.0347, RMSE: 0.0495, R2: 0.9656), PNorm: 189.1941, GNorm: 0.5000
[241/299] timecost: 61.36, lr: 0.000027, Train: (LOSS: 0.0037, MAE: 0.0037, RMSE: 0.0058, R2: 0.9992), Valid: (LOSS: 0.0350, MAE: 0.0350, RMSE: 0.0498, R2: 0.9648), PNorm: 189.1944, GNorm: 0.3489
[242/299] timecost: 63.81, lr: 0.000027, Train: (LOSS: 0.0037, MAE: 0.0037, RMSE: 0.0059, R2: 0.9989), Valid: (LOSS: 0.0339, MAE: 0.0339, RMSE: 0.0486, R2: 0.9668), PNorm: 189.1957, GNorm: 0.4522
[243/299] timecost: 63.34, lr: 0.000027, Train: (LOSS: 0.0037, MAE: 0.0037, RMSE: 0.0058, R2: 0.9990), Valid: (LOSS: 0.0342, MAE: 0.0342, RMSE: 0.0489, R2: 0.9663), PNorm: 189.1963, GNorm: 0.3813
[244/299] timecost: 61.76, lr: 0.000027, Train: (LOSS: 0.0040, MAE: 0.0040, RMSE: 0.0062, R2: 0.9987), Valid: (LOSS: 0.0340, MAE: 0.0340, RMSE: 0.0489, R2: 0.9662), PNorm: 189.1976, GNorm: 0.5000
[245/299] timecost: 60.68, lr: 0.000027, Train: (LOSS: 0.0036, MAE: 0.0036, RMSE: 0.0057, R2: 0.9989), Valid: (LOSS: 0.0345, MAE: 0.0345, RMSE: 0.0493, R2: 0.9658), PNorm: 189.1997, GNorm: 0.3688
[246/299] timecost: 60.98, lr: 0.000027, Train: (LOSS: 0.0036, MAE: 0.0036, RMSE: 0.0057, R2: 0.9991), Valid: (LOSS: 0.0339, MAE: 0.0339, RMSE: 0.0490, R2: 0.9661), PNorm: 189.1999, GNorm: 0.4181
Epoch 00248: reducing learning rate of group 0 to 2.3162e-05.
[247/299] timecost: 63.19, lr: 0.000023, Train: (LOSS: 0.0036, MAE: 0.0036, RMSE: 0.0056, R2: 0.9991), Valid: (LOSS: 0.0346, MAE: 0.0346, RMSE: 0.0494, R2: 0.9655), PNorm: 189.2008, GNorm: 0.4695
[248/299] timecost: 63.65, lr: 0.000023, Train: (LOSS: 0.0034, MAE: 0.0034, RMSE: 0.0055, R2: 0.9990), Valid: (LOSS: 0.0344, MAE: 0.0344, RMSE: 0.0490, R2: 0.9661), PNorm: 189.2021, GNorm: 0.4006
[249/299] timecost: 63.62, lr: 0.000023, Train: (LOSS: 0.0035, MAE: 0.0035, RMSE: 0.0056, R2: 0.9989), Valid: (LOSS: 0.0339, MAE: 0.0339, RMSE: 0.0487, R2: 0.9664), PNorm: 189.2030, GNorm: 0.3369
[250/299] timecost: 62.12, lr: 0.000023, Train: (LOSS: 0.0033, MAE: 0.0033, RMSE: 0.0054, R2: 0.9990), Valid: (LOSS: 0.0345, MAE: 0.0345, RMSE: 0.0497, R2: 0.9651), PNorm: 189.2042, GNorm: 0.4007
[251/299] timecost: 59.96, lr: 0.000023, Train: (LOSS: 0.0033, MAE: 0.0033, RMSE: 0.0054, R2: 0.9990), Valid: (LOSS: 0.0341, MAE: 0.0341, RMSE: 0.0485, R2: 0.9668), PNorm: 189.2053, GNorm: 0.4054
[252/299] timecost: 59.84, lr: 0.000023, Train: (LOSS: 0.0032, MAE: 0.0032, RMSE: 0.0053, R2: 0.9990), Valid: (LOSS: 0.0347, MAE: 0.0347, RMSE: 0.0500, R2: 0.9649), PNorm: 189.2058, GNorm: 0.5000
[253/299] timecost: 59.94, lr: 0.000023, Train: (LOSS: 0.0034, MAE: 0.0034, RMSE: 0.0054, R2: 0.9992), Valid: (LOSS: 0.0342, MAE: 0.0342, RMSE: 0.0492, R2: 0.9658), PNorm: 189.2070, GNorm: 0.3900
[254/299] timecost: 60.26, lr: 0.000023, Train: (LOSS: 0.0032, MAE: 0.0032, RMSE: 0.0052, R2: 0.9990), Valid: (LOSS: 0.0343, MAE: 0.0343, RMSE: 0.0491, R2: 0.9661), PNorm: 189.2073, GNorm: 0.4416
[255/299] timecost: 60.35, lr: 0.000023, Train: (LOSS: 0.0032, MAE: 0.0032, RMSE: 0.0053, R2: 0.9991), Valid: (LOSS: 0.0339, MAE: 0.0339, RMSE: 0.0491, R2: 0.9660), PNorm: 189.2089, GNorm: 0.3575
[256/299] timecost: 59.88, lr: 0.000023, Train: (LOSS: 0.0033, MAE: 0.0033, RMSE: 0.0053, R2: 0.9989), Valid: (LOSS: 0.0348, MAE: 0.0348, RMSE: 0.0499, R2: 0.9650), PNorm: 189.2098, GNorm: 0.5000
[257/299] timecost: 59.76, lr: 0.000023, Train: (LOSS: 0.0032, MAE: 0.0032, RMSE: 0.0053, R2: 0.9990), Valid: (LOSS: 0.0341, MAE: 0.0341, RMSE: 0.0492, R2: 0.9658), PNorm: 189.2111, GNorm: 0.3871
[258/299] timecost: 60.17, lr: 0.000023, Train: (LOSS: 0.0032, MAE: 0.0032, RMSE: 0.0052, R2: 0.9990), Valid: (LOSS: 0.0343, MAE: 0.0343, RMSE: 0.0492, R2: 0.9660), PNorm: 189.2117, GNorm: 0.3409
[259/299] timecost: 59.82, lr: 0.000023, Train: (LOSS: 0.0032, MAE: 0.0032, RMSE: 0.0051, R2: 0.9990), Valid: (LOSS: 0.0340, MAE: 0.0340, RMSE: 0.0489, R2: 0.9663), PNorm: 189.2122, GNorm: 0.4240
[260/299] timecost: 60.41, lr: 0.000023, Train: (LOSS: 0.0031, MAE: 0.0031, RMSE: 0.0051, R2: 0.9990), Valid: (LOSS: 0.0337, MAE: 0.0337, RMSE: 0.0489, R2: 0.9664), PNorm: 189.2130, GNorm: 0.3833
[261/299] timecost: 59.70, lr: 0.000023, Train: (LOSS: 0.0031, MAE: 0.0031, RMSE: 0.0050, R2: 0.9986), Valid: (LOSS: 0.0340, MAE: 0.0340, RMSE: 0.0489, R2: 0.9665), PNorm: 189.2145, GNorm: 0.3478
[262/299] timecost: 59.62, lr: 0.000023, Train: (LOSS: 0.0032, MAE: 0.0032, RMSE: 0.0051, R2: 0.9989), Valid: (LOSS: 0.0339, MAE: 0.0339, RMSE: 0.0488, R2: 0.9667), PNorm: 189.2152, GNorm: 0.3468
Epoch 00264: reducing learning rate of group 0 to 1.9687e-05.
[263/299] timecost: 61.78, lr: 0.000020, Train: (LOSS: 0.0032, MAE: 0.0032, RMSE: 0.0052, R2: 0.9991), Valid: (LOSS: 0.0340, MAE: 0.0340, RMSE: 0.0488, R2: 0.9664), PNorm: 189.2163, GNorm: 0.3171
[264/299] timecost: 61.81, lr: 0.000020, Train: (LOSS: 0.0031, MAE: 0.0031, RMSE: 0.0051, R2: 0.9992), Valid: (LOSS: 0.0338, MAE: 0.0338, RMSE: 0.0485, R2: 0.9668), PNorm: 189.2172, GNorm: 0.3678
[265/299] timecost: 61.91, lr: 0.000020, Train: (LOSS: 0.0029, MAE: 0.0029, RMSE: 0.0047, R2: 0.9992), Valid: (LOSS: 0.0342, MAE: 0.0342, RMSE: 0.0487, R2: 0.9666), PNorm: 189.2173, GNorm: 0.5000
[266/299] timecost: 61.72, lr: 0.000020, Train: (LOSS: 0.0029, MAE: 0.0029, RMSE: 0.0049, R2: 0.9991), Valid: (LOSS: 0.0339, MAE: 0.0339, RMSE: 0.0488, R2: 0.9667), PNorm: 189.2183, GNorm: 0.4213
[267/299] timecost: 62.52, lr: 0.000020, Train: (LOSS: 0.0028, MAE: 0.0028, RMSE: 0.0047, R2: 0.9990), Valid: (LOSS: 0.0340, MAE: 0.0340, RMSE: 0.0489, R2: 0.9663), PNorm: 189.2185, GNorm: 0.4015
[268/299] timecost: 63.43, lr: 0.000020, Train: (LOSS: 0.0028, MAE: 0.0028, RMSE: 0.0047, R2: 0.9993), Valid: (LOSS: 0.0339, MAE: 0.0339, RMSE: 0.0493, R2: 0.9658), PNorm: 189.2191, GNorm: 0.4866
[269/299] timecost: 63.61, lr: 0.000020, Train: (LOSS: 0.0029, MAE: 0.0029, RMSE: 0.0048, R2: 0.9992), Valid: (LOSS: 0.0347, MAE: 0.0347, RMSE: 0.0498, R2: 0.9652), PNorm: 189.2201, GNorm: 0.5000
[270/299] timecost: 62.18, lr: 0.000020, Train: (LOSS: 0.0029, MAE: 0.0029, RMSE: 0.0047, R2: 0.9992), Valid: (LOSS: 0.0340, MAE: 0.0340, RMSE: 0.0486, R2: 0.9667), PNorm: 189.2206, GNorm: 0.5000
[271/299] timecost: 61.00, lr: 0.000020, Train: (LOSS: 0.0028, MAE: 0.0028, RMSE: 0.0046, R2: 0.9992), Valid: (LOSS: 0.0343, MAE: 0.0343, RMSE: 0.0490, R2: 0.9663), PNorm: 189.2214, GNorm: 0.4225
[272/299] timecost: 60.36, lr: 0.000020, Train: (LOSS: 0.0028, MAE: 0.0028, RMSE: 0.0046, R2: 0.9991), Valid: (LOSS: 0.0342, MAE: 0.0342, RMSE: 0.0492, R2: 0.9659), PNorm: 189.2223, GNorm: 0.3739
[273/299] timecost: 61.45, lr: 0.000020, Train: (LOSS: 0.0029, MAE: 0.0029, RMSE: 0.0047, R2: 0.9992), Valid: (LOSS: 0.0341, MAE: 0.0341, RMSE: 0.0489, R2: 0.9664), PNorm: 189.2223, GNorm: 0.3267
[274/299] timecost: 59.92, lr: 0.000020, Train: (LOSS: 0.0027, MAE: 0.0027, RMSE: 0.0044, R2: 0.9990), Valid: (LOSS: 0.0341, MAE: 0.0341, RMSE: 0.0487, R2: 0.9666), PNorm: 189.2238, GNorm: 0.4194
[275/299] timecost: 60.16, lr: 0.000020, Train: (LOSS: 0.0028, MAE: 0.0028, RMSE: 0.0047, R2: 0.9994), Valid: (LOSS: 0.0339, MAE: 0.0339, RMSE: 0.0485, R2: 0.9668), PNorm: 189.2241, GNorm: 0.3883
[276/299] timecost: 59.63, lr: 0.000020, Train: (LOSS: 0.0029, MAE: 0.0029, RMSE: 0.0047, R2: 0.9993), Valid: (LOSS: 0.0335, MAE: 0.0335, RMSE: 0.0483, R2: 0.9672), PNorm: 189.2249, GNorm: 0.3522
[277/299] timecost: 60.94, lr: 0.000020, Train: (LOSS: 0.0029, MAE: 0.0029, RMSE: 0.0048, R2: 0.9992), Valid: (LOSS: 0.0340, MAE: 0.0340, RMSE: 0.0491, R2: 0.9661), PNorm: 189.2256, GNorm: 0.5000
[278/299] timecost: 60.82, lr: 0.000020, Train: (LOSS: 0.0028, MAE: 0.0028, RMSE: 0.0046, R2: 0.9992), Valid: (LOSS: 0.0345, MAE: 0.0345, RMSE: 0.0497, R2: 0.9651), PNorm: 189.2266, GNorm: 0.4391
Epoch 00280: reducing learning rate of group 0 to 1.6734e-05.
[279/299] timecost: 61.31, lr: 0.000017, Train: (LOSS: 0.0028, MAE: 0.0028, RMSE: 0.0047, R2: 0.9993), Valid: (LOSS: 0.0342, MAE: 0.0342, RMSE: 0.0491, R2: 0.9659), PNorm: 189.2271, GNorm: 0.3946
[280/299] timecost: 61.12, lr: 0.000017, Train: (LOSS: 0.0027, MAE: 0.0027, RMSE: 0.0044, R2: 0.9994), Valid: (LOSS: 0.0347, MAE: 0.0347, RMSE: 0.0494, R2: 0.9656), PNorm: 189.2274, GNorm: 0.5000
[281/299] timecost: 61.26, lr: 0.000017, Train: (LOSS: 0.0025, MAE: 0.0025, RMSE: 0.0043, R2: 0.9994), Valid: (LOSS: 0.0343, MAE: 0.0343, RMSE: 0.0490, R2: 0.9662), PNorm: 189.2282, GNorm: 0.3353
[282/299] timecost: 61.05, lr: 0.000017, Train: (LOSS: 0.0024, MAE: 0.0024, RMSE: 0.0043, R2: 0.9992), Valid: (LOSS: 0.0341, MAE: 0.0341, RMSE: 0.0489, R2: 0.9662), PNorm: 189.2288, GNorm: 0.3801
[283/299] timecost: 60.94, lr: 0.000017, Train: (LOSS: 0.0024, MAE: 0.0024, RMSE: 0.0042, R2: 0.9993), Valid: (LOSS: 0.0342, MAE: 0.0342, RMSE: 0.0491, R2: 0.9659), PNorm: 189.2290, GNorm: 0.4661
[284/299] timecost: 60.96, lr: 0.000017, Train: (LOSS: 0.0025, MAE: 0.0025, RMSE: 0.0042, R2: 0.9993), Valid: (LOSS: 0.0342, MAE: 0.0342, RMSE: 0.0490, R2: 0.9660), PNorm: 189.2291, GNorm: 0.4178
[285/299] timecost: 61.20, lr: 0.000017, Train: (LOSS: 0.0025, MAE: 0.0025, RMSE: 0.0043, R2: 0.9993), Valid: (LOSS: 0.0336, MAE: 0.0336, RMSE: 0.0482, R2: 0.9672), PNorm: 189.2292, GNorm: 0.4497
[286/299] timecost: 60.78, lr: 0.000017, Train: (LOSS: 0.0024, MAE: 0.0024, RMSE: 0.0042, R2: 0.9994), Valid: (LOSS: 0.0340, MAE: 0.0340, RMSE: 0.0491, R2: 0.9662), PNorm: 189.2296, GNorm: 0.4412
[287/299] timecost: 60.20, lr: 0.000017, Train: (LOSS: 0.0024, MAE: 0.0024, RMSE: 0.0041, R2: 0.9994), Valid: (LOSS: 0.0339, MAE: 0.0339, RMSE: 0.0490, R2: 0.9661), PNorm: 189.2301, GNorm: 0.3263
[288/299] timecost: 60.43, lr: 0.000017, Train: (LOSS: 0.0025, MAE: 0.0025, RMSE: 0.0043, R2: 0.9994), Valid: (LOSS: 0.0341, MAE: 0.0341, RMSE: 0.0488, R2: 0.9664), PNorm: 189.2311, GNorm: 0.3902
[289/299] timecost: 60.54, lr: 0.000017, Train: (LOSS: 0.0025, MAE: 0.0025, RMSE: 0.0043, R2: 0.9993), Valid: (LOSS: 0.0337, MAE: 0.0337, RMSE: 0.0484, R2: 0.9669), PNorm: 189.2317, GNorm: 0.5000
[290/299] timecost: 59.83, lr: 0.000017, Train: (LOSS: 0.0027, MAE: 0.0027, RMSE: 0.0046, R2: 0.9991), Valid: (LOSS: 0.0339, MAE: 0.0339, RMSE: 0.0487, R2: 0.9665), PNorm: 189.2323, GNorm: 0.3513
[291/299] timecost: 59.81, lr: 0.000017, Train: (LOSS: 0.0024, MAE: 0.0024, RMSE: 0.0042, R2: 0.9994), Valid: (LOSS: 0.0339, MAE: 0.0339, RMSE: 0.0485, R2: 0.9669), PNorm: 189.2326, GNorm: 0.3800
[292/299] timecost: 58.59, lr: 0.000017, Train: (LOSS: 0.0024, MAE: 0.0024, RMSE: 0.0041, R2: 0.9995), Valid: (LOSS: 0.0341, MAE: 0.0341, RMSE: 0.0491, R2: 0.9659), PNorm: 189.2331, GNorm: 0.5000
[293/299] timecost: 58.63, lr: 0.000017, Train: (LOSS: 0.0026, MAE: 0.0026, RMSE: 0.0043, R2: 0.9991), Valid: (LOSS: 0.0345, MAE: 0.0345, RMSE: 0.0492, R2: 0.9658), PNorm: 189.2336, GNorm: 0.4334
[294/299] timecost: 58.64, lr: 0.000017, Train: (LOSS: 0.0024, MAE: 0.0024, RMSE: 0.0042, R2: 0.9993), Valid: (LOSS: 0.0341, MAE: 0.0341, RMSE: 0.0491, R2: 0.9661), PNorm: 189.2344, GNorm: 0.3956
Epoch 00296: reducing learning rate of group 0 to 1.4224e-05.
[295/299] timecost: 58.80, lr: 0.000014, Train: (LOSS: 0.0024, MAE: 0.0024, RMSE: 0.0041, R2: 0.9994), Valid: (LOSS: 0.0337, MAE: 0.0337, RMSE: 0.0487, R2: 0.9665), PNorm: 189.2344, GNorm: 0.4461
[296/299] timecost: 58.75, lr: 0.000014, Train: (LOSS: 0.0024, MAE: 0.0024, RMSE: 0.0039, R2: 0.9994), Valid: (LOSS: 0.0343, MAE: 0.0343, RMSE: 0.0493, R2: 0.9656), PNorm: 189.2346, GNorm: 0.3332
[297/299] timecost: 58.44, lr: 0.000014, Train: (LOSS: 0.0021, MAE: 0.0021, RMSE: 0.0037, R2: 0.9993), Valid: (LOSS: 0.0341, MAE: 0.0341, RMSE: 0.0491, R2: 0.9661), PNorm: 189.2347, GNorm: 0.4211
[298/299] timecost: 58.60, lr: 0.000014, Train: (LOSS: 0.0022, MAE: 0.0022, RMSE: 0.0039, R2: 0.9994), Valid: (LOSS: 0.0339, MAE: 0.0339, RMSE: 0.0489, R2: 0.9663), PNorm: 189.2352, GNorm: 0.4614
[299/299] timecost: 59.99, lr: 0.000014, Train: (LOSS: 0.0023, MAE: 0.0023, RMSE: 0.0040, R2: 0.9994), Valid: (LOSS: 0.0339, MAE: 0.0339, RMSE: 0.0486, R2: 0.9666), PNorm: 189.2360, GNorm: 0.4132
==========Training End==========
==========Test Best Model==========
================Final Results=======================
mse: 0.0339 +- 0.0000:
rmse: 0.0542 +- 0.0000:
mae: 0.0339 +- 0.0000:
r2: 0.9505 +- 0.0000:
tensor([[0.0000, 0.0000],
        [0.0000, 0.0000],
        [0.0000, 0.0000],
        ...,
        [0.0000, 0.0000],
        [0.5895, 0.4354],
        [0.7984, 0.6980]], device='cuda:0')
