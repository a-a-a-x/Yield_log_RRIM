cuda available with GPU: Tesla V100-PCIE-16GB
==========Load Seed==========
set_random_seed
0
==========Training Start==========
Training Graphs:  2491
Valid Graphs:  277
Test Graphs:  1187
============Not pretrained weights used============
============Creating new layers============
============Creating Model============
Training Graphs Batches:  78
Valid Graphs Batches:  9
Test Graphs Batches:  37
[0/299] timecost: 63.86, lr: 0.000100, Train: (LOSS: 0.2048, MAE: 0.2048, RMSE: 0.2534, R2: 0.0487), Valid: (LOSS: 0.1776, MAE: 0.1776, RMSE: 0.2405, R2: 0.2086), PNorm: 187.0335, GNorm: 0.5000
[1/299] timecost: 61.57, lr: 0.000100, Train: (LOSS: 0.1781, MAE: 0.1781, RMSE: 0.2291, R2: 0.2379), Valid: (LOSS: 0.1752, MAE: 0.1752, RMSE: 0.2258, R2: 0.2946), PNorm: 187.0561, GNorm: 0.5000
[2/299] timecost: 62.58, lr: 0.000100, Train: (LOSS: 0.1686, MAE: 0.1686, RMSE: 0.2203, R2: 0.2955), Valid: (LOSS: 0.1726, MAE: 0.1726, RMSE: 0.2232, R2: 0.3086), PNorm: 187.0902, GNorm: 0.5000
[3/299] timecost: 61.28, lr: 0.000100, Train: (LOSS: 0.1597, MAE: 0.1597, RMSE: 0.2085, R2: 0.3685), Valid: (LOSS: 0.1512, MAE: 0.1512, RMSE: 0.2012, R2: 0.4455), PNorm: 187.1227, GNorm: 0.4288
[4/299] timecost: 58.68, lr: 0.000100, Train: (LOSS: 0.1504, MAE: 0.1504, RMSE: 0.1998, R2: 0.4121), Valid: (LOSS: 0.1400, MAE: 0.1400, RMSE: 0.1855, R2: 0.5158), PNorm: 187.1605, GNorm: 0.5000
[5/299] timecost: 58.37, lr: 0.000100, Train: (LOSS: 0.1392, MAE: 0.1392, RMSE: 0.1874, R2: 0.4811), Valid: (LOSS: 0.1302, MAE: 0.1302, RMSE: 0.1755, R2: 0.5694), PNorm: 187.1913, GNorm: 0.5000
[6/299] timecost: 59.52, lr: 0.000100, Train: (LOSS: 0.1272, MAE: 0.1272, RMSE: 0.1759, R2: 0.5403), Valid: (LOSS: 0.1288, MAE: 0.1288, RMSE: 0.1769, R2: 0.5606), PNorm: 187.2167, GNorm: 0.4035
[7/299] timecost: 60.93, lr: 0.000100, Train: (LOSS: 0.1194, MAE: 0.1194, RMSE: 0.1655, R2: 0.5942), Valid: (LOSS: 0.1205, MAE: 0.1205, RMSE: 0.1635, R2: 0.6238), PNorm: 187.2378, GNorm: 0.4188
[8/299] timecost: 63.20, lr: 0.000100, Train: (LOSS: 0.1125, MAE: 0.1125, RMSE: 0.1586, R2: 0.6138), Valid: (LOSS: 0.1313, MAE: 0.1313, RMSE: 0.1835, R2: 0.5185), PNorm: 187.2657, GNorm: 0.5000
[9/299] timecost: 63.25, lr: 0.000100, Train: (LOSS: 0.1080, MAE: 0.1080, RMSE: 0.1539, R2: 0.6460), Valid: (LOSS: 0.1088, MAE: 0.1088, RMSE: 0.1557, R2: 0.6635), PNorm: 187.2961, GNorm: 0.5000
[10/299] timecost: 61.51, lr: 0.000100, Train: (LOSS: 0.0988, MAE: 0.0988, RMSE: 0.1431, R2: 0.6914), Valid: (LOSS: 0.0976, MAE: 0.0976, RMSE: 0.1367, R2: 0.7361), PNorm: 187.3274, GNorm: 0.5000
[11/299] timecost: 61.12, lr: 0.000100, Train: (LOSS: 0.0938, MAE: 0.0938, RMSE: 0.1374, R2: 0.7119), Valid: (LOSS: 0.0936, MAE: 0.0936, RMSE: 0.1374, R2: 0.7262), PNorm: 187.3511, GNorm: 0.5000
[12/299] timecost: 62.72, lr: 0.000100, Train: (LOSS: 0.0878, MAE: 0.0878, RMSE: 0.1294, R2: 0.7421), Valid: (LOSS: 0.0904, MAE: 0.0904, RMSE: 0.1284, R2: 0.7654), PNorm: 187.3720, GNorm: 0.5000
[13/299] timecost: 62.09, lr: 0.000100, Train: (LOSS: 0.0830, MAE: 0.0830, RMSE: 0.1241, R2: 0.7633), Valid: (LOSS: 0.0874, MAE: 0.0874, RMSE: 0.1213, R2: 0.7869), PNorm: 187.3956, GNorm: 0.5000
[14/299] timecost: 58.83, lr: 0.000100, Train: (LOSS: 0.0792, MAE: 0.0792, RMSE: 0.1184, R2: 0.7857), Valid: (LOSS: 0.0828, MAE: 0.0828, RMSE: 0.1187, R2: 0.7977), PNorm: 187.4234, GNorm: 0.5000
[15/299] timecost: 59.92, lr: 0.000100, Train: (LOSS: 0.0729, MAE: 0.0729, RMSE: 0.1111, R2: 0.8088), Valid: (LOSS: 0.0674, MAE: 0.0674, RMSE: 0.0974, R2: 0.8631), PNorm: 187.4507, GNorm: 0.5000
[16/299] timecost: 60.36, lr: 0.000100, Train: (LOSS: 0.0665, MAE: 0.0665, RMSE: 0.0991, R2: 0.8468), Valid: (LOSS: 0.0679, MAE: 0.0679, RMSE: 0.0980, R2: 0.8596), PNorm: 187.4739, GNorm: 0.5000
[17/299] timecost: 60.79, lr: 0.000100, Train: (LOSS: 0.0664, MAE: 0.0664, RMSE: 0.1007, R2: 0.8366), Valid: (LOSS: 0.0610, MAE: 0.0610, RMSE: 0.0877, R2: 0.8829), PNorm: 187.4991, GNorm: 0.5000
[18/299] timecost: 59.79, lr: 0.000100, Train: (LOSS: 0.0633, MAE: 0.0633, RMSE: 0.0957, R2: 0.8536), Valid: (LOSS: 0.0715, MAE: 0.0715, RMSE: 0.1038, R2: 0.8328), PNorm: 187.5179, GNorm: 0.5000
[19/299] timecost: 60.44, lr: 0.000100, Train: (LOSS: 0.0579, MAE: 0.0579, RMSE: 0.0879, R2: 0.8760), Valid: (LOSS: 0.0598, MAE: 0.0598, RMSE: 0.0884, R2: 0.8803), PNorm: 187.5373, GNorm: 0.5000
[20/299] timecost: 61.01, lr: 0.000100, Train: (LOSS: 0.0555, MAE: 0.0555, RMSE: 0.0854, R2: 0.8821), Valid: (LOSS: 0.0555, MAE: 0.0555, RMSE: 0.0842, R2: 0.8924), PNorm: 187.5549, GNorm: 0.4260
[21/299] timecost: 61.06, lr: 0.000100, Train: (LOSS: 0.0533, MAE: 0.0533, RMSE: 0.0841, R2: 0.8897), Valid: (LOSS: 0.0614, MAE: 0.0614, RMSE: 0.0897, R2: 0.8828), PNorm: 187.5713, GNorm: 0.5000
[22/299] timecost: 61.23, lr: 0.000100, Train: (LOSS: 0.0522, MAE: 0.0522, RMSE: 0.0805, R2: 0.8961), Valid: (LOSS: 0.0558, MAE: 0.0558, RMSE: 0.0828, R2: 0.8963), PNorm: 187.5882, GNorm: 0.5000
[23/299] timecost: 62.74, lr: 0.000100, Train: (LOSS: 0.0512, MAE: 0.0512, RMSE: 0.0798, R2: 0.8963), Valid: (LOSS: 0.0518, MAE: 0.0518, RMSE: 0.0797, R2: 0.9063), PNorm: 187.6040, GNorm: 0.5000
[24/299] timecost: 62.95, lr: 0.000100, Train: (LOSS: 0.0493, MAE: 0.0493, RMSE: 0.0767, R2: 0.9042), Valid: (LOSS: 0.0588, MAE: 0.0588, RMSE: 0.0878, R2: 0.8849), PNorm: 187.6191, GNorm: 0.5000
[25/299] timecost: 63.35, lr: 0.000100, Train: (LOSS: 0.0470, MAE: 0.0470, RMSE: 0.0740, R2: 0.9117), Valid: (LOSS: 0.0555, MAE: 0.0555, RMSE: 0.0829, R2: 0.8982), PNorm: 187.6324, GNorm: 0.5000
[26/299] timecost: 63.31, lr: 0.000100, Train: (LOSS: 0.0464, MAE: 0.0464, RMSE: 0.0722, R2: 0.9143), Valid: (LOSS: 0.0560, MAE: 0.0560, RMSE: 0.0847, R2: 0.8904), PNorm: 187.6486, GNorm: 0.3843
[27/299] timecost: 63.53, lr: 0.000100, Train: (LOSS: 0.0441, MAE: 0.0441, RMSE: 0.0690, R2: 0.9222), Valid: (LOSS: 0.0564, MAE: 0.0564, RMSE: 0.0877, R2: 0.8770), PNorm: 187.6628, GNorm: 0.5000
[28/299] timecost: 63.61, lr: 0.000100, Train: (LOSS: 0.0455, MAE: 0.0455, RMSE: 0.0712, R2: 0.9178), Valid: (LOSS: 0.0508, MAE: 0.0508, RMSE: 0.0740, R2: 0.9167), PNorm: 187.6759, GNorm: 0.5000
[29/299] timecost: 63.54, lr: 0.000100, Train: (LOSS: 0.0440, MAE: 0.0440, RMSE: 0.0682, R2: 0.9212), Valid: (LOSS: 0.0511, MAE: 0.0511, RMSE: 0.0765, R2: 0.9111), PNorm: 187.6909, GNorm: 0.5000
[30/299] timecost: 63.14, lr: 0.000100, Train: (LOSS: 0.0437, MAE: 0.0437, RMSE: 0.0672, R2: 0.9267), Valid: (LOSS: 0.0506, MAE: 0.0506, RMSE: 0.0736, R2: 0.9153), PNorm: 187.7072, GNorm: 0.5000
[31/299] timecost: 62.92, lr: 0.000100, Train: (LOSS: 0.0404, MAE: 0.0404, RMSE: 0.0647, R2: 0.9306), Valid: (LOSS: 0.0500, MAE: 0.0500, RMSE: 0.0732, R2: 0.9186), PNorm: 187.7219, GNorm: 0.5000
[32/299] timecost: 62.23, lr: 0.000100, Train: (LOSS: 0.0405, MAE: 0.0405, RMSE: 0.0637, R2: 0.9338), Valid: (LOSS: 0.0485, MAE: 0.0485, RMSE: 0.0736, R2: 0.9192), PNorm: 187.7351, GNorm: 0.5000
[33/299] timecost: 60.21, lr: 0.000100, Train: (LOSS: 0.0388, MAE: 0.0388, RMSE: 0.0620, R2: 0.9349), Valid: (LOSS: 0.0522, MAE: 0.0522, RMSE: 0.0797, R2: 0.9034), PNorm: 187.7480, GNorm: 0.5000
[34/299] timecost: 60.13, lr: 0.000100, Train: (LOSS: 0.0380, MAE: 0.0380, RMSE: 0.0594, R2: 0.9415), Valid: (LOSS: 0.0423, MAE: 0.0423, RMSE: 0.0648, R2: 0.9349), PNorm: 187.7596, GNorm: 0.5000
[35/299] timecost: 60.27, lr: 0.000100, Train: (LOSS: 0.0384, MAE: 0.0384, RMSE: 0.0614, R2: 0.9402), Valid: (LOSS: 0.0483, MAE: 0.0483, RMSE: 0.0691, R2: 0.9294), PNorm: 187.7745, GNorm: 0.5000
[36/299] timecost: 60.11, lr: 0.000100, Train: (LOSS: 0.0379, MAE: 0.0379, RMSE: 0.0593, R2: 0.9400), Valid: (LOSS: 0.0449, MAE: 0.0449, RMSE: 0.0681, R2: 0.9294), PNorm: 187.7885, GNorm: 0.4300
[37/299] timecost: 59.03, lr: 0.000100, Train: (LOSS: 0.0371, MAE: 0.0371, RMSE: 0.0589, R2: 0.9418), Valid: (LOSS: 0.0489, MAE: 0.0489, RMSE: 0.0731, R2: 0.9217), PNorm: 187.8028, GNorm: 0.4621
[38/299] timecost: 59.39, lr: 0.000100, Train: (LOSS: 0.0362, MAE: 0.0362, RMSE: 0.0575, R2: 0.9437), Valid: (LOSS: 0.0459, MAE: 0.0459, RMSE: 0.0682, R2: 0.9296), PNorm: 187.8151, GNorm: 0.5000
[39/299] timecost: 61.04, lr: 0.000100, Train: (LOSS: 0.0338, MAE: 0.0338, RMSE: 0.0547, R2: 0.9487), Valid: (LOSS: 0.0470, MAE: 0.0470, RMSE: 0.0716, R2: 0.9222), PNorm: 187.8269, GNorm: 0.5000
[40/299] timecost: 60.43, lr: 0.000100, Train: (LOSS: 0.0344, MAE: 0.0344, RMSE: 0.0547, R2: 0.9502), Valid: (LOSS: 0.0421, MAE: 0.0421, RMSE: 0.0641, R2: 0.9371), PNorm: 187.8381, GNorm: 0.5000
[41/299] timecost: 59.53, lr: 0.000100, Train: (LOSS: 0.0333, MAE: 0.0333, RMSE: 0.0529, R2: 0.9530), Valid: (LOSS: 0.0402, MAE: 0.0402, RMSE: 0.0613, R2: 0.9428), PNorm: 187.8497, GNorm: 0.5000
[42/299] timecost: 58.82, lr: 0.000100, Train: (LOSS: 0.0329, MAE: 0.0329, RMSE: 0.0522, R2: 0.9538), Valid: (LOSS: 0.0433, MAE: 0.0433, RMSE: 0.0658, R2: 0.9323), PNorm: 187.8633, GNorm: 0.5000
[43/299] timecost: 58.52, lr: 0.000100, Train: (LOSS: 0.0342, MAE: 0.0342, RMSE: 0.0548, R2: 0.9484), Valid: (LOSS: 0.0421, MAE: 0.0421, RMSE: 0.0670, R2: 0.9298), PNorm: 187.8790, GNorm: 0.4039
[44/299] timecost: 58.63, lr: 0.000100, Train: (LOSS: 0.0323, MAE: 0.0323, RMSE: 0.0516, R2: 0.9557), Valid: (LOSS: 0.0426, MAE: 0.0426, RMSE: 0.0667, R2: 0.9323), PNorm: 187.8887, GNorm: 0.5000
[45/299] timecost: 59.08, lr: 0.000100, Train: (LOSS: 0.0312, MAE: 0.0312, RMSE: 0.0502, R2: 0.9568), Valid: (LOSS: 0.0405, MAE: 0.0405, RMSE: 0.0617, R2: 0.9411), PNorm: 187.9003, GNorm: 0.5000
[46/299] timecost: 60.14, lr: 0.000100, Train: (LOSS: 0.0324, MAE: 0.0324, RMSE: 0.0518, R2: 0.9553), Valid: (LOSS: 0.0433, MAE: 0.0433, RMSE: 0.0669, R2: 0.9305), PNorm: 187.9132, GNorm: 0.5000
[47/299] timecost: 58.89, lr: 0.000100, Train: (LOSS: 0.0310, MAE: 0.0310, RMSE: 0.0498, R2: 0.9584), Valid: (LOSS: 0.0398, MAE: 0.0398, RMSE: 0.0618, R2: 0.9416), PNorm: 187.9256, GNorm: 0.5000
[48/299] timecost: 58.75, lr: 0.000100, Train: (LOSS: 0.0305, MAE: 0.0305, RMSE: 0.0493, R2: 0.9591), Valid: (LOSS: 0.0406, MAE: 0.0406, RMSE: 0.0616, R2: 0.9397), PNorm: 187.9375, GNorm: 0.3422
[49/299] timecost: 59.13, lr: 0.000100, Train: (LOSS: 0.0299, MAE: 0.0299, RMSE: 0.0476, R2: 0.9577), Valid: (LOSS: 0.0428, MAE: 0.0428, RMSE: 0.0649, R2: 0.9347), PNorm: 187.9493, GNorm: 0.4393
[50/299] timecost: 58.51, lr: 0.000100, Train: (LOSS: 0.0301, MAE: 0.0301, RMSE: 0.0483, R2: 0.9576), Valid: (LOSS: 0.0407, MAE: 0.0407, RMSE: 0.0605, R2: 0.9417), PNorm: 187.9602, GNorm: 0.5000
[51/299] timecost: 59.73, lr: 0.000100, Train: (LOSS: 0.0305, MAE: 0.0305, RMSE: 0.0493, R2: 0.9578), Valid: (LOSS: 0.0409, MAE: 0.0409, RMSE: 0.0612, R2: 0.9430), PNorm: 187.9753, GNorm: 0.5000
[52/299] timecost: 60.77, lr: 0.000100, Train: (LOSS: 0.0287, MAE: 0.0287, RMSE: 0.0472, R2: 0.9627), Valid: (LOSS: 0.0410, MAE: 0.0410, RMSE: 0.0626, R2: 0.9393), PNorm: 187.9857, GNorm: 0.5000
[53/299] timecost: 61.15, lr: 0.000100, Train: (LOSS: 0.0276, MAE: 0.0276, RMSE: 0.0454, R2: 0.9644), Valid: (LOSS: 0.0413, MAE: 0.0413, RMSE: 0.0646, R2: 0.9352), PNorm: 187.9966, GNorm: 0.4505
[54/299] timecost: 61.59, lr: 0.000100, Train: (LOSS: 0.0283, MAE: 0.0283, RMSE: 0.0455, R2: 0.9639), Valid: (LOSS: 0.0404, MAE: 0.0404, RMSE: 0.0618, R2: 0.9407), PNorm: 188.0090, GNorm: 0.5000
[55/299] timecost: 61.55, lr: 0.000100, Train: (LOSS: 0.0287, MAE: 0.0287, RMSE: 0.0469, R2: 0.9630), Valid: (LOSS: 0.0394, MAE: 0.0394, RMSE: 0.0599, R2: 0.9442), PNorm: 188.0217, GNorm: 0.4898
[56/299] timecost: 61.58, lr: 0.000100, Train: (LOSS: 0.0277, MAE: 0.0277, RMSE: 0.0456, R2: 0.9647), Valid: (LOSS: 0.0398, MAE: 0.0398, RMSE: 0.0611, R2: 0.9415), PNorm: 188.0330, GNorm: 0.3933
[57/299] timecost: 62.96, lr: 0.000100, Train: (LOSS: 0.0271, MAE: 0.0271, RMSE: 0.0441, R2: 0.9650), Valid: (LOSS: 0.0396, MAE: 0.0396, RMSE: 0.0610, R2: 0.9420), PNorm: 188.0440, GNorm: 0.5000
[58/299] timecost: 63.11, lr: 0.000100, Train: (LOSS: 0.0281, MAE: 0.0281, RMSE: 0.0463, R2: 0.9631), Valid: (LOSS: 0.0384, MAE: 0.0384, RMSE: 0.0596, R2: 0.9449), PNorm: 188.0578, GNorm: 0.3994
[59/299] timecost: 61.48, lr: 0.000100, Train: (LOSS: 0.0282, MAE: 0.0282, RMSE: 0.0457, R2: 0.9656), Valid: (LOSS: 0.0380, MAE: 0.0380, RMSE: 0.0580, R2: 0.9469), PNorm: 188.0705, GNorm: 0.4200
[60/299] timecost: 61.98, lr: 0.000100, Train: (LOSS: 0.0261, MAE: 0.0261, RMSE: 0.0433, R2: 0.9665), Valid: (LOSS: 0.0387, MAE: 0.0387, RMSE: 0.0596, R2: 0.9445), PNorm: 188.0793, GNorm: 0.5000
[61/299] timecost: 62.50, lr: 0.000100, Train: (LOSS: 0.0269, MAE: 0.0269, RMSE: 0.0440, R2: 0.9661), Valid: (LOSS: 0.0399, MAE: 0.0399, RMSE: 0.0608, R2: 0.9430), PNorm: 188.0923, GNorm: 0.5000
[62/299] timecost: 62.62, lr: 0.000100, Train: (LOSS: 0.0271, MAE: 0.0271, RMSE: 0.0438, R2: 0.9668), Valid: (LOSS: 0.0415, MAE: 0.0415, RMSE: 0.0633, R2: 0.9382), PNorm: 188.1047, GNorm: 0.4915
[63/299] timecost: 62.34, lr: 0.000100, Train: (LOSS: 0.0263, MAE: 0.0263, RMSE: 0.0425, R2: 0.9689), Valid: (LOSS: 0.0398, MAE: 0.0398, RMSE: 0.0626, R2: 0.9380), PNorm: 188.1186, GNorm: 0.4345
[64/299] timecost: 61.49, lr: 0.000100, Train: (LOSS: 0.0258, MAE: 0.0258, RMSE: 0.0415, R2: 0.9705), Valid: (LOSS: 0.0390, MAE: 0.0390, RMSE: 0.0570, R2: 0.9497), PNorm: 188.1270, GNorm: 0.4143
[65/299] timecost: 60.56, lr: 0.000100, Train: (LOSS: 0.0253, MAE: 0.0253, RMSE: 0.0415, R2: 0.9686), Valid: (LOSS: 0.0398, MAE: 0.0398, RMSE: 0.0595, R2: 0.9439), PNorm: 188.1399, GNorm: 0.4720
[66/299] timecost: 60.72, lr: 0.000100, Train: (LOSS: 0.0253, MAE: 0.0253, RMSE: 0.0414, R2: 0.9700), Valid: (LOSS: 0.0384, MAE: 0.0384, RMSE: 0.0580, R2: 0.9471), PNorm: 188.1523, GNorm: 0.5000
[67/299] timecost: 61.02, lr: 0.000100, Train: (LOSS: 0.0252, MAE: 0.0252, RMSE: 0.0413, R2: 0.9703), Valid: (LOSS: 0.0378, MAE: 0.0378, RMSE: 0.0588, R2: 0.9445), PNorm: 188.1627, GNorm: 0.3871
[68/299] timecost: 60.20, lr: 0.000100, Train: (LOSS: 0.0235, MAE: 0.0235, RMSE: 0.0385, R2: 0.9737), Valid: (LOSS: 0.0386, MAE: 0.0386, RMSE: 0.0592, R2: 0.9450), PNorm: 188.1724, GNorm: 0.5000
[69/299] timecost: 60.48, lr: 0.000100, Train: (LOSS: 0.0250, MAE: 0.0250, RMSE: 0.0403, R2: 0.9696), Valid: (LOSS: 0.0397, MAE: 0.0397, RMSE: 0.0617, R2: 0.9406), PNorm: 188.1867, GNorm: 0.4422
[70/299] timecost: 60.37, lr: 0.000100, Train: (LOSS: 0.0249, MAE: 0.0249, RMSE: 0.0407, R2: 0.9700), Valid: (LOSS: 0.0367, MAE: 0.0367, RMSE: 0.0591, R2: 0.9463), PNorm: 188.1978, GNorm: 0.4779
[71/299] timecost: 59.50, lr: 0.000100, Train: (LOSS: 0.0238, MAE: 0.0238, RMSE: 0.0385, R2: 0.9708), Valid: (LOSS: 0.0398, MAE: 0.0398, RMSE: 0.0608, R2: 0.9416), PNorm: 188.2097, GNorm: 0.4504
[72/299] timecost: 59.16, lr: 0.000100, Train: (LOSS: 0.0234, MAE: 0.0234, RMSE: 0.0382, R2: 0.9717), Valid: (LOSS: 0.0388, MAE: 0.0388, RMSE: 0.0617, R2: 0.9381), PNorm: 188.2206, GNorm: 0.4860
[73/299] timecost: 59.12, lr: 0.000100, Train: (LOSS: 0.0235, MAE: 0.0235, RMSE: 0.0391, R2: 0.9722), Valid: (LOSS: 0.0393, MAE: 0.0393, RMSE: 0.0605, R2: 0.9431), PNorm: 188.2351, GNorm: 0.4438
[74/299] timecost: 58.88, lr: 0.000100, Train: (LOSS: 0.0232, MAE: 0.0232, RMSE: 0.0380, R2: 0.9756), Valid: (LOSS: 0.0366, MAE: 0.0366, RMSE: 0.0561, R2: 0.9511), PNorm: 188.2496, GNorm: 0.4450
[75/299] timecost: 58.85, lr: 0.000100, Train: (LOSS: 0.0229, MAE: 0.0229, RMSE: 0.0379, R2: 0.9740), Valid: (LOSS: 0.0395, MAE: 0.0395, RMSE: 0.0616, R2: 0.9365), PNorm: 188.2637, GNorm: 0.5000
[76/299] timecost: 59.07, lr: 0.000100, Train: (LOSS: 0.0238, MAE: 0.0238, RMSE: 0.0386, R2: 0.9738), Valid: (LOSS: 0.0383, MAE: 0.0383, RMSE: 0.0581, R2: 0.9480), PNorm: 188.2759, GNorm: 0.4177
[77/299] timecost: 58.90, lr: 0.000100, Train: (LOSS: 0.0237, MAE: 0.0237, RMSE: 0.0380, R2: 0.9737), Valid: (LOSS: 0.0378, MAE: 0.0378, RMSE: 0.0591, R2: 0.9447), PNorm: 188.2877, GNorm: 0.4373
[78/299] timecost: 59.28, lr: 0.000100, Train: (LOSS: 0.0231, MAE: 0.0231, RMSE: 0.0378, R2: 0.9755), Valid: (LOSS: 0.0368, MAE: 0.0368, RMSE: 0.0571, R2: 0.9492), PNorm: 188.3009, GNorm: 0.4531
[79/299] timecost: 60.75, lr: 0.000100, Train: (LOSS: 0.0234, MAE: 0.0234, RMSE: 0.0374, R2: 0.9751), Valid: (LOSS: 0.0362, MAE: 0.0362, RMSE: 0.0550, R2: 0.9520), PNorm: 188.3150, GNorm: 0.5000
[80/299] timecost: 60.77, lr: 0.000100, Train: (LOSS: 0.0225, MAE: 0.0225, RMSE: 0.0362, R2: 0.9767), Valid: (LOSS: 0.0370, MAE: 0.0370, RMSE: 0.0566, R2: 0.9503), PNorm: 188.3275, GNorm: 0.5000
[81/299] timecost: 60.95, lr: 0.000100, Train: (LOSS: 0.0223, MAE: 0.0223, RMSE: 0.0351, R2: 0.9788), Valid: (LOSS: 0.0367, MAE: 0.0367, RMSE: 0.0560, R2: 0.9511), PNorm: 188.3391, GNorm: 0.5000
[82/299] timecost: 60.65, lr: 0.000100, Train: (LOSS: 0.0223, MAE: 0.0223, RMSE: 0.0356, R2: 0.9776), Valid: (LOSS: 0.0402, MAE: 0.0402, RMSE: 0.0633, R2: 0.9369), PNorm: 188.3510, GNorm: 0.4310
[83/299] timecost: 60.84, lr: 0.000100, Train: (LOSS: 0.0220, MAE: 0.0220, RMSE: 0.0352, R2: 0.9775), Valid: (LOSS: 0.0380, MAE: 0.0380, RMSE: 0.0557, R2: 0.9531), PNorm: 188.3648, GNorm: 0.4655
[84/299] timecost: 60.77, lr: 0.000100, Train: (LOSS: 0.0215, MAE: 0.0215, RMSE: 0.0350, R2: 0.9789), Valid: (LOSS: 0.0387, MAE: 0.0387, RMSE: 0.0598, R2: 0.9441), PNorm: 188.3752, GNorm: 0.5000
[85/299] timecost: 60.83, lr: 0.000100, Train: (LOSS: 0.0212, MAE: 0.0212, RMSE: 0.0350, R2: 0.9788), Valid: (LOSS: 0.0386, MAE: 0.0386, RMSE: 0.0588, R2: 0.9460), PNorm: 188.3862, GNorm: 0.4901
[86/299] timecost: 60.93, lr: 0.000100, Train: (LOSS: 0.0214, MAE: 0.0214, RMSE: 0.0344, R2: 0.9788), Valid: (LOSS: 0.0367, MAE: 0.0367, RMSE: 0.0562, R2: 0.9507), PNorm: 188.3971, GNorm: 0.4422
[87/299] timecost: 60.74, lr: 0.000100, Train: (LOSS: 0.0211, MAE: 0.0211, RMSE: 0.0335, R2: 0.9803), Valid: (LOSS: 0.0362, MAE: 0.0362, RMSE: 0.0580, R2: 0.9463), PNorm: 188.4116, GNorm: 0.3571
[88/299] timecost: 60.92, lr: 0.000100, Train: (LOSS: 0.0210, MAE: 0.0210, RMSE: 0.0338, R2: 0.9807), Valid: (LOSS: 0.0351, MAE: 0.0351, RMSE: 0.0537, R2: 0.9545), PNorm: 188.4230, GNorm: 0.4571
[89/299] timecost: 59.68, lr: 0.000100, Train: (LOSS: 0.0217, MAE: 0.0217, RMSE: 0.0346, R2: 0.9791), Valid: (LOSS: 0.0377, MAE: 0.0377, RMSE: 0.0576, R2: 0.9465), PNorm: 188.4349, GNorm: 0.4131
[90/299] timecost: 58.87, lr: 0.000100, Train: (LOSS: 0.0205, MAE: 0.0205, RMSE: 0.0333, R2: 0.9812), Valid: (LOSS: 0.0368, MAE: 0.0368, RMSE: 0.0589, R2: 0.9449), PNorm: 188.4453, GNorm: 0.3852
[91/299] timecost: 58.61, lr: 0.000100, Train: (LOSS: 0.0197, MAE: 0.0197, RMSE: 0.0316, R2: 0.9819), Valid: (LOSS: 0.0358, MAE: 0.0358, RMSE: 0.0556, R2: 0.9519), PNorm: 188.4594, GNorm: 0.3766
[92/299] timecost: 58.69, lr: 0.000100, Train: (LOSS: 0.0202, MAE: 0.0202, RMSE: 0.0324, R2: 0.9825), Valid: (LOSS: 0.0370, MAE: 0.0370, RMSE: 0.0583, R2: 0.9456), PNorm: 188.4707, GNorm: 0.3822
[93/299] timecost: 58.91, lr: 0.000100, Train: (LOSS: 0.0201, MAE: 0.0201, RMSE: 0.0328, R2: 0.9813), Valid: (LOSS: 0.0378, MAE: 0.0378, RMSE: 0.0587, R2: 0.9448), PNorm: 188.4827, GNorm: 0.4238
[94/299] timecost: 58.73, lr: 0.000100, Train: (LOSS: 0.0202, MAE: 0.0202, RMSE: 0.0327, R2: 0.9816), Valid: (LOSS: 0.0378, MAE: 0.0378, RMSE: 0.0578, R2: 0.9471), PNorm: 188.4934, GNorm: 0.5000
[95/299] timecost: 58.41, lr: 0.000100, Train: (LOSS: 0.0197, MAE: 0.0197, RMSE: 0.0319, R2: 0.9823), Valid: (LOSS: 0.0349, MAE: 0.0349, RMSE: 0.0552, R2: 0.9506), PNorm: 188.5039, GNorm: 0.3833
[96/299] timecost: 58.91, lr: 0.000100, Train: (LOSS: 0.0187, MAE: 0.0187, RMSE: 0.0303, R2: 0.9833), Valid: (LOSS: 0.0373, MAE: 0.0373, RMSE: 0.0575, R2: 0.9460), PNorm: 188.5158, GNorm: 0.4762
[97/299] timecost: 58.39, lr: 0.000100, Train: (LOSS: 0.0189, MAE: 0.0189, RMSE: 0.0308, R2: 0.9831), Valid: (LOSS: 0.0371, MAE: 0.0371, RMSE: 0.0553, R2: 0.9504), PNorm: 188.5277, GNorm: 0.4019
[98/299] timecost: 58.70, lr: 0.000100, Train: (LOSS: 0.0194, MAE: 0.0194, RMSE: 0.0305, R2: 0.9839), Valid: (LOSS: 0.0369, MAE: 0.0369, RMSE: 0.0580, R2: 0.9459), PNorm: 188.5427, GNorm: 0.5000
[99/299] timecost: 58.50, lr: 0.000100, Train: (LOSS: 0.0186, MAE: 0.0186, RMSE: 0.0303, R2: 0.9837), Valid: (LOSS: 0.0369, MAE: 0.0369, RMSE: 0.0576, R2: 0.9465), PNorm: 188.5557, GNorm: 0.5000
[100/299] timecost: 58.85, lr: 0.000100, Train: (LOSS: 0.0198, MAE: 0.0198, RMSE: 0.0319, R2: 0.9825), Valid: (LOSS: 0.0371, MAE: 0.0371, RMSE: 0.0581, R2: 0.9451), PNorm: 188.5727, GNorm: 0.4478
[101/299] timecost: 58.94, lr: 0.000100, Train: (LOSS: 0.0189, MAE: 0.0189, RMSE: 0.0301, R2: 0.9846), Valid: (LOSS: 0.0357, MAE: 0.0357, RMSE: 0.0568, R2: 0.9495), PNorm: 188.5839, GNorm: 0.3830
[102/299] timecost: 60.15, lr: 0.000100, Train: (LOSS: 0.0185, MAE: 0.0185, RMSE: 0.0298, R2: 0.9846), Valid: (LOSS: 0.0366, MAE: 0.0366, RMSE: 0.0566, R2: 0.9477), PNorm: 188.5928, GNorm: 0.4358
[103/299] timecost: 60.65, lr: 0.000100, Train: (LOSS: 0.0185, MAE: 0.0185, RMSE: 0.0298, R2: 0.9843), Valid: (LOSS: 0.0370, MAE: 0.0370, RMSE: 0.0586, R2: 0.9444), PNorm: 188.6038, GNorm: 0.5000
Epoch 00105: reducing learning rate of group 0 to 8.5000e-05.
[104/299] timecost: 59.87, lr: 0.000085, Train: (LOSS: 0.0183, MAE: 0.0183, RMSE: 0.0293, R2: 0.9848), Valid: (LOSS: 0.0382, MAE: 0.0382, RMSE: 0.0597, R2: 0.9443), PNorm: 188.6130, GNorm: 0.4287
[105/299] timecost: 60.27, lr: 0.000085, Train: (LOSS: 0.0174, MAE: 0.0174, RMSE: 0.0283, R2: 0.9856), Valid: (LOSS: 0.0370, MAE: 0.0370, RMSE: 0.0576, R2: 0.9473), PNorm: 188.6210, GNorm: 0.5000
[106/299] timecost: 60.57, lr: 0.000085, Train: (LOSS: 0.0172, MAE: 0.0172, RMSE: 0.0280, R2: 0.9861), Valid: (LOSS: 0.0370, MAE: 0.0370, RMSE: 0.0586, R2: 0.9444), PNorm: 188.6299, GNorm: 0.2785
[107/299] timecost: 60.49, lr: 0.000085, Train: (LOSS: 0.0174, MAE: 0.0174, RMSE: 0.0277, R2: 0.9874), Valid: (LOSS: 0.0347, MAE: 0.0347, RMSE: 0.0545, R2: 0.9540), PNorm: 188.6384, GNorm: 0.3119
[108/299] timecost: 61.63, lr: 0.000085, Train: (LOSS: 0.0168, MAE: 0.0168, RMSE: 0.0275, R2: 0.9865), Valid: (LOSS: 0.0376, MAE: 0.0376, RMSE: 0.0580, R2: 0.9458), PNorm: 188.6452, GNorm: 0.5000
[109/299] timecost: 62.00, lr: 0.000085, Train: (LOSS: 0.0165, MAE: 0.0165, RMSE: 0.0266, R2: 0.9873), Valid: (LOSS: 0.0361, MAE: 0.0361, RMSE: 0.0563, R2: 0.9496), PNorm: 188.6544, GNorm: 0.3292
[110/299] timecost: 61.85, lr: 0.000085, Train: (LOSS: 0.0162, MAE: 0.0162, RMSE: 0.0266, R2: 0.9878), Valid: (LOSS: 0.0360, MAE: 0.0360, RMSE: 0.0579, R2: 0.9478), PNorm: 188.6636, GNorm: 0.4536
[111/299] timecost: 61.73, lr: 0.000085, Train: (LOSS: 0.0161, MAE: 0.0161, RMSE: 0.0264, R2: 0.9878), Valid: (LOSS: 0.0357, MAE: 0.0357, RMSE: 0.0568, R2: 0.9483), PNorm: 188.6717, GNorm: 0.5000
[112/299] timecost: 60.90, lr: 0.000085, Train: (LOSS: 0.0159, MAE: 0.0159, RMSE: 0.0254, R2: 0.9881), Valid: (LOSS: 0.0369, MAE: 0.0369, RMSE: 0.0565, R2: 0.9513), PNorm: 188.6803, GNorm: 0.4111
[113/299] timecost: 59.67, lr: 0.000085, Train: (LOSS: 0.0152, MAE: 0.0152, RMSE: 0.0254, R2: 0.9893), Valid: (LOSS: 0.0360, MAE: 0.0360, RMSE: 0.0560, R2: 0.9500), PNorm: 188.6877, GNorm: 0.4068
[114/299] timecost: 59.79, lr: 0.000085, Train: (LOSS: 0.0151, MAE: 0.0151, RMSE: 0.0244, R2: 0.9890), Valid: (LOSS: 0.0369, MAE: 0.0369, RMSE: 0.0576, R2: 0.9467), PNorm: 188.6938, GNorm: 0.5000
[115/299] timecost: 59.75, lr: 0.000085, Train: (LOSS: 0.0152, MAE: 0.0152, RMSE: 0.0244, R2: 0.9892), Valid: (LOSS: 0.0368, MAE: 0.0368, RMSE: 0.0587, R2: 0.9445), PNorm: 188.7017, GNorm: 0.5000
[116/299] timecost: 59.40, lr: 0.000085, Train: (LOSS: 0.0156, MAE: 0.0156, RMSE: 0.0251, R2: 0.9888), Valid: (LOSS: 0.0366, MAE: 0.0366, RMSE: 0.0575, R2: 0.9473), PNorm: 188.7111, GNorm: 0.4961
[117/299] timecost: 59.75, lr: 0.000085, Train: (LOSS: 0.0152, MAE: 0.0152, RMSE: 0.0247, R2: 0.9890), Valid: (LOSS: 0.0346, MAE: 0.0346, RMSE: 0.0540, R2: 0.9537), PNorm: 188.7202, GNorm: 0.5000
[118/299] timecost: 60.24, lr: 0.000085, Train: (LOSS: 0.0143, MAE: 0.0143, RMSE: 0.0239, R2: 0.9897), Valid: (LOSS: 0.0360, MAE: 0.0360, RMSE: 0.0587, R2: 0.9445), PNorm: 188.7281, GNorm: 0.5000
[119/299] timecost: 62.29, lr: 0.000085, Train: (LOSS: 0.0145, MAE: 0.0145, RMSE: 0.0234, R2: 0.9898), Valid: (LOSS: 0.0360, MAE: 0.0360, RMSE: 0.0564, R2: 0.9489), PNorm: 188.7337, GNorm: 0.4626
[120/299] timecost: 62.81, lr: 0.000085, Train: (LOSS: 0.0149, MAE: 0.0149, RMSE: 0.0241, R2: 0.9899), Valid: (LOSS: 0.0373, MAE: 0.0373, RMSE: 0.0570, R2: 0.9488), PNorm: 188.7431, GNorm: 0.4480
[121/299] timecost: 61.26, lr: 0.000085, Train: (LOSS: 0.0153, MAE: 0.0153, RMSE: 0.0245, R2: 0.9891), Valid: (LOSS: 0.0383, MAE: 0.0383, RMSE: 0.0594, R2: 0.9432), PNorm: 188.7505, GNorm: 0.3992
[122/299] timecost: 59.80, lr: 0.000085, Train: (LOSS: 0.0151, MAE: 0.0151, RMSE: 0.0246, R2: 0.9895), Valid: (LOSS: 0.0363, MAE: 0.0363, RMSE: 0.0581, R2: 0.9461), PNorm: 188.7613, GNorm: 0.3799
Epoch 00124: reducing learning rate of group 0 to 7.2250e-05.
[123/299] timecost: 59.99, lr: 0.000072, Train: (LOSS: 0.0150, MAE: 0.0150, RMSE: 0.0242, R2: 0.9882), Valid: (LOSS: 0.0358, MAE: 0.0358, RMSE: 0.0564, R2: 0.9503), PNorm: 188.7702, GNorm: 0.3978
[124/299] timecost: 62.78, lr: 0.000072, Train: (LOSS: 0.0139, MAE: 0.0139, RMSE: 0.0234, R2: 0.9899), Valid: (LOSS: 0.0362, MAE: 0.0362, RMSE: 0.0572, R2: 0.9475), PNorm: 188.7783, GNorm: 0.4392
[125/299] timecost: 62.79, lr: 0.000072, Train: (LOSS: 0.0136, MAE: 0.0136, RMSE: 0.0228, R2: 0.9903), Valid: (LOSS: 0.0356, MAE: 0.0356, RMSE: 0.0558, R2: 0.9506), PNorm: 188.7839, GNorm: 0.4216
[126/299] timecost: 62.75, lr: 0.000072, Train: (LOSS: 0.0132, MAE: 0.0132, RMSE: 0.0226, R2: 0.9904), Valid: (LOSS: 0.0365, MAE: 0.0365, RMSE: 0.0589, R2: 0.9438), PNorm: 188.7895, GNorm: 0.4547
[127/299] timecost: 62.31, lr: 0.000072, Train: (LOSS: 0.0132, MAE: 0.0132, RMSE: 0.0225, R2: 0.9909), Valid: (LOSS: 0.0361, MAE: 0.0361, RMSE: 0.0579, R2: 0.9454), PNorm: 188.7949, GNorm: 0.4923
[128/299] timecost: 62.67, lr: 0.000072, Train: (LOSS: 0.0131, MAE: 0.0131, RMSE: 0.0221, R2: 0.9906), Valid: (LOSS: 0.0364, MAE: 0.0364, RMSE: 0.0576, R2: 0.9473), PNorm: 188.8022, GNorm: 0.3201
[129/299] timecost: 62.76, lr: 0.000072, Train: (LOSS: 0.0132, MAE: 0.0132, RMSE: 0.0220, R2: 0.9910), Valid: (LOSS: 0.0351, MAE: 0.0351, RMSE: 0.0557, R2: 0.9486), PNorm: 188.8083, GNorm: 0.5000
[130/299] timecost: 62.27, lr: 0.000072, Train: (LOSS: 0.0129, MAE: 0.0129, RMSE: 0.0216, R2: 0.9905), Valid: (LOSS: 0.0355, MAE: 0.0355, RMSE: 0.0562, R2: 0.9486), PNorm: 188.8133, GNorm: 0.4426
[131/299] timecost: 62.51, lr: 0.000072, Train: (LOSS: 0.0126, MAE: 0.0126, RMSE: 0.0222, R2: 0.9908), Valid: (LOSS: 0.0367, MAE: 0.0367, RMSE: 0.0580, R2: 0.9458), PNorm: 188.8198, GNorm: 0.3800
[132/299] timecost: 62.66, lr: 0.000072, Train: (LOSS: 0.0132, MAE: 0.0132, RMSE: 0.0223, R2: 0.9915), Valid: (LOSS: 0.0355, MAE: 0.0355, RMSE: 0.0550, R2: 0.9520), PNorm: 188.8274, GNorm: 0.3331
[133/299] timecost: 60.36, lr: 0.000072, Train: (LOSS: 0.0125, MAE: 0.0125, RMSE: 0.0217, R2: 0.9911), Valid: (LOSS: 0.0354, MAE: 0.0354, RMSE: 0.0565, R2: 0.9490), PNorm: 188.8336, GNorm: 0.5000
[134/299] timecost: 60.66, lr: 0.000072, Train: (LOSS: 0.0132, MAE: 0.0132, RMSE: 0.0221, R2: 0.9912), Valid: (LOSS: 0.0363, MAE: 0.0363, RMSE: 0.0570, R2: 0.9471), PNorm: 188.8412, GNorm: 0.3346
[135/299] timecost: 61.88, lr: 0.000072, Train: (LOSS: 0.0126, MAE: 0.0126, RMSE: 0.0211, R2: 0.9911), Valid: (LOSS: 0.0359, MAE: 0.0359, RMSE: 0.0565, R2: 0.9485), PNorm: 188.8466, GNorm: 0.4742
[136/299] timecost: 62.75, lr: 0.000072, Train: (LOSS: 0.0121, MAE: 0.0121, RMSE: 0.0209, R2: 0.9914), Valid: (LOSS: 0.0356, MAE: 0.0356, RMSE: 0.0554, R2: 0.9492), PNorm: 188.8537, GNorm: 0.4435
[137/299] timecost: 62.42, lr: 0.000072, Train: (LOSS: 0.0124, MAE: 0.0124, RMSE: 0.0211, R2: 0.9915), Valid: (LOSS: 0.0357, MAE: 0.0357, RMSE: 0.0579, R2: 0.9461), PNorm: 188.8592, GNorm: 0.3476
[138/299] timecost: 62.56, lr: 0.000072, Train: (LOSS: 0.0126, MAE: 0.0126, RMSE: 0.0215, R2: 0.9909), Valid: (LOSS: 0.0358, MAE: 0.0358, RMSE: 0.0563, R2: 0.9490), PNorm: 188.8662, GNorm: 0.4669
Epoch 00140: reducing learning rate of group 0 to 6.1413e-05.
[139/299] timecost: 62.41, lr: 0.000061, Train: (LOSS: 0.0129, MAE: 0.0129, RMSE: 0.0228, R2: 0.9898), Valid: (LOSS: 0.0360, MAE: 0.0360, RMSE: 0.0570, R2: 0.9479), PNorm: 188.8746, GNorm: 0.4709
[140/299] timecost: 62.91, lr: 0.000061, Train: (LOSS: 0.0123, MAE: 0.0123, RMSE: 0.0211, R2: 0.9910), Valid: (LOSS: 0.0348, MAE: 0.0348, RMSE: 0.0552, R2: 0.9514), PNorm: 188.8788, GNorm: 0.4961
[141/299] timecost: 62.42, lr: 0.000061, Train: (LOSS: 0.0120, MAE: 0.0120, RMSE: 0.0207, R2: 0.9914), Valid: (LOSS: 0.0350, MAE: 0.0350, RMSE: 0.0550, R2: 0.9516), PNorm: 188.8840, GNorm: 0.4167
[142/299] timecost: 62.88, lr: 0.000061, Train: (LOSS: 0.0116, MAE: 0.0116, RMSE: 0.0208, R2: 0.9919), Valid: (LOSS: 0.0353, MAE: 0.0353, RMSE: 0.0554, R2: 0.9509), PNorm: 188.8875, GNorm: 0.4183
[143/299] timecost: 62.46, lr: 0.000061, Train: (LOSS: 0.0111, MAE: 0.0111, RMSE: 0.0196, R2: 0.9924), Valid: (LOSS: 0.0366, MAE: 0.0366, RMSE: 0.0579, R2: 0.9465), PNorm: 188.8932, GNorm: 0.4567
[144/299] timecost: 62.71, lr: 0.000061, Train: (LOSS: 0.0112, MAE: 0.0112, RMSE: 0.0197, R2: 0.9919), Valid: (LOSS: 0.0346, MAE: 0.0346, RMSE: 0.0548, R2: 0.9517), PNorm: 188.8968, GNorm: 0.3789
[145/299] timecost: 62.81, lr: 0.000061, Train: (LOSS: 0.0109, MAE: 0.0109, RMSE: 0.0194, R2: 0.9922), Valid: (LOSS: 0.0356, MAE: 0.0356, RMSE: 0.0564, R2: 0.9489), PNorm: 188.9012, GNorm: 0.3782
[146/299] timecost: 60.70, lr: 0.000061, Train: (LOSS: 0.0113, MAE: 0.0113, RMSE: 0.0202, R2: 0.9923), Valid: (LOSS: 0.0352, MAE: 0.0352, RMSE: 0.0551, R2: 0.9506), PNorm: 188.9085, GNorm: 0.3627
[147/299] timecost: 62.98, lr: 0.000061, Train: (LOSS: 0.0117, MAE: 0.0117, RMSE: 0.0207, R2: 0.9917), Valid: (LOSS: 0.0349, MAE: 0.0349, RMSE: 0.0558, R2: 0.9500), PNorm: 188.9125, GNorm: 0.5000
[148/299] timecost: 62.75, lr: 0.000061, Train: (LOSS: 0.0112, MAE: 0.0112, RMSE: 0.0202, R2: 0.9922), Valid: (LOSS: 0.0350, MAE: 0.0350, RMSE: 0.0548, R2: 0.9518), PNorm: 188.9183, GNorm: 0.3883
[149/299] timecost: 62.70, lr: 0.000061, Train: (LOSS: 0.0114, MAE: 0.0114, RMSE: 0.0202, R2: 0.9922), Valid: (LOSS: 0.0349, MAE: 0.0349, RMSE: 0.0541, R2: 0.9530), PNorm: 188.9232, GNorm: 0.4522
[150/299] timecost: 63.17, lr: 0.000061, Train: (LOSS: 0.0113, MAE: 0.0113, RMSE: 0.0198, R2: 0.9919), Valid: (LOSS: 0.0357, MAE: 0.0357, RMSE: 0.0564, R2: 0.9480), PNorm: 188.9292, GNorm: 0.4729
[151/299] timecost: 62.61, lr: 0.000061, Train: (LOSS: 0.0111, MAE: 0.0111, RMSE: 0.0197, R2: 0.9925), Valid: (LOSS: 0.0349, MAE: 0.0349, RMSE: 0.0551, R2: 0.9509), PNorm: 188.9349, GNorm: 0.4847
[152/299] timecost: 62.64, lr: 0.000061, Train: (LOSS: 0.0107, MAE: 0.0107, RMSE: 0.0191, R2: 0.9924), Valid: (LOSS: 0.0351, MAE: 0.0351, RMSE: 0.0563, R2: 0.9482), PNorm: 188.9403, GNorm: 0.4062
[153/299] timecost: 62.61, lr: 0.000061, Train: (LOSS: 0.0108, MAE: 0.0108, RMSE: 0.0195, R2: 0.9925), Valid: (LOSS: 0.0355, MAE: 0.0355, RMSE: 0.0555, R2: 0.9507), PNorm: 188.9449, GNorm: 0.4371
[154/299] timecost: 62.73, lr: 0.000061, Train: (LOSS: 0.0110, MAE: 0.0110, RMSE: 0.0195, R2: 0.9918), Valid: (LOSS: 0.0347, MAE: 0.0347, RMSE: 0.0552, R2: 0.9506), PNorm: 188.9517, GNorm: 0.4384
Epoch 00156: reducing learning rate of group 0 to 5.2201e-05.
[155/299] timecost: 62.74, lr: 0.000052, Train: (LOSS: 0.0109, MAE: 0.0109, RMSE: 0.0194, R2: 0.9923), Valid: (LOSS: 0.0356, MAE: 0.0356, RMSE: 0.0563, R2: 0.9489), PNorm: 188.9572, GNorm: 0.4968
[156/299] timecost: 62.42, lr: 0.000052, Train: (LOSS: 0.0106, MAE: 0.0106, RMSE: 0.0195, R2: 0.9922), Valid: (LOSS: 0.0351, MAE: 0.0351, RMSE: 0.0553, R2: 0.9501), PNorm: 188.9615, GNorm: 0.3050
[157/299] timecost: 62.61, lr: 0.000052, Train: (LOSS: 0.0104, MAE: 0.0104, RMSE: 0.0195, R2: 0.9929), Valid: (LOSS: 0.0361, MAE: 0.0361, RMSE: 0.0565, R2: 0.9484), PNorm: 188.9638, GNorm: 0.4584
[158/299] timecost: 61.88, lr: 0.000052, Train: (LOSS: 0.0105, MAE: 0.0105, RMSE: 0.0193, R2: 0.9927), Valid: (LOSS: 0.0355, MAE: 0.0355, RMSE: 0.0568, R2: 0.9475), PNorm: 188.9697, GNorm: 0.4369
[159/299] timecost: 62.10, lr: 0.000052, Train: (LOSS: 0.0103, MAE: 0.0103, RMSE: 0.0189, R2: 0.9926), Valid: (LOSS: 0.0345, MAE: 0.0345, RMSE: 0.0533, R2: 0.9534), PNorm: 188.9737, GNorm: 0.4021
[160/299] timecost: 62.27, lr: 0.000052, Train: (LOSS: 0.0100, MAE: 0.0100, RMSE: 0.0187, R2: 0.9926), Valid: (LOSS: 0.0353, MAE: 0.0353, RMSE: 0.0544, R2: 0.9524), PNorm: 188.9780, GNorm: 0.3629
[161/299] timecost: 61.78, lr: 0.000052, Train: (LOSS: 0.0099, MAE: 0.0099, RMSE: 0.0182, R2: 0.9924), Valid: (LOSS: 0.0353, MAE: 0.0353, RMSE: 0.0551, R2: 0.9519), PNorm: 188.9825, GNorm: 0.3421
[162/299] timecost: 62.03, lr: 0.000052, Train: (LOSS: 0.0096, MAE: 0.0096, RMSE: 0.0183, R2: 0.9928), Valid: (LOSS: 0.0356, MAE: 0.0356, RMSE: 0.0561, R2: 0.9495), PNorm: 188.9859, GNorm: 0.3614
[163/299] timecost: 62.11, lr: 0.000052, Train: (LOSS: 0.0096, MAE: 0.0096, RMSE: 0.0181, R2: 0.9932), Valid: (LOSS: 0.0349, MAE: 0.0349, RMSE: 0.0554, R2: 0.9492), PNorm: 188.9893, GNorm: 0.3577
[164/299] timecost: 62.03, lr: 0.000052, Train: (LOSS: 0.0096, MAE: 0.0096, RMSE: 0.0182, R2: 0.9934), Valid: (LOSS: 0.0360, MAE: 0.0360, RMSE: 0.0567, R2: 0.9485), PNorm: 188.9946, GNorm: 0.3220
[165/299] timecost: 62.12, lr: 0.000052, Train: (LOSS: 0.0098, MAE: 0.0098, RMSE: 0.0180, R2: 0.9930), Valid: (LOSS: 0.0347, MAE: 0.0347, RMSE: 0.0547, R2: 0.9517), PNorm: 188.9985, GNorm: 0.3564
[166/299] timecost: 62.30, lr: 0.000052, Train: (LOSS: 0.0095, MAE: 0.0095, RMSE: 0.0178, R2: 0.9927), Valid: (LOSS: 0.0344, MAE: 0.0344, RMSE: 0.0550, R2: 0.9514), PNorm: 189.0031, GNorm: 0.3295
[167/299] timecost: 61.58, lr: 0.000052, Train: (LOSS: 0.0095, MAE: 0.0095, RMSE: 0.0180, R2: 0.9925), Valid: (LOSS: 0.0340, MAE: 0.0340, RMSE: 0.0537, R2: 0.9533), PNorm: 189.0070, GNorm: 0.4620
[168/299] timecost: 62.41, lr: 0.000052, Train: (LOSS: 0.0094, MAE: 0.0094, RMSE: 0.0177, R2: 0.9924), Valid: (LOSS: 0.0350, MAE: 0.0350, RMSE: 0.0550, R2: 0.9506), PNorm: 189.0102, GNorm: 0.3319
[169/299] timecost: 62.41, lr: 0.000052, Train: (LOSS: 0.0095, MAE: 0.0095, RMSE: 0.0179, R2: 0.9933), Valid: (LOSS: 0.0343, MAE: 0.0343, RMSE: 0.0544, R2: 0.9523), PNorm: 189.0144, GNorm: 0.4287
[170/299] timecost: 61.75, lr: 0.000052, Train: (LOSS: 0.0095, MAE: 0.0095, RMSE: 0.0180, R2: 0.9929), Valid: (LOSS: 0.0339, MAE: 0.0339, RMSE: 0.0532, R2: 0.9538), PNorm: 189.0190, GNorm: 0.3940
[171/299] timecost: 62.32, lr: 0.000052, Train: (LOSS: 0.0095, MAE: 0.0095, RMSE: 0.0179, R2: 0.9932), Valid: (LOSS: 0.0341, MAE: 0.0341, RMSE: 0.0540, R2: 0.9530), PNorm: 189.0221, GNorm: 0.3890
[172/299] timecost: 62.33, lr: 0.000052, Train: (LOSS: 0.0094, MAE: 0.0094, RMSE: 0.0175, R2: 0.9932), Valid: (LOSS: 0.0342, MAE: 0.0342, RMSE: 0.0552, R2: 0.9505), PNorm: 189.0264, GNorm: 0.5000
[173/299] timecost: 62.16, lr: 0.000052, Train: (LOSS: 0.0094, MAE: 0.0094, RMSE: 0.0174, R2: 0.9937), Valid: (LOSS: 0.0346, MAE: 0.0346, RMSE: 0.0545, R2: 0.9522), PNorm: 189.0304, GNorm: 0.2783
[174/299] timecost: 62.04, lr: 0.000052, Train: (LOSS: 0.0088, MAE: 0.0088, RMSE: 0.0171, R2: 0.9937), Valid: (LOSS: 0.0343, MAE: 0.0343, RMSE: 0.0542, R2: 0.9528), PNorm: 189.0349, GNorm: 0.5000
[175/299] timecost: 60.75, lr: 0.000052, Train: (LOSS: 0.0093, MAE: 0.0093, RMSE: 0.0174, R2: 0.9933), Valid: (LOSS: 0.0340, MAE: 0.0340, RMSE: 0.0533, R2: 0.9535), PNorm: 189.0392, GNorm: 0.3371
[176/299] timecost: 59.68, lr: 0.000052, Train: (LOSS: 0.0092, MAE: 0.0092, RMSE: 0.0174, R2: 0.9930), Valid: (LOSS: 0.0349, MAE: 0.0349, RMSE: 0.0542, R2: 0.9532), PNorm: 189.0438, GNorm: 0.4217
[177/299] timecost: 60.04, lr: 0.000052, Train: (LOSS: 0.0094, MAE: 0.0094, RMSE: 0.0176, R2: 0.9933), Valid: (LOSS: 0.0342, MAE: 0.0342, RMSE: 0.0533, R2: 0.9533), PNorm: 189.0483, GNorm: 0.3991
[178/299] timecost: 61.08, lr: 0.000052, Train: (LOSS: 0.0095, MAE: 0.0095, RMSE: 0.0179, R2: 0.9928), Valid: (LOSS: 0.0348, MAE: 0.0348, RMSE: 0.0545, R2: 0.9519), PNorm: 189.0547, GNorm: 0.3988
[179/299] timecost: 61.43, lr: 0.000052, Train: (LOSS: 0.0093, MAE: 0.0093, RMSE: 0.0172, R2: 0.9938), Valid: (LOSS: 0.0355, MAE: 0.0355, RMSE: 0.0554, R2: 0.9512), PNorm: 189.0584, GNorm: 0.5000
[180/299] timecost: 59.93, lr: 0.000052, Train: (LOSS: 0.0093, MAE: 0.0093, RMSE: 0.0175, R2: 0.9937), Valid: (LOSS: 0.0342, MAE: 0.0342, RMSE: 0.0544, R2: 0.9514), PNorm: 189.0628, GNorm: 0.4005
[181/299] timecost: 59.92, lr: 0.000052, Train: (LOSS: 0.0090, MAE: 0.0090, RMSE: 0.0173, R2: 0.9932), Valid: (LOSS: 0.0342, MAE: 0.0342, RMSE: 0.0533, R2: 0.9542), PNorm: 189.0693, GNorm: 0.5000
[182/299] timecost: 61.13, lr: 0.000052, Train: (LOSS: 0.0092, MAE: 0.0092, RMSE: 0.0170, R2: 0.9937), Valid: (LOSS: 0.0344, MAE: 0.0344, RMSE: 0.0532, R2: 0.9544), PNorm: 189.0739, GNorm: 0.3821
Epoch 00184: reducing learning rate of group 0 to 4.4371e-05.
[183/299] timecost: 61.19, lr: 0.000044, Train: (LOSS: 0.0089, MAE: 0.0089, RMSE: 0.0170, R2: 0.9938), Valid: (LOSS: 0.0339, MAE: 0.0339, RMSE: 0.0526, R2: 0.9554), PNorm: 189.0783, GNorm: 0.3568
[184/299] timecost: 60.79, lr: 0.000044, Train: (LOSS: 0.0084, MAE: 0.0084, RMSE: 0.0167, R2: 0.9940), Valid: (LOSS: 0.0336, MAE: 0.0336, RMSE: 0.0523, R2: 0.9562), PNorm: 189.0816, GNorm: 0.3402
[185/299] timecost: 60.99, lr: 0.000044, Train: (LOSS: 0.0081, MAE: 0.0081, RMSE: 0.0161, R2: 0.9945), Valid: (LOSS: 0.0337, MAE: 0.0337, RMSE: 0.0532, R2: 0.9543), PNorm: 189.0858, GNorm: 0.3285
[186/299] timecost: 61.25, lr: 0.000044, Train: (LOSS: 0.0084, MAE: 0.0084, RMSE: 0.0161, R2: 0.9941), Valid: (LOSS: 0.0336, MAE: 0.0336, RMSE: 0.0523, R2: 0.9563), PNorm: 189.0899, GNorm: 0.4199
[187/299] timecost: 61.52, lr: 0.000044, Train: (LOSS: 0.0084, MAE: 0.0084, RMSE: 0.0162, R2: 0.9941), Valid: (LOSS: 0.0336, MAE: 0.0336, RMSE: 0.0527, R2: 0.9552), PNorm: 189.0937, GNorm: 0.3404
[188/299] timecost: 62.14, lr: 0.000044, Train: (LOSS: 0.0080, MAE: 0.0080, RMSE: 0.0156, R2: 0.9948), Valid: (LOSS: 0.0344, MAE: 0.0344, RMSE: 0.0528, R2: 0.9548), PNorm: 189.0966, GNorm: 0.4539
[189/299] timecost: 60.34, lr: 0.000044, Train: (LOSS: 0.0084, MAE: 0.0084, RMSE: 0.0156, R2: 0.9943), Valid: (LOSS: 0.0338, MAE: 0.0338, RMSE: 0.0530, R2: 0.9550), PNorm: 189.1003, GNorm: 0.3879
[190/299] timecost: 61.66, lr: 0.000044, Train: (LOSS: 0.0079, MAE: 0.0079, RMSE: 0.0153, R2: 0.9945), Valid: (LOSS: 0.0334, MAE: 0.0334, RMSE: 0.0524, R2: 0.9549), PNorm: 189.1046, GNorm: 0.4045
[191/299] timecost: 62.52, lr: 0.000044, Train: (LOSS: 0.0079, MAE: 0.0079, RMSE: 0.0154, R2: 0.9950), Valid: (LOSS: 0.0338, MAE: 0.0338, RMSE: 0.0527, R2: 0.9552), PNorm: 189.1090, GNorm: 0.4845
[192/299] timecost: 62.77, lr: 0.000044, Train: (LOSS: 0.0078, MAE: 0.0078, RMSE: 0.0146, R2: 0.9950), Valid: (LOSS: 0.0335, MAE: 0.0335, RMSE: 0.0524, R2: 0.9561), PNorm: 189.1134, GNorm: 0.3868
[193/299] timecost: 60.89, lr: 0.000044, Train: (LOSS: 0.0079, MAE: 0.0079, RMSE: 0.0152, R2: 0.9950), Valid: (LOSS: 0.0345, MAE: 0.0345, RMSE: 0.0541, R2: 0.9530), PNorm: 189.1159, GNorm: 0.3889
[194/299] timecost: 61.37, lr: 0.000044, Train: (LOSS: 0.0081, MAE: 0.0081, RMSE: 0.0151, R2: 0.9944), Valid: (LOSS: 0.0336, MAE: 0.0336, RMSE: 0.0519, R2: 0.9575), PNorm: 189.1194, GNorm: 0.5000
[195/299] timecost: 61.14, lr: 0.000044, Train: (LOSS: 0.0077, MAE: 0.0077, RMSE: 0.0144, R2: 0.9950), Valid: (LOSS: 0.0339, MAE: 0.0339, RMSE: 0.0529, R2: 0.9553), PNorm: 189.1238, GNorm: 0.3855
[196/299] timecost: 61.71, lr: 0.000044, Train: (LOSS: 0.0076, MAE: 0.0076, RMSE: 0.0145, R2: 0.9953), Valid: (LOSS: 0.0333, MAE: 0.0333, RMSE: 0.0515, R2: 0.9581), PNorm: 189.1276, GNorm: 0.4994
[197/299] timecost: 62.61, lr: 0.000044, Train: (LOSS: 0.0076, MAE: 0.0076, RMSE: 0.0146, R2: 0.9954), Valid: (LOSS: 0.0342, MAE: 0.0342, RMSE: 0.0541, R2: 0.9523), PNorm: 189.1319, GNorm: 0.4255
[198/299] timecost: 60.37, lr: 0.000044, Train: (LOSS: 0.0079, MAE: 0.0079, RMSE: 0.0145, R2: 0.9954), Valid: (LOSS: 0.0329, MAE: 0.0329, RMSE: 0.0515, R2: 0.9574), PNorm: 189.1364, GNorm: 0.4374
[199/299] timecost: 59.92, lr: 0.000044, Train: (LOSS: 0.0076, MAE: 0.0076, RMSE: 0.0142, R2: 0.9956), Valid: (LOSS: 0.0327, MAE: 0.0327, RMSE: 0.0511, R2: 0.9575), PNorm: 189.1401, GNorm: 0.3522
[200/299] timecost: 60.01, lr: 0.000044, Train: (LOSS: 0.0076, MAE: 0.0076, RMSE: 0.0138, R2: 0.9958), Valid: (LOSS: 0.0330, MAE: 0.0330, RMSE: 0.0519, R2: 0.9574), PNorm: 189.1437, GNorm: 0.4165
[201/299] timecost: 60.37, lr: 0.000044, Train: (LOSS: 0.0072, MAE: 0.0072, RMSE: 0.0135, R2: 0.9957), Valid: (LOSS: 0.0325, MAE: 0.0325, RMSE: 0.0502, R2: 0.9593), PNorm: 189.1468, GNorm: 0.3444
[202/299] timecost: 60.74, lr: 0.000044, Train: (LOSS: 0.0073, MAE: 0.0073, RMSE: 0.0137, R2: 0.9959), Valid: (LOSS: 0.0325, MAE: 0.0325, RMSE: 0.0497, R2: 0.9607), PNorm: 189.1530, GNorm: 0.3288
[203/299] timecost: 59.05, lr: 0.000044, Train: (LOSS: 0.0073, MAE: 0.0073, RMSE: 0.0134, R2: 0.9961), Valid: (LOSS: 0.0320, MAE: 0.0320, RMSE: 0.0501, R2: 0.9598), PNorm: 189.1553, GNorm: 0.4631
[204/299] timecost: 59.04, lr: 0.000044, Train: (LOSS: 0.0071, MAE: 0.0071, RMSE: 0.0127, R2: 0.9963), Valid: (LOSS: 0.0331, MAE: 0.0331, RMSE: 0.0511, R2: 0.9587), PNorm: 189.1590, GNorm: 0.5000
[205/299] timecost: 59.11, lr: 0.000044, Train: (LOSS: 0.0075, MAE: 0.0075, RMSE: 0.0133, R2: 0.9962), Valid: (LOSS: 0.0332, MAE: 0.0332, RMSE: 0.0512, R2: 0.9581), PNorm: 189.1628, GNorm: 0.4080
[206/299] timecost: 59.30, lr: 0.000044, Train: (LOSS: 0.0071, MAE: 0.0071, RMSE: 0.0129, R2: 0.9963), Valid: (LOSS: 0.0320, MAE: 0.0320, RMSE: 0.0494, R2: 0.9617), PNorm: 189.1672, GNorm: 0.3802
[207/299] timecost: 59.70, lr: 0.000044, Train: (LOSS: 0.0072, MAE: 0.0072, RMSE: 0.0130, R2: 0.9963), Valid: (LOSS: 0.0327, MAE: 0.0327, RMSE: 0.0502, R2: 0.9604), PNorm: 189.1728, GNorm: 0.3376
[208/299] timecost: 62.47, lr: 0.000044, Train: (LOSS: 0.0072, MAE: 0.0072, RMSE: 0.0125, R2: 0.9965), Valid: (LOSS: 0.0325, MAE: 0.0325, RMSE: 0.0501, R2: 0.9608), PNorm: 189.1754, GNorm: 0.4619
[209/299] timecost: 62.34, lr: 0.000044, Train: (LOSS: 0.0071, MAE: 0.0071, RMSE: 0.0123, R2: 0.9968), Valid: (LOSS: 0.0323, MAE: 0.0323, RMSE: 0.0491, R2: 0.9622), PNorm: 189.1789, GNorm: 0.4048
[210/299] timecost: 62.87, lr: 0.000044, Train: (LOSS: 0.0071, MAE: 0.0071, RMSE: 0.0123, R2: 0.9962), Valid: (LOSS: 0.0331, MAE: 0.0331, RMSE: 0.0503, R2: 0.9597), PNorm: 189.1813, GNorm: 0.4717
[211/299] timecost: 59.40, lr: 0.000044, Train: (LOSS: 0.0070, MAE: 0.0070, RMSE: 0.0123, R2: 0.9966), Valid: (LOSS: 0.0322, MAE: 0.0322, RMSE: 0.0497, R2: 0.9611), PNorm: 189.1866, GNorm: 0.5000
[212/299] timecost: 59.54, lr: 0.000044, Train: (LOSS: 0.0070, MAE: 0.0070, RMSE: 0.0120, R2: 0.9969), Valid: (LOSS: 0.0332, MAE: 0.0332, RMSE: 0.0499, R2: 0.9608), PNorm: 189.1893, GNorm: 0.4576
[213/299] timecost: 59.51, lr: 0.000044, Train: (LOSS: 0.0074, MAE: 0.0074, RMSE: 0.0129, R2: 0.9964), Valid: (LOSS: 0.0326, MAE: 0.0326, RMSE: 0.0494, R2: 0.9622), PNorm: 189.1949, GNorm: 0.5000
[214/299] timecost: 59.33, lr: 0.000044, Train: (LOSS: 0.0068, MAE: 0.0068, RMSE: 0.0114, R2: 0.9970), Valid: (LOSS: 0.0324, MAE: 0.0324, RMSE: 0.0498, R2: 0.9618), PNorm: 189.1989, GNorm: 0.4080
[215/299] timecost: 59.56, lr: 0.000044, Train: (LOSS: 0.0067, MAE: 0.0067, RMSE: 0.0113, R2: 0.9962), Valid: (LOSS: 0.0318, MAE: 0.0318, RMSE: 0.0487, R2: 0.9638), PNorm: 189.2025, GNorm: 0.4617
[216/299] timecost: 59.70, lr: 0.000044, Train: (LOSS: 0.0064, MAE: 0.0064, RMSE: 0.0109, R2: 0.9973), Valid: (LOSS: 0.0311, MAE: 0.0311, RMSE: 0.0480, R2: 0.9640), PNorm: 189.2055, GNorm: 0.4129
[217/299] timecost: 62.30, lr: 0.000044, Train: (LOSS: 0.0066, MAE: 0.0066, RMSE: 0.0114, R2: 0.9970), Valid: (LOSS: 0.0314, MAE: 0.0314, RMSE: 0.0485, R2: 0.9638), PNorm: 189.2089, GNorm: 0.4062
[218/299] timecost: 62.54, lr: 0.000044, Train: (LOSS: 0.0068, MAE: 0.0068, RMSE: 0.0117, R2: 0.9967), Valid: (LOSS: 0.0318, MAE: 0.0318, RMSE: 0.0484, R2: 0.9641), PNorm: 189.2123, GNorm: 0.4825
[219/299] timecost: 61.82, lr: 0.000044, Train: (LOSS: 0.0065, MAE: 0.0065, RMSE: 0.0110, R2: 0.9972), Valid: (LOSS: 0.0312, MAE: 0.0312, RMSE: 0.0475, R2: 0.9658), PNorm: 189.2158, GNorm: 0.4666
[220/299] timecost: 60.36, lr: 0.000044, Train: (LOSS: 0.0065, MAE: 0.0065, RMSE: 0.0109, R2: 0.9973), Valid: (LOSS: 0.0319, MAE: 0.0319, RMSE: 0.0489, R2: 0.9636), PNorm: 189.2193, GNorm: 0.3053
[221/299] timecost: 60.41, lr: 0.000044, Train: (LOSS: 0.0065, MAE: 0.0065, RMSE: 0.0107, R2: 0.9974), Valid: (LOSS: 0.0317, MAE: 0.0317, RMSE: 0.0490, R2: 0.9636), PNorm: 189.2221, GNorm: 0.3826
[222/299] timecost: 59.94, lr: 0.000044, Train: (LOSS: 0.0063, MAE: 0.0063, RMSE: 0.0105, R2: 0.9975), Valid: (LOSS: 0.0317, MAE: 0.0317, RMSE: 0.0484, R2: 0.9639), PNorm: 189.2257, GNorm: 0.3208
[223/299] timecost: 58.47, lr: 0.000044, Train: (LOSS: 0.0064, MAE: 0.0064, RMSE: 0.0107, R2: 0.9974), Valid: (LOSS: 0.0318, MAE: 0.0318, RMSE: 0.0494, R2: 0.9625), PNorm: 189.2286, GNorm: 0.4345
[224/299] timecost: 58.63, lr: 0.000044, Train: (LOSS: 0.0063, MAE: 0.0063, RMSE: 0.0105, R2: 0.9971), Valid: (LOSS: 0.0328, MAE: 0.0328, RMSE: 0.0504, R2: 0.9614), PNorm: 189.2322, GNorm: 0.4547
[225/299] timecost: 60.34, lr: 0.000044, Train: (LOSS: 0.0065, MAE: 0.0065, RMSE: 0.0107, R2: 0.9969), Valid: (LOSS: 0.0319, MAE: 0.0319, RMSE: 0.0485, R2: 0.9641), PNorm: 189.2333, GNorm: 0.5000
[226/299] timecost: 60.74, lr: 0.000044, Train: (LOSS: 0.0063, MAE: 0.0063, RMSE: 0.0104, R2: 0.9976), Valid: (LOSS: 0.0318, MAE: 0.0318, RMSE: 0.0481, R2: 0.9650), PNorm: 189.2377, GNorm: 0.3551
[227/299] timecost: 60.84, lr: 0.000044, Train: (LOSS: 0.0064, MAE: 0.0064, RMSE: 0.0104, R2: 0.9972), Valid: (LOSS: 0.0322, MAE: 0.0322, RMSE: 0.0499, R2: 0.9622), PNorm: 189.2396, GNorm: 0.5000
[228/299] timecost: 60.87, lr: 0.000044, Train: (LOSS: 0.0063, MAE: 0.0063, RMSE: 0.0102, R2: 0.9976), Valid: (LOSS: 0.0330, MAE: 0.0330, RMSE: 0.0516, R2: 0.9590), PNorm: 189.2440, GNorm: 0.3419
[229/299] timecost: 60.65, lr: 0.000044, Train: (LOSS: 0.0064, MAE: 0.0064, RMSE: 0.0102, R2: 0.9975), Valid: (LOSS: 0.0326, MAE: 0.0326, RMSE: 0.0504, R2: 0.9613), PNorm: 189.2471, GNorm: 0.5000
[230/299] timecost: 59.80, lr: 0.000044, Train: (LOSS: 0.0059, MAE: 0.0059, RMSE: 0.0099, R2: 0.9975), Valid: (LOSS: 0.0325, MAE: 0.0325, RMSE: 0.0500, R2: 0.9615), PNorm: 189.2504, GNorm: 0.4543
[231/299] timecost: 58.70, lr: 0.000044, Train: (LOSS: 0.0059, MAE: 0.0059, RMSE: 0.0097, R2: 0.9976), Valid: (LOSS: 0.0325, MAE: 0.0325, RMSE: 0.0499, R2: 0.9624), PNorm: 189.2535, GNorm: 0.3820
Epoch 00233: reducing learning rate of group 0 to 3.7715e-05.
[232/299] timecost: 58.46, lr: 0.000038, Train: (LOSS: 0.0060, MAE: 0.0060, RMSE: 0.0103, R2: 0.9975), Valid: (LOSS: 0.0319, MAE: 0.0319, RMSE: 0.0494, R2: 0.9629), PNorm: 189.2571, GNorm: 0.3940
[233/299] timecost: 58.85, lr: 0.000038, Train: (LOSS: 0.0058, MAE: 0.0058, RMSE: 0.0097, R2: 0.9978), Valid: (LOSS: 0.0318, MAE: 0.0318, RMSE: 0.0496, R2: 0.9621), PNorm: 189.2593, GNorm: 0.3678
[234/299] timecost: 59.49, lr: 0.000038, Train: (LOSS: 0.0055, MAE: 0.0055, RMSE: 0.0092, R2: 0.9978), Valid: (LOSS: 0.0313, MAE: 0.0313, RMSE: 0.0483, R2: 0.9644), PNorm: 189.2615, GNorm: 0.4557
[235/299] timecost: 60.14, lr: 0.000038, Train: (LOSS: 0.0053, MAE: 0.0053, RMSE: 0.0090, R2: 0.9975), Valid: (LOSS: 0.0316, MAE: 0.0316, RMSE: 0.0485, R2: 0.9641), PNorm: 189.2640, GNorm: 0.3373
[236/299] timecost: 60.42, lr: 0.000038, Train: (LOSS: 0.0052, MAE: 0.0052, RMSE: 0.0090, R2: 0.9978), Valid: (LOSS: 0.0317, MAE: 0.0317, RMSE: 0.0488, R2: 0.9637), PNorm: 189.2665, GNorm: 0.4027
[237/299] timecost: 59.48, lr: 0.000038, Train: (LOSS: 0.0052, MAE: 0.0052, RMSE: 0.0088, R2: 0.9981), Valid: (LOSS: 0.0316, MAE: 0.0316, RMSE: 0.0489, R2: 0.9635), PNorm: 189.2684, GNorm: 0.3540
[238/299] timecost: 59.65, lr: 0.000038, Train: (LOSS: 0.0052, MAE: 0.0052, RMSE: 0.0088, R2: 0.9980), Valid: (LOSS: 0.0315, MAE: 0.0315, RMSE: 0.0484, R2: 0.9642), PNorm: 189.2706, GNorm: 0.3144
[239/299] timecost: 59.47, lr: 0.000038, Train: (LOSS: 0.0053, MAE: 0.0053, RMSE: 0.0088, R2: 0.9981), Valid: (LOSS: 0.0316, MAE: 0.0316, RMSE: 0.0491, R2: 0.9626), PNorm: 189.2723, GNorm: 0.3760
[240/299] timecost: 60.63, lr: 0.000038, Train: (LOSS: 0.0052, MAE: 0.0052, RMSE: 0.0088, R2: 0.9978), Valid: (LOSS: 0.0316, MAE: 0.0316, RMSE: 0.0492, R2: 0.9623), PNorm: 189.2746, GNorm: 0.5000
[241/299] timecost: 61.71, lr: 0.000038, Train: (LOSS: 0.0051, MAE: 0.0051, RMSE: 0.0087, R2: 0.9979), Valid: (LOSS: 0.0314, MAE: 0.0314, RMSE: 0.0488, R2: 0.9634), PNorm: 189.2770, GNorm: 0.3742
[242/299] timecost: 62.32, lr: 0.000038, Train: (LOSS: 0.0048, MAE: 0.0048, RMSE: 0.0085, R2: 0.9979), Valid: (LOSS: 0.0316, MAE: 0.0316, RMSE: 0.0483, R2: 0.9643), PNorm: 189.2786, GNorm: 0.3841
[243/299] timecost: 62.00, lr: 0.000038, Train: (LOSS: 0.0053, MAE: 0.0053, RMSE: 0.0088, R2: 0.9977), Valid: (LOSS: 0.0318, MAE: 0.0318, RMSE: 0.0490, R2: 0.9631), PNorm: 189.2801, GNorm: 0.4730
[244/299] timecost: 62.06, lr: 0.000038, Train: (LOSS: 0.0052, MAE: 0.0052, RMSE: 0.0088, R2: 0.9980), Valid: (LOSS: 0.0317, MAE: 0.0317, RMSE: 0.0491, R2: 0.9631), PNorm: 189.2830, GNorm: 0.5000
[245/299] timecost: 61.96, lr: 0.000038, Train: (LOSS: 0.0052, MAE: 0.0052, RMSE: 0.0087, R2: 0.9980), Valid: (LOSS: 0.0313, MAE: 0.0313, RMSE: 0.0480, R2: 0.9643), PNorm: 189.2854, GNorm: 0.3341
[246/299] timecost: 62.10, lr: 0.000038, Train: (LOSS: 0.0049, MAE: 0.0049, RMSE: 0.0082, R2: 0.9983), Valid: (LOSS: 0.0318, MAE: 0.0318, RMSE: 0.0486, R2: 0.9642), PNorm: 189.2865, GNorm: 0.3595
[247/299] timecost: 61.93, lr: 0.000038, Train: (LOSS: 0.0050, MAE: 0.0050, RMSE: 0.0085, R2: 0.9980), Valid: (LOSS: 0.0317, MAE: 0.0317, RMSE: 0.0482, R2: 0.9645), PNorm: 189.2884, GNorm: 0.4524
Epoch 00249: reducing learning rate of group 0 to 3.2058e-05.
[248/299] timecost: 62.15, lr: 0.000032, Train: (LOSS: 0.0053, MAE: 0.0053, RMSE: 0.0088, R2: 0.9977), Valid: (LOSS: 0.0323, MAE: 0.0323, RMSE: 0.0495, R2: 0.9622), PNorm: 189.2913, GNorm: 0.3490
[249/299] timecost: 62.20, lr: 0.000032, Train: (LOSS: 0.0049, MAE: 0.0049, RMSE: 0.0083, R2: 0.9981), Valid: (LOSS: 0.0311, MAE: 0.0311, RMSE: 0.0477, R2: 0.9648), PNorm: 189.2930, GNorm: 0.4486
[250/299] timecost: 62.79, lr: 0.000032, Train: (LOSS: 0.0049, MAE: 0.0049, RMSE: 0.0081, R2: 0.9984), Valid: (LOSS: 0.0317, MAE: 0.0317, RMSE: 0.0484, R2: 0.9639), PNorm: 189.2946, GNorm: 0.4097
[251/299] timecost: 62.66, lr: 0.000032, Train: (LOSS: 0.0046, MAE: 0.0046, RMSE: 0.0079, R2: 0.9985), Valid: (LOSS: 0.0318, MAE: 0.0318, RMSE: 0.0491, R2: 0.9625), PNorm: 189.2972, GNorm: 0.3514
[252/299] timecost: 60.65, lr: 0.000032, Train: (LOSS: 0.0044, MAE: 0.0044, RMSE: 0.0076, R2: 0.9985), Valid: (LOSS: 0.0310, MAE: 0.0310, RMSE: 0.0483, R2: 0.9639), PNorm: 189.2983, GNorm: 0.3837
[253/299] timecost: 60.27, lr: 0.000032, Train: (LOSS: 0.0044, MAE: 0.0044, RMSE: 0.0077, R2: 0.9982), Valid: (LOSS: 0.0315, MAE: 0.0315, RMSE: 0.0485, R2: 0.9634), PNorm: 189.3001, GNorm: 0.3752
[254/299] timecost: 60.74, lr: 0.000032, Train: (LOSS: 0.0043, MAE: 0.0043, RMSE: 0.0076, R2: 0.9975), Valid: (LOSS: 0.0317, MAE: 0.0317, RMSE: 0.0487, R2: 0.9634), PNorm: 189.3014, GNorm: 0.4537
[255/299] timecost: 62.29, lr: 0.000032, Train: (LOSS: 0.0045, MAE: 0.0045, RMSE: 0.0078, R2: 0.9984), Valid: (LOSS: 0.0314, MAE: 0.0314, RMSE: 0.0483, R2: 0.9638), PNorm: 189.3026, GNorm: 0.4181
[256/299] timecost: 61.83, lr: 0.000032, Train: (LOSS: 0.0044, MAE: 0.0044, RMSE: 0.0076, R2: 0.9982), Valid: (LOSS: 0.0311, MAE: 0.0311, RMSE: 0.0474, R2: 0.9655), PNorm: 189.3045, GNorm: 0.4788
[257/299] timecost: 62.19, lr: 0.000032, Train: (LOSS: 0.0045, MAE: 0.0045, RMSE: 0.0077, R2: 0.9983), Valid: (LOSS: 0.0314, MAE: 0.0314, RMSE: 0.0481, R2: 0.9643), PNorm: 189.3060, GNorm: 0.4523
[258/299] timecost: 61.81, lr: 0.000032, Train: (LOSS: 0.0043, MAE: 0.0043, RMSE: 0.0073, R2: 0.9983), Valid: (LOSS: 0.0315, MAE: 0.0315, RMSE: 0.0488, R2: 0.9632), PNorm: 189.3079, GNorm: 0.5000
[259/299] timecost: 60.42, lr: 0.000032, Train: (LOSS: 0.0044, MAE: 0.0044, RMSE: 0.0077, R2: 0.9982), Valid: (LOSS: 0.0321, MAE: 0.0321, RMSE: 0.0495, R2: 0.9622), PNorm: 189.3092, GNorm: 0.4546
[260/299] timecost: 60.37, lr: 0.000032, Train: (LOSS: 0.0045, MAE: 0.0045, RMSE: 0.0076, R2: 0.9983), Valid: (LOSS: 0.0313, MAE: 0.0313, RMSE: 0.0484, R2: 0.9639), PNorm: 189.3114, GNorm: 0.4140
[261/299] timecost: 60.04, lr: 0.000032, Train: (LOSS: 0.0044, MAE: 0.0044, RMSE: 0.0076, R2: 0.9979), Valid: (LOSS: 0.0318, MAE: 0.0318, RMSE: 0.0487, R2: 0.9632), PNorm: 189.3127, GNorm: 0.5000
[262/299] timecost: 60.45, lr: 0.000032, Train: (LOSS: 0.0044, MAE: 0.0044, RMSE: 0.0074, R2: 0.9985), Valid: (LOSS: 0.0318, MAE: 0.0318, RMSE: 0.0486, R2: 0.9638), PNorm: 189.3152, GNorm: 0.5000
[263/299] timecost: 60.22, lr: 0.000032, Train: (LOSS: 0.0045, MAE: 0.0045, RMSE: 0.0075, R2: 0.9982), Valid: (LOSS: 0.0321, MAE: 0.0321, RMSE: 0.0491, R2: 0.9633), PNorm: 189.3161, GNorm: 0.4957
Epoch 00265: reducing learning rate of group 0 to 2.7249e-05.
[264/299] timecost: 60.14, lr: 0.000027, Train: (LOSS: 0.0043, MAE: 0.0043, RMSE: 0.0073, R2: 0.9983), Valid: (LOSS: 0.0313, MAE: 0.0313, RMSE: 0.0479, R2: 0.9649), PNorm: 189.3171, GNorm: 0.3495
[265/299] timecost: 60.43, lr: 0.000027, Train: (LOSS: 0.0042, MAE: 0.0042, RMSE: 0.0071, R2: 0.9980), Valid: (LOSS: 0.0318, MAE: 0.0318, RMSE: 0.0488, R2: 0.9633), PNorm: 189.3191, GNorm: 0.4582
[266/299] timecost: 59.57, lr: 0.000027, Train: (LOSS: 0.0041, MAE: 0.0041, RMSE: 0.0071, R2: 0.9984), Valid: (LOSS: 0.0314, MAE: 0.0314, RMSE: 0.0487, R2: 0.9631), PNorm: 189.3194, GNorm: 0.4228
[267/299] timecost: 60.33, lr: 0.000027, Train: (LOSS: 0.0039, MAE: 0.0039, RMSE: 0.0070, R2: 0.9985), Valid: (LOSS: 0.0318, MAE: 0.0318, RMSE: 0.0490, R2: 0.9629), PNorm: 189.3215, GNorm: 0.3326
[268/299] timecost: 60.63, lr: 0.000027, Train: (LOSS: 0.0040, MAE: 0.0040, RMSE: 0.0070, R2: 0.9981), Valid: (LOSS: 0.0313, MAE: 0.0313, RMSE: 0.0486, R2: 0.9635), PNorm: 189.3230, GNorm: 0.4003
[269/299] timecost: 60.55, lr: 0.000027, Train: (LOSS: 0.0038, MAE: 0.0038, RMSE: 0.0067, R2: 0.9984), Valid: (LOSS: 0.0312, MAE: 0.0312, RMSE: 0.0482, R2: 0.9643), PNorm: 189.3244, GNorm: 0.3699
[270/299] timecost: 59.45, lr: 0.000027, Train: (LOSS: 0.0039, MAE: 0.0039, RMSE: 0.0069, R2: 0.9984), Valid: (LOSS: 0.0319, MAE: 0.0319, RMSE: 0.0490, R2: 0.9633), PNorm: 189.3253, GNorm: 0.3733
[271/299] timecost: 58.90, lr: 0.000027, Train: (LOSS: 0.0038, MAE: 0.0038, RMSE: 0.0067, R2: 0.9985), Valid: (LOSS: 0.0310, MAE: 0.0310, RMSE: 0.0484, R2: 0.9639), PNorm: 189.3265, GNorm: 0.3473
[272/299] timecost: 59.43, lr: 0.000027, Train: (LOSS: 0.0038, MAE: 0.0038, RMSE: 0.0067, R2: 0.9987), Valid: (LOSS: 0.0312, MAE: 0.0312, RMSE: 0.0483, R2: 0.9640), PNorm: 189.3274, GNorm: 0.2864
[273/299] timecost: 59.69, lr: 0.000027, Train: (LOSS: 0.0037, MAE: 0.0037, RMSE: 0.0065, R2: 0.9986), Valid: (LOSS: 0.0306, MAE: 0.0306, RMSE: 0.0474, R2: 0.9656), PNorm: 189.3285, GNorm: 0.2929
[274/299] timecost: 59.77, lr: 0.000027, Train: (LOSS: 0.0038, MAE: 0.0038, RMSE: 0.0067, R2: 0.9986), Valid: (LOSS: 0.0313, MAE: 0.0313, RMSE: 0.0479, R2: 0.9648), PNorm: 189.3297, GNorm: 0.5000
[275/299] timecost: 60.72, lr: 0.000027, Train: (LOSS: 0.0039, MAE: 0.0039, RMSE: 0.0068, R2: 0.9983), Valid: (LOSS: 0.0317, MAE: 0.0317, RMSE: 0.0489, R2: 0.9632), PNorm: 189.3304, GNorm: 0.3415
[276/299] timecost: 61.47, lr: 0.000027, Train: (LOSS: 0.0039, MAE: 0.0039, RMSE: 0.0069, R2: 0.9983), Valid: (LOSS: 0.0307, MAE: 0.0307, RMSE: 0.0474, R2: 0.9654), PNorm: 189.3323, GNorm: 0.3345
[277/299] timecost: 62.15, lr: 0.000027, Train: (LOSS: 0.0039, MAE: 0.0039, RMSE: 0.0066, R2: 0.9987), Valid: (LOSS: 0.0315, MAE: 0.0315, RMSE: 0.0483, R2: 0.9642), PNorm: 189.3336, GNorm: 0.3592
[278/299] timecost: 62.02, lr: 0.000027, Train: (LOSS: 0.0036, MAE: 0.0036, RMSE: 0.0064, R2: 0.9986), Valid: (LOSS: 0.0314, MAE: 0.0314, RMSE: 0.0481, R2: 0.9649), PNorm: 189.3352, GNorm: 0.5000
[279/299] timecost: 60.43, lr: 0.000027, Train: (LOSS: 0.0039, MAE: 0.0039, RMSE: 0.0066, R2: 0.9985), Valid: (LOSS: 0.0316, MAE: 0.0316, RMSE: 0.0489, R2: 0.9633), PNorm: 189.3367, GNorm: 0.3740
[280/299] timecost: 61.54, lr: 0.000027, Train: (LOSS: 0.0037, MAE: 0.0037, RMSE: 0.0064, R2: 0.9986), Valid: (LOSS: 0.0320, MAE: 0.0320, RMSE: 0.0493, R2: 0.9626), PNorm: 189.3374, GNorm: 0.3739
[281/299] timecost: 62.25, lr: 0.000027, Train: (LOSS: 0.0039, MAE: 0.0039, RMSE: 0.0067, R2: 0.9986), Valid: (LOSS: 0.0313, MAE: 0.0313, RMSE: 0.0483, R2: 0.9649), PNorm: 189.3393, GNorm: 0.4051
[282/299] timecost: 61.94, lr: 0.000027, Train: (LOSS: 0.0039, MAE: 0.0039, RMSE: 0.0066, R2: 0.9984), Valid: (LOSS: 0.0313, MAE: 0.0313, RMSE: 0.0481, R2: 0.9644), PNorm: 189.3403, GNorm: 0.3306
[283/299] timecost: 62.33, lr: 0.000027, Train: (LOSS: 0.0038, MAE: 0.0038, RMSE: 0.0066, R2: 0.9985), Valid: (LOSS: 0.0317, MAE: 0.0317, RMSE: 0.0481, R2: 0.9644), PNorm: 189.3418, GNorm: 0.4101
[284/299] timecost: 61.68, lr: 0.000027, Train: (LOSS: 0.0037, MAE: 0.0037, RMSE: 0.0062, R2: 0.9983), Valid: (LOSS: 0.0319, MAE: 0.0319, RMSE: 0.0490, R2: 0.9634), PNorm: 189.3428, GNorm: 0.2963
[285/299] timecost: 62.26, lr: 0.000027, Train: (LOSS: 0.0036, MAE: 0.0036, RMSE: 0.0064, R2: 0.9984), Valid: (LOSS: 0.0311, MAE: 0.0311, RMSE: 0.0482, R2: 0.9644), PNorm: 189.3435, GNorm: 0.3545
[286/299] timecost: 62.11, lr: 0.000027, Train: (LOSS: 0.0039, MAE: 0.0039, RMSE: 0.0064, R2: 0.9986), Valid: (LOSS: 0.0312, MAE: 0.0312, RMSE: 0.0480, R2: 0.9646), PNorm: 189.3453, GNorm: 0.3269
[287/299] timecost: 61.86, lr: 0.000027, Train: (LOSS: 0.0035, MAE: 0.0035, RMSE: 0.0063, R2: 0.9987), Valid: (LOSS: 0.0315, MAE: 0.0315, RMSE: 0.0482, R2: 0.9642), PNorm: 189.3463, GNorm: 0.3261
[288/299] timecost: 62.48, lr: 0.000027, Train: (LOSS: 0.0038, MAE: 0.0038, RMSE: 0.0066, R2: 0.9987), Valid: (LOSS: 0.0316, MAE: 0.0316, RMSE: 0.0484, R2: 0.9643), PNorm: 189.3475, GNorm: 0.3666
Epoch 00290: reducing learning rate of group 0 to 2.3162e-05.
[289/299] timecost: 62.25, lr: 0.000023, Train: (LOSS: 0.0036, MAE: 0.0036, RMSE: 0.0063, R2: 0.9987), Valid: (LOSS: 0.0311, MAE: 0.0311, RMSE: 0.0482, R2: 0.9644), PNorm: 189.3489, GNorm: 0.5000
[290/299] timecost: 62.76, lr: 0.000023, Train: (LOSS: 0.0037, MAE: 0.0037, RMSE: 0.0064, R2: 0.9985), Valid: (LOSS: 0.0310, MAE: 0.0310, RMSE: 0.0480, R2: 0.9645), PNorm: 189.3502, GNorm: 0.4990
[291/299] timecost: 62.37, lr: 0.000023, Train: (LOSS: 0.0035, MAE: 0.0035, RMSE: 0.0061, R2: 0.9987), Valid: (LOSS: 0.0314, MAE: 0.0314, RMSE: 0.0487, R2: 0.9635), PNorm: 189.3509, GNorm: 0.4413
[292/299] timecost: 62.23, lr: 0.000023, Train: (LOSS: 0.0033, MAE: 0.0033, RMSE: 0.0059, R2: 0.9987), Valid: (LOSS: 0.0311, MAE: 0.0311, RMSE: 0.0479, R2: 0.9648), PNorm: 189.3516, GNorm: 0.3511
[293/299] timecost: 62.12, lr: 0.000023, Train: (LOSS: 0.0033, MAE: 0.0033, RMSE: 0.0059, R2: 0.9987), Valid: (LOSS: 0.0313, MAE: 0.0313, RMSE: 0.0487, R2: 0.9635), PNorm: 189.3525, GNorm: 0.3737
[294/299] timecost: 61.80, lr: 0.000023, Train: (LOSS: 0.0034, MAE: 0.0034, RMSE: 0.0061, R2: 0.9986), Valid: (LOSS: 0.0309, MAE: 0.0309, RMSE: 0.0482, R2: 0.9643), PNorm: 189.3536, GNorm: 0.5000
[295/299] timecost: 62.00, lr: 0.000023, Train: (LOSS: 0.0032, MAE: 0.0032, RMSE: 0.0058, R2: 0.9987), Valid: (LOSS: 0.0313, MAE: 0.0313, RMSE: 0.0482, R2: 0.9642), PNorm: 189.3544, GNorm: 0.4282
[296/299] timecost: 61.73, lr: 0.000023, Train: (LOSS: 0.0032, MAE: 0.0032, RMSE: 0.0058, R2: 0.9981), Valid: (LOSS: 0.0311, MAE: 0.0311, RMSE: 0.0478, R2: 0.9650), PNorm: 189.3555, GNorm: 0.3794
[297/299] timecost: 60.69, lr: 0.000023, Train: (LOSS: 0.0033, MAE: 0.0033, RMSE: 0.0058, R2: 0.9987), Valid: (LOSS: 0.0309, MAE: 0.0309, RMSE: 0.0480, R2: 0.9647), PNorm: 189.3557, GNorm: 0.4565
[298/299] timecost: 60.04, lr: 0.000023, Train: (LOSS: 0.0033, MAE: 0.0033, RMSE: 0.0060, R2: 0.9988), Valid: (LOSS: 0.0313, MAE: 0.0313, RMSE: 0.0481, R2: 0.9645), PNorm: 189.3572, GNorm: 0.4240
[299/299] timecost: 60.56, lr: 0.000023, Train: (LOSS: 0.0033, MAE: 0.0033, RMSE: 0.0059, R2: 0.9987), Valid: (LOSS: 0.0313, MAE: 0.0313, RMSE: 0.0487, R2: 0.9634), PNorm: 189.3575, GNorm: 0.3494
==========Training End==========
==========Test Best Model==========
================Final Results=======================
mse: 0.0344 +- 0.0000:
rmse: 0.0536 +- 0.0000:
mae: 0.0344 +- 0.0000:
r2: 0.9550 +- 0.0000:
tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        ...,
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
