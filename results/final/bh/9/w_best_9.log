cuda available with GPU: Tesla V100-PCIE-16GB
==========Load Seed==========
set_random_seed
0
==========Training Start==========
Training Graphs:  2491
Valid Graphs:  277
Test Graphs:  1187
============Loading pretrained weights to generate initialization============
============Creating new layers============
============Creating Model============
Training Graphs Batches:  78
Valid Graphs Batches:  9
Test Graphs Batches:  37
[0/299] timecost: 65.62, lr: 0.000030, Train: (LOSS: 0.2311, MAE: 0.2311, RMSE: 0.2719, R2: -0.0736), Valid: (LOSS: 0.2254, MAE: 0.2254, RMSE: 0.2629, R2: 0.0438), PNorm: 174.3767, GNorm: 0.6737
[1/299] timecost: 66.06, lr: 0.000030, Train: (LOSS: 0.2177, MAE: 0.2177, RMSE: 0.2574, R2: 0.0453), Valid: (LOSS: 0.2141, MAE: 0.2141, RMSE: 0.2656, R2: 0.0205), PNorm: 173.7244, GNorm: 0.5175
[2/299] timecost: 64.22, lr: 0.000030, Train: (LOSS: 0.2046, MAE: 0.2046, RMSE: 0.2465, R2: 0.1276), Valid: (LOSS: 0.1978, MAE: 0.1978, RMSE: 0.2396, R2: 0.2016), PNorm: 173.1799, GNorm: 0.5096
[3/299] timecost: 64.90, lr: 0.000030, Train: (LOSS: 0.1861, MAE: 0.1861, RMSE: 0.2314, R2: 0.2103), Valid: (LOSS: 0.1719, MAE: 0.1719, RMSE: 0.2085, R2: 0.3970), PNorm: 172.7377, GNorm: 5.0278
[4/299] timecost: 64.38, lr: 0.000030, Train: (LOSS: 0.1656, MAE: 0.1656, RMSE: 0.2131, R2: 0.3221), Valid: (LOSS: 0.1664, MAE: 0.1664, RMSE: 0.2016, R2: 0.4400), PNorm: 172.3884, GNorm: 3.1432
[5/299] timecost: 64.42, lr: 0.000030, Train: (LOSS: 0.1522, MAE: 0.1522, RMSE: 0.1980, R2: 0.4182), Valid: (LOSS: 0.1526, MAE: 0.1526, RMSE: 0.1978, R2: 0.4566), PNorm: 172.0765, GNorm: 5.4556
[6/299] timecost: 64.18, lr: 0.000030, Train: (LOSS: 0.1504, MAE: 0.1504, RMSE: 0.1959, R2: 0.4334), Valid: (LOSS: 0.1521, MAE: 0.1521, RMSE: 0.1989, R2: 0.4438), PNorm: 171.7850, GNorm: 4.2191
[7/299] timecost: 64.37, lr: 0.000030, Train: (LOSS: 0.1377, MAE: 0.1377, RMSE: 0.1827, R2: 0.4892), Valid: (LOSS: 0.1362, MAE: 0.1362, RMSE: 0.1843, R2: 0.5238), PNorm: 171.4963, GNorm: 1.3768
[8/299] timecost: 63.90, lr: 0.000030, Train: (LOSS: 0.1334, MAE: 0.1334, RMSE: 0.1816, R2: 0.5075), Valid: (LOSS: 0.1399, MAE: 0.1399, RMSE: 0.1858, R2: 0.5239), PNorm: 171.2193, GNorm: 0.8805
[9/299] timecost: 64.11, lr: 0.000030, Train: (LOSS: 0.1267, MAE: 0.1267, RMSE: 0.1726, R2: 0.5592), Valid: (LOSS: 0.1417, MAE: 0.1417, RMSE: 0.1943, R2: 0.4699), PNorm: 170.9491, GNorm: 1.5529
[10/299] timecost: 63.96, lr: 0.000030, Train: (LOSS: 0.1250, MAE: 0.1250, RMSE: 0.1732, R2: 0.5550), Valid: (LOSS: 0.1220, MAE: 0.1220, RMSE: 0.1652, R2: 0.6221), PNorm: 170.6900, GNorm: 2.2696
[11/299] timecost: 64.45, lr: 0.000030, Train: (LOSS: 0.1206, MAE: 0.1206, RMSE: 0.1663, R2: 0.5827), Valid: (LOSS: 0.1243, MAE: 0.1243, RMSE: 0.1652, R2: 0.6172), PNorm: 170.4413, GNorm: 4.3621
[12/299] timecost: 65.60, lr: 0.000030, Train: (LOSS: 0.1319, MAE: 0.1319, RMSE: 0.1783, R2: 0.5126), Valid: (LOSS: 0.1193, MAE: 0.1193, RMSE: 0.1593, R2: 0.6489), PNorm: 170.2088, GNorm: 1.2798
[13/299] timecost: 66.68, lr: 0.000030, Train: (LOSS: 0.1140, MAE: 0.1140, RMSE: 0.1591, R2: 0.6195), Valid: (LOSS: 0.1303, MAE: 0.1303, RMSE: 0.1810, R2: 0.5444), PNorm: 169.9816, GNorm: 1.0737
[14/299] timecost: 66.60, lr: 0.000030, Train: (LOSS: 0.1198, MAE: 0.1198, RMSE: 0.1682, R2: 0.5788), Valid: (LOSS: 0.1198, MAE: 0.1198, RMSE: 0.1658, R2: 0.6178), PNorm: 169.7627, GNorm: 1.0378
[15/299] timecost: 66.82, lr: 0.000030, Train: (LOSS: 0.1105, MAE: 0.1105, RMSE: 0.1568, R2: 0.6234), Valid: (LOSS: 0.1196, MAE: 0.1196, RMSE: 0.1732, R2: 0.5800), PNorm: 169.5489, GNorm: 2.7986
[16/299] timecost: 66.66, lr: 0.000030, Train: (LOSS: 0.1070, MAE: 0.1070, RMSE: 0.1532, R2: 0.6302), Valid: (LOSS: 0.1160, MAE: 0.1160, RMSE: 0.1616, R2: 0.6363), PNorm: 169.3395, GNorm: 1.9629
[17/299] timecost: 66.44, lr: 0.000030, Train: (LOSS: 0.1073, MAE: 0.1073, RMSE: 0.1548, R2: 0.6437), Valid: (LOSS: 0.1147, MAE: 0.1147, RMSE: 0.1652, R2: 0.6137), PNorm: 169.1370, GNorm: 2.4822
[18/299] timecost: 66.92, lr: 0.000030, Train: (LOSS: 0.1071, MAE: 0.1071, RMSE: 0.1536, R2: 0.6362), Valid: (LOSS: 0.1084, MAE: 0.1084, RMSE: 0.1565, R2: 0.6585), PNorm: 168.9387, GNorm: 2.4898
[19/299] timecost: 66.52, lr: 0.000030, Train: (LOSS: 0.1019, MAE: 0.1019, RMSE: 0.1466, R2: 0.6713), Valid: (LOSS: 0.0992, MAE: 0.0992, RMSE: 0.1478, R2: 0.6779), PNorm: 168.7459, GNorm: 1.7077
[20/299] timecost: 67.11, lr: 0.000030, Train: (LOSS: 0.1006, MAE: 0.1006, RMSE: 0.1465, R2: 0.6782), Valid: (LOSS: 0.1135, MAE: 0.1135, RMSE: 0.1581, R2: 0.6469), PNorm: 168.5596, GNorm: 2.5236
[21/299] timecost: 66.91, lr: 0.000030, Train: (LOSS: 0.0970, MAE: 0.0970, RMSE: 0.1428, R2: 0.6948), Valid: (LOSS: 0.1069, MAE: 0.1069, RMSE: 0.1531, R2: 0.6696), PNorm: 168.3764, GNorm: 2.9421
[22/299] timecost: 66.47, lr: 0.000030, Train: (LOSS: 0.0908, MAE: 0.0908, RMSE: 0.1345, R2: 0.7281), Valid: (LOSS: 0.0966, MAE: 0.0966, RMSE: 0.1381, R2: 0.7254), PNorm: 168.1969, GNorm: 3.1942
[23/299] timecost: 66.31, lr: 0.000030, Train: (LOSS: 0.0921, MAE: 0.0921, RMSE: 0.1356, R2: 0.7235), Valid: (LOSS: 0.1058, MAE: 0.1058, RMSE: 0.1484, R2: 0.6839), PNorm: 168.0205, GNorm: 1.3914
[24/299] timecost: 66.91, lr: 0.000030, Train: (LOSS: 0.0889, MAE: 0.0889, RMSE: 0.1310, R2: 0.7408), Valid: (LOSS: 0.0935, MAE: 0.0935, RMSE: 0.1386, R2: 0.7203), PNorm: 167.8470, GNorm: 1.7335
[25/299] timecost: 67.01, lr: 0.000030, Train: (LOSS: 0.0853, MAE: 0.0853, RMSE: 0.1276, R2: 0.7501), Valid: (LOSS: 0.0905, MAE: 0.0905, RMSE: 0.1301, R2: 0.7522), PNorm: 167.6752, GNorm: 1.7580
[26/299] timecost: 67.03, lr: 0.000030, Train: (LOSS: 0.0832, MAE: 0.0832, RMSE: 0.1246, R2: 0.7601), Valid: (LOSS: 0.0832, MAE: 0.0832, RMSE: 0.1168, R2: 0.7989), PNorm: 167.5057, GNorm: 2.8753
[27/299] timecost: 65.90, lr: 0.000030, Train: (LOSS: 0.0817, MAE: 0.0817, RMSE: 0.1231, R2: 0.7658), Valid: (LOSS: 0.0989, MAE: 0.0989, RMSE: 0.1479, R2: 0.6821), PNorm: 167.3409, GNorm: 1.7215
[28/299] timecost: 66.28, lr: 0.000030, Train: (LOSS: 0.0855, MAE: 0.0855, RMSE: 0.1253, R2: 0.7609), Valid: (LOSS: 0.0986, MAE: 0.0986, RMSE: 0.1394, R2: 0.7162), PNorm: 167.1828, GNorm: 2.4287
[29/299] timecost: 66.56, lr: 0.000030, Train: (LOSS: 0.0812, MAE: 0.0812, RMSE: 0.1205, R2: 0.7718), Valid: (LOSS: 0.0781, MAE: 0.0781, RMSE: 0.1085, R2: 0.8303), PNorm: 167.0286, GNorm: 1.4350
[30/299] timecost: 65.24, lr: 0.000030, Train: (LOSS: 0.0756, MAE: 0.0756, RMSE: 0.1133, R2: 0.8035), Valid: (LOSS: 0.0748, MAE: 0.0748, RMSE: 0.1111, R2: 0.8144), PNorm: 166.8742, GNorm: 2.3305
[31/299] timecost: 66.73, lr: 0.000030, Train: (LOSS: 0.0768, MAE: 0.0768, RMSE: 0.1161, R2: 0.7913), Valid: (LOSS: 0.0842, MAE: 0.0842, RMSE: 0.1211, R2: 0.7859), PNorm: 166.7233, GNorm: 4.6351
[32/299] timecost: 66.73, lr: 0.000030, Train: (LOSS: 0.0737, MAE: 0.0737, RMSE: 0.1123, R2: 0.8037), Valid: (LOSS: 0.0699, MAE: 0.0699, RMSE: 0.1086, R2: 0.8269), PNorm: 166.5741, GNorm: 1.2259
[33/299] timecost: 66.66, lr: 0.000030, Train: (LOSS: 0.0713, MAE: 0.0713, RMSE: 0.1094, R2: 0.8141), Valid: (LOSS: 0.0715, MAE: 0.0715, RMSE: 0.1028, R2: 0.8431), PNorm: 166.4260, GNorm: 1.4120
[34/299] timecost: 67.20, lr: 0.000030, Train: (LOSS: 0.0672, MAE: 0.0672, RMSE: 0.1039, R2: 0.8313), Valid: (LOSS: 0.0895, MAE: 0.0895, RMSE: 0.1295, R2: 0.7522), PNorm: 166.2807, GNorm: 2.5742
[35/299] timecost: 66.32, lr: 0.000030, Train: (LOSS: 0.0695, MAE: 0.0695, RMSE: 0.1070, R2: 0.8230), Valid: (LOSS: 0.0710, MAE: 0.0710, RMSE: 0.1074, R2: 0.8255), PNorm: 166.1369, GNorm: 1.1124
[36/299] timecost: 66.88, lr: 0.000030, Train: (LOSS: 0.0681, MAE: 0.0681, RMSE: 0.1040, R2: 0.8335), Valid: (LOSS: 0.0734, MAE: 0.0734, RMSE: 0.1090, R2: 0.8240), PNorm: 165.9972, GNorm: 1.3704
[37/299] timecost: 66.79, lr: 0.000030, Train: (LOSS: 0.0647, MAE: 0.0647, RMSE: 0.1008, R2: 0.8416), Valid: (LOSS: 0.0818, MAE: 0.0818, RMSE: 0.1167, R2: 0.8038), PNorm: 165.8582, GNorm: 1.7406
[38/299] timecost: 66.59, lr: 0.000030, Train: (LOSS: 0.0637, MAE: 0.0637, RMSE: 0.0997, R2: 0.8455), Valid: (LOSS: 0.0583, MAE: 0.0583, RMSE: 0.0838, R2: 0.8974), PNorm: 165.7209, GNorm: 1.3670
[39/299] timecost: 67.03, lr: 0.000030, Train: (LOSS: 0.0631, MAE: 0.0631, RMSE: 0.0989, R2: 0.8450), Valid: (LOSS: 0.0606, MAE: 0.0606, RMSE: 0.0866, R2: 0.8921), PNorm: 165.5848, GNorm: 1.6858
[40/299] timecost: 66.83, lr: 0.000030, Train: (LOSS: 0.0583, MAE: 0.0583, RMSE: 0.0936, R2: 0.8604), Valid: (LOSS: 0.0581, MAE: 0.0581, RMSE: 0.0856, R2: 0.8915), PNorm: 165.4508, GNorm: 3.4033
[41/299] timecost: 66.51, lr: 0.000030, Train: (LOSS: 0.0602, MAE: 0.0602, RMSE: 0.0950, R2: 0.8572), Valid: (LOSS: 0.0637, MAE: 0.0637, RMSE: 0.0925, R2: 0.8745), PNorm: 165.3199, GNorm: 1.8730
[42/299] timecost: 66.34, lr: 0.000030, Train: (LOSS: 0.0625, MAE: 0.0625, RMSE: 0.0997, R2: 0.8384), Valid: (LOSS: 0.0598, MAE: 0.0598, RMSE: 0.0883, R2: 0.8875), PNorm: 165.1912, GNorm: 2.0343
[43/299] timecost: 66.96, lr: 0.000030, Train: (LOSS: 0.0536, MAE: 0.0536, RMSE: 0.0843, R2: 0.8868), Valid: (LOSS: 0.0569, MAE: 0.0569, RMSE: 0.0811, R2: 0.9017), PNorm: 165.0624, GNorm: 2.1518
[44/299] timecost: 67.20, lr: 0.000030, Train: (LOSS: 0.0536, MAE: 0.0536, RMSE: 0.0851, R2: 0.8832), Valid: (LOSS: 0.0516, MAE: 0.0516, RMSE: 0.0761, R2: 0.9142), PNorm: 164.9353, GNorm: 1.2967
[45/299] timecost: 66.97, lr: 0.000030, Train: (LOSS: 0.0510, MAE: 0.0510, RMSE: 0.0795, R2: 0.8946), Valid: (LOSS: 0.0537, MAE: 0.0537, RMSE: 0.0784, R2: 0.9070), PNorm: 164.8088, GNorm: 1.1833
[46/299] timecost: 67.08, lr: 0.000030, Train: (LOSS: 0.0519, MAE: 0.0519, RMSE: 0.0821, R2: 0.8929), Valid: (LOSS: 0.0565, MAE: 0.0565, RMSE: 0.0834, R2: 0.8982), PNorm: 164.6836, GNorm: 1.3853
[47/299] timecost: 66.87, lr: 0.000030, Train: (LOSS: 0.0523, MAE: 0.0523, RMSE: 0.0817, R2: 0.8927), Valid: (LOSS: 0.0653, MAE: 0.0653, RMSE: 0.1007, R2: 0.8457), PNorm: 164.5606, GNorm: 3.0182
[48/299] timecost: 66.74, lr: 0.000030, Train: (LOSS: 0.0504, MAE: 0.0504, RMSE: 0.0797, R2: 0.8976), Valid: (LOSS: 0.0498, MAE: 0.0498, RMSE: 0.0735, R2: 0.9178), PNorm: 164.4390, GNorm: 0.9619
[49/299] timecost: 66.43, lr: 0.000030, Train: (LOSS: 0.0487, MAE: 0.0487, RMSE: 0.0776, R2: 0.9028), Valid: (LOSS: 0.0543, MAE: 0.0543, RMSE: 0.0818, R2: 0.9021), PNorm: 164.3169, GNorm: 1.7417
[50/299] timecost: 66.63, lr: 0.000030, Train: (LOSS: 0.0501, MAE: 0.0501, RMSE: 0.0776, R2: 0.8997), Valid: (LOSS: 0.0555, MAE: 0.0555, RMSE: 0.0827, R2: 0.8965), PNorm: 164.1980, GNorm: 1.9402
[51/299] timecost: 66.54, lr: 0.000030, Train: (LOSS: 0.0478, MAE: 0.0478, RMSE: 0.0749, R2: 0.9109), Valid: (LOSS: 0.0551, MAE: 0.0551, RMSE: 0.0804, R2: 0.9043), PNorm: 164.0792, GNorm: 1.6785
[52/299] timecost: 66.99, lr: 0.000030, Train: (LOSS: 0.0495, MAE: 0.0495, RMSE: 0.0783, R2: 0.9019), Valid: (LOSS: 0.0522, MAE: 0.0522, RMSE: 0.0758, R2: 0.9167), PNorm: 163.9603, GNorm: 1.5808
[53/299] timecost: 67.04, lr: 0.000030, Train: (LOSS: 0.0492, MAE: 0.0492, RMSE: 0.0770, R2: 0.9029), Valid: (LOSS: 0.0523, MAE: 0.0523, RMSE: 0.0795, R2: 0.9081), PNorm: 163.8434, GNorm: 1.1903
[54/299] timecost: 66.98, lr: 0.000030, Train: (LOSS: 0.0479, MAE: 0.0479, RMSE: 0.0748, R2: 0.9074), Valid: (LOSS: 0.0466, MAE: 0.0466, RMSE: 0.0705, R2: 0.9258), PNorm: 163.7279, GNorm: 1.4325
[55/299] timecost: 66.67, lr: 0.000030, Train: (LOSS: 0.0450, MAE: 0.0450, RMSE: 0.0722, R2: 0.9134), Valid: (LOSS: 0.0477, MAE: 0.0477, RMSE: 0.0716, R2: 0.9236), PNorm: 163.6126, GNorm: 1.4058
[56/299] timecost: 66.75, lr: 0.000030, Train: (LOSS: 0.0437, MAE: 0.0437, RMSE: 0.0690, R2: 0.9228), Valid: (LOSS: 0.0473, MAE: 0.0473, RMSE: 0.0713, R2: 0.9225), PNorm: 163.4963, GNorm: 0.9059
[57/299] timecost: 67.15, lr: 0.000030, Train: (LOSS: 0.0427, MAE: 0.0427, RMSE: 0.0685, R2: 0.9254), Valid: (LOSS: 0.0547, MAE: 0.0547, RMSE: 0.0777, R2: 0.9100), PNorm: 163.3817, GNorm: 1.4050
[58/299] timecost: 66.58, lr: 0.000030, Train: (LOSS: 0.0447, MAE: 0.0447, RMSE: 0.0707, R2: 0.9182), Valid: (LOSS: 0.0485, MAE: 0.0485, RMSE: 0.0769, R2: 0.9112), PNorm: 163.2682, GNorm: 1.0471
[59/299] timecost: 66.17, lr: 0.000030, Train: (LOSS: 0.0430, MAE: 0.0430, RMSE: 0.0689, R2: 0.9231), Valid: (LOSS: 0.0490, MAE: 0.0490, RMSE: 0.0716, R2: 0.9244), PNorm: 163.1547, GNorm: 1.1443
[60/299] timecost: 65.68, lr: 0.000030, Train: (LOSS: 0.0426, MAE: 0.0426, RMSE: 0.0681, R2: 0.9233), Valid: (LOSS: 0.0482, MAE: 0.0482, RMSE: 0.0722, R2: 0.9215), PNorm: 163.0418, GNorm: 1.5009
[61/299] timecost: 65.98, lr: 0.000030, Train: (LOSS: 0.0415, MAE: 0.0415, RMSE: 0.0670, R2: 0.9258), Valid: (LOSS: 0.0474, MAE: 0.0474, RMSE: 0.0704, R2: 0.9260), PNorm: 162.9296, GNorm: 2.7353
[62/299] timecost: 66.15, lr: 0.000030, Train: (LOSS: 0.0398, MAE: 0.0398, RMSE: 0.0655, R2: 0.9274), Valid: (LOSS: 0.0462, MAE: 0.0462, RMSE: 0.0682, R2: 0.9286), PNorm: 162.8172, GNorm: 1.0614
[63/299] timecost: 65.70, lr: 0.000030, Train: (LOSS: 0.0425, MAE: 0.0425, RMSE: 0.0673, R2: 0.9261), Valid: (LOSS: 0.0433, MAE: 0.0433, RMSE: 0.0645, R2: 0.9369), PNorm: 162.7062, GNorm: 1.6883
[64/299] timecost: 66.04, lr: 0.000030, Train: (LOSS: 0.0414, MAE: 0.0414, RMSE: 0.0664, R2: 0.9267), Valid: (LOSS: 0.0542, MAE: 0.0542, RMSE: 0.0808, R2: 0.9027), PNorm: 162.5956, GNorm: 1.3373
[65/299] timecost: 66.15, lr: 0.000030, Train: (LOSS: 0.0406, MAE: 0.0406, RMSE: 0.0658, R2: 0.9296), Valid: (LOSS: 0.0440, MAE: 0.0440, RMSE: 0.0648, R2: 0.9363), PNorm: 162.4855, GNorm: 1.8563
[66/299] timecost: 66.70, lr: 0.000030, Train: (LOSS: 0.0391, MAE: 0.0391, RMSE: 0.0628, R2: 0.9346), Valid: (LOSS: 0.0432, MAE: 0.0432, RMSE: 0.0646, R2: 0.9363), PNorm: 162.3760, GNorm: 1.2410
[67/299] timecost: 66.58, lr: 0.000030, Train: (LOSS: 0.0373, MAE: 0.0373, RMSE: 0.0604, R2: 0.9389), Valid: (LOSS: 0.0433, MAE: 0.0433, RMSE: 0.0653, R2: 0.9348), PNorm: 162.2659, GNorm: 1.2176
[68/299] timecost: 65.96, lr: 0.000030, Train: (LOSS: 0.0363, MAE: 0.0363, RMSE: 0.0598, R2: 0.9402), Valid: (LOSS: 0.0455, MAE: 0.0455, RMSE: 0.0672, R2: 0.9320), PNorm: 162.1576, GNorm: 1.5078
[69/299] timecost: 66.59, lr: 0.000030, Train: (LOSS: 0.0390, MAE: 0.0390, RMSE: 0.0618, R2: 0.9368), Valid: (LOSS: 0.0462, MAE: 0.0462, RMSE: 0.0678, R2: 0.9329), PNorm: 162.0498, GNorm: 1.3249
[70/299] timecost: 66.28, lr: 0.000030, Train: (LOSS: 0.0379, MAE: 0.0379, RMSE: 0.0621, R2: 0.9351), Valid: (LOSS: 0.0424, MAE: 0.0424, RMSE: 0.0650, R2: 0.9379), PNorm: 161.9421, GNorm: 1.4011
[71/299] timecost: 66.96, lr: 0.000030, Train: (LOSS: 0.0370, MAE: 0.0370, RMSE: 0.0597, R2: 0.9415), Valid: (LOSS: 0.0468, MAE: 0.0468, RMSE: 0.0697, R2: 0.9273), PNorm: 161.8343, GNorm: 1.3450
[72/299] timecost: 66.54, lr: 0.000030, Train: (LOSS: 0.0374, MAE: 0.0374, RMSE: 0.0608, R2: 0.9383), Valid: (LOSS: 0.0417, MAE: 0.0417, RMSE: 0.0605, R2: 0.9456), PNorm: 161.7266, GNorm: 1.4729
[73/299] timecost: 66.58, lr: 0.000030, Train: (LOSS: 0.0369, MAE: 0.0369, RMSE: 0.0590, R2: 0.9412), Valid: (LOSS: 0.0388, MAE: 0.0388, RMSE: 0.0600, R2: 0.9456), PNorm: 161.6200, GNorm: 1.1987
[74/299] timecost: 66.90, lr: 0.000030, Train: (LOSS: 0.0373, MAE: 0.0373, RMSE: 0.0610, R2: 0.9380), Valid: (LOSS: 0.0433, MAE: 0.0433, RMSE: 0.0624, R2: 0.9424), PNorm: 161.5141, GNorm: 1.0790
[75/299] timecost: 66.94, lr: 0.000030, Train: (LOSS: 0.0349, MAE: 0.0349, RMSE: 0.0562, R2: 0.9439), Valid: (LOSS: 0.0388, MAE: 0.0388, RMSE: 0.0585, R2: 0.9484), PNorm: 161.4076, GNorm: 1.0711
[76/299] timecost: 66.19, lr: 0.000030, Train: (LOSS: 0.0351, MAE: 0.0351, RMSE: 0.0567, R2: 0.9437), Valid: (LOSS: 0.0378, MAE: 0.0378, RMSE: 0.0568, R2: 0.9504), PNorm: 161.3010, GNorm: 1.2989
[77/299] timecost: 65.85, lr: 0.000030, Train: (LOSS: 0.0341, MAE: 0.0341, RMSE: 0.0556, R2: 0.9465), Valid: (LOSS: 0.0405, MAE: 0.0405, RMSE: 0.0616, R2: 0.9435), PNorm: 161.1948, GNorm: 1.9940
[78/299] timecost: 65.86, lr: 0.000030, Train: (LOSS: 0.0338, MAE: 0.0338, RMSE: 0.0557, R2: 0.9470), Valid: (LOSS: 0.0382, MAE: 0.0382, RMSE: 0.0577, R2: 0.9486), PNorm: 161.0879, GNorm: 1.3344
[79/299] timecost: 66.54, lr: 0.000030, Train: (LOSS: 0.0347, MAE: 0.0347, RMSE: 0.0559, R2: 0.9471), Valid: (LOSS: 0.0396, MAE: 0.0396, RMSE: 0.0614, R2: 0.9448), PNorm: 160.9815, GNorm: 1.2588
[80/299] timecost: 66.87, lr: 0.000030, Train: (LOSS: 0.0334, MAE: 0.0334, RMSE: 0.0537, R2: 0.9485), Valid: (LOSS: 0.0374, MAE: 0.0374, RMSE: 0.0582, R2: 0.9496), PNorm: 160.8763, GNorm: 1.2867
[81/299] timecost: 66.70, lr: 0.000030, Train: (LOSS: 0.0332, MAE: 0.0332, RMSE: 0.0543, R2: 0.9471), Valid: (LOSS: 0.0420, MAE: 0.0420, RMSE: 0.0624, R2: 0.9415), PNorm: 160.7700, GNorm: 1.5591
[82/299] timecost: 66.68, lr: 0.000030, Train: (LOSS: 0.0325, MAE: 0.0325, RMSE: 0.0530, R2: 0.9515), Valid: (LOSS: 0.0396, MAE: 0.0396, RMSE: 0.0597, R2: 0.9437), PNorm: 160.6642, GNorm: 1.6269
[83/299] timecost: 66.57, lr: 0.000030, Train: (LOSS: 0.0331, MAE: 0.0331, RMSE: 0.0546, R2: 0.9494), Valid: (LOSS: 0.0388, MAE: 0.0388, RMSE: 0.0562, R2: 0.9522), PNorm: 160.5584, GNorm: 0.9188
[84/299] timecost: 66.71, lr: 0.000030, Train: (LOSS: 0.0322, MAE: 0.0322, RMSE: 0.0525, R2: 0.9493), Valid: (LOSS: 0.0376, MAE: 0.0376, RMSE: 0.0567, R2: 0.9505), PNorm: 160.4523, GNorm: 1.0341
[85/299] timecost: 64.44, lr: 0.000030, Train: (LOSS: 0.0324, MAE: 0.0324, RMSE: 0.0523, R2: 0.9514), Valid: (LOSS: 0.0443, MAE: 0.0443, RMSE: 0.0639, R2: 0.9371), PNorm: 160.3464, GNorm: 0.8124
[86/299] timecost: 64.23, lr: 0.000030, Train: (LOSS: 0.0317, MAE: 0.0317, RMSE: 0.0516, R2: 0.9544), Valid: (LOSS: 0.0380, MAE: 0.0380, RMSE: 0.0571, R2: 0.9497), PNorm: 160.2417, GNorm: 1.3635
[87/299] timecost: 64.25, lr: 0.000030, Train: (LOSS: 0.0307, MAE: 0.0307, RMSE: 0.0516, R2: 0.9556), Valid: (LOSS: 0.0373, MAE: 0.0373, RMSE: 0.0555, R2: 0.9529), PNorm: 160.1360, GNorm: 1.0953
[88/299] timecost: 64.11, lr: 0.000030, Train: (LOSS: 0.0298, MAE: 0.0298, RMSE: 0.0488, R2: 0.9585), Valid: (LOSS: 0.0374, MAE: 0.0374, RMSE: 0.0573, R2: 0.9516), PNorm: 160.0305, GNorm: 1.7826
[89/299] timecost: 64.88, lr: 0.000030, Train: (LOSS: 0.0316, MAE: 0.0316, RMSE: 0.0509, R2: 0.9542), Valid: (LOSS: 0.0391, MAE: 0.0391, RMSE: 0.0585, R2: 0.9474), PNorm: 159.9257, GNorm: 0.8911
[90/299] timecost: 64.14, lr: 0.000030, Train: (LOSS: 0.0311, MAE: 0.0311, RMSE: 0.0502, R2: 0.9537), Valid: (LOSS: 0.0364, MAE: 0.0364, RMSE: 0.0548, R2: 0.9536), PNorm: 159.8212, GNorm: 0.8551
[91/299] timecost: 64.33, lr: 0.000030, Train: (LOSS: 0.0307, MAE: 0.0307, RMSE: 0.0511, R2: 0.9542), Valid: (LOSS: 0.0379, MAE: 0.0379, RMSE: 0.0555, R2: 0.9515), PNorm: 159.7176, GNorm: 2.9337
[92/299] timecost: 64.02, lr: 0.000030, Train: (LOSS: 0.0330, MAE: 0.0330, RMSE: 0.0522, R2: 0.9507), Valid: (LOSS: 0.0405, MAE: 0.0405, RMSE: 0.0611, R2: 0.9430), PNorm: 159.6136, GNorm: 1.3352
[93/299] timecost: 63.94, lr: 0.000030, Train: (LOSS: 0.0291, MAE: 0.0291, RMSE: 0.0479, R2: 0.9596), Valid: (LOSS: 0.0369, MAE: 0.0369, RMSE: 0.0555, R2: 0.9527), PNorm: 159.5098, GNorm: 1.1871
[94/299] timecost: 64.01, lr: 0.000030, Train: (LOSS: 0.0301, MAE: 0.0301, RMSE: 0.0489, R2: 0.9573), Valid: (LOSS: 0.0403, MAE: 0.0403, RMSE: 0.0587, R2: 0.9492), PNorm: 159.4050, GNorm: 1.8525
[95/299] timecost: 64.14, lr: 0.000030, Train: (LOSS: 0.0297, MAE: 0.0297, RMSE: 0.0482, R2: 0.9589), Valid: (LOSS: 0.0367, MAE: 0.0367, RMSE: 0.0554, R2: 0.9540), PNorm: 159.3014, GNorm: 0.7862
[96/299] timecost: 63.76, lr: 0.000030, Train: (LOSS: 0.0283, MAE: 0.0283, RMSE: 0.0462, R2: 0.9614), Valid: (LOSS: 0.0345, MAE: 0.0345, RMSE: 0.0528, R2: 0.9573), PNorm: 159.1975, GNorm: 2.7750
[97/299] timecost: 63.67, lr: 0.000030, Train: (LOSS: 0.0286, MAE: 0.0286, RMSE: 0.0457, R2: 0.9619), Valid: (LOSS: 0.0373, MAE: 0.0373, RMSE: 0.0544, R2: 0.9545), PNorm: 159.0935, GNorm: 1.5435
[98/299] timecost: 65.23, lr: 0.000030, Train: (LOSS: 0.0283, MAE: 0.0283, RMSE: 0.0469, R2: 0.9607), Valid: (LOSS: 0.0364, MAE: 0.0364, RMSE: 0.0519, R2: 0.9594), PNorm: 158.9896, GNorm: 1.2300
[99/299] timecost: 65.47, lr: 0.000030, Train: (LOSS: 0.0282, MAE: 0.0282, RMSE: 0.0461, R2: 0.9603), Valid: (LOSS: 0.0381, MAE: 0.0381, RMSE: 0.0567, R2: 0.9508), PNorm: 158.8858, GNorm: 1.3496
[100/299] timecost: 65.15, lr: 0.000030, Train: (LOSS: 0.0279, MAE: 0.0279, RMSE: 0.0459, R2: 0.9631), Valid: (LOSS: 0.0350, MAE: 0.0350, RMSE: 0.0536, R2: 0.9559), PNorm: 158.7825, GNorm: 1.0387
[101/299] timecost: 65.94, lr: 0.000030, Train: (LOSS: 0.0283, MAE: 0.0283, RMSE: 0.0461, R2: 0.9613), Valid: (LOSS: 0.0349, MAE: 0.0349, RMSE: 0.0531, R2: 0.9566), PNorm: 158.6793, GNorm: 1.5085
[102/299] timecost: 66.52, lr: 0.000030, Train: (LOSS: 0.0274, MAE: 0.0274, RMSE: 0.0450, R2: 0.9628), Valid: (LOSS: 0.0363, MAE: 0.0363, RMSE: 0.0536, R2: 0.9545), PNorm: 158.5754, GNorm: 2.2530
[103/299] timecost: 65.90, lr: 0.000030, Train: (LOSS: 0.0281, MAE: 0.0281, RMSE: 0.0464, R2: 0.9626), Valid: (LOSS: 0.0354, MAE: 0.0354, RMSE: 0.0531, R2: 0.9567), PNorm: 158.4727, GNorm: 1.3108
[104/299] timecost: 66.21, lr: 0.000030, Train: (LOSS: 0.0267, MAE: 0.0267, RMSE: 0.0438, R2: 0.9650), Valid: (LOSS: 0.0369, MAE: 0.0369, RMSE: 0.0558, R2: 0.9540), PNorm: 158.3700, GNorm: 1.0470
[105/299] timecost: 66.25, lr: 0.000030, Train: (LOSS: 0.0274, MAE: 0.0274, RMSE: 0.0445, R2: 0.9618), Valid: (LOSS: 0.0356, MAE: 0.0356, RMSE: 0.0533, R2: 0.9571), PNorm: 158.2671, GNorm: 1.0081
[106/299] timecost: 63.25, lr: 0.000030, Train: (LOSS: 0.0252, MAE: 0.0252, RMSE: 0.0417, R2: 0.9656), Valid: (LOSS: 0.0368, MAE: 0.0368, RMSE: 0.0542, R2: 0.9554), PNorm: 158.1634, GNorm: 0.8358
[107/299] timecost: 64.39, lr: 0.000030, Train: (LOSS: 0.0268, MAE: 0.0268, RMSE: 0.0444, R2: 0.9651), Valid: (LOSS: 0.0346, MAE: 0.0346, RMSE: 0.0527, R2: 0.9568), PNorm: 158.0592, GNorm: 1.2999
[108/299] timecost: 65.48, lr: 0.000030, Train: (LOSS: 0.0260, MAE: 0.0260, RMSE: 0.0420, R2: 0.9656), Valid: (LOSS: 0.0344, MAE: 0.0344, RMSE: 0.0512, R2: 0.9599), PNorm: 157.9549, GNorm: 1.0404
[109/299] timecost: 65.55, lr: 0.000030, Train: (LOSS: 0.0254, MAE: 0.0254, RMSE: 0.0421, R2: 0.9687), Valid: (LOSS: 0.0389, MAE: 0.0389, RMSE: 0.0573, R2: 0.9488), PNorm: 157.8516, GNorm: 1.6429
[110/299] timecost: 65.59, lr: 0.000030, Train: (LOSS: 0.0255, MAE: 0.0255, RMSE: 0.0426, R2: 0.9670), Valid: (LOSS: 0.0375, MAE: 0.0375, RMSE: 0.0562, R2: 0.9517), PNorm: 157.7485, GNorm: 1.6702
[111/299] timecost: 65.98, lr: 0.000030, Train: (LOSS: 0.0275, MAE: 0.0275, RMSE: 0.0440, R2: 0.9656), Valid: (LOSS: 0.0368, MAE: 0.0368, RMSE: 0.0548, R2: 0.9536), PNorm: 157.6460, GNorm: 0.9714
[112/299] timecost: 65.93, lr: 0.000030, Train: (LOSS: 0.0266, MAE: 0.0266, RMSE: 0.0434, R2: 0.9654), Valid: (LOSS: 0.0378, MAE: 0.0378, RMSE: 0.0567, R2: 0.9511), PNorm: 157.5433, GNorm: 1.9449
[113/299] timecost: 66.73, lr: 0.000030, Train: (LOSS: 0.0262, MAE: 0.0262, RMSE: 0.0431, R2: 0.9643), Valid: (LOSS: 0.0372, MAE: 0.0372, RMSE: 0.0545, R2: 0.9563), PNorm: 157.4411, GNorm: 1.1239
[114/299] timecost: 66.19, lr: 0.000030, Train: (LOSS: 0.0277, MAE: 0.0277, RMSE: 0.0443, R2: 0.9634), Valid: (LOSS: 0.0374, MAE: 0.0374, RMSE: 0.0571, R2: 0.9518), PNorm: 157.3392, GNorm: 1.3032
[115/299] timecost: 66.32, lr: 0.000030, Train: (LOSS: 0.0247, MAE: 0.0247, RMSE: 0.0417, R2: 0.9673), Valid: (LOSS: 0.0396, MAE: 0.0396, RMSE: 0.0593, R2: 0.9461), PNorm: 157.2362, GNorm: 1.5387
[116/299] timecost: 66.57, lr: 0.000030, Train: (LOSS: 0.0250, MAE: 0.0250, RMSE: 0.0412, R2: 0.9681), Valid: (LOSS: 0.0353, MAE: 0.0353, RMSE: 0.0529, R2: 0.9574), PNorm: 157.1334, GNorm: 0.9365
[117/299] timecost: 66.42, lr: 0.000030, Train: (LOSS: 0.0235, MAE: 0.0235, RMSE: 0.0391, R2: 0.9685), Valid: (LOSS: 0.0332, MAE: 0.0332, RMSE: 0.0488, R2: 0.9638), PNorm: 157.0291, GNorm: 0.9020
[118/299] timecost: 65.86, lr: 0.000030, Train: (LOSS: 0.0249, MAE: 0.0249, RMSE: 0.0430, R2: 0.9641), Valid: (LOSS: 0.0397, MAE: 0.0397, RMSE: 0.0562, R2: 0.9550), PNorm: 156.9266, GNorm: 1.3962
[119/299] timecost: 66.19, lr: 0.000030, Train: (LOSS: 0.0244, MAE: 0.0244, RMSE: 0.0404, R2: 0.9700), Valid: (LOSS: 0.0384, MAE: 0.0384, RMSE: 0.0590, R2: 0.9461), PNorm: 156.8242, GNorm: 1.1902
[120/299] timecost: 66.33, lr: 0.000030, Train: (LOSS: 0.0242, MAE: 0.0242, RMSE: 0.0407, R2: 0.9705), Valid: (LOSS: 0.0342, MAE: 0.0342, RMSE: 0.0491, R2: 0.9639), PNorm: 156.7209, GNorm: 1.0540
[121/299] timecost: 66.41, lr: 0.000030, Train: (LOSS: 0.0242, MAE: 0.0242, RMSE: 0.0400, R2: 0.9695), Valid: (LOSS: 0.0361, MAE: 0.0361, RMSE: 0.0538, R2: 0.9561), PNorm: 156.6175, GNorm: 1.2917
[122/299] timecost: 66.38, lr: 0.000030, Train: (LOSS: 0.0232, MAE: 0.0232, RMSE: 0.0388, R2: 0.9703), Valid: (LOSS: 0.0342, MAE: 0.0342, RMSE: 0.0512, R2: 0.9595), PNorm: 156.5147, GNorm: 1.0873
[123/299] timecost: 65.86, lr: 0.000030, Train: (LOSS: 0.0242, MAE: 0.0242, RMSE: 0.0395, R2: 0.9694), Valid: (LOSS: 0.0407, MAE: 0.0407, RMSE: 0.0605, R2: 0.9450), PNorm: 156.4123, GNorm: 2.1028
[124/299] timecost: 66.00, lr: 0.000030, Train: (LOSS: 0.0233, MAE: 0.0233, RMSE: 0.0382, R2: 0.9724), Valid: (LOSS: 0.0395, MAE: 0.0395, RMSE: 0.0589, R2: 0.9475), PNorm: 156.3097, GNorm: 1.1635
[125/299] timecost: 66.13, lr: 0.000030, Train: (LOSS: 0.0243, MAE: 0.0243, RMSE: 0.0398, R2: 0.9679), Valid: (LOSS: 0.0361, MAE: 0.0361, RMSE: 0.0565, R2: 0.9537), PNorm: 156.2069, GNorm: 1.4750
[126/299] timecost: 66.12, lr: 0.000030, Train: (LOSS: 0.0224, MAE: 0.0224, RMSE: 0.0379, R2: 0.9736), Valid: (LOSS: 0.0347, MAE: 0.0347, RMSE: 0.0521, R2: 0.9593), PNorm: 156.1037, GNorm: 1.2993
[127/299] timecost: 65.95, lr: 0.000030, Train: (LOSS: 0.0232, MAE: 0.0232, RMSE: 0.0382, R2: 0.9723), Valid: (LOSS: 0.0345, MAE: 0.0345, RMSE: 0.0517, R2: 0.9588), PNorm: 156.0008, GNorm: 0.9366
[128/299] timecost: 66.06, lr: 0.000030, Train: (LOSS: 0.0230, MAE: 0.0230, RMSE: 0.0374, R2: 0.9725), Valid: (LOSS: 0.0383, MAE: 0.0383, RMSE: 0.0554, R2: 0.9546), PNorm: 155.8977, GNorm: 1.9443
[129/299] timecost: 65.95, lr: 0.000030, Train: (LOSS: 0.0238, MAE: 0.0238, RMSE: 0.0392, R2: 0.9700), Valid: (LOSS: 0.0347, MAE: 0.0347, RMSE: 0.0516, R2: 0.9592), PNorm: 155.7963, GNorm: 1.9105
[130/299] timecost: 66.14, lr: 0.000030, Train: (LOSS: 0.0220, MAE: 0.0220, RMSE: 0.0365, R2: 0.9747), Valid: (LOSS: 0.0341, MAE: 0.0341, RMSE: 0.0518, R2: 0.9594), PNorm: 155.6926, GNorm: 1.2177
[131/299] timecost: 65.97, lr: 0.000030, Train: (LOSS: 0.0221, MAE: 0.0221, RMSE: 0.0368, R2: 0.9735), Valid: (LOSS: 0.0362, MAE: 0.0362, RMSE: 0.0550, R2: 0.9549), PNorm: 155.5898, GNorm: 1.0467
[132/299] timecost: 66.23, lr: 0.000030, Train: (LOSS: 0.0217, MAE: 0.0217, RMSE: 0.0357, R2: 0.9757), Valid: (LOSS: 0.0347, MAE: 0.0347, RMSE: 0.0520, R2: 0.9603), PNorm: 155.4854, GNorm: 0.8857
[133/299] timecost: 66.04, lr: 0.000030, Train: (LOSS: 0.0220, MAE: 0.0220, RMSE: 0.0362, R2: 0.9744), Valid: (LOSS: 0.0336, MAE: 0.0336, RMSE: 0.0504, R2: 0.9613), PNorm: 155.3820, GNorm: 1.1603
[134/299] timecost: 66.21, lr: 0.000030, Train: (LOSS: 0.0220, MAE: 0.0220, RMSE: 0.0366, R2: 0.9754), Valid: (LOSS: 0.0345, MAE: 0.0345, RMSE: 0.0518, R2: 0.9603), PNorm: 155.2784, GNorm: 1.7668
[135/299] timecost: 65.82, lr: 0.000030, Train: (LOSS: 0.0218, MAE: 0.0218, RMSE: 0.0361, R2: 0.9745), Valid: (LOSS: 0.0349, MAE: 0.0349, RMSE: 0.0514, R2: 0.9618), PNorm: 155.1751, GNorm: 1.2428
[136/299] timecost: 64.13, lr: 0.000030, Train: (LOSS: 0.0217, MAE: 0.0217, RMSE: 0.0356, R2: 0.9758), Valid: (LOSS: 0.0353, MAE: 0.0353, RMSE: 0.0517, R2: 0.9594), PNorm: 155.0727, GNorm: 1.0637
[137/299] timecost: 64.41, lr: 0.000030, Train: (LOSS: 0.0219, MAE: 0.0219, RMSE: 0.0361, R2: 0.9750), Valid: (LOSS: 0.0330, MAE: 0.0330, RMSE: 0.0499, R2: 0.9629), PNorm: 154.9691, GNorm: 1.4016
Epoch 00139: reducing learning rate of group 0 to 2.7000e-05.
[138/299] timecost: 63.26, lr: 0.000027, Train: (LOSS: 0.0207, MAE: 0.0207, RMSE: 0.0351, R2: 0.9750), Valid: (LOSS: 0.0340, MAE: 0.0340, RMSE: 0.0514, R2: 0.9601), PNorm: 154.8655, GNorm: 1.3166
[139/299] timecost: 63.12, lr: 0.000027, Train: (LOSS: 0.0204, MAE: 0.0204, RMSE: 0.0343, R2: 0.9773), Valid: (LOSS: 0.0316, MAE: 0.0316, RMSE: 0.0468, R2: 0.9679), PNorm: 154.7719, GNorm: 1.0464
[140/299] timecost: 64.37, lr: 0.000027, Train: (LOSS: 0.0207, MAE: 0.0207, RMSE: 0.0343, R2: 0.9772), Valid: (LOSS: 0.0334, MAE: 0.0334, RMSE: 0.0484, R2: 0.9654), PNorm: 154.6776, GNorm: 1.4032
[141/299] timecost: 65.48, lr: 0.000027, Train: (LOSS: 0.0199, MAE: 0.0199, RMSE: 0.0335, R2: 0.9782), Valid: (LOSS: 0.0358, MAE: 0.0358, RMSE: 0.0524, R2: 0.9593), PNorm: 154.5841, GNorm: 1.2068
[142/299] timecost: 65.44, lr: 0.000027, Train: (LOSS: 0.0194, MAE: 0.0194, RMSE: 0.0332, R2: 0.9781), Valid: (LOSS: 0.0326, MAE: 0.0326, RMSE: 0.0499, R2: 0.9612), PNorm: 154.4889, GNorm: 1.6805
[143/299] timecost: 65.13, lr: 0.000027, Train: (LOSS: 0.0194, MAE: 0.0194, RMSE: 0.0323, R2: 0.9803), Valid: (LOSS: 0.0342, MAE: 0.0342, RMSE: 0.0506, R2: 0.9619), PNorm: 154.3943, GNorm: 1.2826
[144/299] timecost: 65.67, lr: 0.000027, Train: (LOSS: 0.0199, MAE: 0.0199, RMSE: 0.0324, R2: 0.9793), Valid: (LOSS: 0.0324, MAE: 0.0324, RMSE: 0.0486, R2: 0.9634), PNorm: 154.3000, GNorm: 1.3348
[145/299] timecost: 66.51, lr: 0.000027, Train: (LOSS: 0.0188, MAE: 0.0188, RMSE: 0.0311, R2: 0.9795), Valid: (LOSS: 0.0321, MAE: 0.0321, RMSE: 0.0455, R2: 0.9695), PNorm: 154.2052, GNorm: 1.8636
[146/299] timecost: 66.18, lr: 0.000027, Train: (LOSS: 0.0193, MAE: 0.0193, RMSE: 0.0313, R2: 0.9803), Valid: (LOSS: 0.0354, MAE: 0.0354, RMSE: 0.0518, R2: 0.9598), PNorm: 154.1112, GNorm: 0.8786
[147/299] timecost: 66.18, lr: 0.000027, Train: (LOSS: 0.0200, MAE: 0.0200, RMSE: 0.0331, R2: 0.9792), Valid: (LOSS: 0.0340, MAE: 0.0340, RMSE: 0.0520, R2: 0.9591), PNorm: 154.0169, GNorm: 1.3461
[148/299] timecost: 65.92, lr: 0.000027, Train: (LOSS: 0.0195, MAE: 0.0195, RMSE: 0.0326, R2: 0.9796), Valid: (LOSS: 0.0321, MAE: 0.0321, RMSE: 0.0481, R2: 0.9658), PNorm: 153.9218, GNorm: 1.3519
[149/299] timecost: 66.29, lr: 0.000027, Train: (LOSS: 0.0184, MAE: 0.0184, RMSE: 0.0306, R2: 0.9820), Valid: (LOSS: 0.0342, MAE: 0.0342, RMSE: 0.0517, R2: 0.9599), PNorm: 153.8273, GNorm: 0.8283
[150/299] timecost: 64.04, lr: 0.000027, Train: (LOSS: 0.0201, MAE: 0.0201, RMSE: 0.0339, R2: 0.9763), Valid: (LOSS: 0.0369, MAE: 0.0369, RMSE: 0.0531, R2: 0.9578), PNorm: 153.7343, GNorm: 1.7839
[151/299] timecost: 63.35, lr: 0.000027, Train: (LOSS: 0.0197, MAE: 0.0197, RMSE: 0.0328, R2: 0.9798), Valid: (LOSS: 0.0315, MAE: 0.0315, RMSE: 0.0444, R2: 0.9703), PNorm: 153.6403, GNorm: 1.2385
[152/299] timecost: 63.57, lr: 0.000027, Train: (LOSS: 0.0179, MAE: 0.0179, RMSE: 0.0299, R2: 0.9804), Valid: (LOSS: 0.0324, MAE: 0.0324, RMSE: 0.0459, R2: 0.9685), PNorm: 153.5460, GNorm: 1.2271
[153/299] timecost: 64.95, lr: 0.000027, Train: (LOSS: 0.0183, MAE: 0.0183, RMSE: 0.0303, R2: 0.9824), Valid: (LOSS: 0.0325, MAE: 0.0325, RMSE: 0.0499, R2: 0.9617), PNorm: 153.4511, GNorm: 1.4750
[154/299] timecost: 66.99, lr: 0.000027, Train: (LOSS: 0.0186, MAE: 0.0186, RMSE: 0.0300, R2: 0.9805), Valid: (LOSS: 0.0337, MAE: 0.0337, RMSE: 0.0499, R2: 0.9626), PNorm: 153.3573, GNorm: 1.0618
[155/299] timecost: 66.40, lr: 0.000027, Train: (LOSS: 0.0183, MAE: 0.0183, RMSE: 0.0299, R2: 0.9824), Valid: (LOSS: 0.0334, MAE: 0.0334, RMSE: 0.0495, R2: 0.9630), PNorm: 153.2629, GNorm: 1.3863
[156/299] timecost: 67.07, lr: 0.000027, Train: (LOSS: 0.0183, MAE: 0.0183, RMSE: 0.0297, R2: 0.9808), Valid: (LOSS: 0.0311, MAE: 0.0311, RMSE: 0.0444, R2: 0.9708), PNorm: 153.1692, GNorm: 1.2655
[157/299] timecost: 67.11, lr: 0.000027, Train: (LOSS: 0.0180, MAE: 0.0180, RMSE: 0.0296, R2: 0.9814), Valid: (LOSS: 0.0306, MAE: 0.0306, RMSE: 0.0459, R2: 0.9681), PNorm: 153.0740, GNorm: 0.9016
[158/299] timecost: 66.57, lr: 0.000027, Train: (LOSS: 0.0190, MAE: 0.0190, RMSE: 0.0308, R2: 0.9820), Valid: (LOSS: 0.0324, MAE: 0.0324, RMSE: 0.0462, R2: 0.9677), PNorm: 152.9803, GNorm: 1.7710
[159/299] timecost: 65.87, lr: 0.000027, Train: (LOSS: 0.0189, MAE: 0.0189, RMSE: 0.0306, R2: 0.9806), Valid: (LOSS: 0.0339, MAE: 0.0339, RMSE: 0.0479, R2: 0.9650), PNorm: 152.8869, GNorm: 1.0675
[160/299] timecost: 66.14, lr: 0.000027, Train: (LOSS: 0.0175, MAE: 0.0175, RMSE: 0.0290, R2: 0.9827), Valid: (LOSS: 0.0318, MAE: 0.0318, RMSE: 0.0459, R2: 0.9683), PNorm: 152.7929, GNorm: 1.7239
[161/299] timecost: 66.02, lr: 0.000027, Train: (LOSS: 0.0177, MAE: 0.0177, RMSE: 0.0291, R2: 0.9836), Valid: (LOSS: 0.0343, MAE: 0.0343, RMSE: 0.0504, R2: 0.9621), PNorm: 152.6990, GNorm: 0.6922
[162/299] timecost: 66.31, lr: 0.000027, Train: (LOSS: 0.0180, MAE: 0.0180, RMSE: 0.0298, R2: 0.9819), Valid: (LOSS: 0.0330, MAE: 0.0330, RMSE: 0.0457, R2: 0.9690), PNorm: 152.6053, GNorm: 1.1582
[163/299] timecost: 66.14, lr: 0.000027, Train: (LOSS: 0.0189, MAE: 0.0189, RMSE: 0.0314, R2: 0.9808), Valid: (LOSS: 0.0342, MAE: 0.0342, RMSE: 0.0482, R2: 0.9657), PNorm: 152.5128, GNorm: 0.9209
[164/299] timecost: 66.07, lr: 0.000027, Train: (LOSS: 0.0193, MAE: 0.0193, RMSE: 0.0313, R2: 0.9812), Valid: (LOSS: 0.0337, MAE: 0.0337, RMSE: 0.0468, R2: 0.9669), PNorm: 152.4208, GNorm: 1.5264
[165/299] timecost: 66.23, lr: 0.000027, Train: (LOSS: 0.0180, MAE: 0.0180, RMSE: 0.0294, R2: 0.9830), Valid: (LOSS: 0.0312, MAE: 0.0312, RMSE: 0.0453, R2: 0.9695), PNorm: 152.3283, GNorm: 1.2833
[166/299] timecost: 65.87, lr: 0.000027, Train: (LOSS: 0.0174, MAE: 0.0174, RMSE: 0.0294, R2: 0.9822), Valid: (LOSS: 0.0317, MAE: 0.0317, RMSE: 0.0470, R2: 0.9659), PNorm: 152.2354, GNorm: 0.7940
[167/299] timecost: 66.14, lr: 0.000027, Train: (LOSS: 0.0166, MAE: 0.0166, RMSE: 0.0273, R2: 0.9849), Valid: (LOSS: 0.0317, MAE: 0.0317, RMSE: 0.0443, R2: 0.9705), PNorm: 152.1423, GNorm: 1.1445
[168/299] timecost: 66.10, lr: 0.000027, Train: (LOSS: 0.0165, MAE: 0.0165, RMSE: 0.0280, R2: 0.9829), Valid: (LOSS: 0.0336, MAE: 0.0336, RMSE: 0.0475, R2: 0.9660), PNorm: 152.0475, GNorm: 1.6569
[169/299] timecost: 66.21, lr: 0.000027, Train: (LOSS: 0.0169, MAE: 0.0169, RMSE: 0.0280, R2: 0.9839), Valid: (LOSS: 0.0329, MAE: 0.0329, RMSE: 0.0491, R2: 0.9635), PNorm: 151.9546, GNorm: 1.5839
[170/299] timecost: 66.03, lr: 0.000027, Train: (LOSS: 0.0163, MAE: 0.0163, RMSE: 0.0273, R2: 0.9845), Valid: (LOSS: 0.0310, MAE: 0.0310, RMSE: 0.0426, R2: 0.9726), PNorm: 151.8604, GNorm: 0.8620
[171/299] timecost: 66.08, lr: 0.000027, Train: (LOSS: 0.0166, MAE: 0.0166, RMSE: 0.0271, R2: 0.9844), Valid: (LOSS: 0.0324, MAE: 0.0324, RMSE: 0.0485, R2: 0.9640), PNorm: 151.7670, GNorm: 1.1951
[172/299] timecost: 66.58, lr: 0.000027, Train: (LOSS: 0.0164, MAE: 0.0164, RMSE: 0.0274, R2: 0.9847), Valid: (LOSS: 0.0335, MAE: 0.0335, RMSE: 0.0482, R2: 0.9645), PNorm: 151.6733, GNorm: 1.7787
[173/299] timecost: 65.44, lr: 0.000027, Train: (LOSS: 0.0154, MAE: 0.0154, RMSE: 0.0257, R2: 0.9845), Valid: (LOSS: 0.0342, MAE: 0.0342, RMSE: 0.0506, R2: 0.9613), PNorm: 151.5792, GNorm: 0.8671
[174/299] timecost: 64.05, lr: 0.000027, Train: (LOSS: 0.0160, MAE: 0.0160, RMSE: 0.0273, R2: 0.9846), Valid: (LOSS: 0.0343, MAE: 0.0343, RMSE: 0.0496, R2: 0.9628), PNorm: 151.4856, GNorm: 0.9826
[175/299] timecost: 64.23, lr: 0.000027, Train: (LOSS: 0.0164, MAE: 0.0164, RMSE: 0.0273, R2: 0.9839), Valid: (LOSS: 0.0327, MAE: 0.0327, RMSE: 0.0471, R2: 0.9674), PNorm: 151.3919, GNorm: 0.8397
[176/299] timecost: 64.40, lr: 0.000027, Train: (LOSS: 0.0164, MAE: 0.0164, RMSE: 0.0267, R2: 0.9850), Valid: (LOSS: 0.0317, MAE: 0.0317, RMSE: 0.0458, R2: 0.9680), PNorm: 151.2972, GNorm: 1.4588
[177/299] timecost: 63.68, lr: 0.000027, Train: (LOSS: 0.0155, MAE: 0.0155, RMSE: 0.0260, R2: 0.9851), Valid: (LOSS: 0.0337, MAE: 0.0337, RMSE: 0.0471, R2: 0.9665), PNorm: 151.2031, GNorm: 0.9268
Epoch 00179: reducing learning rate of group 0 to 2.4300e-05.
[178/299] timecost: 65.27, lr: 0.000024, Train: (LOSS: 0.0157, MAE: 0.0157, RMSE: 0.0264, R2: 0.9842), Valid: (LOSS: 0.0332, MAE: 0.0332, RMSE: 0.0484, R2: 0.9641), PNorm: 151.1089, GNorm: 0.7257
[179/299] timecost: 65.52, lr: 0.000024, Train: (LOSS: 0.0150, MAE: 0.0150, RMSE: 0.0255, R2: 0.9862), Valid: (LOSS: 0.0315, MAE: 0.0315, RMSE: 0.0434, R2: 0.9720), PNorm: 151.0249, GNorm: 1.2642
[180/299] timecost: 65.63, lr: 0.000024, Train: (LOSS: 0.0150, MAE: 0.0150, RMSE: 0.0254, R2: 0.9857), Valid: (LOSS: 0.0312, MAE: 0.0312, RMSE: 0.0439, R2: 0.9711), PNorm: 150.9396, GNorm: 1.3181
[181/299] timecost: 65.62, lr: 0.000024, Train: (LOSS: 0.0139, MAE: 0.0139, RMSE: 0.0245, R2: 0.9864), Valid: (LOSS: 0.0305, MAE: 0.0305, RMSE: 0.0432, R2: 0.9718), PNorm: 150.8538, GNorm: 1.1916
[182/299] timecost: 65.60, lr: 0.000024, Train: (LOSS: 0.0149, MAE: 0.0149, RMSE: 0.0255, R2: 0.9848), Valid: (LOSS: 0.0311, MAE: 0.0311, RMSE: 0.0440, R2: 0.9715), PNorm: 150.7686, GNorm: 0.8841
[183/299] timecost: 65.20, lr: 0.000024, Train: (LOSS: 0.0148, MAE: 0.0148, RMSE: 0.0254, R2: 0.9858), Valid: (LOSS: 0.0317, MAE: 0.0317, RMSE: 0.0449, R2: 0.9694), PNorm: 150.6837, GNorm: 0.9549
[184/299] timecost: 65.96, lr: 0.000024, Train: (LOSS: 0.0140, MAE: 0.0140, RMSE: 0.0244, R2: 0.9872), Valid: (LOSS: 0.0320, MAE: 0.0320, RMSE: 0.0452, R2: 0.9695), PNorm: 150.5982, GNorm: 2.0752
[185/299] timecost: 66.42, lr: 0.000024, Train: (LOSS: 0.0149, MAE: 0.0149, RMSE: 0.0250, R2: 0.9867), Valid: (LOSS: 0.0333, MAE: 0.0333, RMSE: 0.0473, R2: 0.9670), PNorm: 150.5122, GNorm: 0.8636
[186/299] timecost: 65.82, lr: 0.000024, Train: (LOSS: 0.0147, MAE: 0.0147, RMSE: 0.0244, R2: 0.9869), Valid: (LOSS: 0.0319, MAE: 0.0319, RMSE: 0.0443, R2: 0.9704), PNorm: 150.4272, GNorm: 0.9670
[187/299] timecost: 66.25, lr: 0.000024, Train: (LOSS: 0.0147, MAE: 0.0147, RMSE: 0.0250, R2: 0.9865), Valid: (LOSS: 0.0318, MAE: 0.0318, RMSE: 0.0465, R2: 0.9664), PNorm: 150.3428, GNorm: 1.1344
[188/299] timecost: 65.96, lr: 0.000024, Train: (LOSS: 0.0141, MAE: 0.0141, RMSE: 0.0242, R2: 0.9868), Valid: (LOSS: 0.0309, MAE: 0.0309, RMSE: 0.0423, R2: 0.9726), PNorm: 150.2571, GNorm: 1.1807
[189/299] timecost: 66.31, lr: 0.000024, Train: (LOSS: 0.0144, MAE: 0.0144, RMSE: 0.0247, R2: 0.9862), Valid: (LOSS: 0.0321, MAE: 0.0321, RMSE: 0.0459, R2: 0.9678), PNorm: 150.1731, GNorm: 1.2264
[190/299] timecost: 66.42, lr: 0.000024, Train: (LOSS: 0.0140, MAE: 0.0140, RMSE: 0.0240, R2: 0.9879), Valid: (LOSS: 0.0323, MAE: 0.0323, RMSE: 0.0469, R2: 0.9673), PNorm: 150.0876, GNorm: 0.8207
[191/299] timecost: 66.00, lr: 0.000024, Train: (LOSS: 0.0140, MAE: 0.0140, RMSE: 0.0237, R2: 0.9879), Valid: (LOSS: 0.0300, MAE: 0.0300, RMSE: 0.0436, R2: 0.9711), PNorm: 150.0019, GNorm: 1.3361
[192/299] timecost: 65.17, lr: 0.000024, Train: (LOSS: 0.0134, MAE: 0.0134, RMSE: 0.0232, R2: 0.9881), Valid: (LOSS: 0.0323, MAE: 0.0323, RMSE: 0.0446, R2: 0.9703), PNorm: 149.9172, GNorm: 1.1333
[193/299] timecost: 65.54, lr: 0.000024, Train: (LOSS: 0.0158, MAE: 0.0158, RMSE: 0.0255, R2: 0.9859), Valid: (LOSS: 0.0313, MAE: 0.0313, RMSE: 0.0445, R2: 0.9697), PNorm: 149.8339, GNorm: 0.7782
[194/299] timecost: 65.39, lr: 0.000024, Train: (LOSS: 0.0139, MAE: 0.0139, RMSE: 0.0235, R2: 0.9878), Valid: (LOSS: 0.0330, MAE: 0.0330, RMSE: 0.0451, R2: 0.9696), PNorm: 149.7493, GNorm: 1.3471
[195/299] timecost: 65.43, lr: 0.000024, Train: (LOSS: 0.0143, MAE: 0.0143, RMSE: 0.0245, R2: 0.9877), Valid: (LOSS: 0.0298, MAE: 0.0298, RMSE: 0.0436, R2: 0.9711), PNorm: 149.6650, GNorm: 1.1612
[196/299] timecost: 65.82, lr: 0.000024, Train: (LOSS: 0.0142, MAE: 0.0142, RMSE: 0.0239, R2: 0.9869), Valid: (LOSS: 0.0313, MAE: 0.0313, RMSE: 0.0435, R2: 0.9719), PNorm: 149.5817, GNorm: 1.1570
[197/299] timecost: 65.53, lr: 0.000024, Train: (LOSS: 0.0138, MAE: 0.0138, RMSE: 0.0240, R2: 0.9871), Valid: (LOSS: 0.0306, MAE: 0.0306, RMSE: 0.0442, R2: 0.9697), PNorm: 149.4974, GNorm: 0.9508
[198/299] timecost: 65.34, lr: 0.000024, Train: (LOSS: 0.0133, MAE: 0.0133, RMSE: 0.0231, R2: 0.9876), Valid: (LOSS: 0.0297, MAE: 0.0297, RMSE: 0.0431, R2: 0.9717), PNorm: 149.4129, GNorm: 1.0554
[199/299] timecost: 65.55, lr: 0.000024, Train: (LOSS: 0.0137, MAE: 0.0137, RMSE: 0.0230, R2: 0.9877), Valid: (LOSS: 0.0318, MAE: 0.0318, RMSE: 0.0445, R2: 0.9696), PNorm: 149.3294, GNorm: 0.6434
[200/299] timecost: 65.54, lr: 0.000024, Train: (LOSS: 0.0136, MAE: 0.0136, RMSE: 0.0234, R2: 0.9884), Valid: (LOSS: 0.0327, MAE: 0.0327, RMSE: 0.0477, R2: 0.9649), PNorm: 149.2453, GNorm: 1.2118
[201/299] timecost: 65.70, lr: 0.000024, Train: (LOSS: 0.0142, MAE: 0.0142, RMSE: 0.0232, R2: 0.9885), Valid: (LOSS: 0.0302, MAE: 0.0302, RMSE: 0.0431, R2: 0.9710), PNorm: 149.1620, GNorm: 1.0739
[202/299] timecost: 65.37, lr: 0.000024, Train: (LOSS: 0.0143, MAE: 0.0143, RMSE: 0.0246, R2: 0.9866), Valid: (LOSS: 0.0324, MAE: 0.0324, RMSE: 0.0459, R2: 0.9684), PNorm: 149.0789, GNorm: 0.9409
[203/299] timecost: 65.53, lr: 0.000024, Train: (LOSS: 0.0133, MAE: 0.0133, RMSE: 0.0222, R2: 0.9880), Valid: (LOSS: 0.0334, MAE: 0.0334, RMSE: 0.0474, R2: 0.9667), PNorm: 148.9951, GNorm: 0.6224
[204/299] timecost: 65.68, lr: 0.000024, Train: (LOSS: 0.0141, MAE: 0.0141, RMSE: 0.0234, R2: 0.9880), Valid: (LOSS: 0.0312, MAE: 0.0312, RMSE: 0.0433, R2: 0.9722), PNorm: 148.9118, GNorm: 0.9461
[205/299] timecost: 65.11, lr: 0.000024, Train: (LOSS: 0.0138, MAE: 0.0138, RMSE: 0.0230, R2: 0.9887), Valid: (LOSS: 0.0308, MAE: 0.0308, RMSE: 0.0420, R2: 0.9733), PNorm: 148.8282, GNorm: 1.0253
[206/299] timecost: 65.39, lr: 0.000024, Train: (LOSS: 0.0133, MAE: 0.0133, RMSE: 0.0228, R2: 0.9881), Valid: (LOSS: 0.0330, MAE: 0.0330, RMSE: 0.0492, R2: 0.9623), PNorm: 148.7455, GNorm: 0.9034
[207/299] timecost: 65.47, lr: 0.000024, Train: (LOSS: 0.0143, MAE: 0.0143, RMSE: 0.0240, R2: 0.9878), Valid: (LOSS: 0.0306, MAE: 0.0306, RMSE: 0.0421, R2: 0.9737), PNorm: 148.6632, GNorm: 0.9507
[208/299] timecost: 65.48, lr: 0.000024, Train: (LOSS: 0.0132, MAE: 0.0132, RMSE: 0.0221, R2: 0.9887), Valid: (LOSS: 0.0306, MAE: 0.0306, RMSE: 0.0425, R2: 0.9732), PNorm: 148.5794, GNorm: 1.4706
[209/299] timecost: 66.24, lr: 0.000024, Train: (LOSS: 0.0128, MAE: 0.0128, RMSE: 0.0218, R2: 0.9901), Valid: (LOSS: 0.0307, MAE: 0.0307, RMSE: 0.0429, R2: 0.9716), PNorm: 148.4966, GNorm: 1.8863
[210/299] timecost: 65.94, lr: 0.000024, Train: (LOSS: 0.0133, MAE: 0.0133, RMSE: 0.0226, R2: 0.9893), Valid: (LOSS: 0.0301, MAE: 0.0301, RMSE: 0.0440, R2: 0.9700), PNorm: 148.4137, GNorm: 1.0183
[211/299] timecost: 66.00, lr: 0.000024, Train: (LOSS: 0.0134, MAE: 0.0134, RMSE: 0.0221, R2: 0.9885), Valid: (LOSS: 0.0324, MAE: 0.0324, RMSE: 0.0466, R2: 0.9671), PNorm: 148.3312, GNorm: 1.7137
Epoch 00213: reducing learning rate of group 0 to 2.1870e-05.
[212/299] timecost: 65.85, lr: 0.000022, Train: (LOSS: 0.0136, MAE: 0.0136, RMSE: 0.0228, R2: 0.9890), Valid: (LOSS: 0.0309, MAE: 0.0309, RMSE: 0.0425, R2: 0.9735), PNorm: 148.2487, GNorm: 1.6983
[213/299] timecost: 66.30, lr: 0.000022, Train: (LOSS: 0.0129, MAE: 0.0129, RMSE: 0.0217, R2: 0.9888), Valid: (LOSS: 0.0306, MAE: 0.0306, RMSE: 0.0445, R2: 0.9700), PNorm: 148.1742, GNorm: 1.5607
[214/299] timecost: 65.77, lr: 0.000022, Train: (LOSS: 0.0121, MAE: 0.0121, RMSE: 0.0213, R2: 0.9896), Valid: (LOSS: 0.0296, MAE: 0.0296, RMSE: 0.0428, R2: 0.9725), PNorm: 148.1005, GNorm: 1.1094
[215/299] timecost: 66.02, lr: 0.000022, Train: (LOSS: 0.0120, MAE: 0.0120, RMSE: 0.0207, R2: 0.9902), Valid: (LOSS: 0.0310, MAE: 0.0310, RMSE: 0.0434, R2: 0.9709), PNorm: 148.0257, GNorm: 1.0650
[216/299] timecost: 65.96, lr: 0.000022, Train: (LOSS: 0.0123, MAE: 0.0123, RMSE: 0.0208, R2: 0.9904), Valid: (LOSS: 0.0319, MAE: 0.0319, RMSE: 0.0449, R2: 0.9692), PNorm: 147.9510, GNorm: 1.0405
[217/299] timecost: 66.42, lr: 0.000022, Train: (LOSS: 0.0125, MAE: 0.0125, RMSE: 0.0208, R2: 0.9910), Valid: (LOSS: 0.0309, MAE: 0.0309, RMSE: 0.0437, R2: 0.9705), PNorm: 147.8766, GNorm: 0.6915
[218/299] timecost: 66.37, lr: 0.000022, Train: (LOSS: 0.0122, MAE: 0.0122, RMSE: 0.0204, R2: 0.9910), Valid: (LOSS: 0.0304, MAE: 0.0304, RMSE: 0.0426, R2: 0.9726), PNorm: 147.8018, GNorm: 0.7653
[219/299] timecost: 65.88, lr: 0.000022, Train: (LOSS: 0.0117, MAE: 0.0117, RMSE: 0.0198, R2: 0.9902), Valid: (LOSS: 0.0312, MAE: 0.0312, RMSE: 0.0447, R2: 0.9696), PNorm: 147.7274, GNorm: 0.9108
[220/299] timecost: 65.85, lr: 0.000022, Train: (LOSS: 0.0117, MAE: 0.0117, RMSE: 0.0200, R2: 0.9904), Valid: (LOSS: 0.0295, MAE: 0.0295, RMSE: 0.0406, R2: 0.9749), PNorm: 147.6517, GNorm: 0.7757
[221/299] timecost: 65.72, lr: 0.000022, Train: (LOSS: 0.0115, MAE: 0.0115, RMSE: 0.0197, R2: 0.9909), Valid: (LOSS: 0.0301, MAE: 0.0301, RMSE: 0.0430, R2: 0.9720), PNorm: 147.5776, GNorm: 0.7342
[222/299] timecost: 65.59, lr: 0.000022, Train: (LOSS: 0.0122, MAE: 0.0122, RMSE: 0.0206, R2: 0.9911), Valid: (LOSS: 0.0313, MAE: 0.0313, RMSE: 0.0435, R2: 0.9714), PNorm: 147.5031, GNorm: 0.9696
[223/299] timecost: 65.68, lr: 0.000022, Train: (LOSS: 0.0122, MAE: 0.0122, RMSE: 0.0203, R2: 0.9905), Valid: (LOSS: 0.0307, MAE: 0.0307, RMSE: 0.0449, R2: 0.9693), PNorm: 147.4292, GNorm: 1.7157
[224/299] timecost: 65.63, lr: 0.000022, Train: (LOSS: 0.0113, MAE: 0.0113, RMSE: 0.0194, R2: 0.9915), Valid: (LOSS: 0.0297, MAE: 0.0297, RMSE: 0.0413, R2: 0.9742), PNorm: 147.3549, GNorm: 0.9519
[225/299] timecost: 65.07, lr: 0.000022, Train: (LOSS: 0.0120, MAE: 0.0120, RMSE: 0.0197, R2: 0.9920), Valid: (LOSS: 0.0295, MAE: 0.0295, RMSE: 0.0415, R2: 0.9741), PNorm: 147.2809, GNorm: 1.5333
[226/299] timecost: 65.61, lr: 0.000022, Train: (LOSS: 0.0120, MAE: 0.0120, RMSE: 0.0201, R2: 0.9917), Valid: (LOSS: 0.0303, MAE: 0.0303, RMSE: 0.0437, R2: 0.9710), PNorm: 147.2067, GNorm: 0.8544
[227/299] timecost: 65.62, lr: 0.000022, Train: (LOSS: 0.0119, MAE: 0.0119, RMSE: 0.0189, R2: 0.9926), Valid: (LOSS: 0.0321, MAE: 0.0321, RMSE: 0.0436, R2: 0.9708), PNorm: 147.1330, GNorm: 1.2759
[228/299] timecost: 65.48, lr: 0.000022, Train: (LOSS: 0.0117, MAE: 0.0117, RMSE: 0.0198, R2: 0.9915), Valid: (LOSS: 0.0293, MAE: 0.0293, RMSE: 0.0414, R2: 0.9738), PNorm: 147.0597, GNorm: 0.8624
[229/299] timecost: 65.61, lr: 0.000022, Train: (LOSS: 0.0113, MAE: 0.0113, RMSE: 0.0189, R2: 0.9927), Valid: (LOSS: 0.0318, MAE: 0.0318, RMSE: 0.0457, R2: 0.9689), PNorm: 146.9855, GNorm: 1.3203
[230/299] timecost: 65.26, lr: 0.000022, Train: (LOSS: 0.0117, MAE: 0.0117, RMSE: 0.0192, R2: 0.9924), Valid: (LOSS: 0.0305, MAE: 0.0305, RMSE: 0.0421, R2: 0.9729), PNorm: 146.9122, GNorm: 0.9427
[231/299] timecost: 65.54, lr: 0.000022, Train: (LOSS: 0.0116, MAE: 0.0116, RMSE: 0.0192, R2: 0.9924), Valid: (LOSS: 0.0309, MAE: 0.0309, RMSE: 0.0457, R2: 0.9678), PNorm: 146.8400, GNorm: 0.9488
[232/299] timecost: 65.43, lr: 0.000022, Train: (LOSS: 0.0121, MAE: 0.0121, RMSE: 0.0197, R2: 0.9920), Valid: (LOSS: 0.0316, MAE: 0.0316, RMSE: 0.0436, R2: 0.9716), PNorm: 146.7670, GNorm: 1.0652
[233/299] timecost: 65.86, lr: 0.000022, Train: (LOSS: 0.0115, MAE: 0.0115, RMSE: 0.0187, R2: 0.9922), Valid: (LOSS: 0.0314, MAE: 0.0314, RMSE: 0.0439, R2: 0.9708), PNorm: 146.6942, GNorm: 0.7061
[234/299] timecost: 65.99, lr: 0.000022, Train: (LOSS: 0.0115, MAE: 0.0115, RMSE: 0.0185, R2: 0.9936), Valid: (LOSS: 0.0315, MAE: 0.0315, RMSE: 0.0423, R2: 0.9731), PNorm: 146.6219, GNorm: 0.9144
Epoch 00236: reducing learning rate of group 0 to 1.9683e-05.
[235/299] timecost: 66.48, lr: 0.000020, Train: (LOSS: 0.0121, MAE: 0.0121, RMSE: 0.0190, R2: 0.9930), Valid: (LOSS: 0.0300, MAE: 0.0300, RMSE: 0.0413, R2: 0.9744), PNorm: 146.5497, GNorm: 0.8207
[236/299] timecost: 66.01, lr: 0.000020, Train: (LOSS: 0.0106, MAE: 0.0106, RMSE: 0.0169, R2: 0.9942), Valid: (LOSS: 0.0308, MAE: 0.0308, RMSE: 0.0413, R2: 0.9746), PNorm: 146.4838, GNorm: 0.9620
[237/299] timecost: 66.17, lr: 0.000020, Train: (LOSS: 0.0106, MAE: 0.0106, RMSE: 0.0166, R2: 0.9945), Valid: (LOSS: 0.0311, MAE: 0.0311, RMSE: 0.0430, R2: 0.9722), PNorm: 146.4189, GNorm: 1.2678
[238/299] timecost: 65.98, lr: 0.000020, Train: (LOSS: 0.0107, MAE: 0.0107, RMSE: 0.0171, R2: 0.9945), Valid: (LOSS: 0.0304, MAE: 0.0304, RMSE: 0.0420, R2: 0.9733), PNorm: 146.3539, GNorm: 1.0342
[239/299] timecost: 65.96, lr: 0.000020, Train: (LOSS: 0.0105, MAE: 0.0105, RMSE: 0.0166, R2: 0.9945), Valid: (LOSS: 0.0300, MAE: 0.0300, RMSE: 0.0412, R2: 0.9744), PNorm: 146.2880, GNorm: 0.8832
[240/299] timecost: 65.63, lr: 0.000020, Train: (LOSS: 0.0100, MAE: 0.0100, RMSE: 0.0161, R2: 0.9950), Valid: (LOSS: 0.0300, MAE: 0.0300, RMSE: 0.0420, R2: 0.9731), PNorm: 146.2232, GNorm: 1.3670
[241/299] timecost: 66.06, lr: 0.000020, Train: (LOSS: 0.0103, MAE: 0.0103, RMSE: 0.0163, R2: 0.9951), Valid: (LOSS: 0.0302, MAE: 0.0302, RMSE: 0.0414, R2: 0.9740), PNorm: 146.1576, GNorm: 0.8247
[242/299] timecost: 66.08, lr: 0.000020, Train: (LOSS: 0.0098, MAE: 0.0098, RMSE: 0.0158, R2: 0.9956), Valid: (LOSS: 0.0306, MAE: 0.0306, RMSE: 0.0418, R2: 0.9733), PNorm: 146.0925, GNorm: 1.0426
[243/299] timecost: 65.84, lr: 0.000020, Train: (LOSS: 0.0101, MAE: 0.0101, RMSE: 0.0163, R2: 0.9953), Valid: (LOSS: 0.0303, MAE: 0.0303, RMSE: 0.0418, R2: 0.9731), PNorm: 146.0275, GNorm: 0.6812
[244/299] timecost: 66.61, lr: 0.000020, Train: (LOSS: 0.0104, MAE: 0.0104, RMSE: 0.0162, R2: 0.9955), Valid: (LOSS: 0.0293, MAE: 0.0293, RMSE: 0.0417, R2: 0.9733), PNorm: 145.9630, GNorm: 0.8003
[245/299] timecost: 66.05, lr: 0.000020, Train: (LOSS: 0.0099, MAE: 0.0099, RMSE: 0.0156, R2: 0.9956), Valid: (LOSS: 0.0308, MAE: 0.0308, RMSE: 0.0437, R2: 0.9709), PNorm: 145.8980, GNorm: 0.6187
[246/299] timecost: 66.10, lr: 0.000020, Train: (LOSS: 0.0104, MAE: 0.0104, RMSE: 0.0163, R2: 0.9955), Valid: (LOSS: 0.0318, MAE: 0.0318, RMSE: 0.0445, R2: 0.9705), PNorm: 145.8332, GNorm: 0.9440
[247/299] timecost: 66.15, lr: 0.000020, Train: (LOSS: 0.0101, MAE: 0.0101, RMSE: 0.0156, R2: 0.9955), Valid: (LOSS: 0.0289, MAE: 0.0289, RMSE: 0.0402, R2: 0.9749), PNorm: 145.7695, GNorm: 1.9842
[248/299] timecost: 65.89, lr: 0.000020, Train: (LOSS: 0.0106, MAE: 0.0106, RMSE: 0.0161, R2: 0.9954), Valid: (LOSS: 0.0277, MAE: 0.0277, RMSE: 0.0386, R2: 0.9777), PNorm: 145.7054, GNorm: 1.1415
[249/299] timecost: 66.25, lr: 0.000020, Train: (LOSS: 0.0099, MAE: 0.0099, RMSE: 0.0151, R2: 0.9961), Valid: (LOSS: 0.0307, MAE: 0.0307, RMSE: 0.0431, R2: 0.9717), PNorm: 145.6416, GNorm: 1.1982
[250/299] timecost: 65.71, lr: 0.000020, Train: (LOSS: 0.0103, MAE: 0.0103, RMSE: 0.0151, R2: 0.9962), Valid: (LOSS: 0.0306, MAE: 0.0306, RMSE: 0.0424, R2: 0.9724), PNorm: 145.5779, GNorm: 0.7473
[251/299] timecost: 65.96, lr: 0.000020, Train: (LOSS: 0.0097, MAE: 0.0097, RMSE: 0.0148, R2: 0.9962), Valid: (LOSS: 0.0303, MAE: 0.0303, RMSE: 0.0415, R2: 0.9735), PNorm: 145.5138, GNorm: 0.7416
[252/299] timecost: 65.85, lr: 0.000020, Train: (LOSS: 0.0092, MAE: 0.0092, RMSE: 0.0143, R2: 0.9963), Valid: (LOSS: 0.0290, MAE: 0.0290, RMSE: 0.0405, R2: 0.9745), PNorm: 145.4497, GNorm: 1.3005
[253/299] timecost: 66.58, lr: 0.000020, Train: (LOSS: 0.0092, MAE: 0.0092, RMSE: 0.0140, R2: 0.9967), Valid: (LOSS: 0.0304, MAE: 0.0304, RMSE: 0.0429, R2: 0.9718), PNorm: 145.3851, GNorm: 1.7931
[254/299] timecost: 66.03, lr: 0.000020, Train: (LOSS: 0.0096, MAE: 0.0096, RMSE: 0.0147, R2: 0.9963), Valid: (LOSS: 0.0313, MAE: 0.0313, RMSE: 0.0433, R2: 0.9715), PNorm: 145.3209, GNorm: 0.9316
[255/299] timecost: 66.19, lr: 0.000020, Train: (LOSS: 0.0094, MAE: 0.0094, RMSE: 0.0141, R2: 0.9966), Valid: (LOSS: 0.0303, MAE: 0.0303, RMSE: 0.0420, R2: 0.9729), PNorm: 145.2567, GNorm: 0.9658
[256/299] timecost: 66.37, lr: 0.000020, Train: (LOSS: 0.0097, MAE: 0.0097, RMSE: 0.0147, R2: 0.9963), Valid: (LOSS: 0.0308, MAE: 0.0308, RMSE: 0.0439, R2: 0.9702), PNorm: 145.1931, GNorm: 0.6989
[257/299] timecost: 65.97, lr: 0.000020, Train: (LOSS: 0.0101, MAE: 0.0101, RMSE: 0.0151, R2: 0.9963), Valid: (LOSS: 0.0292, MAE: 0.0292, RMSE: 0.0399, R2: 0.9754), PNorm: 145.1294, GNorm: 1.0449
[258/299] timecost: 65.56, lr: 0.000020, Train: (LOSS: 0.0094, MAE: 0.0094, RMSE: 0.0139, R2: 0.9967), Valid: (LOSS: 0.0306, MAE: 0.0306, RMSE: 0.0425, R2: 0.9724), PNorm: 145.0661, GNorm: 0.8787
[259/299] timecost: 66.18, lr: 0.000020, Train: (LOSS: 0.0092, MAE: 0.0092, RMSE: 0.0138, R2: 0.9967), Valid: (LOSS: 0.0293, MAE: 0.0293, RMSE: 0.0412, R2: 0.9742), PNorm: 145.0027, GNorm: 1.1591
[260/299] timecost: 65.89, lr: 0.000020, Train: (LOSS: 0.0088, MAE: 0.0088, RMSE: 0.0134, R2: 0.9967), Valid: (LOSS: 0.0297, MAE: 0.0297, RMSE: 0.0412, R2: 0.9740), PNorm: 144.9385, GNorm: 0.9295
[261/299] timecost: 66.28, lr: 0.000020, Train: (LOSS: 0.0090, MAE: 0.0090, RMSE: 0.0134, R2: 0.9971), Valid: (LOSS: 0.0301, MAE: 0.0301, RMSE: 0.0417, R2: 0.9736), PNorm: 144.8747, GNorm: 0.9490
[262/299] timecost: 66.35, lr: 0.000020, Train: (LOSS: 0.0094, MAE: 0.0094, RMSE: 0.0143, R2: 0.9967), Valid: (LOSS: 0.0310, MAE: 0.0310, RMSE: 0.0428, R2: 0.9726), PNorm: 144.8115, GNorm: 1.0776
[263/299] timecost: 66.11, lr: 0.000020, Train: (LOSS: 0.0092, MAE: 0.0092, RMSE: 0.0138, R2: 0.9965), Valid: (LOSS: 0.0316, MAE: 0.0316, RMSE: 0.0440, R2: 0.9705), PNorm: 144.7485, GNorm: 1.8575
[264/299] timecost: 66.64, lr: 0.000020, Train: (LOSS: 0.0096, MAE: 0.0096, RMSE: 0.0143, R2: 0.9966), Valid: (LOSS: 0.0299, MAE: 0.0299, RMSE: 0.0424, R2: 0.9727), PNorm: 144.6850, GNorm: 1.0795
[265/299] timecost: 66.04, lr: 0.000020, Train: (LOSS: 0.0089, MAE: 0.0089, RMSE: 0.0133, R2: 0.9971), Valid: (LOSS: 0.0297, MAE: 0.0297, RMSE: 0.0419, R2: 0.9732), PNorm: 144.6213, GNorm: 1.2951
[266/299] timecost: 66.07, lr: 0.000020, Train: (LOSS: 0.0101, MAE: 0.0101, RMSE: 0.0149, R2: 0.9963), Valid: (LOSS: 0.0306, MAE: 0.0306, RMSE: 0.0414, R2: 0.9743), PNorm: 144.5587, GNorm: 1.4553
[267/299] timecost: 65.98, lr: 0.000020, Train: (LOSS: 0.0089, MAE: 0.0089, RMSE: 0.0132, R2: 0.9971), Valid: (LOSS: 0.0299, MAE: 0.0299, RMSE: 0.0413, R2: 0.9735), PNorm: 144.4960, GNorm: 1.4776
[268/299] timecost: 66.10, lr: 0.000020, Train: (LOSS: 0.0086, MAE: 0.0086, RMSE: 0.0130, R2: 0.9972), Valid: (LOSS: 0.0300, MAE: 0.0300, RMSE: 0.0422, R2: 0.9723), PNorm: 144.4323, GNorm: 1.1484
Epoch 00270: reducing learning rate of group 0 to 1.7715e-05.
[269/299] timecost: 66.18, lr: 0.000018, Train: (LOSS: 0.0084, MAE: 0.0084, RMSE: 0.0125, R2: 0.9974), Valid: (LOSS: 0.0301, MAE: 0.0301, RMSE: 0.0421, R2: 0.9726), PNorm: 144.3692, GNorm: 1.0191
[270/299] timecost: 66.10, lr: 0.000018, Train: (LOSS: 0.0083, MAE: 0.0083, RMSE: 0.0127, R2: 0.9971), Valid: (LOSS: 0.0302, MAE: 0.0302, RMSE: 0.0429, R2: 0.9721), PNorm: 144.3127, GNorm: 1.1282
[271/299] timecost: 66.25, lr: 0.000018, Train: (LOSS: 0.0082, MAE: 0.0082, RMSE: 0.0122, R2: 0.9975), Valid: (LOSS: 0.0303, MAE: 0.0303, RMSE: 0.0425, R2: 0.9722), PNorm: 144.2553, GNorm: 0.9558
[272/299] timecost: 66.05, lr: 0.000018, Train: (LOSS: 0.0087, MAE: 0.0087, RMSE: 0.0128, R2: 0.9972), Valid: (LOSS: 0.0298, MAE: 0.0298, RMSE: 0.0416, R2: 0.9733), PNorm: 144.1987, GNorm: 0.6901
[273/299] timecost: 66.06, lr: 0.000018, Train: (LOSS: 0.0084, MAE: 0.0084, RMSE: 0.0126, R2: 0.9974), Valid: (LOSS: 0.0294, MAE: 0.0294, RMSE: 0.0411, R2: 0.9734), PNorm: 144.1423, GNorm: 1.1724
[274/299] timecost: 65.65, lr: 0.000018, Train: (LOSS: 0.0085, MAE: 0.0085, RMSE: 0.0126, R2: 0.9974), Valid: (LOSS: 0.0300, MAE: 0.0300, RMSE: 0.0420, R2: 0.9727), PNorm: 144.0860, GNorm: 1.3115
[275/299] timecost: 66.42, lr: 0.000018, Train: (LOSS: 0.0079, MAE: 0.0079, RMSE: 0.0119, R2: 0.9977), Valid: (LOSS: 0.0296, MAE: 0.0296, RMSE: 0.0418, R2: 0.9725), PNorm: 144.0292, GNorm: 1.2829
[276/299] timecost: 66.07, lr: 0.000018, Train: (LOSS: 0.0081, MAE: 0.0081, RMSE: 0.0121, R2: 0.9976), Valid: (LOSS: 0.0295, MAE: 0.0295, RMSE: 0.0409, R2: 0.9744), PNorm: 143.9731, GNorm: 0.8663
[277/299] timecost: 66.18, lr: 0.000018, Train: (LOSS: 0.0084, MAE: 0.0084, RMSE: 0.0123, R2: 0.9974), Valid: (LOSS: 0.0300, MAE: 0.0300, RMSE: 0.0415, R2: 0.9729), PNorm: 143.9168, GNorm: 1.2151
[278/299] timecost: 65.99, lr: 0.000018, Train: (LOSS: 0.0085, MAE: 0.0085, RMSE: 0.0125, R2: 0.9975), Valid: (LOSS: 0.0300, MAE: 0.0300, RMSE: 0.0418, R2: 0.9733), PNorm: 143.8605, GNorm: 1.0232
[279/299] timecost: 66.95, lr: 0.000018, Train: (LOSS: 0.0081, MAE: 0.0081, RMSE: 0.0121, R2: 0.9976), Valid: (LOSS: 0.0290, MAE: 0.0290, RMSE: 0.0409, R2: 0.9742), PNorm: 143.8044, GNorm: 0.7581
[280/299] timecost: 66.77, lr: 0.000018, Train: (LOSS: 0.0083, MAE: 0.0083, RMSE: 0.0123, R2: 0.9974), Valid: (LOSS: 0.0303, MAE: 0.0303, RMSE: 0.0421, R2: 0.9727), PNorm: 143.7483, GNorm: 1.1904
[281/299] timecost: 66.38, lr: 0.000018, Train: (LOSS: 0.0085, MAE: 0.0085, RMSE: 0.0125, R2: 0.9974), Valid: (LOSS: 0.0301, MAE: 0.0301, RMSE: 0.0437, R2: 0.9703), PNorm: 143.6930, GNorm: 1.0651
[282/299] timecost: 66.05, lr: 0.000018, Train: (LOSS: 0.0088, MAE: 0.0088, RMSE: 0.0128, R2: 0.9973), Valid: (LOSS: 0.0299, MAE: 0.0299, RMSE: 0.0418, R2: 0.9735), PNorm: 143.6374, GNorm: 1.0863
[283/299] timecost: 66.01, lr: 0.000018, Train: (LOSS: 0.0083, MAE: 0.0083, RMSE: 0.0122, R2: 0.9976), Valid: (LOSS: 0.0298, MAE: 0.0298, RMSE: 0.0422, R2: 0.9727), PNorm: 143.5821, GNorm: 0.8526
[284/299] timecost: 66.06, lr: 0.000018, Train: (LOSS: 0.0083, MAE: 0.0083, RMSE: 0.0122, R2: 0.9974), Valid: (LOSS: 0.0308, MAE: 0.0308, RMSE: 0.0422, R2: 0.9728), PNorm: 143.5263, GNorm: 0.8909
[285/299] timecost: 64.89, lr: 0.000018, Train: (LOSS: 0.0083, MAE: 0.0083, RMSE: 0.0123, R2: 0.9976), Valid: (LOSS: 0.0310, MAE: 0.0310, RMSE: 0.0443, R2: 0.9704), PNorm: 143.4710, GNorm: 1.0498
[286/299] timecost: 64.96, lr: 0.000018, Train: (LOSS: 0.0082, MAE: 0.0082, RMSE: 0.0122, R2: 0.9975), Valid: (LOSS: 0.0300, MAE: 0.0300, RMSE: 0.0433, R2: 0.9715), PNorm: 143.4153, GNorm: 0.7708
[287/299] timecost: 64.91, lr: 0.000018, Train: (LOSS: 0.0076, MAE: 0.0076, RMSE: 0.0112, R2: 0.9980), Valid: (LOSS: 0.0295, MAE: 0.0295, RMSE: 0.0408, R2: 0.9746), PNorm: 143.3592, GNorm: 0.7492
[288/299] timecost: 65.23, lr: 0.000018, Train: (LOSS: 0.0074, MAE: 0.0074, RMSE: 0.0112, R2: 0.9979), Valid: (LOSS: 0.0293, MAE: 0.0293, RMSE: 0.0407, R2: 0.9745), PNorm: 143.3034, GNorm: 0.9227
[289/299] timecost: 65.34, lr: 0.000018, Train: (LOSS: 0.0080, MAE: 0.0080, RMSE: 0.0117, R2: 0.9977), Valid: (LOSS: 0.0290, MAE: 0.0290, RMSE: 0.0405, R2: 0.9747), PNorm: 143.2475, GNorm: 0.9643
Epoch 00291: reducing learning rate of group 0 to 1.5943e-05.
[290/299] timecost: 65.75, lr: 0.000016, Train: (LOSS: 0.0076, MAE: 0.0076, RMSE: 0.0113, R2: 0.9979), Valid: (LOSS: 0.0291, MAE: 0.0291, RMSE: 0.0402, R2: 0.9753), PNorm: 143.1919, GNorm: 0.8007
[291/299] timecost: 65.41, lr: 0.000016, Train: (LOSS: 0.0076, MAE: 0.0076, RMSE: 0.0114, R2: 0.9978), Valid: (LOSS: 0.0290, MAE: 0.0290, RMSE: 0.0409, R2: 0.9744), PNorm: 143.1413, GNorm: 1.4409
[292/299] timecost: 65.19, lr: 0.000016, Train: (LOSS: 0.0075, MAE: 0.0075, RMSE: 0.0111, R2: 0.9979), Valid: (LOSS: 0.0299, MAE: 0.0299, RMSE: 0.0414, R2: 0.9736), PNorm: 143.0912, GNorm: 1.6330
[293/299] timecost: 65.35, lr: 0.000016, Train: (LOSS: 0.0077, MAE: 0.0077, RMSE: 0.0115, R2: 0.9978), Valid: (LOSS: 0.0299, MAE: 0.0299, RMSE: 0.0414, R2: 0.9738), PNorm: 143.0410, GNorm: 1.4944
[294/299] timecost: 65.15, lr: 0.000016, Train: (LOSS: 0.0072, MAE: 0.0072, RMSE: 0.0108, R2: 0.9980), Valid: (LOSS: 0.0297, MAE: 0.0297, RMSE: 0.0423, R2: 0.9725), PNorm: 142.9904, GNorm: 1.6368
[295/299] timecost: 65.21, lr: 0.000016, Train: (LOSS: 0.0073, MAE: 0.0073, RMSE: 0.0108, R2: 0.9981), Valid: (LOSS: 0.0287, MAE: 0.0287, RMSE: 0.0402, R2: 0.9751), PNorm: 142.9402, GNorm: 1.2595
[296/299] timecost: 65.17, lr: 0.000016, Train: (LOSS: 0.0071, MAE: 0.0071, RMSE: 0.0108, R2: 0.9981), Valid: (LOSS: 0.0291, MAE: 0.0291, RMSE: 0.0407, R2: 0.9745), PNorm: 142.8902, GNorm: 0.8264
[297/299] timecost: 65.08, lr: 0.000016, Train: (LOSS: 0.0071, MAE: 0.0071, RMSE: 0.0106, R2: 0.9980), Valid: (LOSS: 0.0295, MAE: 0.0295, RMSE: 0.0413, R2: 0.9737), PNorm: 142.8401, GNorm: 0.6745
[298/299] timecost: 65.21, lr: 0.000016, Train: (LOSS: 0.0070, MAE: 0.0070, RMSE: 0.0105, R2: 0.9981), Valid: (LOSS: 0.0295, MAE: 0.0295, RMSE: 0.0410, R2: 0.9742), PNorm: 142.7894, GNorm: 1.3039
[299/299] timecost: 65.50, lr: 0.000016, Train: (LOSS: 0.0068, MAE: 0.0068, RMSE: 0.0103, R2: 0.9981), Valid: (LOSS: 0.0300, MAE: 0.0300, RMSE: 0.0420, R2: 0.9725), PNorm: 142.7384, GNorm: 1.3335
==========Training End==========
==========Test Best Model==========
================Final Results=======================
mse: 0.0338 +- 0.0000:
rmse: 0.0501 +- 0.0000:
mae: 0.0338 +- 0.0000:
r2: 0.9612 +- 0.0000:
tensor([[0., 0.],
        [0., 0.],
        [0., 0.],
        ...,
        [0., 0.],
        [0., 0.],
        [0., 0.]], device='cuda:0')
