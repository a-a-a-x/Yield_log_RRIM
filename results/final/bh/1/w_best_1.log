cuda available with GPU: Tesla V100-PCIE-16GB
==========Load Seed==========
set_random_seed
0
==========Training Start==========
Training Graphs:  2491
Valid Graphs:  277
Test Graphs:  1187
============Loading pretrained weights to generate initialization============
============Creating new layers============
============Creating Model============
Training Graphs Batches:  78
Valid Graphs Batches:  9
Test Graphs Batches:  37
[0/299] timecost: 67.11, lr: 0.000030, Train: (LOSS: 0.2309, MAE: 0.2309, RMSE: 0.2732, R2: -0.0584), Valid: (LOSS: 0.2220, MAE: 0.2220, RMSE: 0.2571, R2: 0.0647), PNorm: 174.3928, GNorm: 1.3711
[1/299] timecost: 64.95, lr: 0.000030, Train: (LOSS: 0.2109, MAE: 0.2109, RMSE: 0.2533, R2: 0.0871), Valid: (LOSS: 0.2067, MAE: 0.2067, RMSE: 0.2393, R2: 0.1867), PNorm: 173.7532, GNorm: 2.0207
[2/299] timecost: 64.34, lr: 0.000030, Train: (LOSS: 0.1993, MAE: 0.1993, RMSE: 0.2419, R2: 0.1668), Valid: (LOSS: 0.1850, MAE: 0.1850, RMSE: 0.2221, R2: 0.3000), PNorm: 173.2767, GNorm: 2.4839
[3/299] timecost: 64.28, lr: 0.000030, Train: (LOSS: 0.1797, MAE: 0.1797, RMSE: 0.2221, R2: 0.2740), Valid: (LOSS: 0.1642, MAE: 0.1642, RMSE: 0.2056, R2: 0.3879), PNorm: 172.9096, GNorm: 1.7887
[4/299] timecost: 64.03, lr: 0.000030, Train: (LOSS: 0.1487, MAE: 0.1487, RMSE: 0.1945, R2: 0.4484), Valid: (LOSS: 0.1383, MAE: 0.1383, RMSE: 0.1841, R2: 0.5158), PNorm: 172.6017, GNorm: 1.3432
[5/299] timecost: 64.64, lr: 0.000030, Train: (LOSS: 0.1433, MAE: 0.1433, RMSE: 0.1900, R2: 0.4641), Valid: (LOSS: 0.1273, MAE: 0.1273, RMSE: 0.1660, R2: 0.6047), PNorm: 172.3091, GNorm: 1.5760
[6/299] timecost: 65.20, lr: 0.000030, Train: (LOSS: 0.1282, MAE: 0.1282, RMSE: 0.1756, R2: 0.5426), Valid: (LOSS: 0.1197, MAE: 0.1197, RMSE: 0.1634, R2: 0.6162), PNorm: 172.0265, GNorm: 3.4307
[7/299] timecost: 64.69, lr: 0.000030, Train: (LOSS: 0.1273, MAE: 0.1273, RMSE: 0.1758, R2: 0.5303), Valid: (LOSS: 0.1214, MAE: 0.1214, RMSE: 0.1631, R2: 0.6154), PNorm: 171.7540, GNorm: 3.0531
[8/299] timecost: 64.26, lr: 0.000030, Train: (LOSS: 0.1291, MAE: 0.1291, RMSE: 0.1763, R2: 0.5239), Valid: (LOSS: 0.1192, MAE: 0.1192, RMSE: 0.1643, R2: 0.6077), PNorm: 171.4900, GNorm: 1.6488
[9/299] timecost: 64.01, lr: 0.000030, Train: (LOSS: 0.1198, MAE: 0.1198, RMSE: 0.1659, R2: 0.5940), Valid: (LOSS: 0.1191, MAE: 0.1191, RMSE: 0.1613, R2: 0.6252), PNorm: 171.2295, GNorm: 1.4352
[10/299] timecost: 64.17, lr: 0.000030, Train: (LOSS: 0.1152, MAE: 0.1152, RMSE: 0.1632, R2: 0.6015), Valid: (LOSS: 0.1050, MAE: 0.1050, RMSE: 0.1494, R2: 0.6806), PNorm: 170.9728, GNorm: 1.4816
[11/299] timecost: 64.08, lr: 0.000030, Train: (LOSS: 0.1136, MAE: 0.1136, RMSE: 0.1604, R2: 0.6186), Valid: (LOSS: 0.1106, MAE: 0.1106, RMSE: 0.1549, R2: 0.6575), PNorm: 170.7189, GNorm: 4.0012
[12/299] timecost: 63.97, lr: 0.000030, Train: (LOSS: 0.1084, MAE: 0.1084, RMSE: 0.1554, R2: 0.6379), Valid: (LOSS: 0.1167, MAE: 0.1167, RMSE: 0.1654, R2: 0.6005), PNorm: 170.4689, GNorm: 1.2763
[13/299] timecost: 64.45, lr: 0.000030, Train: (LOSS: 0.1064, MAE: 0.1064, RMSE: 0.1528, R2: 0.6543), Valid: (LOSS: 0.1132, MAE: 0.1132, RMSE: 0.1618, R2: 0.6191), PNorm: 170.2215, GNorm: 1.5087
[14/299] timecost: 64.57, lr: 0.000030, Train: (LOSS: 0.1054, MAE: 0.1054, RMSE: 0.1511, R2: 0.6574), Valid: (LOSS: 0.0968, MAE: 0.0968, RMSE: 0.1373, R2: 0.7284), PNorm: 169.9791, GNorm: 1.7377
[15/299] timecost: 64.09, lr: 0.000030, Train: (LOSS: 0.0993, MAE: 0.0993, RMSE: 0.1434, R2: 0.6797), Valid: (LOSS: 0.1043, MAE: 0.1043, RMSE: 0.1519, R2: 0.6685), PNorm: 169.7394, GNorm: 1.4991
[16/299] timecost: 64.03, lr: 0.000030, Train: (LOSS: 0.0984, MAE: 0.0984, RMSE: 0.1428, R2: 0.6934), Valid: (LOSS: 0.0915, MAE: 0.0915, RMSE: 0.1333, R2: 0.7458), PNorm: 169.5032, GNorm: 2.3696
[17/299] timecost: 63.87, lr: 0.000030, Train: (LOSS: 0.0931, MAE: 0.0931, RMSE: 0.1371, R2: 0.7115), Valid: (LOSS: 0.0915, MAE: 0.0915, RMSE: 0.1326, R2: 0.7443), PNorm: 169.2699, GNorm: 1.9940
[18/299] timecost: 63.90, lr: 0.000030, Train: (LOSS: 0.0932, MAE: 0.0932, RMSE: 0.1360, R2: 0.7203), Valid: (LOSS: 0.0950, MAE: 0.0950, RMSE: 0.1367, R2: 0.7326), PNorm: 169.0390, GNorm: 2.6477
[19/299] timecost: 63.95, lr: 0.000030, Train: (LOSS: 0.0889, MAE: 0.0889, RMSE: 0.1316, R2: 0.7370), Valid: (LOSS: 0.0911, MAE: 0.0911, RMSE: 0.1357, R2: 0.7318), PNorm: 168.8091, GNorm: 2.8305
[20/299] timecost: 64.73, lr: 0.000030, Train: (LOSS: 0.0910, MAE: 0.0910, RMSE: 0.1340, R2: 0.7221), Valid: (LOSS: 0.0860, MAE: 0.0860, RMSE: 0.1289, R2: 0.7557), PNorm: 168.5820, GNorm: 1.6592
[21/299] timecost: 64.30, lr: 0.000030, Train: (LOSS: 0.0848, MAE: 0.0848, RMSE: 0.1295, R2: 0.7477), Valid: (LOSS: 0.0853, MAE: 0.0853, RMSE: 0.1280, R2: 0.7594), PNorm: 168.3562, GNorm: 3.2125
[22/299] timecost: 63.97, lr: 0.000030, Train: (LOSS: 0.0831, MAE: 0.0831, RMSE: 0.1260, R2: 0.7572), Valid: (LOSS: 0.0845, MAE: 0.0845, RMSE: 0.1236, R2: 0.7749), PNorm: 168.1319, GNorm: 2.3123
[23/299] timecost: 63.98, lr: 0.000030, Train: (LOSS: 0.0847, MAE: 0.0847, RMSE: 0.1246, R2: 0.7632), Valid: (LOSS: 0.0866, MAE: 0.0866, RMSE: 0.1238, R2: 0.7774), PNorm: 167.9116, GNorm: 2.9976
[24/299] timecost: 63.96, lr: 0.000030, Train: (LOSS: 0.0835, MAE: 0.0835, RMSE: 0.1262, R2: 0.7584), Valid: (LOSS: 0.0874, MAE: 0.0874, RMSE: 0.1317, R2: 0.7441), PNorm: 167.6930, GNorm: 2.8017
[25/299] timecost: 63.83, lr: 0.000030, Train: (LOSS: 0.0868, MAE: 0.0868, RMSE: 0.1298, R2: 0.7414), Valid: (LOSS: 0.0855, MAE: 0.0855, RMSE: 0.1274, R2: 0.7580), PNorm: 167.4787, GNorm: 1.6906
[26/299] timecost: 63.82, lr: 0.000030, Train: (LOSS: 0.0807, MAE: 0.0807, RMSE: 0.1225, R2: 0.7699), Valid: (LOSS: 0.0846, MAE: 0.0846, RMSE: 0.1263, R2: 0.7656), PNorm: 167.2650, GNorm: 2.8858
[27/299] timecost: 64.05, lr: 0.000030, Train: (LOSS: 0.0752, MAE: 0.0752, RMSE: 0.1167, R2: 0.8000), Valid: (LOSS: 0.0825, MAE: 0.0825, RMSE: 0.1211, R2: 0.7810), PNorm: 167.0524, GNorm: 1.6536
[28/299] timecost: 64.25, lr: 0.000030, Train: (LOSS: 0.0737, MAE: 0.0737, RMSE: 0.1141, R2: 0.7996), Valid: (LOSS: 0.0894, MAE: 0.0894, RMSE: 0.1339, R2: 0.7358), PNorm: 166.8420, GNorm: 1.5036
[29/299] timecost: 63.92, lr: 0.000030, Train: (LOSS: 0.0745, MAE: 0.0745, RMSE: 0.1155, R2: 0.7875), Valid: (LOSS: 0.0788, MAE: 0.0788, RMSE: 0.1113, R2: 0.8159), PNorm: 166.6339, GNorm: 2.7670
[30/299] timecost: 63.86, lr: 0.000030, Train: (LOSS: 0.0758, MAE: 0.0758, RMSE: 0.1172, R2: 0.7865), Valid: (LOSS: 0.0843, MAE: 0.0843, RMSE: 0.1239, R2: 0.7698), PNorm: 166.4283, GNorm: 2.0939
[31/299] timecost: 63.90, lr: 0.000030, Train: (LOSS: 0.0734, MAE: 0.0734, RMSE: 0.1124, R2: 0.8017), Valid: (LOSS: 0.0779, MAE: 0.0779, RMSE: 0.1162, R2: 0.7986), PNorm: 166.2230, GNorm: 1.0524
[32/299] timecost: 63.85, lr: 0.000030, Train: (LOSS: 0.0732, MAE: 0.0732, RMSE: 0.1124, R2: 0.8083), Valid: (LOSS: 0.0765, MAE: 0.0765, RMSE: 0.1139, R2: 0.8021), PNorm: 166.0195, GNorm: 1.8295
[33/299] timecost: 63.98, lr: 0.000030, Train: (LOSS: 0.0721, MAE: 0.0721, RMSE: 0.1118, R2: 0.8114), Valid: (LOSS: 0.0777, MAE: 0.0777, RMSE: 0.1199, R2: 0.7840), PNorm: 165.8182, GNorm: 2.0631
[34/299] timecost: 64.66, lr: 0.000030, Train: (LOSS: 0.0645, MAE: 0.0645, RMSE: 0.1028, R2: 0.8378), Valid: (LOSS: 0.0697, MAE: 0.0697, RMSE: 0.1050, R2: 0.8365), PNorm: 165.6165, GNorm: 1.6677
[35/299] timecost: 64.91, lr: 0.000030, Train: (LOSS: 0.0667, MAE: 0.0667, RMSE: 0.1056, R2: 0.8253), Valid: (LOSS: 0.0722, MAE: 0.0722, RMSE: 0.1089, R2: 0.8253), PNorm: 165.4180, GNorm: 1.2049
[36/299] timecost: 63.99, lr: 0.000030, Train: (LOSS: 0.0653, MAE: 0.0653, RMSE: 0.1037, R2: 0.8366), Valid: (LOSS: 0.0878, MAE: 0.0878, RMSE: 0.1250, R2: 0.7706), PNorm: 165.2207, GNorm: 1.3759
[37/299] timecost: 63.86, lr: 0.000030, Train: (LOSS: 0.0647, MAE: 0.0647, RMSE: 0.1020, R2: 0.8396), Valid: (LOSS: 0.0814, MAE: 0.0814, RMSE: 0.1278, R2: 0.7609), PNorm: 165.0263, GNorm: 1.0793
[38/299] timecost: 63.99, lr: 0.000030, Train: (LOSS: 0.0634, MAE: 0.0634, RMSE: 0.1015, R2: 0.8406), Valid: (LOSS: 0.0732, MAE: 0.0732, RMSE: 0.1129, R2: 0.8070), PNorm: 164.8330, GNorm: 1.6749
[39/299] timecost: 63.92, lr: 0.000030, Train: (LOSS: 0.0629, MAE: 0.0629, RMSE: 0.0991, R2: 0.8442), Valid: (LOSS: 0.0704, MAE: 0.0704, RMSE: 0.1075, R2: 0.8240), PNorm: 164.6408, GNorm: 1.6683
[40/299] timecost: 64.02, lr: 0.000030, Train: (LOSS: 0.0582, MAE: 0.0582, RMSE: 0.0932, R2: 0.8677), Valid: (LOSS: 0.0683, MAE: 0.0683, RMSE: 0.1034, R2: 0.8373), PNorm: 164.4495, GNorm: 1.4046
[41/299] timecost: 64.25, lr: 0.000030, Train: (LOSS: 0.0621, MAE: 0.0621, RMSE: 0.0984, R2: 0.8453), Valid: (LOSS: 0.0837, MAE: 0.0837, RMSE: 0.1271, R2: 0.7540), PNorm: 164.2604, GNorm: 1.7362
[42/299] timecost: 64.16, lr: 0.000030, Train: (LOSS: 0.0631, MAE: 0.0631, RMSE: 0.1007, R2: 0.8376), Valid: (LOSS: 0.0700, MAE: 0.0700, RMSE: 0.1029, R2: 0.8405), PNorm: 164.0725, GNorm: 1.1218
[43/299] timecost: 64.20, lr: 0.000030, Train: (LOSS: 0.0561, MAE: 0.0561, RMSE: 0.0883, R2: 0.8800), Valid: (LOSS: 0.0601, MAE: 0.0601, RMSE: 0.0923, R2: 0.8731), PNorm: 163.8854, GNorm: 1.5375
[44/299] timecost: 64.45, lr: 0.000030, Train: (LOSS: 0.0513, MAE: 0.0513, RMSE: 0.0834, R2: 0.8860), Valid: (LOSS: 0.0620, MAE: 0.0620, RMSE: 0.0950, R2: 0.8662), PNorm: 163.6989, GNorm: 1.5056
[45/299] timecost: 64.44, lr: 0.000030, Train: (LOSS: 0.0542, MAE: 0.0542, RMSE: 0.0851, R2: 0.8810), Valid: (LOSS: 0.0588, MAE: 0.0588, RMSE: 0.0911, R2: 0.8754), PNorm: 163.5140, GNorm: 2.3604
[46/299] timecost: 64.62, lr: 0.000030, Train: (LOSS: 0.0530, MAE: 0.0530, RMSE: 0.0845, R2: 0.8891), Valid: (LOSS: 0.0607, MAE: 0.0607, RMSE: 0.0935, R2: 0.8685), PNorm: 163.3304, GNorm: 1.5471
[47/299] timecost: 64.79, lr: 0.000030, Train: (LOSS: 0.0508, MAE: 0.0508, RMSE: 0.0800, R2: 0.8967), Valid: (LOSS: 0.0631, MAE: 0.0631, RMSE: 0.0969, R2: 0.8591), PNorm: 163.1477, GNorm: 1.2746
[48/299] timecost: 63.98, lr: 0.000030, Train: (LOSS: 0.0477, MAE: 0.0477, RMSE: 0.0781, R2: 0.8996), Valid: (LOSS: 0.0657, MAE: 0.0657, RMSE: 0.0996, R2: 0.8486), PNorm: 162.9651, GNorm: 2.3425
[49/299] timecost: 63.66, lr: 0.000030, Train: (LOSS: 0.0503, MAE: 0.0503, RMSE: 0.0812, R2: 0.8943), Valid: (LOSS: 0.0590, MAE: 0.0590, RMSE: 0.0883, R2: 0.8800), PNorm: 162.7837, GNorm: 1.7074
[50/299] timecost: 66.97, lr: 0.000030, Train: (LOSS: 0.0480, MAE: 0.0480, RMSE: 0.0777, R2: 0.9042), Valid: (LOSS: 0.0689, MAE: 0.0689, RMSE: 0.1035, R2: 0.8448), PNorm: 162.6031, GNorm: 1.5587
[51/299] timecost: 64.58, lr: 0.000030, Train: (LOSS: 0.0474, MAE: 0.0474, RMSE: 0.0763, R2: 0.9071), Valid: (LOSS: 0.0585, MAE: 0.0585, RMSE: 0.0915, R2: 0.8744), PNorm: 162.4236, GNorm: 1.8099
[52/299] timecost: 64.30, lr: 0.000030, Train: (LOSS: 0.0474, MAE: 0.0474, RMSE: 0.0761, R2: 0.9029), Valid: (LOSS: 0.0567, MAE: 0.0567, RMSE: 0.0891, R2: 0.8798), PNorm: 162.2450, GNorm: 1.2360
[53/299] timecost: 65.23, lr: 0.000030, Train: (LOSS: 0.0442, MAE: 0.0442, RMSE: 0.0714, R2: 0.9168), Valid: (LOSS: 0.0551, MAE: 0.0551, RMSE: 0.0836, R2: 0.8943), PNorm: 162.0664, GNorm: 1.2718
[54/299] timecost: 65.94, lr: 0.000030, Train: (LOSS: 0.0428, MAE: 0.0428, RMSE: 0.0701, R2: 0.9200), Valid: (LOSS: 0.0562, MAE: 0.0562, RMSE: 0.0859, R2: 0.8896), PNorm: 161.8880, GNorm: 1.3941
[55/299] timecost: 66.85, lr: 0.000030, Train: (LOSS: 0.0422, MAE: 0.0422, RMSE: 0.0687, R2: 0.9229), Valid: (LOSS: 0.0528, MAE: 0.0528, RMSE: 0.0835, R2: 0.8944), PNorm: 161.7097, GNorm: 1.2371
[56/299] timecost: 66.87, lr: 0.000030, Train: (LOSS: 0.0438, MAE: 0.0438, RMSE: 0.0711, R2: 0.9213), Valid: (LOSS: 0.0603, MAE: 0.0603, RMSE: 0.0905, R2: 0.8764), PNorm: 161.5324, GNorm: 1.0957
[57/299] timecost: 66.95, lr: 0.000030, Train: (LOSS: 0.0411, MAE: 0.0411, RMSE: 0.0685, R2: 0.9238), Valid: (LOSS: 0.0551, MAE: 0.0551, RMSE: 0.0861, R2: 0.8882), PNorm: 161.3550, GNorm: 1.2946
[58/299] timecost: 66.88, lr: 0.000030, Train: (LOSS: 0.0415, MAE: 0.0415, RMSE: 0.0683, R2: 0.9207), Valid: (LOSS: 0.0540, MAE: 0.0540, RMSE: 0.0834, R2: 0.8930), PNorm: 161.1779, GNorm: 2.2623
[59/299] timecost: 67.10, lr: 0.000030, Train: (LOSS: 0.0421, MAE: 0.0421, RMSE: 0.0693, R2: 0.9215), Valid: (LOSS: 0.0550, MAE: 0.0550, RMSE: 0.0865, R2: 0.8884), PNorm: 161.0017, GNorm: 1.3875
[60/299] timecost: 66.82, lr: 0.000030, Train: (LOSS: 0.0401, MAE: 0.0401, RMSE: 0.0647, R2: 0.9299), Valid: (LOSS: 0.0530, MAE: 0.0530, RMSE: 0.0843, R2: 0.8949), PNorm: 160.8254, GNorm: 1.6489
[61/299] timecost: 66.88, lr: 0.000030, Train: (LOSS: 0.0399, MAE: 0.0399, RMSE: 0.0651, R2: 0.9289), Valid: (LOSS: 0.0508, MAE: 0.0508, RMSE: 0.0799, R2: 0.9031), PNorm: 160.6490, GNorm: 1.1183
[62/299] timecost: 66.97, lr: 0.000030, Train: (LOSS: 0.0390, MAE: 0.0390, RMSE: 0.0623, R2: 0.9319), Valid: (LOSS: 0.0544, MAE: 0.0544, RMSE: 0.0837, R2: 0.8898), PNorm: 160.4729, GNorm: 1.5124
[63/299] timecost: 66.88, lr: 0.000030, Train: (LOSS: 0.0399, MAE: 0.0399, RMSE: 0.0666, R2: 0.9298), Valid: (LOSS: 0.0542, MAE: 0.0542, RMSE: 0.0847, R2: 0.8925), PNorm: 160.2962, GNorm: 0.9726
[64/299] timecost: 64.93, lr: 0.000030, Train: (LOSS: 0.0430, MAE: 0.0430, RMSE: 0.0701, R2: 0.9167), Valid: (LOSS: 0.0501, MAE: 0.0501, RMSE: 0.0796, R2: 0.9015), PNorm: 160.1221, GNorm: 1.3497
[65/299] timecost: 63.40, lr: 0.000030, Train: (LOSS: 0.0369, MAE: 0.0369, RMSE: 0.0613, R2: 0.9362), Valid: (LOSS: 0.0476, MAE: 0.0476, RMSE: 0.0772, R2: 0.9111), PNorm: 159.9455, GNorm: 1.5545
[66/299] timecost: 63.92, lr: 0.000030, Train: (LOSS: 0.0365, MAE: 0.0365, RMSE: 0.0607, R2: 0.9393), Valid: (LOSS: 0.0482, MAE: 0.0482, RMSE: 0.0770, R2: 0.9104), PNorm: 159.7683, GNorm: 1.5583
[67/299] timecost: 63.30, lr: 0.000030, Train: (LOSS: 0.0372, MAE: 0.0372, RMSE: 0.0607, R2: 0.9377), Valid: (LOSS: 0.0497, MAE: 0.0497, RMSE: 0.0768, R2: 0.9100), PNorm: 159.5910, GNorm: 1.2334
[68/299] timecost: 64.00, lr: 0.000030, Train: (LOSS: 0.0363, MAE: 0.0363, RMSE: 0.0603, R2: 0.9395), Valid: (LOSS: 0.0495, MAE: 0.0495, RMSE: 0.0795, R2: 0.9046), PNorm: 159.4132, GNorm: 1.0679
[69/299] timecost: 65.36, lr: 0.000030, Train: (LOSS: 0.0353, MAE: 0.0353, RMSE: 0.0592, R2: 0.9413), Valid: (LOSS: 0.0495, MAE: 0.0495, RMSE: 0.0775, R2: 0.9091), PNorm: 159.2352, GNorm: 1.1912
[70/299] timecost: 65.12, lr: 0.000030, Train: (LOSS: 0.0344, MAE: 0.0344, RMSE: 0.0571, R2: 0.9430), Valid: (LOSS: 0.0481, MAE: 0.0481, RMSE: 0.0790, R2: 0.9042), PNorm: 159.0558, GNorm: 1.3976
[71/299] timecost: 64.55, lr: 0.000030, Train: (LOSS: 0.0346, MAE: 0.0346, RMSE: 0.0566, R2: 0.9429), Valid: (LOSS: 0.0477, MAE: 0.0477, RMSE: 0.0742, R2: 0.9156), PNorm: 158.8769, GNorm: 1.4727
[72/299] timecost: 64.62, lr: 0.000030, Train: (LOSS: 0.0344, MAE: 0.0344, RMSE: 0.0567, R2: 0.9450), Valid: (LOSS: 0.0477, MAE: 0.0477, RMSE: 0.0768, R2: 0.9090), PNorm: 158.6964, GNorm: 1.6520
[73/299] timecost: 64.73, lr: 0.000030, Train: (LOSS: 0.0337, MAE: 0.0337, RMSE: 0.0560, R2: 0.9476), Valid: (LOSS: 0.0485, MAE: 0.0485, RMSE: 0.0761, R2: 0.9103), PNorm: 158.5153, GNorm: 1.1544
[74/299] timecost: 65.03, lr: 0.000030, Train: (LOSS: 0.0345, MAE: 0.0345, RMSE: 0.0574, R2: 0.9454), Valid: (LOSS: 0.0470, MAE: 0.0470, RMSE: 0.0755, R2: 0.9132), PNorm: 158.3338, GNorm: 1.2798
[75/299] timecost: 64.86, lr: 0.000030, Train: (LOSS: 0.0326, MAE: 0.0326, RMSE: 0.0539, R2: 0.9522), Valid: (LOSS: 0.0491, MAE: 0.0491, RMSE: 0.0770, R2: 0.9087), PNorm: 158.1508, GNorm: 1.3148
[76/299] timecost: 64.65, lr: 0.000030, Train: (LOSS: 0.0343, MAE: 0.0343, RMSE: 0.0572, R2: 0.9466), Valid: (LOSS: 0.0474, MAE: 0.0474, RMSE: 0.0761, R2: 0.9109), PNorm: 157.9674, GNorm: 1.4354
[77/299] timecost: 64.70, lr: 0.000030, Train: (LOSS: 0.0332, MAE: 0.0332, RMSE: 0.0537, R2: 0.9483), Valid: (LOSS: 0.0468, MAE: 0.0468, RMSE: 0.0745, R2: 0.9140), PNorm: 157.7832, GNorm: 1.0562
[78/299] timecost: 63.36, lr: 0.000030, Train: (LOSS: 0.0330, MAE: 0.0330, RMSE: 0.0540, R2: 0.9491), Valid: (LOSS: 0.0481, MAE: 0.0481, RMSE: 0.0764, R2: 0.9101), PNorm: 157.5988, GNorm: 1.2747
[79/299] timecost: 63.69, lr: 0.000030, Train: (LOSS: 0.0321, MAE: 0.0321, RMSE: 0.0527, R2: 0.9522), Valid: (LOSS: 0.0468, MAE: 0.0468, RMSE: 0.0753, R2: 0.9143), PNorm: 157.4130, GNorm: 1.6461
[80/299] timecost: 63.37, lr: 0.000030, Train: (LOSS: 0.0320, MAE: 0.0320, RMSE: 0.0529, R2: 0.9527), Valid: (LOSS: 0.0468, MAE: 0.0468, RMSE: 0.0751, R2: 0.9123), PNorm: 157.2265, GNorm: 1.4865
[81/299] timecost: 64.04, lr: 0.000030, Train: (LOSS: 0.0331, MAE: 0.0331, RMSE: 0.0543, R2: 0.9489), Valid: (LOSS: 0.0512, MAE: 0.0512, RMSE: 0.0808, R2: 0.9011), PNorm: 157.0389, GNorm: 0.7314
[82/299] timecost: 63.92, lr: 0.000030, Train: (LOSS: 0.0343, MAE: 0.0343, RMSE: 0.0570, R2: 0.9420), Valid: (LOSS: 0.0501, MAE: 0.0501, RMSE: 0.0778, R2: 0.9062), PNorm: 156.8523, GNorm: 0.9973
[83/299] timecost: 65.82, lr: 0.000030, Train: (LOSS: 0.0341, MAE: 0.0341, RMSE: 0.0561, R2: 0.9467), Valid: (LOSS: 0.0479, MAE: 0.0479, RMSE: 0.0775, R2: 0.9084), PNorm: 156.6650, GNorm: 0.8341
[84/299] timecost: 66.79, lr: 0.000030, Train: (LOSS: 0.0309, MAE: 0.0309, RMSE: 0.0526, R2: 0.9520), Valid: (LOSS: 0.0472, MAE: 0.0472, RMSE: 0.0721, R2: 0.9186), PNorm: 156.4754, GNorm: 0.8142
[85/299] timecost: 66.77, lr: 0.000030, Train: (LOSS: 0.0300, MAE: 0.0300, RMSE: 0.0508, R2: 0.9551), Valid: (LOSS: 0.0477, MAE: 0.0477, RMSE: 0.0755, R2: 0.9117), PNorm: 156.2843, GNorm: 1.3361
[86/299] timecost: 66.90, lr: 0.000030, Train: (LOSS: 0.0309, MAE: 0.0309, RMSE: 0.0519, R2: 0.9531), Valid: (LOSS: 0.0469, MAE: 0.0469, RMSE: 0.0740, R2: 0.9166), PNorm: 156.0926, GNorm: 1.1295
[87/299] timecost: 66.92, lr: 0.000030, Train: (LOSS: 0.0302, MAE: 0.0302, RMSE: 0.0514, R2: 0.9537), Valid: (LOSS: 0.0501, MAE: 0.0501, RMSE: 0.0779, R2: 0.9074), PNorm: 155.8997, GNorm: 1.6771
[88/299] timecost: 66.87, lr: 0.000030, Train: (LOSS: 0.0297, MAE: 0.0297, RMSE: 0.0493, R2: 0.9566), Valid: (LOSS: 0.0465, MAE: 0.0465, RMSE: 0.0724, R2: 0.9202), PNorm: 155.7056, GNorm: 1.1099
[89/299] timecost: 66.78, lr: 0.000030, Train: (LOSS: 0.0297, MAE: 0.0297, RMSE: 0.0493, R2: 0.9580), Valid: (LOSS: 0.0458, MAE: 0.0458, RMSE: 0.0728, R2: 0.9171), PNorm: 155.5102, GNorm: 1.0091
[90/299] timecost: 66.93, lr: 0.000030, Train: (LOSS: 0.0295, MAE: 0.0295, RMSE: 0.0489, R2: 0.9572), Valid: (LOSS: 0.0451, MAE: 0.0451, RMSE: 0.0705, R2: 0.9196), PNorm: 155.3129, GNorm: 1.2020
[91/299] timecost: 66.78, lr: 0.000030, Train: (LOSS: 0.0293, MAE: 0.0293, RMSE: 0.0493, R2: 0.9558), Valid: (LOSS: 0.0439, MAE: 0.0439, RMSE: 0.0690, R2: 0.9257), PNorm: 155.1157, GNorm: 1.0694
[92/299] timecost: 66.70, lr: 0.000030, Train: (LOSS: 0.0290, MAE: 0.0290, RMSE: 0.0485, R2: 0.9554), Valid: (LOSS: 0.0434, MAE: 0.0434, RMSE: 0.0730, R2: 0.9182), PNorm: 154.9167, GNorm: 0.8963
[93/299] timecost: 67.04, lr: 0.000030, Train: (LOSS: 0.0296, MAE: 0.0296, RMSE: 0.0497, R2: 0.9567), Valid: (LOSS: 0.0440, MAE: 0.0440, RMSE: 0.0691, R2: 0.9232), PNorm: 154.7160, GNorm: 1.2557
[94/299] timecost: 66.62, lr: 0.000030, Train: (LOSS: 0.0307, MAE: 0.0307, RMSE: 0.0493, R2: 0.9553), Valid: (LOSS: 0.0464, MAE: 0.0464, RMSE: 0.0735, R2: 0.9147), PNorm: 154.5153, GNorm: 0.9311
[95/299] timecost: 66.74, lr: 0.000030, Train: (LOSS: 0.0298, MAE: 0.0298, RMSE: 0.0486, R2: 0.9573), Valid: (LOSS: 0.0420, MAE: 0.0420, RMSE: 0.0669, R2: 0.9288), PNorm: 154.3132, GNorm: 1.7291
[96/299] timecost: 66.87, lr: 0.000030, Train: (LOSS: 0.0275, MAE: 0.0275, RMSE: 0.0454, R2: 0.9613), Valid: (LOSS: 0.0439, MAE: 0.0439, RMSE: 0.0713, R2: 0.9229), PNorm: 154.1090, GNorm: 1.1492
[97/299] timecost: 66.82, lr: 0.000030, Train: (LOSS: 0.0289, MAE: 0.0289, RMSE: 0.0484, R2: 0.9602), Valid: (LOSS: 0.0425, MAE: 0.0425, RMSE: 0.0696, R2: 0.9248), PNorm: 153.9042, GNorm: 0.9117
[98/299] timecost: 66.73, lr: 0.000030, Train: (LOSS: 0.0281, MAE: 0.0281, RMSE: 0.0461, R2: 0.9604), Valid: (LOSS: 0.0443, MAE: 0.0443, RMSE: 0.0712, R2: 0.9223), PNorm: 153.6980, GNorm: 0.9189
[99/299] timecost: 66.76, lr: 0.000030, Train: (LOSS: 0.0275, MAE: 0.0275, RMSE: 0.0462, R2: 0.9603), Valid: (LOSS: 0.0413, MAE: 0.0413, RMSE: 0.0674, R2: 0.9295), PNorm: 153.4910, GNorm: 1.2610
[100/299] timecost: 66.83, lr: 0.000030, Train: (LOSS: 0.0272, MAE: 0.0272, RMSE: 0.0461, R2: 0.9611), Valid: (LOSS: 0.0461, MAE: 0.0461, RMSE: 0.0739, R2: 0.9157), PNorm: 153.2819, GNorm: 1.1664
[101/299] timecost: 66.74, lr: 0.000030, Train: (LOSS: 0.0275, MAE: 0.0275, RMSE: 0.0464, R2: 0.9615), Valid: (LOSS: 0.0419, MAE: 0.0419, RMSE: 0.0679, R2: 0.9272), PNorm: 153.0731, GNorm: 1.3308
[102/299] timecost: 66.95, lr: 0.000030, Train: (LOSS: 0.0280, MAE: 0.0280, RMSE: 0.0468, R2: 0.9626), Valid: (LOSS: 0.0421, MAE: 0.0421, RMSE: 0.0688, R2: 0.9268), PNorm: 152.8625, GNorm: 1.2771
[103/299] timecost: 64.60, lr: 0.000030, Train: (LOSS: 0.0272, MAE: 0.0272, RMSE: 0.0461, R2: 0.9599), Valid: (LOSS: 0.0433, MAE: 0.0433, RMSE: 0.0706, R2: 0.9222), PNorm: 152.6516, GNorm: 0.9245
[104/299] timecost: 63.56, lr: 0.000030, Train: (LOSS: 0.0268, MAE: 0.0268, RMSE: 0.0451, R2: 0.9621), Valid: (LOSS: 0.0419, MAE: 0.0419, RMSE: 0.0692, R2: 0.9250), PNorm: 152.4396, GNorm: 1.1620
[105/299] timecost: 64.01, lr: 0.000030, Train: (LOSS: 0.0267, MAE: 0.0267, RMSE: 0.0460, R2: 0.9630), Valid: (LOSS: 0.0432, MAE: 0.0432, RMSE: 0.0709, R2: 0.9220), PNorm: 152.2262, GNorm: 0.9991
[106/299] timecost: 63.53, lr: 0.000030, Train: (LOSS: 0.0265, MAE: 0.0265, RMSE: 0.0442, R2: 0.9633), Valid: (LOSS: 0.0412, MAE: 0.0412, RMSE: 0.0665, R2: 0.9283), PNorm: 152.0119, GNorm: 1.4586
[107/299] timecost: 66.43, lr: 0.000030, Train: (LOSS: 0.0254, MAE: 0.0254, RMSE: 0.0432, R2: 0.9668), Valid: (LOSS: 0.0406, MAE: 0.0406, RMSE: 0.0649, R2: 0.9337), PNorm: 151.7974, GNorm: 1.1523
[108/299] timecost: 66.76, lr: 0.000030, Train: (LOSS: 0.0260, MAE: 0.0260, RMSE: 0.0439, R2: 0.9657), Valid: (LOSS: 0.0444, MAE: 0.0444, RMSE: 0.0713, R2: 0.9204), PNorm: 151.5826, GNorm: 1.0467
[109/299] timecost: 66.85, lr: 0.000030, Train: (LOSS: 0.0245, MAE: 0.0245, RMSE: 0.0415, R2: 0.9661), Valid: (LOSS: 0.0403, MAE: 0.0403, RMSE: 0.0649, R2: 0.9340), PNorm: 151.3659, GNorm: 1.1486
[110/299] timecost: 66.85, lr: 0.000030, Train: (LOSS: 0.0260, MAE: 0.0260, RMSE: 0.0440, R2: 0.9659), Valid: (LOSS: 0.0416, MAE: 0.0416, RMSE: 0.0658, R2: 0.9327), PNorm: 151.1490, GNorm: 0.8743
[111/299] timecost: 66.88, lr: 0.000030, Train: (LOSS: 0.0249, MAE: 0.0249, RMSE: 0.0420, R2: 0.9668), Valid: (LOSS: 0.0439, MAE: 0.0439, RMSE: 0.0690, R2: 0.9252), PNorm: 150.9318, GNorm: 0.9861
[112/299] timecost: 66.87, lr: 0.000030, Train: (LOSS: 0.0252, MAE: 0.0252, RMSE: 0.0429, R2: 0.9667), Valid: (LOSS: 0.0408, MAE: 0.0408, RMSE: 0.0666, R2: 0.9318), PNorm: 150.7140, GNorm: 0.9775
[113/299] timecost: 66.88, lr: 0.000030, Train: (LOSS: 0.0252, MAE: 0.0252, RMSE: 0.0425, R2: 0.9660), Valid: (LOSS: 0.0441, MAE: 0.0441, RMSE: 0.0703, R2: 0.9253), PNorm: 150.4959, GNorm: 1.1556
[114/299] timecost: 66.89, lr: 0.000030, Train: (LOSS: 0.0243, MAE: 0.0243, RMSE: 0.0419, R2: 0.9688), Valid: (LOSS: 0.0412, MAE: 0.0412, RMSE: 0.0676, R2: 0.9285), PNorm: 150.2767, GNorm: 0.9158
[115/299] timecost: 67.02, lr: 0.000030, Train: (LOSS: 0.0238, MAE: 0.0238, RMSE: 0.0409, R2: 0.9692), Valid: (LOSS: 0.0438, MAE: 0.0438, RMSE: 0.0671, R2: 0.9307), PNorm: 150.0569, GNorm: 1.2091
[116/299] timecost: 66.76, lr: 0.000030, Train: (LOSS: 0.0244, MAE: 0.0244, RMSE: 0.0415, R2: 0.9678), Valid: (LOSS: 0.0447, MAE: 0.0447, RMSE: 0.0741, R2: 0.9121), PNorm: 149.8372, GNorm: 1.6329
[117/299] timecost: 66.85, lr: 0.000030, Train: (LOSS: 0.0237, MAE: 0.0237, RMSE: 0.0404, R2: 0.9690), Valid: (LOSS: 0.0404, MAE: 0.0404, RMSE: 0.0663, R2: 0.9312), PNorm: 149.6170, GNorm: 0.8331
[118/299] timecost: 66.71, lr: 0.000030, Train: (LOSS: 0.0238, MAE: 0.0238, RMSE: 0.0401, R2: 0.9698), Valid: (LOSS: 0.0405, MAE: 0.0405, RMSE: 0.0636, R2: 0.9361), PNorm: 149.3961, GNorm: 1.2106
[119/299] timecost: 66.72, lr: 0.000030, Train: (LOSS: 0.0249, MAE: 0.0249, RMSE: 0.0409, R2: 0.9666), Valid: (LOSS: 0.0433, MAE: 0.0433, RMSE: 0.0684, R2: 0.9267), PNorm: 149.1763, GNorm: 0.8004
[120/299] timecost: 66.85, lr: 0.000030, Train: (LOSS: 0.0236, MAE: 0.0236, RMSE: 0.0393, R2: 0.9702), Valid: (LOSS: 0.0412, MAE: 0.0412, RMSE: 0.0640, R2: 0.9353), PNorm: 148.9552, GNorm: 1.0474
[121/299] timecost: 66.87, lr: 0.000030, Train: (LOSS: 0.0234, MAE: 0.0234, RMSE: 0.0404, R2: 0.9704), Valid: (LOSS: 0.0424, MAE: 0.0424, RMSE: 0.0650, R2: 0.9310), PNorm: 148.7345, GNorm: 1.0834
[122/299] timecost: 66.76, lr: 0.000030, Train: (LOSS: 0.0232, MAE: 0.0232, RMSE: 0.0393, R2: 0.9706), Valid: (LOSS: 0.0420, MAE: 0.0420, RMSE: 0.0667, R2: 0.9309), PNorm: 148.5133, GNorm: 1.0041
[123/299] timecost: 66.73, lr: 0.000030, Train: (LOSS: 0.0235, MAE: 0.0235, RMSE: 0.0397, R2: 0.9693), Valid: (LOSS: 0.0407, MAE: 0.0407, RMSE: 0.0644, R2: 0.9346), PNorm: 148.2930, GNorm: 1.0397
[124/299] timecost: 66.85, lr: 0.000030, Train: (LOSS: 0.0233, MAE: 0.0233, RMSE: 0.0396, R2: 0.9697), Valid: (LOSS: 0.0383, MAE: 0.0383, RMSE: 0.0625, R2: 0.9391), PNorm: 148.0731, GNorm: 0.7438
[125/299] timecost: 66.81, lr: 0.000030, Train: (LOSS: 0.0229, MAE: 0.0229, RMSE: 0.0380, R2: 0.9732), Valid: (LOSS: 0.0436, MAE: 0.0436, RMSE: 0.0699, R2: 0.9231), PNorm: 147.8532, GNorm: 1.3154
[126/299] timecost: 67.01, lr: 0.000030, Train: (LOSS: 0.0218, MAE: 0.0218, RMSE: 0.0368, R2: 0.9734), Valid: (LOSS: 0.0398, MAE: 0.0398, RMSE: 0.0623, R2: 0.9385), PNorm: 147.6335, GNorm: 1.0029
[127/299] timecost: 66.77, lr: 0.000030, Train: (LOSS: 0.0225, MAE: 0.0225, RMSE: 0.0378, R2: 0.9724), Valid: (LOSS: 0.0406, MAE: 0.0406, RMSE: 0.0643, R2: 0.9326), PNorm: 147.4141, GNorm: 1.1364
[128/299] timecost: 66.70, lr: 0.000030, Train: (LOSS: 0.0218, MAE: 0.0218, RMSE: 0.0374, R2: 0.9720), Valid: (LOSS: 0.0383, MAE: 0.0383, RMSE: 0.0600, R2: 0.9420), PNorm: 147.1950, GNorm: 1.4054
[129/299] timecost: 66.83, lr: 0.000030, Train: (LOSS: 0.0222, MAE: 0.0222, RMSE: 0.0376, R2: 0.9727), Valid: (LOSS: 0.0409, MAE: 0.0409, RMSE: 0.0647, R2: 0.9350), PNorm: 146.9765, GNorm: 0.9215
[130/299] timecost: 66.89, lr: 0.000030, Train: (LOSS: 0.0214, MAE: 0.0214, RMSE: 0.0361, R2: 0.9756), Valid: (LOSS: 0.0412, MAE: 0.0412, RMSE: 0.0646, R2: 0.9338), PNorm: 146.7586, GNorm: 1.2194
[131/299] timecost: 66.83, lr: 0.000030, Train: (LOSS: 0.0225, MAE: 0.0225, RMSE: 0.0379, R2: 0.9726), Valid: (LOSS: 0.0415, MAE: 0.0415, RMSE: 0.0683, R2: 0.9281), PNorm: 146.5415, GNorm: 1.0893
[132/299] timecost: 66.79, lr: 0.000030, Train: (LOSS: 0.0219, MAE: 0.0219, RMSE: 0.0369, R2: 0.9735), Valid: (LOSS: 0.0387, MAE: 0.0387, RMSE: 0.0623, R2: 0.9382), PNorm: 146.3261, GNorm: 0.9836
[133/299] timecost: 66.71, lr: 0.000030, Train: (LOSS: 0.0214, MAE: 0.0214, RMSE: 0.0357, R2: 0.9751), Valid: (LOSS: 0.0403, MAE: 0.0403, RMSE: 0.0629, R2: 0.9352), PNorm: 146.1111, GNorm: 1.2290
[134/299] timecost: 66.12, lr: 0.000030, Train: (LOSS: 0.0212, MAE: 0.0212, RMSE: 0.0361, R2: 0.9745), Valid: (LOSS: 0.0381, MAE: 0.0381, RMSE: 0.0592, R2: 0.9423), PNorm: 145.8966, GNorm: 0.7464
[135/299] timecost: 64.47, lr: 0.000030, Train: (LOSS: 0.0210, MAE: 0.0210, RMSE: 0.0352, R2: 0.9763), Valid: (LOSS: 0.0384, MAE: 0.0384, RMSE: 0.0608, R2: 0.9406), PNorm: 145.6825, GNorm: 1.1244
[136/299] timecost: 64.52, lr: 0.000030, Train: (LOSS: 0.0214, MAE: 0.0214, RMSE: 0.0357, R2: 0.9746), Valid: (LOSS: 0.0403, MAE: 0.0403, RMSE: 0.0631, R2: 0.9378), PNorm: 145.4703, GNorm: 1.0959
[137/299] timecost: 64.29, lr: 0.000030, Train: (LOSS: 0.0209, MAE: 0.0209, RMSE: 0.0349, R2: 0.9750), Valid: (LOSS: 0.0368, MAE: 0.0368, RMSE: 0.0563, R2: 0.9487), PNorm: 145.2587, GNorm: 0.7985
[138/299] timecost: 64.01, lr: 0.000030, Train: (LOSS: 0.0206, MAE: 0.0206, RMSE: 0.0348, R2: 0.9767), Valid: (LOSS: 0.0395, MAE: 0.0395, RMSE: 0.0623, R2: 0.9382), PNorm: 145.0476, GNorm: 0.9995
[139/299] timecost: 65.26, lr: 0.000030, Train: (LOSS: 0.0206, MAE: 0.0206, RMSE: 0.0341, R2: 0.9768), Valid: (LOSS: 0.0379, MAE: 0.0379, RMSE: 0.0580, R2: 0.9441), PNorm: 144.8387, GNorm: 0.9616
[140/299] timecost: 64.33, lr: 0.000030, Train: (LOSS: 0.0204, MAE: 0.0204, RMSE: 0.0340, R2: 0.9755), Valid: (LOSS: 0.0394, MAE: 0.0394, RMSE: 0.0610, R2: 0.9398), PNorm: 144.6308, GNorm: 1.1734
[141/299] timecost: 65.08, lr: 0.000030, Train: (LOSS: 0.0211, MAE: 0.0211, RMSE: 0.0346, R2: 0.9777), Valid: (LOSS: 0.0395, MAE: 0.0395, RMSE: 0.0608, R2: 0.9386), PNorm: 144.4255, GNorm: 1.2689
[142/299] timecost: 64.98, lr: 0.000030, Train: (LOSS: 0.0203, MAE: 0.0203, RMSE: 0.0343, R2: 0.9775), Valid: (LOSS: 0.0389, MAE: 0.0389, RMSE: 0.0606, R2: 0.9426), PNorm: 144.2202, GNorm: 0.8430
[143/299] timecost: 64.84, lr: 0.000030, Train: (LOSS: 0.0206, MAE: 0.0206, RMSE: 0.0347, R2: 0.9766), Valid: (LOSS: 0.0389, MAE: 0.0389, RMSE: 0.0590, R2: 0.9449), PNorm: 144.0170, GNorm: 0.8637
[144/299] timecost: 65.44, lr: 0.000030, Train: (LOSS: 0.0196, MAE: 0.0196, RMSE: 0.0336, R2: 0.9774), Valid: (LOSS: 0.0396, MAE: 0.0396, RMSE: 0.0605, R2: 0.9394), PNorm: 143.8146, GNorm: 0.8945
[145/299] timecost: 65.33, lr: 0.000030, Train: (LOSS: 0.0195, MAE: 0.0195, RMSE: 0.0322, R2: 0.9780), Valid: (LOSS: 0.0383, MAE: 0.0383, RMSE: 0.0590, R2: 0.9427), PNorm: 143.6138, GNorm: 1.1832
[146/299] timecost: 64.64, lr: 0.000030, Train: (LOSS: 0.0189, MAE: 0.0189, RMSE: 0.0317, R2: 0.9797), Valid: (LOSS: 0.0402, MAE: 0.0402, RMSE: 0.0607, R2: 0.9405), PNorm: 143.4138, GNorm: 1.0886
[147/299] timecost: 64.72, lr: 0.000030, Train: (LOSS: 0.0187, MAE: 0.0187, RMSE: 0.0313, R2: 0.9801), Valid: (LOSS: 0.0394, MAE: 0.0394, RMSE: 0.0606, R2: 0.9399), PNorm: 143.2158, GNorm: 0.9610
[148/299] timecost: 64.85, lr: 0.000030, Train: (LOSS: 0.0194, MAE: 0.0194, RMSE: 0.0323, R2: 0.9793), Valid: (LOSS: 0.0402, MAE: 0.0402, RMSE: 0.0618, R2: 0.9359), PNorm: 143.0201, GNorm: 1.1168
[149/299] timecost: 64.88, lr: 0.000030, Train: (LOSS: 0.0197, MAE: 0.0197, RMSE: 0.0328, R2: 0.9778), Valid: (LOSS: 0.0375, MAE: 0.0375, RMSE: 0.0578, R2: 0.9446), PNorm: 142.8279, GNorm: 0.8960
[150/299] timecost: 64.03, lr: 0.000030, Train: (LOSS: 0.0189, MAE: 0.0189, RMSE: 0.0317, R2: 0.9791), Valid: (LOSS: 0.0380, MAE: 0.0380, RMSE: 0.0581, R2: 0.9442), PNorm: 142.6369, GNorm: 0.9026
[151/299] timecost: 64.04, lr: 0.000030, Train: (LOSS: 0.0189, MAE: 0.0189, RMSE: 0.0320, R2: 0.9786), Valid: (LOSS: 0.0384, MAE: 0.0384, RMSE: 0.0587, R2: 0.9409), PNorm: 142.4477, GNorm: 0.8567
[152/299] timecost: 63.53, lr: 0.000030, Train: (LOSS: 0.0187, MAE: 0.0187, RMSE: 0.0314, R2: 0.9804), Valid: (LOSS: 0.0385, MAE: 0.0385, RMSE: 0.0624, R2: 0.9392), PNorm: 142.2601, GNorm: 0.7010
[153/299] timecost: 63.81, lr: 0.000030, Train: (LOSS: 0.0182, MAE: 0.0182, RMSE: 0.0311, R2: 0.9810), Valid: (LOSS: 0.0354, MAE: 0.0354, RMSE: 0.0556, R2: 0.9498), PNorm: 142.0746, GNorm: 0.8444
[154/299] timecost: 66.74, lr: 0.000030, Train: (LOSS: 0.0183, MAE: 0.0183, RMSE: 0.0306, R2: 0.9811), Valid: (LOSS: 0.0371, MAE: 0.0371, RMSE: 0.0571, R2: 0.9462), PNorm: 141.8903, GNorm: 0.9423
[155/299] timecost: 66.80, lr: 0.000030, Train: (LOSS: 0.0177, MAE: 0.0177, RMSE: 0.0297, R2: 0.9803), Valid: (LOSS: 0.0352, MAE: 0.0352, RMSE: 0.0541, R2: 0.9508), PNorm: 141.7073, GNorm: 1.0707
[156/299] timecost: 66.89, lr: 0.000030, Train: (LOSS: 0.0173, MAE: 0.0173, RMSE: 0.0294, R2: 0.9822), Valid: (LOSS: 0.0349, MAE: 0.0349, RMSE: 0.0557, R2: 0.9464), PNorm: 141.5267, GNorm: 0.7281
[157/299] timecost: 66.72, lr: 0.000030, Train: (LOSS: 0.0184, MAE: 0.0184, RMSE: 0.0307, R2: 0.9793), Valid: (LOSS: 0.0354, MAE: 0.0354, RMSE: 0.0548, R2: 0.9476), PNorm: 141.3487, GNorm: 0.7314
[158/299] timecost: 66.58, lr: 0.000030, Train: (LOSS: 0.0181, MAE: 0.0181, RMSE: 0.0302, R2: 0.9804), Valid: (LOSS: 0.0367, MAE: 0.0367, RMSE: 0.0576, R2: 0.9453), PNorm: 141.1730, GNorm: 0.9587
[159/299] timecost: 66.83, lr: 0.000030, Train: (LOSS: 0.0182, MAE: 0.0182, RMSE: 0.0308, R2: 0.9796), Valid: (LOSS: 0.0399, MAE: 0.0399, RMSE: 0.0608, R2: 0.9396), PNorm: 140.9991, GNorm: 0.6972
[160/299] timecost: 66.82, lr: 0.000030, Train: (LOSS: 0.0177, MAE: 0.0177, RMSE: 0.0295, R2: 0.9796), Valid: (LOSS: 0.0387, MAE: 0.0387, RMSE: 0.0603, R2: 0.9417), PNorm: 140.8284, GNorm: 0.8988
[161/299] timecost: 66.91, lr: 0.000030, Train: (LOSS: 0.0177, MAE: 0.0177, RMSE: 0.0300, R2: 0.9799), Valid: (LOSS: 0.0364, MAE: 0.0364, RMSE: 0.0576, R2: 0.9446), PNorm: 140.6590, GNorm: 1.0749
[162/299] timecost: 66.75, lr: 0.000030, Train: (LOSS: 0.0170, MAE: 0.0170, RMSE: 0.0281, R2: 0.9813), Valid: (LOSS: 0.0374, MAE: 0.0374, RMSE: 0.0582, R2: 0.9426), PNorm: 140.4917, GNorm: 0.8241
[163/299] timecost: 65.72, lr: 0.000030, Train: (LOSS: 0.0173, MAE: 0.0173, RMSE: 0.0296, R2: 0.9814), Valid: (LOSS: 0.0376, MAE: 0.0376, RMSE: 0.0590, R2: 0.9402), PNorm: 140.3259, GNorm: 0.8555
[164/299] timecost: 64.08, lr: 0.000030, Train: (LOSS: 0.0162, MAE: 0.0162, RMSE: 0.0277, R2: 0.9832), Valid: (LOSS: 0.0369, MAE: 0.0369, RMSE: 0.0564, R2: 0.9473), PNorm: 140.1625, GNorm: 0.7967
[165/299] timecost: 63.90, lr: 0.000030, Train: (LOSS: 0.0166, MAE: 0.0166, RMSE: 0.0279, R2: 0.9836), Valid: (LOSS: 0.0371, MAE: 0.0371, RMSE: 0.0573, R2: 0.9461), PNorm: 140.0016, GNorm: 0.6899
[166/299] timecost: 63.54, lr: 0.000030, Train: (LOSS: 0.0189, MAE: 0.0189, RMSE: 0.0313, R2: 0.9793), Valid: (LOSS: 0.0376, MAE: 0.0376, RMSE: 0.0594, R2: 0.9421), PNorm: 139.8435, GNorm: 1.5750
[167/299] timecost: 63.83, lr: 0.000030, Train: (LOSS: 0.0166, MAE: 0.0166, RMSE: 0.0278, R2: 0.9827), Valid: (LOSS: 0.0372, MAE: 0.0372, RMSE: 0.0582, R2: 0.9447), PNorm: 139.6863, GNorm: 0.9299
[168/299] timecost: 63.74, lr: 0.000030, Train: (LOSS: 0.0163, MAE: 0.0163, RMSE: 0.0280, R2: 0.9833), Valid: (LOSS: 0.0373, MAE: 0.0373, RMSE: 0.0584, R2: 0.9435), PNorm: 139.5316, GNorm: 0.7604
[169/299] timecost: 63.40, lr: 0.000030, Train: (LOSS: 0.0159, MAE: 0.0159, RMSE: 0.0274, R2: 0.9828), Valid: (LOSS: 0.0356, MAE: 0.0356, RMSE: 0.0538, R2: 0.9508), PNorm: 139.3787, GNorm: 0.7658
[170/299] timecost: 63.51, lr: 0.000030, Train: (LOSS: 0.0173, MAE: 0.0173, RMSE: 0.0292, R2: 0.9807), Valid: (LOSS: 0.0386, MAE: 0.0386, RMSE: 0.0612, R2: 0.9405), PNorm: 139.2290, GNorm: 0.8085
[171/299] timecost: 63.59, lr: 0.000030, Train: (LOSS: 0.0171, MAE: 0.0171, RMSE: 0.0281, R2: 0.9827), Valid: (LOSS: 0.0364, MAE: 0.0364, RMSE: 0.0553, R2: 0.9503), PNorm: 139.0816, GNorm: 0.7559
[172/299] timecost: 64.05, lr: 0.000030, Train: (LOSS: 0.0161, MAE: 0.0161, RMSE: 0.0274, R2: 0.9845), Valid: (LOSS: 0.0357, MAE: 0.0357, RMSE: 0.0569, R2: 0.9459), PNorm: 138.9354, GNorm: 0.7999
[173/299] timecost: 63.72, lr: 0.000030, Train: (LOSS: 0.0157, MAE: 0.0157, RMSE: 0.0268, R2: 0.9832), Valid: (LOSS: 0.0359, MAE: 0.0359, RMSE: 0.0559, R2: 0.9484), PNorm: 138.7919, GNorm: 0.6836
[174/299] timecost: 63.83, lr: 0.000030, Train: (LOSS: 0.0162, MAE: 0.0162, RMSE: 0.0276, R2: 0.9831), Valid: (LOSS: 0.0363, MAE: 0.0363, RMSE: 0.0570, R2: 0.9467), PNorm: 138.6501, GNorm: 0.7829
[175/299] timecost: 63.81, lr: 0.000030, Train: (LOSS: 0.0161, MAE: 0.0161, RMSE: 0.0277, R2: 0.9845), Valid: (LOSS: 0.0384, MAE: 0.0384, RMSE: 0.0611, R2: 0.9403), PNorm: 138.5106, GNorm: 0.8058
[176/299] timecost: 63.67, lr: 0.000030, Train: (LOSS: 0.0156, MAE: 0.0156, RMSE: 0.0272, R2: 0.9838), Valid: (LOSS: 0.0363, MAE: 0.0363, RMSE: 0.0557, R2: 0.9468), PNorm: 138.3724, GNorm: 0.7150
Epoch 00178: reducing learning rate of group 0 to 2.7000e-05.
[177/299] timecost: 63.53, lr: 0.000027, Train: (LOSS: 0.0155, MAE: 0.0155, RMSE: 0.0267, R2: 0.9843), Valid: (LOSS: 0.0351, MAE: 0.0351, RMSE: 0.0550, R2: 0.9503), PNorm: 138.2365, GNorm: 0.9337
[178/299] timecost: 63.37, lr: 0.000027, Train: (LOSS: 0.0152, MAE: 0.0152, RMSE: 0.0261, R2: 0.9837), Valid: (LOSS: 0.0373, MAE: 0.0373, RMSE: 0.0574, R2: 0.9459), PNorm: 138.1155, GNorm: 0.8219
[179/299] timecost: 63.44, lr: 0.000027, Train: (LOSS: 0.0144, MAE: 0.0144, RMSE: 0.0251, R2: 0.9853), Valid: (LOSS: 0.0375, MAE: 0.0375, RMSE: 0.0579, R2: 0.9455), PNorm: 137.9955, GNorm: 0.9583
[180/299] timecost: 66.12, lr: 0.000027, Train: (LOSS: 0.0143, MAE: 0.0143, RMSE: 0.0250, R2: 0.9857), Valid: (LOSS: 0.0358, MAE: 0.0358, RMSE: 0.0567, R2: 0.9470), PNorm: 137.8771, GNorm: 0.7034
[181/299] timecost: 67.03, lr: 0.000027, Train: (LOSS: 0.0137, MAE: 0.0137, RMSE: 0.0241, R2: 0.9857), Valid: (LOSS: 0.0349, MAE: 0.0349, RMSE: 0.0528, R2: 0.9542), PNorm: 137.7591, GNorm: 0.8652
[182/299] timecost: 66.92, lr: 0.000027, Train: (LOSS: 0.0142, MAE: 0.0142, RMSE: 0.0247, R2: 0.9856), Valid: (LOSS: 0.0346, MAE: 0.0346, RMSE: 0.0538, R2: 0.9522), PNorm: 137.6429, GNorm: 0.8053
[183/299] timecost: 64.96, lr: 0.000027, Train: (LOSS: 0.0151, MAE: 0.0151, RMSE: 0.0260, R2: 0.9848), Valid: (LOSS: 0.0347, MAE: 0.0347, RMSE: 0.0546, R2: 0.9509), PNorm: 137.5286, GNorm: 0.7633
[184/299] timecost: 66.84, lr: 0.000027, Train: (LOSS: 0.0147, MAE: 0.0147, RMSE: 0.0252, R2: 0.9844), Valid: (LOSS: 0.0364, MAE: 0.0364, RMSE: 0.0581, R2: 0.9436), PNorm: 137.4158, GNorm: 0.6668
[185/299] timecost: 66.88, lr: 0.000027, Train: (LOSS: 0.0136, MAE: 0.0136, RMSE: 0.0238, R2: 0.9851), Valid: (LOSS: 0.0356, MAE: 0.0356, RMSE: 0.0551, R2: 0.9492), PNorm: 137.3030, GNorm: 1.0168
[186/299] timecost: 66.89, lr: 0.000027, Train: (LOSS: 0.0141, MAE: 0.0141, RMSE: 0.0245, R2: 0.9847), Valid: (LOSS: 0.0363, MAE: 0.0363, RMSE: 0.0570, R2: 0.9468), PNorm: 137.1924, GNorm: 0.7938
[187/299] timecost: 66.87, lr: 0.000027, Train: (LOSS: 0.0141, MAE: 0.0141, RMSE: 0.0241, R2: 0.9855), Valid: (LOSS: 0.0350, MAE: 0.0350, RMSE: 0.0543, R2: 0.9512), PNorm: 137.0823, GNorm: 0.8577
[188/299] timecost: 66.76, lr: 0.000027, Train: (LOSS: 0.0143, MAE: 0.0143, RMSE: 0.0240, R2: 0.9859), Valid: (LOSS: 0.0345, MAE: 0.0345, RMSE: 0.0552, R2: 0.9494), PNorm: 136.9741, GNorm: 0.8274
[189/299] timecost: 66.87, lr: 0.000027, Train: (LOSS: 0.0136, MAE: 0.0136, RMSE: 0.0237, R2: 0.9869), Valid: (LOSS: 0.0346, MAE: 0.0346, RMSE: 0.0542, R2: 0.9521), PNorm: 136.8671, GNorm: 0.7418
[190/299] timecost: 64.05, lr: 0.000027, Train: (LOSS: 0.0139, MAE: 0.0139, RMSE: 0.0243, R2: 0.9865), Valid: (LOSS: 0.0369, MAE: 0.0369, RMSE: 0.0561, R2: 0.9480), PNorm: 136.7603, GNorm: 0.7372
[191/299] timecost: 63.80, lr: 0.000027, Train: (LOSS: 0.0141, MAE: 0.0141, RMSE: 0.0244, R2: 0.9858), Valid: (LOSS: 0.0351, MAE: 0.0351, RMSE: 0.0556, R2: 0.9482), PNorm: 136.6559, GNorm: 0.8890
[192/299] timecost: 63.47, lr: 0.000027, Train: (LOSS: 0.0142, MAE: 0.0142, RMSE: 0.0243, R2: 0.9872), Valid: (LOSS: 0.0344, MAE: 0.0344, RMSE: 0.0538, R2: 0.9517), PNorm: 136.5534, GNorm: 0.8762
[193/299] timecost: 63.92, lr: 0.000027, Train: (LOSS: 0.0139, MAE: 0.0139, RMSE: 0.0233, R2: 0.9872), Valid: (LOSS: 0.0358, MAE: 0.0358, RMSE: 0.0553, R2: 0.9508), PNorm: 136.4511, GNorm: 1.0363
[194/299] timecost: 63.77, lr: 0.000027, Train: (LOSS: 0.0142, MAE: 0.0142, RMSE: 0.0243, R2: 0.9865), Valid: (LOSS: 0.0351, MAE: 0.0351, RMSE: 0.0550, R2: 0.9511), PNorm: 136.3507, GNorm: 1.1012
[195/299] timecost: 63.78, lr: 0.000027, Train: (LOSS: 0.0137, MAE: 0.0137, RMSE: 0.0238, R2: 0.9874), Valid: (LOSS: 0.0330, MAE: 0.0330, RMSE: 0.0536, R2: 0.9512), PNorm: 136.2508, GNorm: 0.6831
[196/299] timecost: 66.07, lr: 0.000027, Train: (LOSS: 0.0138, MAE: 0.0138, RMSE: 0.0235, R2: 0.9863), Valid: (LOSS: 0.0363, MAE: 0.0363, RMSE: 0.0569, R2: 0.9474), PNorm: 136.1519, GNorm: 1.1469
[197/299] timecost: 64.81, lr: 0.000027, Train: (LOSS: 0.0140, MAE: 0.0140, RMSE: 0.0242, R2: 0.9868), Valid: (LOSS: 0.0359, MAE: 0.0359, RMSE: 0.0548, R2: 0.9506), PNorm: 136.0548, GNorm: 0.7261
[198/299] timecost: 65.10, lr: 0.000027, Train: (LOSS: 0.0141, MAE: 0.0141, RMSE: 0.0237, R2: 0.9868), Valid: (LOSS: 0.0334, MAE: 0.0334, RMSE: 0.0532, R2: 0.9522), PNorm: 135.9596, GNorm: 0.8794
[199/299] timecost: 64.99, lr: 0.000027, Train: (LOSS: 0.0131, MAE: 0.0131, RMSE: 0.0230, R2: 0.9874), Valid: (LOSS: 0.0331, MAE: 0.0331, RMSE: 0.0524, R2: 0.9540), PNorm: 135.8635, GNorm: 0.9431
[200/299] timecost: 65.46, lr: 0.000027, Train: (LOSS: 0.0131, MAE: 0.0131, RMSE: 0.0229, R2: 0.9868), Valid: (LOSS: 0.0337, MAE: 0.0337, RMSE: 0.0531, R2: 0.9533), PNorm: 135.7680, GNorm: 0.8237
[201/299] timecost: 64.96, lr: 0.000027, Train: (LOSS: 0.0131, MAE: 0.0131, RMSE: 0.0229, R2: 0.9877), Valid: (LOSS: 0.0336, MAE: 0.0336, RMSE: 0.0528, R2: 0.9528), PNorm: 135.6744, GNorm: 0.7226
[202/299] timecost: 64.71, lr: 0.000027, Train: (LOSS: 0.0131, MAE: 0.0131, RMSE: 0.0222, R2: 0.9868), Valid: (LOSS: 0.0346, MAE: 0.0346, RMSE: 0.0558, R2: 0.9488), PNorm: 135.5807, GNorm: 0.7366
[203/299] timecost: 64.96, lr: 0.000027, Train: (LOSS: 0.0131, MAE: 0.0131, RMSE: 0.0222, R2: 0.9886), Valid: (LOSS: 0.0361, MAE: 0.0361, RMSE: 0.0560, R2: 0.9470), PNorm: 135.4879, GNorm: 0.9731
[204/299] timecost: 64.97, lr: 0.000027, Train: (LOSS: 0.0131, MAE: 0.0131, RMSE: 0.0222, R2: 0.9869), Valid: (LOSS: 0.0349, MAE: 0.0349, RMSE: 0.0536, R2: 0.9527), PNorm: 135.3963, GNorm: 3.5709
[205/299] timecost: 65.04, lr: 0.000027, Train: (LOSS: 0.0157, MAE: 0.0157, RMSE: 0.0282, R2: 0.9792), Valid: (LOSS: 0.0349, MAE: 0.0349, RMSE: 0.0550, R2: 0.9493), PNorm: 135.3100, GNorm: 0.8884
[206/299] timecost: 64.79, lr: 0.000027, Train: (LOSS: 0.0130, MAE: 0.0130, RMSE: 0.0223, R2: 0.9884), Valid: (LOSS: 0.0341, MAE: 0.0341, RMSE: 0.0551, R2: 0.9503), PNorm: 135.2216, GNorm: 0.6845
[207/299] timecost: 64.42, lr: 0.000027, Train: (LOSS: 0.0128, MAE: 0.0128, RMSE: 0.0224, R2: 0.9882), Valid: (LOSS: 0.0341, MAE: 0.0341, RMSE: 0.0529, R2: 0.9540), PNorm: 135.1332, GNorm: 0.7808
[208/299] timecost: 64.35, lr: 0.000027, Train: (LOSS: 0.0122, MAE: 0.0122, RMSE: 0.0215, R2: 0.9880), Valid: (LOSS: 0.0343, MAE: 0.0343, RMSE: 0.0542, R2: 0.9508), PNorm: 135.0451, GNorm: 0.7407
[209/299] timecost: 64.45, lr: 0.000027, Train: (LOSS: 0.0123, MAE: 0.0123, RMSE: 0.0211, R2: 0.9879), Valid: (LOSS: 0.0337, MAE: 0.0337, RMSE: 0.0527, R2: 0.9535), PNorm: 134.9572, GNorm: 0.8813
[210/299] timecost: 64.27, lr: 0.000027, Train: (LOSS: 0.0128, MAE: 0.0128, RMSE: 0.0217, R2: 0.9890), Valid: (LOSS: 0.0328, MAE: 0.0328, RMSE: 0.0522, R2: 0.9550), PNorm: 134.8705, GNorm: 0.6709
[211/299] timecost: 64.03, lr: 0.000027, Train: (LOSS: 0.0121, MAE: 0.0121, RMSE: 0.0209, R2: 0.9890), Valid: (LOSS: 0.0335, MAE: 0.0335, RMSE: 0.0522, R2: 0.9545), PNorm: 134.7841, GNorm: 0.8580
[212/299] timecost: 64.47, lr: 0.000027, Train: (LOSS: 0.0131, MAE: 0.0131, RMSE: 0.0219, R2: 0.9890), Valid: (LOSS: 0.0357, MAE: 0.0357, RMSE: 0.0551, R2: 0.9507), PNorm: 134.6977, GNorm: 0.8732
[213/299] timecost: 65.20, lr: 0.000027, Train: (LOSS: 0.0118, MAE: 0.0118, RMSE: 0.0209, R2: 0.9892), Valid: (LOSS: 0.0337, MAE: 0.0337, RMSE: 0.0544, R2: 0.9490), PNorm: 134.6120, GNorm: 1.1334
[214/299] timecost: 64.71, lr: 0.000027, Train: (LOSS: 0.0123, MAE: 0.0123, RMSE: 0.0212, R2: 0.9892), Valid: (LOSS: 0.0346, MAE: 0.0346, RMSE: 0.0556, R2: 0.9492), PNorm: 134.5267, GNorm: 1.1304
[215/299] timecost: 65.11, lr: 0.000027, Train: (LOSS: 0.0127, MAE: 0.0127, RMSE: 0.0212, R2: 0.9902), Valid: (LOSS: 0.0339, MAE: 0.0339, RMSE: 0.0521, R2: 0.9543), PNorm: 134.4426, GNorm: 0.6936
Epoch 00217: reducing learning rate of group 0 to 2.4300e-05.
[216/299] timecost: 64.92, lr: 0.000024, Train: (LOSS: 0.0120, MAE: 0.0120, RMSE: 0.0210, R2: 0.9896), Valid: (LOSS: 0.0342, MAE: 0.0342, RMSE: 0.0532, R2: 0.9522), PNorm: 134.3576, GNorm: 1.0235
[217/299] timecost: 64.68, lr: 0.000024, Train: (LOSS: 0.0115, MAE: 0.0115, RMSE: 0.0201, R2: 0.9895), Valid: (LOSS: 0.0329, MAE: 0.0329, RMSE: 0.0520, R2: 0.9530), PNorm: 134.2826, GNorm: 0.6794
[218/299] timecost: 65.03, lr: 0.000024, Train: (LOSS: 0.0112, MAE: 0.0112, RMSE: 0.0196, R2: 0.9897), Valid: (LOSS: 0.0329, MAE: 0.0329, RMSE: 0.0511, R2: 0.9569), PNorm: 134.2069, GNorm: 0.6623
[219/299] timecost: 64.86, lr: 0.000024, Train: (LOSS: 0.0112, MAE: 0.0112, RMSE: 0.0198, R2: 0.9899), Valid: (LOSS: 0.0331, MAE: 0.0331, RMSE: 0.0523, R2: 0.9539), PNorm: 134.1314, GNorm: 0.8780
[220/299] timecost: 65.08, lr: 0.000024, Train: (LOSS: 0.0113, MAE: 0.0113, RMSE: 0.0197, R2: 0.9904), Valid: (LOSS: 0.0345, MAE: 0.0345, RMSE: 0.0543, R2: 0.9512), PNorm: 134.0553, GNorm: 0.7054
[221/299] timecost: 65.21, lr: 0.000024, Train: (LOSS: 0.0108, MAE: 0.0108, RMSE: 0.0192, R2: 0.9895), Valid: (LOSS: 0.0337, MAE: 0.0337, RMSE: 0.0527, R2: 0.9538), PNorm: 133.9794, GNorm: 0.7905
[222/299] timecost: 64.87, lr: 0.000024, Train: (LOSS: 0.0111, MAE: 0.0111, RMSE: 0.0185, R2: 0.9897), Valid: (LOSS: 0.0340, MAE: 0.0340, RMSE: 0.0524, R2: 0.9549), PNorm: 133.9040, GNorm: 0.8049
[223/299] timecost: 65.26, lr: 0.000024, Train: (LOSS: 0.0110, MAE: 0.0110, RMSE: 0.0194, R2: 0.9898), Valid: (LOSS: 0.0360, MAE: 0.0360, RMSE: 0.0554, R2: 0.9504), PNorm: 133.8287, GNorm: 0.7184
[224/299] timecost: 66.71, lr: 0.000024, Train: (LOSS: 0.0116, MAE: 0.0116, RMSE: 0.0203, R2: 0.9897), Valid: (LOSS: 0.0343, MAE: 0.0343, RMSE: 0.0535, R2: 0.9526), PNorm: 133.7536, GNorm: 0.7110
[225/299] timecost: 66.83, lr: 0.000024, Train: (LOSS: 0.0115, MAE: 0.0115, RMSE: 0.0199, R2: 0.9906), Valid: (LOSS: 0.0347, MAE: 0.0347, RMSE: 0.0545, R2: 0.9513), PNorm: 133.6795, GNorm: 0.6782
[226/299] timecost: 65.15, lr: 0.000024, Train: (LOSS: 0.0110, MAE: 0.0110, RMSE: 0.0196, R2: 0.9890), Valid: (LOSS: 0.0330, MAE: 0.0330, RMSE: 0.0513, R2: 0.9569), PNorm: 133.6033, GNorm: 1.0436
[227/299] timecost: 64.10, lr: 0.000024, Train: (LOSS: 0.0116, MAE: 0.0116, RMSE: 0.0201, R2: 0.9904), Valid: (LOSS: 0.0344, MAE: 0.0344, RMSE: 0.0536, R2: 0.9536), PNorm: 133.5289, GNorm: 0.6175
[228/299] timecost: 64.38, lr: 0.000024, Train: (LOSS: 0.0113, MAE: 0.0113, RMSE: 0.0196, R2: 0.9911), Valid: (LOSS: 0.0331, MAE: 0.0331, RMSE: 0.0513, R2: 0.9570), PNorm: 133.4549, GNorm: 0.6517
[229/299] timecost: 63.78, lr: 0.000024, Train: (LOSS: 0.0108, MAE: 0.0108, RMSE: 0.0191, R2: 0.9913), Valid: (LOSS: 0.0348, MAE: 0.0348, RMSE: 0.0533, R2: 0.9537), PNorm: 133.3806, GNorm: 0.6613
[230/299] timecost: 66.25, lr: 0.000024, Train: (LOSS: 0.0105, MAE: 0.0105, RMSE: 0.0184, R2: 0.9909), Valid: (LOSS: 0.0333, MAE: 0.0333, RMSE: 0.0534, R2: 0.9523), PNorm: 133.3063, GNorm: 0.6521
[231/299] timecost: 66.75, lr: 0.000024, Train: (LOSS: 0.0104, MAE: 0.0104, RMSE: 0.0188, R2: 0.9908), Valid: (LOSS: 0.0326, MAE: 0.0326, RMSE: 0.0507, R2: 0.9570), PNorm: 133.2314, GNorm: 0.6632
[232/299] timecost: 66.87, lr: 0.000024, Train: (LOSS: 0.0103, MAE: 0.0103, RMSE: 0.0178, R2: 0.9916), Valid: (LOSS: 0.0348, MAE: 0.0348, RMSE: 0.0550, R2: 0.9507), PNorm: 133.1572, GNorm: 0.9329
[233/299] timecost: 66.72, lr: 0.000024, Train: (LOSS: 0.0107, MAE: 0.0107, RMSE: 0.0189, R2: 0.9906), Valid: (LOSS: 0.0339, MAE: 0.0339, RMSE: 0.0534, R2: 0.9536), PNorm: 133.0834, GNorm: 0.6997
[234/299] timecost: 64.89, lr: 0.000024, Train: (LOSS: 0.0109, MAE: 0.0109, RMSE: 0.0194, R2: 0.9898), Valid: (LOSS: 0.0347, MAE: 0.0347, RMSE: 0.0540, R2: 0.9527), PNorm: 133.0096, GNorm: 0.8060
[235/299] timecost: 65.34, lr: 0.000024, Train: (LOSS: 0.0103, MAE: 0.0103, RMSE: 0.0183, R2: 0.9912), Valid: (LOSS: 0.0342, MAE: 0.0342, RMSE: 0.0533, R2: 0.9535), PNorm: 132.9356, GNorm: 0.8090
[236/299] timecost: 64.94, lr: 0.000024, Train: (LOSS: 0.0102, MAE: 0.0102, RMSE: 0.0179, R2: 0.9922), Valid: (LOSS: 0.0339, MAE: 0.0339, RMSE: 0.0529, R2: 0.9540), PNorm: 132.8619, GNorm: 0.6491
[237/299] timecost: 64.62, lr: 0.000024, Train: (LOSS: 0.0103, MAE: 0.0103, RMSE: 0.0184, R2: 0.9915), Valid: (LOSS: 0.0334, MAE: 0.0334, RMSE: 0.0514, R2: 0.9569), PNorm: 132.7875, GNorm: 0.5310
[238/299] timecost: 64.25, lr: 0.000024, Train: (LOSS: 0.0101, MAE: 0.0101, RMSE: 0.0175, R2: 0.9912), Valid: (LOSS: 0.0336, MAE: 0.0336, RMSE: 0.0532, R2: 0.9516), PNorm: 132.7136, GNorm: 0.5732
[239/299] timecost: 63.57, lr: 0.000024, Train: (LOSS: 0.0102, MAE: 0.0102, RMSE: 0.0180, R2: 0.9919), Valid: (LOSS: 0.0342, MAE: 0.0342, RMSE: 0.0522, R2: 0.9550), PNorm: 132.6396, GNorm: 0.8429
[240/299] timecost: 63.91, lr: 0.000024, Train: (LOSS: 0.0107, MAE: 0.0107, RMSE: 0.0187, R2: 0.9913), Valid: (LOSS: 0.0342, MAE: 0.0342, RMSE: 0.0528, R2: 0.9534), PNorm: 132.5660, GNorm: 0.8385
[241/299] timecost: 64.04, lr: 0.000024, Train: (LOSS: 0.0107, MAE: 0.0107, RMSE: 0.0186, R2: 0.9907), Valid: (LOSS: 0.0344, MAE: 0.0344, RMSE: 0.0544, R2: 0.9503), PNorm: 132.4924, GNorm: 0.7537
[242/299] timecost: 64.25, lr: 0.000024, Train: (LOSS: 0.0099, MAE: 0.0099, RMSE: 0.0177, R2: 0.9922), Valid: (LOSS: 0.0347, MAE: 0.0347, RMSE: 0.0538, R2: 0.9529), PNorm: 132.4183, GNorm: 0.6526
[243/299] timecost: 63.95, lr: 0.000024, Train: (LOSS: 0.0100, MAE: 0.0100, RMSE: 0.0178, R2: 0.9921), Valid: (LOSS: 0.0341, MAE: 0.0341, RMSE: 0.0522, R2: 0.9554), PNorm: 132.3446, GNorm: 0.4935
[244/299] timecost: 64.05, lr: 0.000024, Train: (LOSS: 0.0096, MAE: 0.0096, RMSE: 0.0174, R2: 0.9918), Valid: (LOSS: 0.0335, MAE: 0.0335, RMSE: 0.0507, R2: 0.9576), PNorm: 132.2710, GNorm: 0.7177
[245/299] timecost: 63.95, lr: 0.000024, Train: (LOSS: 0.0097, MAE: 0.0097, RMSE: 0.0165, R2: 0.9919), Valid: (LOSS: 0.0342, MAE: 0.0342, RMSE: 0.0527, R2: 0.9550), PNorm: 132.1973, GNorm: 0.8435
[246/299] timecost: 64.91, lr: 0.000024, Train: (LOSS: 0.0099, MAE: 0.0099, RMSE: 0.0173, R2: 0.9922), Valid: (LOSS: 0.0334, MAE: 0.0334, RMSE: 0.0516, R2: 0.9565), PNorm: 132.1237, GNorm: 0.8319
[247/299] timecost: 66.80, lr: 0.000024, Train: (LOSS: 0.0102, MAE: 0.0102, RMSE: 0.0179, R2: 0.9923), Valid: (LOSS: 0.0328, MAE: 0.0328, RMSE: 0.0507, R2: 0.9564), PNorm: 132.0504, GNorm: 0.6670
[248/299] timecost: 66.81, lr: 0.000024, Train: (LOSS: 0.0101, MAE: 0.0101, RMSE: 0.0176, R2: 0.9907), Valid: (LOSS: 0.0332, MAE: 0.0332, RMSE: 0.0512, R2: 0.9562), PNorm: 131.9774, GNorm: 0.6648
[249/299] timecost: 66.85, lr: 0.000024, Train: (LOSS: 0.0097, MAE: 0.0097, RMSE: 0.0172, R2: 0.9925), Valid: (LOSS: 0.0334, MAE: 0.0334, RMSE: 0.0523, R2: 0.9545), PNorm: 131.9045, GNorm: 0.7150
[250/299] timecost: 66.85, lr: 0.000024, Train: (LOSS: 0.0095, MAE: 0.0095, RMSE: 0.0171, R2: 0.9929), Valid: (LOSS: 0.0332, MAE: 0.0332, RMSE: 0.0518, R2: 0.9569), PNorm: 131.8312, GNorm: 0.5871
[251/299] timecost: 66.98, lr: 0.000024, Train: (LOSS: 0.0099, MAE: 0.0099, RMSE: 0.0175, R2: 0.9920), Valid: (LOSS: 0.0347, MAE: 0.0347, RMSE: 0.0533, R2: 0.9553), PNorm: 131.7580, GNorm: 0.8185
Epoch 00253: reducing learning rate of group 0 to 2.1870e-05.
[252/299] timecost: 67.01, lr: 0.000022, Train: (LOSS: 0.0100, MAE: 0.0100, RMSE: 0.0174, R2: 0.9930), Valid: (LOSS: 0.0332, MAE: 0.0332, RMSE: 0.0513, R2: 0.9576), PNorm: 131.6851, GNorm: 0.8024
[253/299] timecost: 66.77, lr: 0.000022, Train: (LOSS: 0.0094, MAE: 0.0094, RMSE: 0.0166, R2: 0.9926), Valid: (LOSS: 0.0331, MAE: 0.0331, RMSE: 0.0518, R2: 0.9559), PNorm: 131.6191, GNorm: 0.5945
[254/299] timecost: 66.77, lr: 0.000022, Train: (LOSS: 0.0095, MAE: 0.0095, RMSE: 0.0167, R2: 0.9929), Valid: (LOSS: 0.0326, MAE: 0.0326, RMSE: 0.0497, R2: 0.9598), PNorm: 131.5535, GNorm: 0.6792
[255/299] timecost: 67.08, lr: 0.000022, Train: (LOSS: 0.0089, MAE: 0.0089, RMSE: 0.0159, R2: 0.9936), Valid: (LOSS: 0.0337, MAE: 0.0337, RMSE: 0.0533, R2: 0.9529), PNorm: 131.4878, GNorm: 0.7698
[256/299] timecost: 66.78, lr: 0.000022, Train: (LOSS: 0.0096, MAE: 0.0096, RMSE: 0.0165, R2: 0.9928), Valid: (LOSS: 0.0339, MAE: 0.0339, RMSE: 0.0521, R2: 0.9547), PNorm: 131.4221, GNorm: 0.6738
[257/299] timecost: 66.92, lr: 0.000022, Train: (LOSS: 0.0096, MAE: 0.0096, RMSE: 0.0166, R2: 0.9930), Valid: (LOSS: 0.0332, MAE: 0.0332, RMSE: 0.0503, R2: 0.9592), PNorm: 131.3572, GNorm: 0.7572
[258/299] timecost: 66.82, lr: 0.000022, Train: (LOSS: 0.0098, MAE: 0.0098, RMSE: 0.0171, R2: 0.9931), Valid: (LOSS: 0.0336, MAE: 0.0336, RMSE: 0.0514, R2: 0.9576), PNorm: 131.2922, GNorm: 0.7958
[259/299] timecost: 66.93, lr: 0.000022, Train: (LOSS: 0.0090, MAE: 0.0090, RMSE: 0.0159, R2: 0.9927), Valid: (LOSS: 0.0339, MAE: 0.0339, RMSE: 0.0514, R2: 0.9575), PNorm: 131.2261, GNorm: 0.9305
[260/299] timecost: 66.77, lr: 0.000022, Train: (LOSS: 0.0091, MAE: 0.0091, RMSE: 0.0160, R2: 0.9934), Valid: (LOSS: 0.0325, MAE: 0.0325, RMSE: 0.0504, R2: 0.9574), PNorm: 131.1618, GNorm: 0.7816
[261/299] timecost: 66.94, lr: 0.000022, Train: (LOSS: 0.0085, MAE: 0.0085, RMSE: 0.0153, R2: 0.9937), Valid: (LOSS: 0.0347, MAE: 0.0347, RMSE: 0.0540, R2: 0.9535), PNorm: 131.0964, GNorm: 0.9329
[262/299] timecost: 66.76, lr: 0.000022, Train: (LOSS: 0.0086, MAE: 0.0086, RMSE: 0.0154, R2: 0.9934), Valid: (LOSS: 0.0340, MAE: 0.0340, RMSE: 0.0517, R2: 0.9562), PNorm: 131.0306, GNorm: 0.8405
[263/299] timecost: 66.82, lr: 0.000022, Train: (LOSS: 0.0092, MAE: 0.0092, RMSE: 0.0158, R2: 0.9934), Valid: (LOSS: 0.0332, MAE: 0.0332, RMSE: 0.0521, R2: 0.9555), PNorm: 130.9651, GNorm: 0.6847
[264/299] timecost: 66.91, lr: 0.000022, Train: (LOSS: 0.0089, MAE: 0.0089, RMSE: 0.0156, R2: 0.9928), Valid: (LOSS: 0.0336, MAE: 0.0336, RMSE: 0.0509, R2: 0.9573), PNorm: 130.9001, GNorm: 0.5449
[265/299] timecost: 66.85, lr: 0.000022, Train: (LOSS: 0.0090, MAE: 0.0090, RMSE: 0.0156, R2: 0.9938), Valid: (LOSS: 0.0347, MAE: 0.0347, RMSE: 0.0520, R2: 0.9559), PNorm: 130.8352, GNorm: 0.7145
[266/299] timecost: 66.96, lr: 0.000022, Train: (LOSS: 0.0086, MAE: 0.0086, RMSE: 0.0152, R2: 0.9944), Valid: (LOSS: 0.0341, MAE: 0.0341, RMSE: 0.0531, R2: 0.9539), PNorm: 130.7699, GNorm: 0.8557
[267/299] timecost: 66.79, lr: 0.000022, Train: (LOSS: 0.0085, MAE: 0.0085, RMSE: 0.0152, R2: 0.9942), Valid: (LOSS: 0.0332, MAE: 0.0332, RMSE: 0.0510, R2: 0.9587), PNorm: 130.7043, GNorm: 0.7429
[268/299] timecost: 66.72, lr: 0.000022, Train: (LOSS: 0.0086, MAE: 0.0086, RMSE: 0.0153, R2: 0.9939), Valid: (LOSS: 0.0333, MAE: 0.0333, RMSE: 0.0511, R2: 0.9572), PNorm: 130.6397, GNorm: 0.6548
[269/299] timecost: 66.92, lr: 0.000022, Train: (LOSS: 0.0087, MAE: 0.0087, RMSE: 0.0153, R2: 0.9938), Valid: (LOSS: 0.0351, MAE: 0.0351, RMSE: 0.0530, R2: 0.9546), PNorm: 130.5750, GNorm: 0.6662
[270/299] timecost: 66.82, lr: 0.000022, Train: (LOSS: 0.0093, MAE: 0.0093, RMSE: 0.0160, R2: 0.9941), Valid: (LOSS: 0.0339, MAE: 0.0339, RMSE: 0.0512, R2: 0.9566), PNorm: 130.5100, GNorm: 0.7831
[271/299] timecost: 66.68, lr: 0.000022, Train: (LOSS: 0.0091, MAE: 0.0091, RMSE: 0.0157, R2: 0.9939), Valid: (LOSS: 0.0342, MAE: 0.0342, RMSE: 0.0521, R2: 0.9547), PNorm: 130.4453, GNorm: 0.6884
[272/299] timecost: 66.83, lr: 0.000022, Train: (LOSS: 0.0085, MAE: 0.0085, RMSE: 0.0149, R2: 0.9942), Valid: (LOSS: 0.0334, MAE: 0.0334, RMSE: 0.0504, R2: 0.9579), PNorm: 130.3807, GNorm: 0.4837
Epoch 00274: reducing learning rate of group 0 to 1.9683e-05.
[273/299] timecost: 66.88, lr: 0.000020, Train: (LOSS: 0.0086, MAE: 0.0086, RMSE: 0.0155, R2: 0.9933), Valid: (LOSS: 0.0341, MAE: 0.0341, RMSE: 0.0515, R2: 0.9575), PNorm: 130.3164, GNorm: 1.0441
[274/299] timecost: 66.52, lr: 0.000020, Train: (LOSS: 0.0080, MAE: 0.0080, RMSE: 0.0145, R2: 0.9947), Valid: (LOSS: 0.0330, MAE: 0.0330, RMSE: 0.0496, R2: 0.9595), PNorm: 130.2579, GNorm: 0.4829
[275/299] timecost: 64.71, lr: 0.000020, Train: (LOSS: 0.0076, MAE: 0.0076, RMSE: 0.0139, R2: 0.9944), Valid: (LOSS: 0.0332, MAE: 0.0332, RMSE: 0.0494, R2: 0.9596), PNorm: 130.1995, GNorm: 0.7706
[276/299] timecost: 64.99, lr: 0.000020, Train: (LOSS: 0.0076, MAE: 0.0076, RMSE: 0.0134, R2: 0.9952), Valid: (LOSS: 0.0318, MAE: 0.0318, RMSE: 0.0485, R2: 0.9609), PNorm: 130.1412, GNorm: 0.6165
[277/299] timecost: 65.06, lr: 0.000020, Train: (LOSS: 0.0077, MAE: 0.0077, RMSE: 0.0138, R2: 0.9946), Valid: (LOSS: 0.0341, MAE: 0.0341, RMSE: 0.0529, R2: 0.9547), PNorm: 130.0821, GNorm: 0.9304
[278/299] timecost: 65.09, lr: 0.000020, Train: (LOSS: 0.0074, MAE: 0.0074, RMSE: 0.0132, R2: 0.9939), Valid: (LOSS: 0.0325, MAE: 0.0325, RMSE: 0.0499, R2: 0.9600), PNorm: 130.0238, GNorm: 0.8276
[279/299] timecost: 65.49, lr: 0.000020, Train: (LOSS: 0.0080, MAE: 0.0080, RMSE: 0.0141, R2: 0.9948), Valid: (LOSS: 0.0321, MAE: 0.0321, RMSE: 0.0489, R2: 0.9610), PNorm: 129.9653, GNorm: 0.7415
[280/299] timecost: 66.89, lr: 0.000020, Train: (LOSS: 0.0077, MAE: 0.0077, RMSE: 0.0139, R2: 0.9949), Valid: (LOSS: 0.0325, MAE: 0.0325, RMSE: 0.0494, R2: 0.9608), PNorm: 129.9064, GNorm: 0.5502
[281/299] timecost: 66.92, lr: 0.000020, Train: (LOSS: 0.0081, MAE: 0.0081, RMSE: 0.0143, R2: 0.9948), Valid: (LOSS: 0.0328, MAE: 0.0328, RMSE: 0.0501, R2: 0.9588), PNorm: 129.8481, GNorm: 0.8212
[282/299] timecost: 66.95, lr: 0.000020, Train: (LOSS: 0.0079, MAE: 0.0079, RMSE: 0.0141, R2: 0.9947), Valid: (LOSS: 0.0332, MAE: 0.0332, RMSE: 0.0510, R2: 0.9583), PNorm: 129.7896, GNorm: 0.6421
[283/299] timecost: 65.41, lr: 0.000020, Train: (LOSS: 0.0074, MAE: 0.0074, RMSE: 0.0135, R2: 0.9950), Valid: (LOSS: 0.0337, MAE: 0.0337, RMSE: 0.0507, R2: 0.9588), PNorm: 129.7318, GNorm: 0.6233
[284/299] timecost: 64.10, lr: 0.000020, Train: (LOSS: 0.0076, MAE: 0.0076, RMSE: 0.0137, R2: 0.9950), Valid: (LOSS: 0.0332, MAE: 0.0332, RMSE: 0.0510, R2: 0.9575), PNorm: 129.6727, GNorm: 0.8755
[285/299] timecost: 64.25, lr: 0.000020, Train: (LOSS: 0.0080, MAE: 0.0080, RMSE: 0.0140, R2: 0.9952), Valid: (LOSS: 0.0331, MAE: 0.0331, RMSE: 0.0500, R2: 0.9591), PNorm: 129.6160, GNorm: 0.8446
[286/299] timecost: 64.38, lr: 0.000020, Train: (LOSS: 0.0076, MAE: 0.0076, RMSE: 0.0133, R2: 0.9949), Valid: (LOSS: 0.0340, MAE: 0.0340, RMSE: 0.0505, R2: 0.9589), PNorm: 129.5577, GNorm: 0.8758
[287/299] timecost: 65.40, lr: 0.000020, Train: (LOSS: 0.0073, MAE: 0.0073, RMSE: 0.0130, R2: 0.9957), Valid: (LOSS: 0.0331, MAE: 0.0331, RMSE: 0.0509, R2: 0.9576), PNorm: 129.4997, GNorm: 0.7840
[288/299] timecost: 63.75, lr: 0.000020, Train: (LOSS: 0.0078, MAE: 0.0078, RMSE: 0.0138, R2: 0.9950), Valid: (LOSS: 0.0352, MAE: 0.0352, RMSE: 0.0525, R2: 0.9554), PNorm: 129.4411, GNorm: 0.8802
[289/299] timecost: 63.62, lr: 0.000020, Train: (LOSS: 0.0076, MAE: 0.0076, RMSE: 0.0130, R2: 0.9953), Valid: (LOSS: 0.0340, MAE: 0.0340, RMSE: 0.0517, R2: 0.9563), PNorm: 129.3834, GNorm: 0.8144
[290/299] timecost: 63.52, lr: 0.000020, Train: (LOSS: 0.0075, MAE: 0.0075, RMSE: 0.0134, R2: 0.9950), Valid: (LOSS: 0.0327, MAE: 0.0327, RMSE: 0.0502, R2: 0.9597), PNorm: 129.3251, GNorm: 0.6137
[291/299] timecost: 65.07, lr: 0.000020, Train: (LOSS: 0.0077, MAE: 0.0077, RMSE: 0.0135, R2: 0.9952), Valid: (LOSS: 0.0335, MAE: 0.0335, RMSE: 0.0510, R2: 0.9578), PNorm: 129.2675, GNorm: 0.6230
[292/299] timecost: 65.49, lr: 0.000020, Train: (LOSS: 0.0080, MAE: 0.0080, RMSE: 0.0137, R2: 0.9956), Valid: (LOSS: 0.0350, MAE: 0.0350, RMSE: 0.0531, R2: 0.9543), PNorm: 129.2092, GNorm: 0.6041
[293/299] timecost: 65.69, lr: 0.000020, Train: (LOSS: 0.0077, MAE: 0.0077, RMSE: 0.0135, R2: 0.9957), Valid: (LOSS: 0.0334, MAE: 0.0334, RMSE: 0.0508, R2: 0.9584), PNorm: 129.1519, GNorm: 0.6921
[294/299] timecost: 66.88, lr: 0.000020, Train: (LOSS: 0.0074, MAE: 0.0074, RMSE: 0.0127, R2: 0.9955), Valid: (LOSS: 0.0348, MAE: 0.0348, RMSE: 0.0523, R2: 0.9562), PNorm: 129.0941, GNorm: 0.7325
[295/299] timecost: 66.82, lr: 0.000020, Train: (LOSS: 0.0074, MAE: 0.0074, RMSE: 0.0131, R2: 0.9953), Valid: (LOSS: 0.0330, MAE: 0.0330, RMSE: 0.0501, R2: 0.9595), PNorm: 129.0364, GNorm: 0.6289
[296/299] timecost: 66.77, lr: 0.000020, Train: (LOSS: 0.0075, MAE: 0.0075, RMSE: 0.0132, R2: 0.9950), Valid: (LOSS: 0.0336, MAE: 0.0336, RMSE: 0.0497, R2: 0.9594), PNorm: 128.9788, GNorm: 0.6380
Epoch 00298: reducing learning rate of group 0 to 1.7715e-05.
[297/299] timecost: 66.74, lr: 0.000018, Train: (LOSS: 0.0079, MAE: 0.0079, RMSE: 0.0134, R2: 0.9954), Valid: (LOSS: 0.0345, MAE: 0.0345, RMSE: 0.0522, R2: 0.9563), PNorm: 128.9214, GNorm: 0.6583
[298/299] timecost: 66.79, lr: 0.000018, Train: (LOSS: 0.0073, MAE: 0.0073, RMSE: 0.0129, R2: 0.9953), Valid: (LOSS: 0.0335, MAE: 0.0335, RMSE: 0.0509, R2: 0.9585), PNorm: 128.8701, GNorm: 0.5707
[299/299] timecost: 66.69, lr: 0.000018, Train: (LOSS: 0.0070, MAE: 0.0070, RMSE: 0.0125, R2: 0.9958), Valid: (LOSS: 0.0322, MAE: 0.0322, RMSE: 0.0495, R2: 0.9603), PNorm: 128.8184, GNorm: 0.6846
==========Training End==========
==========Test Best Model==========
================Final Results=======================
mse: 0.0363 +- 0.0000:
rmse: 0.0561 +- 0.0000:
mae: 0.0363 +- 0.0000:
r2: 0.9515 +- 0.0000:
tensor([[0.0000, 0.0000],
        [0.0938, 0.1475],
        [0.1247, 0.1828],
        ...,
        [0.0000, 0.0000],
        [0.0000, 0.0000],
        [0.0000, 0.0000]], device='cuda:0')
