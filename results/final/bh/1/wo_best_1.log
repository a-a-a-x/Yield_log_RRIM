cuda available with GPU: Tesla V100S-PCIE-32GB
==========Load Seed==========
set_random_seed
0
==========Training Start==========
Training Graphs:  2491
Valid Graphs:  277
Test Graphs:  1187
============Not pretrained weights used============
============Creating new layers============
============Creating Model============
Training Graphs Batches:  78
Valid Graphs Batches:  9
Test Graphs Batches:  37
[0/299] timecost: 57.64, lr: 0.000100, Train: (LOSS: 0.2103, MAE: 0.2103, RMSE: 0.2555, R2: 0.0487), Valid: (LOSS: 0.1921, MAE: 0.1921, RMSE: 0.2343, R2: 0.2056), PNorm: 187.0394, GNorm: 0.5000
[1/299] timecost: 56.65, lr: 0.000100, Train: (LOSS: 0.1760, MAE: 0.1760, RMSE: 0.2268, R2: 0.2617), Valid: (LOSS: 0.1707, MAE: 0.1707, RMSE: 0.2183, R2: 0.3109), PNorm: 187.0691, GNorm: 0.5000
[2/299] timecost: 56.84, lr: 0.000100, Train: (LOSS: 0.1590, MAE: 0.1590, RMSE: 0.2080, R2: 0.3815), Valid: (LOSS: 0.1370, MAE: 0.1370, RMSE: 0.1787, R2: 0.5422), PNorm: 187.1065, GNorm: 0.5000
[3/299] timecost: 56.64, lr: 0.000100, Train: (LOSS: 0.1456, MAE: 0.1456, RMSE: 0.1926, R2: 0.4495), Valid: (LOSS: 0.1329, MAE: 0.1329, RMSE: 0.1767, R2: 0.5524), PNorm: 187.1443, GNorm: 0.5000
[4/299] timecost: 56.62, lr: 0.000100, Train: (LOSS: 0.1426, MAE: 0.1426, RMSE: 0.1918, R2: 0.4543), Valid: (LOSS: 0.1449, MAE: 0.1449, RMSE: 0.1852, R2: 0.5011), PNorm: 187.1752, GNorm: 0.5000
[5/299] timecost: 56.53, lr: 0.000100, Train: (LOSS: 0.1326, MAE: 0.1326, RMSE: 0.1815, R2: 0.5148), Valid: (LOSS: 0.1249, MAE: 0.1249, RMSE: 0.1699, R2: 0.5895), PNorm: 187.2044, GNorm: 0.5000
[6/299] timecost: 56.54, lr: 0.000100, Train: (LOSS: 0.1334, MAE: 0.1334, RMSE: 0.1816, R2: 0.5118), Valid: (LOSS: 0.1302, MAE: 0.1302, RMSE: 0.1804, R2: 0.5244), PNorm: 187.2366, GNorm: 0.5000
[7/299] timecost: 56.68, lr: 0.000100, Train: (LOSS: 0.1262, MAE: 0.1262, RMSE: 0.1744, R2: 0.5486), Valid: (LOSS: 0.1596, MAE: 0.1596, RMSE: 0.2319, R2: 0.2384), PNorm: 187.2720, GNorm: 0.5000
[8/299] timecost: 56.71, lr: 0.000100, Train: (LOSS: 0.1312, MAE: 0.1312, RMSE: 0.1805, R2: 0.5118), Valid: (LOSS: 0.1225, MAE: 0.1225, RMSE: 0.1667, R2: 0.6011), PNorm: 187.3035, GNorm: 0.5000
[9/299] timecost: 56.60, lr: 0.000100, Train: (LOSS: 0.1168, MAE: 0.1168, RMSE: 0.1620, R2: 0.6050), Valid: (LOSS: 0.1181, MAE: 0.1181, RMSE: 0.1680, R2: 0.5652), PNorm: 187.3461, GNorm: 0.4146
[10/299] timecost: 57.04, lr: 0.000100, Train: (LOSS: 0.1106, MAE: 0.1106, RMSE: 0.1567, R2: 0.6322), Valid: (LOSS: 0.1016, MAE: 0.1016, RMSE: 0.1505, R2: 0.6714), PNorm: 187.3781, GNorm: 0.3905
[11/299] timecost: 56.81, lr: 0.000100, Train: (LOSS: 0.1072, MAE: 0.1072, RMSE: 0.1518, R2: 0.6504), Valid: (LOSS: 0.1013, MAE: 0.1013, RMSE: 0.1472, R2: 0.6689), PNorm: 187.4063, GNorm: 0.5000
[12/299] timecost: 57.09, lr: 0.000100, Train: (LOSS: 0.1000, MAE: 0.1000, RMSE: 0.1441, R2: 0.6827), Valid: (LOSS: 0.0997, MAE: 0.0997, RMSE: 0.1480, R2: 0.6743), PNorm: 187.4383, GNorm: 0.5000
[13/299] timecost: 56.91, lr: 0.000100, Train: (LOSS: 0.0941, MAE: 0.0941, RMSE: 0.1375, R2: 0.7181), Valid: (LOSS: 0.0898, MAE: 0.0898, RMSE: 0.1333, R2: 0.7257), PNorm: 187.4709, GNorm: 0.5000
[14/299] timecost: 56.62, lr: 0.000100, Train: (LOSS: 0.0913, MAE: 0.0913, RMSE: 0.1341, R2: 0.7173), Valid: (LOSS: 0.0902, MAE: 0.0902, RMSE: 0.1346, R2: 0.7286), PNorm: 187.4999, GNorm: 0.5000
[15/299] timecost: 56.13, lr: 0.000100, Train: (LOSS: 0.0881, MAE: 0.0881, RMSE: 0.1302, R2: 0.7400), Valid: (LOSS: 0.0909, MAE: 0.0909, RMSE: 0.1278, R2: 0.7575), PNorm: 187.5311, GNorm: 0.5000
[16/299] timecost: 56.49, lr: 0.000100, Train: (LOSS: 0.0819, MAE: 0.0819, RMSE: 0.1217, R2: 0.7729), Valid: (LOSS: 0.0935, MAE: 0.0935, RMSE: 0.1309, R2: 0.7486), PNorm: 187.5589, GNorm: 0.5000
[17/299] timecost: 56.63, lr: 0.000100, Train: (LOSS: 0.0810, MAE: 0.0810, RMSE: 0.1212, R2: 0.7786), Valid: (LOSS: 0.0799, MAE: 0.0799, RMSE: 0.1201, R2: 0.7797), PNorm: 187.5886, GNorm: 0.5000
[18/299] timecost: 56.63, lr: 0.000100, Train: (LOSS: 0.0766, MAE: 0.0766, RMSE: 0.1164, R2: 0.7895), Valid: (LOSS: 0.0803, MAE: 0.0803, RMSE: 0.1207, R2: 0.7814), PNorm: 187.6123, GNorm: 0.5000
[19/299] timecost: 56.77, lr: 0.000100, Train: (LOSS: 0.0758, MAE: 0.0758, RMSE: 0.1157, R2: 0.7971), Valid: (LOSS: 0.0874, MAE: 0.0874, RMSE: 0.1312, R2: 0.7302), PNorm: 187.6342, GNorm: 0.5000
[20/299] timecost: 56.96, lr: 0.000100, Train: (LOSS: 0.0726, MAE: 0.0726, RMSE: 0.1116, R2: 0.8081), Valid: (LOSS: 0.0759, MAE: 0.0759, RMSE: 0.1139, R2: 0.8025), PNorm: 187.6563, GNorm: 0.5000
[21/299] timecost: 56.82, lr: 0.000100, Train: (LOSS: 0.0670, MAE: 0.0670, RMSE: 0.1030, R2: 0.8370), Valid: (LOSS: 0.0722, MAE: 0.0722, RMSE: 0.1107, R2: 0.8124), PNorm: 187.6751, GNorm: 0.5000
[22/299] timecost: 56.27, lr: 0.000100, Train: (LOSS: 0.0683, MAE: 0.0683, RMSE: 0.1067, R2: 0.8190), Valid: (LOSS: 0.0735, MAE: 0.0735, RMSE: 0.1108, R2: 0.8167), PNorm: 187.7003, GNorm: 0.4634
[23/299] timecost: 56.20, lr: 0.000100, Train: (LOSS: 0.0616, MAE: 0.0616, RMSE: 0.0953, R2: 0.8521), Valid: (LOSS: 0.0627, MAE: 0.0627, RMSE: 0.0938, R2: 0.8662), PNorm: 187.7268, GNorm: 0.5000
[24/299] timecost: 56.13, lr: 0.000100, Train: (LOSS: 0.0571, MAE: 0.0571, RMSE: 0.0905, R2: 0.8719), Valid: (LOSS: 0.0639, MAE: 0.0639, RMSE: 0.0923, R2: 0.8731), PNorm: 187.7440, GNorm: 0.5000
[25/299] timecost: 56.11, lr: 0.000100, Train: (LOSS: 0.0580, MAE: 0.0580, RMSE: 0.0900, R2: 0.8635), Valid: (LOSS: 0.0642, MAE: 0.0642, RMSE: 0.0992, R2: 0.8489), PNorm: 187.7664, GNorm: 0.5000
[26/299] timecost: 56.24, lr: 0.000100, Train: (LOSS: 0.0548, MAE: 0.0548, RMSE: 0.0863, R2: 0.8839), Valid: (LOSS: 0.0667, MAE: 0.0667, RMSE: 0.1081, R2: 0.8091), PNorm: 187.7919, GNorm: 0.4344
[27/299] timecost: 56.99, lr: 0.000100, Train: (LOSS: 0.0538, MAE: 0.0538, RMSE: 0.0834, R2: 0.8894), Valid: (LOSS: 0.0614, MAE: 0.0614, RMSE: 0.0888, R2: 0.8833), PNorm: 187.8113, GNorm: 0.5000
[28/299] timecost: 56.14, lr: 0.000100, Train: (LOSS: 0.0525, MAE: 0.0525, RMSE: 0.0821, R2: 0.8932), Valid: (LOSS: 0.0600, MAE: 0.0600, RMSE: 0.0879, R2: 0.8867), PNorm: 187.8316, GNorm: 0.5000
[29/299] timecost: 56.26, lr: 0.000100, Train: (LOSS: 0.0477, MAE: 0.0477, RMSE: 0.0768, R2: 0.9021), Valid: (LOSS: 0.0563, MAE: 0.0563, RMSE: 0.0856, R2: 0.8891), PNorm: 187.8544, GNorm: 0.4077
[30/299] timecost: 56.35, lr: 0.000100, Train: (LOSS: 0.0483, MAE: 0.0483, RMSE: 0.0768, R2: 0.9051), Valid: (LOSS: 0.0537, MAE: 0.0537, RMSE: 0.0801, R2: 0.9023), PNorm: 187.8742, GNorm: 0.5000
[31/299] timecost: 55.99, lr: 0.000100, Train: (LOSS: 0.0462, MAE: 0.0462, RMSE: 0.0722, R2: 0.9137), Valid: (LOSS: 0.0709, MAE: 0.0709, RMSE: 0.1089, R2: 0.8157), PNorm: 187.8941, GNorm: 0.5000
[32/299] timecost: 57.08, lr: 0.000100, Train: (LOSS: 0.0468, MAE: 0.0468, RMSE: 0.0731, R2: 0.9152), Valid: (LOSS: 0.0529, MAE: 0.0529, RMSE: 0.0783, R2: 0.9078), PNorm: 187.9140, GNorm: 0.5000
[33/299] timecost: 57.06, lr: 0.000100, Train: (LOSS: 0.0451, MAE: 0.0451, RMSE: 0.0700, R2: 0.9206), Valid: (LOSS: 0.0513, MAE: 0.0513, RMSE: 0.0799, R2: 0.9028), PNorm: 187.9334, GNorm: 0.4963
[34/299] timecost: 56.99, lr: 0.000100, Train: (LOSS: 0.0421, MAE: 0.0421, RMSE: 0.0665, R2: 0.9269), Valid: (LOSS: 0.0487, MAE: 0.0487, RMSE: 0.0728, R2: 0.9184), PNorm: 187.9476, GNorm: 0.4964
[35/299] timecost: 57.28, lr: 0.000100, Train: (LOSS: 0.0416, MAE: 0.0416, RMSE: 0.0645, R2: 0.9323), Valid: (LOSS: 0.0493, MAE: 0.0493, RMSE: 0.0754, R2: 0.9148), PNorm: 187.9660, GNorm: 0.5000
[36/299] timecost: 57.34, lr: 0.000100, Train: (LOSS: 0.0416, MAE: 0.0416, RMSE: 0.0673, R2: 0.9256), Valid: (LOSS: 0.0464, MAE: 0.0464, RMSE: 0.0727, R2: 0.9202), PNorm: 187.9871, GNorm: 0.5000
[37/299] timecost: 57.52, lr: 0.000100, Train: (LOSS: 0.0391, MAE: 0.0391, RMSE: 0.0626, R2: 0.9342), Valid: (LOSS: 0.0444, MAE: 0.0444, RMSE: 0.0679, R2: 0.9291), PNorm: 188.0015, GNorm: 0.5000
[38/299] timecost: 57.06, lr: 0.000100, Train: (LOSS: 0.0386, MAE: 0.0386, RMSE: 0.0610, R2: 0.9363), Valid: (LOSS: 0.0433, MAE: 0.0433, RMSE: 0.0687, R2: 0.9278), PNorm: 188.0215, GNorm: 0.5000
[39/299] timecost: 57.03, lr: 0.000100, Train: (LOSS: 0.0378, MAE: 0.0378, RMSE: 0.0596, R2: 0.9390), Valid: (LOSS: 0.0437, MAE: 0.0437, RMSE: 0.0680, R2: 0.9301), PNorm: 188.0352, GNorm: 0.4324
[40/299] timecost: 56.74, lr: 0.000100, Train: (LOSS: 0.0355, MAE: 0.0355, RMSE: 0.0566, R2: 0.9484), Valid: (LOSS: 0.0419, MAE: 0.0419, RMSE: 0.0644, R2: 0.9376), PNorm: 188.0488, GNorm: 0.5000
[41/299] timecost: 56.72, lr: 0.000100, Train: (LOSS: 0.0357, MAE: 0.0357, RMSE: 0.0563, R2: 0.9484), Valid: (LOSS: 0.0434, MAE: 0.0434, RMSE: 0.0678, R2: 0.9306), PNorm: 188.0672, GNorm: 0.4798
[42/299] timecost: 56.78, lr: 0.000100, Train: (LOSS: 0.0351, MAE: 0.0351, RMSE: 0.0560, R2: 0.9493), Valid: (LOSS: 0.0449, MAE: 0.0449, RMSE: 0.0683, R2: 0.9294), PNorm: 188.0837, GNorm: 0.5000
[43/299] timecost: 56.12, lr: 0.000100, Train: (LOSS: 0.0344, MAE: 0.0344, RMSE: 0.0541, R2: 0.9492), Valid: (LOSS: 0.0438, MAE: 0.0438, RMSE: 0.0696, R2: 0.9269), PNorm: 188.1012, GNorm: 0.5000
[44/299] timecost: 55.91, lr: 0.000100, Train: (LOSS: 0.0343, MAE: 0.0343, RMSE: 0.0542, R2: 0.9533), Valid: (LOSS: 0.0406, MAE: 0.0406, RMSE: 0.0657, R2: 0.9327), PNorm: 188.1169, GNorm: 0.5000
[45/299] timecost: 56.43, lr: 0.000100, Train: (LOSS: 0.0335, MAE: 0.0335, RMSE: 0.0524, R2: 0.9544), Valid: (LOSS: 0.0435, MAE: 0.0435, RMSE: 0.0675, R2: 0.9295), PNorm: 188.1315, GNorm: 0.5000
[46/299] timecost: 56.25, lr: 0.000100, Train: (LOSS: 0.0340, MAE: 0.0340, RMSE: 0.0535, R2: 0.9514), Valid: (LOSS: 0.0413, MAE: 0.0413, RMSE: 0.0664, R2: 0.9316), PNorm: 188.1471, GNorm: 0.5000
[47/299] timecost: 57.11, lr: 0.000100, Train: (LOSS: 0.0349, MAE: 0.0349, RMSE: 0.0546, R2: 0.9524), Valid: (LOSS: 0.0427, MAE: 0.0427, RMSE: 0.0669, R2: 0.9333), PNorm: 188.1672, GNorm: 0.5000
[48/299] timecost: 57.26, lr: 0.000100, Train: (LOSS: 0.0317, MAE: 0.0317, RMSE: 0.0496, R2: 0.9578), Valid: (LOSS: 0.0416, MAE: 0.0416, RMSE: 0.0708, R2: 0.9221), PNorm: 188.1812, GNorm: 0.5000
[49/299] timecost: 57.18, lr: 0.000100, Train: (LOSS: 0.0328, MAE: 0.0328, RMSE: 0.0524, R2: 0.9546), Valid: (LOSS: 0.0433, MAE: 0.0433, RMSE: 0.0672, R2: 0.9295), PNorm: 188.1975, GNorm: 0.4914
[50/299] timecost: 56.99, lr: 0.000100, Train: (LOSS: 0.0319, MAE: 0.0319, RMSE: 0.0506, R2: 0.9576), Valid: (LOSS: 0.0425, MAE: 0.0425, RMSE: 0.0626, R2: 0.9401), PNorm: 188.2142, GNorm: 0.5000
[51/299] timecost: 57.07, lr: 0.000100, Train: (LOSS: 0.0308, MAE: 0.0308, RMSE: 0.0482, R2: 0.9610), Valid: (LOSS: 0.0405, MAE: 0.0405, RMSE: 0.0649, R2: 0.9359), PNorm: 188.2282, GNorm: 0.4902
[52/299] timecost: 57.00, lr: 0.000100, Train: (LOSS: 0.0315, MAE: 0.0315, RMSE: 0.0495, R2: 0.9591), Valid: (LOSS: 0.0438, MAE: 0.0438, RMSE: 0.0680, R2: 0.9312), PNorm: 188.2442, GNorm: 0.4589
[53/299] timecost: 55.65, lr: 0.000100, Train: (LOSS: 0.0312, MAE: 0.0312, RMSE: 0.0492, R2: 0.9601), Valid: (LOSS: 0.0392, MAE: 0.0392, RMSE: 0.0598, R2: 0.9468), PNorm: 188.2620, GNorm: 0.4237
[54/299] timecost: 55.74, lr: 0.000100, Train: (LOSS: 0.0293, MAE: 0.0293, RMSE: 0.0464, R2: 0.9633), Valid: (LOSS: 0.0410, MAE: 0.0410, RMSE: 0.0628, R2: 0.9402), PNorm: 188.2774, GNorm: 0.5000
[55/299] timecost: 55.47, lr: 0.000100, Train: (LOSS: 0.0297, MAE: 0.0297, RMSE: 0.0467, R2: 0.9636), Valid: (LOSS: 0.0408, MAE: 0.0408, RMSE: 0.0645, R2: 0.9354), PNorm: 188.2936, GNorm: 0.4370
[56/299] timecost: 55.66, lr: 0.000100, Train: (LOSS: 0.0290, MAE: 0.0290, RMSE: 0.0460, R2: 0.9662), Valid: (LOSS: 0.0388, MAE: 0.0388, RMSE: 0.0608, R2: 0.9435), PNorm: 188.3069, GNorm: 0.4276
[57/299] timecost: 55.75, lr: 0.000100, Train: (LOSS: 0.0287, MAE: 0.0287, RMSE: 0.0451, R2: 0.9645), Valid: (LOSS: 0.0408, MAE: 0.0408, RMSE: 0.0603, R2: 0.9449), PNorm: 188.3231, GNorm: 0.5000
[58/299] timecost: 55.71, lr: 0.000100, Train: (LOSS: 0.0280, MAE: 0.0280, RMSE: 0.0448, R2: 0.9649), Valid: (LOSS: 0.0450, MAE: 0.0450, RMSE: 0.0653, R2: 0.9350), PNorm: 188.3426, GNorm: 0.5000
[59/299] timecost: 55.74, lr: 0.000100, Train: (LOSS: 0.0287, MAE: 0.0287, RMSE: 0.0456, R2: 0.9643), Valid: (LOSS: 0.0381, MAE: 0.0381, RMSE: 0.0594, R2: 0.9464), PNorm: 188.3573, GNorm: 0.4633
[60/299] timecost: 55.62, lr: 0.000100, Train: (LOSS: 0.0273, MAE: 0.0273, RMSE: 0.0435, R2: 0.9684), Valid: (LOSS: 0.0413, MAE: 0.0413, RMSE: 0.0609, R2: 0.9449), PNorm: 188.3742, GNorm: 0.5000
[61/299] timecost: 55.79, lr: 0.000100, Train: (LOSS: 0.0275, MAE: 0.0275, RMSE: 0.0431, R2: 0.9695), Valid: (LOSS: 0.0392, MAE: 0.0392, RMSE: 0.0594, R2: 0.9456), PNorm: 188.3897, GNorm: 0.4678
[62/299] timecost: 55.63, lr: 0.000100, Train: (LOSS: 0.0266, MAE: 0.0266, RMSE: 0.0421, R2: 0.9696), Valid: (LOSS: 0.0377, MAE: 0.0377, RMSE: 0.0586, R2: 0.9459), PNorm: 188.4043, GNorm: 0.4759
[63/299] timecost: 55.65, lr: 0.000100, Train: (LOSS: 0.0259, MAE: 0.0259, RMSE: 0.0414, R2: 0.9717), Valid: (LOSS: 0.0369, MAE: 0.0369, RMSE: 0.0570, R2: 0.9487), PNorm: 188.4188, GNorm: 0.5000
[64/299] timecost: 55.45, lr: 0.000100, Train: (LOSS: 0.0261, MAE: 0.0261, RMSE: 0.0408, R2: 0.9713), Valid: (LOSS: 0.0391, MAE: 0.0391, RMSE: 0.0585, R2: 0.9480), PNorm: 188.4329, GNorm: 0.5000
[65/299] timecost: 55.47, lr: 0.000100, Train: (LOSS: 0.0265, MAE: 0.0265, RMSE: 0.0409, R2: 0.9719), Valid: (LOSS: 0.0374, MAE: 0.0374, RMSE: 0.0587, R2: 0.9442), PNorm: 188.4509, GNorm: 0.5000
[66/299] timecost: 55.59, lr: 0.000100, Train: (LOSS: 0.0271, MAE: 0.0271, RMSE: 0.0440, R2: 0.9664), Valid: (LOSS: 0.0370, MAE: 0.0370, RMSE: 0.0559, R2: 0.9516), PNorm: 188.4622, GNorm: 0.4920
[67/299] timecost: 55.45, lr: 0.000100, Train: (LOSS: 0.0247, MAE: 0.0247, RMSE: 0.0397, R2: 0.9731), Valid: (LOSS: 0.0392, MAE: 0.0392, RMSE: 0.0614, R2: 0.9391), PNorm: 188.4791, GNorm: 0.4889
[68/299] timecost: 55.54, lr: 0.000100, Train: (LOSS: 0.0242, MAE: 0.0242, RMSE: 0.0387, R2: 0.9737), Valid: (LOSS: 0.0374, MAE: 0.0374, RMSE: 0.0590, R2: 0.9448), PNorm: 188.4932, GNorm: 0.4424
[69/299] timecost: 55.55, lr: 0.000100, Train: (LOSS: 0.0243, MAE: 0.0243, RMSE: 0.0385, R2: 0.9755), Valid: (LOSS: 0.0360, MAE: 0.0360, RMSE: 0.0548, R2: 0.9536), PNorm: 188.5090, GNorm: 0.4956
[70/299] timecost: 55.62, lr: 0.000100, Train: (LOSS: 0.0248, MAE: 0.0248, RMSE: 0.0387, R2: 0.9743), Valid: (LOSS: 0.0349, MAE: 0.0349, RMSE: 0.0538, R2: 0.9559), PNorm: 188.5228, GNorm: 0.4625
[71/299] timecost: 55.48, lr: 0.000100, Train: (LOSS: 0.0248, MAE: 0.0248, RMSE: 0.0388, R2: 0.9735), Valid: (LOSS: 0.0384, MAE: 0.0384, RMSE: 0.0589, R2: 0.9468), PNorm: 188.5375, GNorm: 0.4493
[72/299] timecost: 55.69, lr: 0.000100, Train: (LOSS: 0.0242, MAE: 0.0242, RMSE: 0.0382, R2: 0.9755), Valid: (LOSS: 0.0399, MAE: 0.0399, RMSE: 0.0591, R2: 0.9467), PNorm: 188.5562, GNorm: 0.4475
[73/299] timecost: 55.48, lr: 0.000100, Train: (LOSS: 0.0240, MAE: 0.0240, RMSE: 0.0375, R2: 0.9764), Valid: (LOSS: 0.0365, MAE: 0.0365, RMSE: 0.0568, R2: 0.9472), PNorm: 188.5714, GNorm: 0.4749
[74/299] timecost: 55.84, lr: 0.000100, Train: (LOSS: 0.0250, MAE: 0.0250, RMSE: 0.0386, R2: 0.9739), Valid: (LOSS: 0.0380, MAE: 0.0380, RMSE: 0.0552, R2: 0.9522), PNorm: 188.5900, GNorm: 0.5000
[75/299] timecost: 55.96, lr: 0.000100, Train: (LOSS: 0.0240, MAE: 0.0240, RMSE: 0.0372, R2: 0.9774), Valid: (LOSS: 0.0379, MAE: 0.0379, RMSE: 0.0578, R2: 0.9470), PNorm: 188.6072, GNorm: 0.4854
[76/299] timecost: 56.05, lr: 0.000100, Train: (LOSS: 0.0234, MAE: 0.0234, RMSE: 0.0366, R2: 0.9772), Valid: (LOSS: 0.0367, MAE: 0.0367, RMSE: 0.0552, R2: 0.9519), PNorm: 188.6237, GNorm: 0.4773
[77/299] timecost: 56.30, lr: 0.000100, Train: (LOSS: 0.0221, MAE: 0.0221, RMSE: 0.0349, R2: 0.9799), Valid: (LOSS: 0.0362, MAE: 0.0362, RMSE: 0.0567, R2: 0.9502), PNorm: 188.6381, GNorm: 0.5000
[78/299] timecost: 56.43, lr: 0.000100, Train: (LOSS: 0.0219, MAE: 0.0219, RMSE: 0.0351, R2: 0.9795), Valid: (LOSS: 0.0414, MAE: 0.0414, RMSE: 0.0607, R2: 0.9434), PNorm: 188.6594, GNorm: 0.5000
[79/299] timecost: 56.22, lr: 0.000100, Train: (LOSS: 0.0232, MAE: 0.0232, RMSE: 0.0364, R2: 0.9776), Valid: (LOSS: 0.0361, MAE: 0.0361, RMSE: 0.0532, R2: 0.9547), PNorm: 188.6732, GNorm: 0.4997
[80/299] timecost: 56.18, lr: 0.000100, Train: (LOSS: 0.0219, MAE: 0.0219, RMSE: 0.0340, R2: 0.9798), Valid: (LOSS: 0.0383, MAE: 0.0383, RMSE: 0.0583, R2: 0.9475), PNorm: 188.6897, GNorm: 0.5000
[81/299] timecost: 56.60, lr: 0.000100, Train: (LOSS: 0.0225, MAE: 0.0225, RMSE: 0.0355, R2: 0.9787), Valid: (LOSS: 0.0366, MAE: 0.0366, RMSE: 0.0563, R2: 0.9505), PNorm: 188.7069, GNorm: 0.5000
[82/299] timecost: 56.48, lr: 0.000100, Train: (LOSS: 0.0224, MAE: 0.0224, RMSE: 0.0348, R2: 0.9788), Valid: (LOSS: 0.0364, MAE: 0.0364, RMSE: 0.0574, R2: 0.9485), PNorm: 188.7283, GNorm: 0.3557
[83/299] timecost: 56.28, lr: 0.000100, Train: (LOSS: 0.0225, MAE: 0.0225, RMSE: 0.0352, R2: 0.9791), Valid: (LOSS: 0.0362, MAE: 0.0362, RMSE: 0.0557, R2: 0.9510), PNorm: 188.7437, GNorm: 0.3923
[84/299] timecost: 56.01, lr: 0.000100, Train: (LOSS: 0.0208, MAE: 0.0208, RMSE: 0.0323, R2: 0.9830), Valid: (LOSS: 0.0361, MAE: 0.0361, RMSE: 0.0557, R2: 0.9520), PNorm: 188.7638, GNorm: 0.3679
[85/299] timecost: 56.35, lr: 0.000100, Train: (LOSS: 0.0221, MAE: 0.0221, RMSE: 0.0346, R2: 0.9795), Valid: (LOSS: 0.0370, MAE: 0.0370, RMSE: 0.0563, R2: 0.9504), PNorm: 188.7771, GNorm: 0.5000
Epoch 00087: reducing learning rate of group 0 to 8.5000e-05.
[86/299] timecost: 56.78, lr: 0.000085, Train: (LOSS: 0.0210, MAE: 0.0210, RMSE: 0.0335, R2: 0.9811), Valid: (LOSS: 0.0383, MAE: 0.0383, RMSE: 0.0584, R2: 0.9464), PNorm: 188.7952, GNorm: 0.4166
[87/299] timecost: 56.26, lr: 0.000085, Train: (LOSS: 0.0194, MAE: 0.0194, RMSE: 0.0312, R2: 0.9832), Valid: (LOSS: 0.0370, MAE: 0.0370, RMSE: 0.0567, R2: 0.9503), PNorm: 188.8015, GNorm: 0.5000
[88/299] timecost: 56.27, lr: 0.000085, Train: (LOSS: 0.0193, MAE: 0.0193, RMSE: 0.0305, R2: 0.9835), Valid: (LOSS: 0.0374, MAE: 0.0374, RMSE: 0.0586, R2: 0.9469), PNorm: 188.8157, GNorm: 0.5000
[89/299] timecost: 56.09, lr: 0.000085, Train: (LOSS: 0.0184, MAE: 0.0184, RMSE: 0.0294, R2: 0.9855), Valid: (LOSS: 0.0356, MAE: 0.0356, RMSE: 0.0544, R2: 0.9529), PNorm: 188.8257, GNorm: 0.5000
[90/299] timecost: 56.29, lr: 0.000085, Train: (LOSS: 0.0189, MAE: 0.0189, RMSE: 0.0303, R2: 0.9827), Valid: (LOSS: 0.0363, MAE: 0.0363, RMSE: 0.0569, R2: 0.9487), PNorm: 188.8364, GNorm: 0.3225
[91/299] timecost: 55.93, lr: 0.000085, Train: (LOSS: 0.0178, MAE: 0.0178, RMSE: 0.0286, R2: 0.9854), Valid: (LOSS: 0.0370, MAE: 0.0370, RMSE: 0.0579, R2: 0.9484), PNorm: 188.8468, GNorm: 0.4989
[92/299] timecost: 56.09, lr: 0.000085, Train: (LOSS: 0.0181, MAE: 0.0181, RMSE: 0.0290, R2: 0.9848), Valid: (LOSS: 0.0358, MAE: 0.0358, RMSE: 0.0542, R2: 0.9547), PNorm: 188.8567, GNorm: 0.4263
[93/299] timecost: 56.11, lr: 0.000085, Train: (LOSS: 0.0177, MAE: 0.0177, RMSE: 0.0284, R2: 0.9844), Valid: (LOSS: 0.0360, MAE: 0.0360, RMSE: 0.0565, R2: 0.9500), PNorm: 188.8664, GNorm: 0.4112
[94/299] timecost: 55.78, lr: 0.000085, Train: (LOSS: 0.0188, MAE: 0.0188, RMSE: 0.0302, R2: 0.9840), Valid: (LOSS: 0.0370, MAE: 0.0370, RMSE: 0.0568, R2: 0.9507), PNorm: 188.8815, GNorm: 0.2955
[95/299] timecost: 55.48, lr: 0.000085, Train: (LOSS: 0.0178, MAE: 0.0178, RMSE: 0.0286, R2: 0.9846), Valid: (LOSS: 0.0372, MAE: 0.0372, RMSE: 0.0573, R2: 0.9488), PNorm: 188.8955, GNorm: 0.3773
[96/299] timecost: 56.10, lr: 0.000085, Train: (LOSS: 0.0177, MAE: 0.0177, RMSE: 0.0287, R2: 0.9852), Valid: (LOSS: 0.0346, MAE: 0.0346, RMSE: 0.0535, R2: 0.9551), PNorm: 188.9048, GNorm: 0.5000
[97/299] timecost: 55.92, lr: 0.000085, Train: (LOSS: 0.0178, MAE: 0.0178, RMSE: 0.0285, R2: 0.9861), Valid: (LOSS: 0.0357, MAE: 0.0357, RMSE: 0.0536, R2: 0.9557), PNorm: 188.9169, GNorm: 0.4449
[98/299] timecost: 55.93, lr: 0.000085, Train: (LOSS: 0.0172, MAE: 0.0172, RMSE: 0.0268, R2: 0.9869), Valid: (LOSS: 0.0373, MAE: 0.0373, RMSE: 0.0579, R2: 0.9477), PNorm: 188.9307, GNorm: 0.3787
[99/299] timecost: 55.47, lr: 0.000085, Train: (LOSS: 0.0167, MAE: 0.0167, RMSE: 0.0276, R2: 0.9867), Valid: (LOSS: 0.0355, MAE: 0.0355, RMSE: 0.0560, R2: 0.9511), PNorm: 188.9427, GNorm: 0.5000
[100/299] timecost: 55.71, lr: 0.000085, Train: (LOSS: 0.0169, MAE: 0.0169, RMSE: 0.0271, R2: 0.9868), Valid: (LOSS: 0.0345, MAE: 0.0345, RMSE: 0.0525, R2: 0.9579), PNorm: 188.9537, GNorm: 0.4865
[101/299] timecost: 55.55, lr: 0.000085, Train: (LOSS: 0.0169, MAE: 0.0169, RMSE: 0.0270, R2: 0.9870), Valid: (LOSS: 0.0352, MAE: 0.0352, RMSE: 0.0551, R2: 0.9520), PNorm: 188.9642, GNorm: 0.3911
[102/299] timecost: 55.95, lr: 0.000085, Train: (LOSS: 0.0170, MAE: 0.0170, RMSE: 0.0266, R2: 0.9876), Valid: (LOSS: 0.0373, MAE: 0.0373, RMSE: 0.0575, R2: 0.9482), PNorm: 188.9788, GNorm: 0.4342
[103/299] timecost: 56.57, lr: 0.000085, Train: (LOSS: 0.0170, MAE: 0.0170, RMSE: 0.0272, R2: 0.9857), Valid: (LOSS: 0.0372, MAE: 0.0372, RMSE: 0.0564, R2: 0.9506), PNorm: 188.9944, GNorm: 0.5000
[104/299] timecost: 56.79, lr: 0.000085, Train: (LOSS: 0.0170, MAE: 0.0170, RMSE: 0.0274, R2: 0.9869), Valid: (LOSS: 0.0375, MAE: 0.0375, RMSE: 0.0568, R2: 0.9492), PNorm: 189.0087, GNorm: 0.4379
[105/299] timecost: 56.71, lr: 0.000085, Train: (LOSS: 0.0163, MAE: 0.0163, RMSE: 0.0260, R2: 0.9869), Valid: (LOSS: 0.0356, MAE: 0.0356, RMSE: 0.0553, R2: 0.9539), PNorm: 189.0194, GNorm: 0.4079
[106/299] timecost: 56.36, lr: 0.000085, Train: (LOSS: 0.0165, MAE: 0.0165, RMSE: 0.0256, R2: 0.9883), Valid: (LOSS: 0.0358, MAE: 0.0358, RMSE: 0.0533, R2: 0.9567), PNorm: 189.0331, GNorm: 0.4653
[107/299] timecost: 56.43, lr: 0.000085, Train: (LOSS: 0.0166, MAE: 0.0166, RMSE: 0.0246, R2: 0.9896), Valid: (LOSS: 0.0361, MAE: 0.0361, RMSE: 0.0521, R2: 0.9585), PNorm: 189.0496, GNorm: 0.4192
[108/299] timecost: 56.42, lr: 0.000085, Train: (LOSS: 0.0158, MAE: 0.0158, RMSE: 0.0236, R2: 0.9905), Valid: (LOSS: 0.0347, MAE: 0.0347, RMSE: 0.0518, R2: 0.9585), PNorm: 189.0626, GNorm: 0.5000
[109/299] timecost: 56.92, lr: 0.000085, Train: (LOSS: 0.0155, MAE: 0.0155, RMSE: 0.0232, R2: 0.9909), Valid: (LOSS: 0.0346, MAE: 0.0346, RMSE: 0.0527, R2: 0.9573), PNorm: 189.0747, GNorm: 0.4671
[110/299] timecost: 57.23, lr: 0.000085, Train: (LOSS: 0.0153, MAE: 0.0153, RMSE: 0.0230, R2: 0.9909), Valid: (LOSS: 0.0341, MAE: 0.0341, RMSE: 0.0496, R2: 0.9612), PNorm: 189.0897, GNorm: 0.5000
[111/299] timecost: 56.56, lr: 0.000085, Train: (LOSS: 0.0159, MAE: 0.0159, RMSE: 0.0237, R2: 0.9908), Valid: (LOSS: 0.0347, MAE: 0.0347, RMSE: 0.0520, R2: 0.9587), PNorm: 189.1023, GNorm: 0.4318
[112/299] timecost: 56.67, lr: 0.000085, Train: (LOSS: 0.0151, MAE: 0.0151, RMSE: 0.0225, R2: 0.9918), Valid: (LOSS: 0.0355, MAE: 0.0355, RMSE: 0.0560, R2: 0.9526), PNorm: 189.1193, GNorm: 0.3898
[113/299] timecost: 55.69, lr: 0.000085, Train: (LOSS: 0.0155, MAE: 0.0155, RMSE: 0.0243, R2: 0.9904), Valid: (LOSS: 0.0366, MAE: 0.0366, RMSE: 0.0563, R2: 0.9507), PNorm: 189.1293, GNorm: 0.4926
[114/299] timecost: 55.76, lr: 0.000085, Train: (LOSS: 0.0146, MAE: 0.0146, RMSE: 0.0214, R2: 0.9929), Valid: (LOSS: 0.0346, MAE: 0.0346, RMSE: 0.0512, R2: 0.9595), PNorm: 189.1410, GNorm: 0.4720
[115/299] timecost: 56.41, lr: 0.000085, Train: (LOSS: 0.0146, MAE: 0.0146, RMSE: 0.0216, R2: 0.9924), Valid: (LOSS: 0.0363, MAE: 0.0363, RMSE: 0.0530, R2: 0.9565), PNorm: 189.1557, GNorm: 0.3692
[116/299] timecost: 56.09, lr: 0.000085, Train: (LOSS: 0.0143, MAE: 0.0143, RMSE: 0.0212, R2: 0.9929), Valid: (LOSS: 0.0356, MAE: 0.0356, RMSE: 0.0516, R2: 0.9603), PNorm: 189.1693, GNorm: 0.5000
[117/299] timecost: 56.43, lr: 0.000085, Train: (LOSS: 0.0147, MAE: 0.0147, RMSE: 0.0209, R2: 0.9930), Valid: (LOSS: 0.0345, MAE: 0.0345, RMSE: 0.0511, R2: 0.9599), PNorm: 189.1797, GNorm: 0.4774
[118/299] timecost: 56.60, lr: 0.000085, Train: (LOSS: 0.0146, MAE: 0.0146, RMSE: 0.0214, R2: 0.9927), Valid: (LOSS: 0.0337, MAE: 0.0337, RMSE: 0.0493, R2: 0.9630), PNorm: 189.1974, GNorm: 0.5000
[119/299] timecost: 56.13, lr: 0.000085, Train: (LOSS: 0.0148, MAE: 0.0148, RMSE: 0.0223, R2: 0.9915), Valid: (LOSS: 0.0357, MAE: 0.0357, RMSE: 0.0555, R2: 0.9531), PNorm: 189.2114, GNorm: 0.5000
[120/299] timecost: 56.56, lr: 0.000085, Train: (LOSS: 0.0152, MAE: 0.0152, RMSE: 0.0231, R2: 0.9907), Valid: (LOSS: 0.0321, MAE: 0.0321, RMSE: 0.0467, R2: 0.9665), PNorm: 189.2243, GNorm: 0.3812
[121/299] timecost: 55.87, lr: 0.000085, Train: (LOSS: 0.0148, MAE: 0.0148, RMSE: 0.0215, R2: 0.9928), Valid: (LOSS: 0.0360, MAE: 0.0360, RMSE: 0.0523, R2: 0.9573), PNorm: 189.2403, GNorm: 0.3415
[122/299] timecost: 56.21, lr: 0.000085, Train: (LOSS: 0.0144, MAE: 0.0144, RMSE: 0.0209, R2: 0.9931), Valid: (LOSS: 0.0351, MAE: 0.0351, RMSE: 0.0509, R2: 0.9575), PNorm: 189.2559, GNorm: 0.4017
[123/299] timecost: 55.62, lr: 0.000085, Train: (LOSS: 0.0142, MAE: 0.0142, RMSE: 0.0208, R2: 0.9931), Valid: (LOSS: 0.0341, MAE: 0.0341, RMSE: 0.0506, R2: 0.9606), PNorm: 189.2696, GNorm: 0.4569
[124/299] timecost: 56.21, lr: 0.000085, Train: (LOSS: 0.0140, MAE: 0.0140, RMSE: 0.0198, R2: 0.9936), Valid: (LOSS: 0.0326, MAE: 0.0326, RMSE: 0.0463, R2: 0.9674), PNorm: 189.2844, GNorm: 0.4708
[125/299] timecost: 56.12, lr: 0.000085, Train: (LOSS: 0.0137, MAE: 0.0137, RMSE: 0.0194, R2: 0.9940), Valid: (LOSS: 0.0342, MAE: 0.0342, RMSE: 0.0501, R2: 0.9606), PNorm: 189.2954, GNorm: 0.5000
[126/299] timecost: 55.99, lr: 0.000085, Train: (LOSS: 0.0145, MAE: 0.0145, RMSE: 0.0214, R2: 0.9915), Valid: (LOSS: 0.0354, MAE: 0.0354, RMSE: 0.0518, R2: 0.9594), PNorm: 189.3050, GNorm: 0.4960
[127/299] timecost: 56.16, lr: 0.000085, Train: (LOSS: 0.0139, MAE: 0.0139, RMSE: 0.0200, R2: 0.9938), Valid: (LOSS: 0.0344, MAE: 0.0344, RMSE: 0.0489, R2: 0.9630), PNorm: 189.3221, GNorm: 0.5000
[128/299] timecost: 56.69, lr: 0.000085, Train: (LOSS: 0.0135, MAE: 0.0135, RMSE: 0.0192, R2: 0.9940), Valid: (LOSS: 0.0342, MAE: 0.0342, RMSE: 0.0505, R2: 0.9621), PNorm: 189.3334, GNorm: 0.3732
[129/299] timecost: 55.85, lr: 0.000085, Train: (LOSS: 0.0130, MAE: 0.0130, RMSE: 0.0185, R2: 0.9946), Valid: (LOSS: 0.0342, MAE: 0.0342, RMSE: 0.0508, R2: 0.9607), PNorm: 189.3466, GNorm: 0.5000
[130/299] timecost: 57.10, lr: 0.000085, Train: (LOSS: 0.0127, MAE: 0.0127, RMSE: 0.0182, R2: 0.9949), Valid: (LOSS: 0.0350, MAE: 0.0350, RMSE: 0.0512, R2: 0.9587), PNorm: 189.3604, GNorm: 0.5000
[131/299] timecost: 56.30, lr: 0.000085, Train: (LOSS: 0.0125, MAE: 0.0125, RMSE: 0.0182, R2: 0.9950), Valid: (LOSS: 0.0316, MAE: 0.0316, RMSE: 0.0451, R2: 0.9695), PNorm: 189.3717, GNorm: 0.5000
[132/299] timecost: 56.30, lr: 0.000085, Train: (LOSS: 0.0122, MAE: 0.0122, RMSE: 0.0175, R2: 0.9952), Valid: (LOSS: 0.0344, MAE: 0.0344, RMSE: 0.0499, R2: 0.9619), PNorm: 189.3838, GNorm: 0.4708
[133/299] timecost: 56.83, lr: 0.000085, Train: (LOSS: 0.0125, MAE: 0.0125, RMSE: 0.0176, R2: 0.9950), Valid: (LOSS: 0.0327, MAE: 0.0327, RMSE: 0.0456, R2: 0.9680), PNorm: 189.3928, GNorm: 0.4165
[134/299] timecost: 56.49, lr: 0.000085, Train: (LOSS: 0.0122, MAE: 0.0122, RMSE: 0.0177, R2: 0.9949), Valid: (LOSS: 0.0340, MAE: 0.0340, RMSE: 0.0492, R2: 0.9629), PNorm: 189.4082, GNorm: 0.4076
[135/299] timecost: 56.75, lr: 0.000085, Train: (LOSS: 0.0134, MAE: 0.0134, RMSE: 0.0198, R2: 0.9936), Valid: (LOSS: 0.0323, MAE: 0.0323, RMSE: 0.0469, R2: 0.9658), PNorm: 189.4265, GNorm: 0.5000
[136/299] timecost: 56.24, lr: 0.000085, Train: (LOSS: 0.0146, MAE: 0.0146, RMSE: 0.0233, R2: 0.9908), Valid: (LOSS: 0.0328, MAE: 0.0328, RMSE: 0.0477, R2: 0.9658), PNorm: 189.4404, GNorm: 0.4709
[137/299] timecost: 55.73, lr: 0.000085, Train: (LOSS: 0.0127, MAE: 0.0127, RMSE: 0.0184, R2: 0.9945), Valid: (LOSS: 0.0333, MAE: 0.0333, RMSE: 0.0493, R2: 0.9629), PNorm: 189.4570, GNorm: 0.3569
[138/299] timecost: 56.40, lr: 0.000085, Train: (LOSS: 0.0120, MAE: 0.0120, RMSE: 0.0167, R2: 0.9956), Valid: (LOSS: 0.0345, MAE: 0.0345, RMSE: 0.0505, R2: 0.9603), PNorm: 189.4692, GNorm: 0.4447
[139/299] timecost: 56.32, lr: 0.000085, Train: (LOSS: 0.0123, MAE: 0.0123, RMSE: 0.0175, R2: 0.9952), Valid: (LOSS: 0.0352, MAE: 0.0352, RMSE: 0.0503, R2: 0.9619), PNorm: 189.4800, GNorm: 0.4059
[140/299] timecost: 56.26, lr: 0.000085, Train: (LOSS: 0.0123, MAE: 0.0123, RMSE: 0.0179, R2: 0.9947), Valid: (LOSS: 0.0329, MAE: 0.0329, RMSE: 0.0488, R2: 0.9631), PNorm: 189.4940, GNorm: 0.4634
[141/299] timecost: 56.49, lr: 0.000085, Train: (LOSS: 0.0119, MAE: 0.0119, RMSE: 0.0168, R2: 0.9955), Valid: (LOSS: 0.0338, MAE: 0.0338, RMSE: 0.0487, R2: 0.9637), PNorm: 189.5065, GNorm: 0.3354
[142/299] timecost: 56.05, lr: 0.000085, Train: (LOSS: 0.0116, MAE: 0.0116, RMSE: 0.0167, R2: 0.9958), Valid: (LOSS: 0.0344, MAE: 0.0344, RMSE: 0.0490, R2: 0.9633), PNorm: 189.5149, GNorm: 0.3769
[143/299] timecost: 55.50, lr: 0.000085, Train: (LOSS: 0.0123, MAE: 0.0123, RMSE: 0.0175, R2: 0.9951), Valid: (LOSS: 0.0342, MAE: 0.0342, RMSE: 0.0490, R2: 0.9631), PNorm: 189.5267, GNorm: 0.3531
[144/299] timecost: 55.83, lr: 0.000085, Train: (LOSS: 0.0116, MAE: 0.0116, RMSE: 0.0164, R2: 0.9958), Valid: (LOSS: 0.0366, MAE: 0.0366, RMSE: 0.0542, R2: 0.9548), PNorm: 189.5384, GNorm: 0.4247
[145/299] timecost: 55.55, lr: 0.000085, Train: (LOSS: 0.0116, MAE: 0.0116, RMSE: 0.0165, R2: 0.9958), Valid: (LOSS: 0.0342, MAE: 0.0342, RMSE: 0.0490, R2: 0.9638), PNorm: 189.5489, GNorm: 0.4928
[146/299] timecost: 55.79, lr: 0.000085, Train: (LOSS: 0.0121, MAE: 0.0121, RMSE: 0.0176, R2: 0.9951), Valid: (LOSS: 0.0339, MAE: 0.0339, RMSE: 0.0481, R2: 0.9647), PNorm: 189.5615, GNorm: 0.4072
Epoch 00148: reducing learning rate of group 0 to 7.2250e-05.
[147/299] timecost: 55.32, lr: 0.000072, Train: (LOSS: 0.0117, MAE: 0.0117, RMSE: 0.0171, R2: 0.9954), Valid: (LOSS: 0.0329, MAE: 0.0329, RMSE: 0.0475, R2: 0.9650), PNorm: 189.5719, GNorm: 0.3443
[148/299] timecost: 55.80, lr: 0.000072, Train: (LOSS: 0.0109, MAE: 0.0109, RMSE: 0.0156, R2: 0.9961), Valid: (LOSS: 0.0341, MAE: 0.0341, RMSE: 0.0482, R2: 0.9645), PNorm: 189.5804, GNorm: 0.5000
[149/299] timecost: 56.30, lr: 0.000072, Train: (LOSS: 0.0101, MAE: 0.0101, RMSE: 0.0144, R2: 0.9968), Valid: (LOSS: 0.0344, MAE: 0.0344, RMSE: 0.0492, R2: 0.9635), PNorm: 189.5864, GNorm: 0.4535
[150/299] timecost: 56.65, lr: 0.000072, Train: (LOSS: 0.0102, MAE: 0.0102, RMSE: 0.0145, R2: 0.9965), Valid: (LOSS: 0.0332, MAE: 0.0332, RMSE: 0.0481, R2: 0.9651), PNorm: 189.5967, GNorm: 0.4342
[151/299] timecost: 56.68, lr: 0.000072, Train: (LOSS: 0.0098, MAE: 0.0098, RMSE: 0.0141, R2: 0.9969), Valid: (LOSS: 0.0338, MAE: 0.0338, RMSE: 0.0501, R2: 0.9614), PNorm: 189.6020, GNorm: 0.4584
[152/299] timecost: 56.45, lr: 0.000072, Train: (LOSS: 0.0096, MAE: 0.0096, RMSE: 0.0139, R2: 0.9970), Valid: (LOSS: 0.0316, MAE: 0.0316, RMSE: 0.0454, R2: 0.9678), PNorm: 189.6122, GNorm: 0.5000
[153/299] timecost: 56.42, lr: 0.000072, Train: (LOSS: 0.0099, MAE: 0.0099, RMSE: 0.0141, R2: 0.9969), Valid: (LOSS: 0.0332, MAE: 0.0332, RMSE: 0.0475, R2: 0.9660), PNorm: 189.6203, GNorm: 0.3653
[154/299] timecost: 56.79, lr: 0.000072, Train: (LOSS: 0.0099, MAE: 0.0099, RMSE: 0.0143, R2: 0.9967), Valid: (LOSS: 0.0342, MAE: 0.0342, RMSE: 0.0484, R2: 0.9640), PNorm: 189.6279, GNorm: 0.4397
[155/299] timecost: 56.37, lr: 0.000072, Train: (LOSS: 0.0094, MAE: 0.0094, RMSE: 0.0136, R2: 0.9970), Valid: (LOSS: 0.0320, MAE: 0.0320, RMSE: 0.0454, R2: 0.9682), PNorm: 189.6376, GNorm: 0.3191
[156/299] timecost: 56.69, lr: 0.000072, Train: (LOSS: 0.0093, MAE: 0.0093, RMSE: 0.0133, R2: 0.9972), Valid: (LOSS: 0.0327, MAE: 0.0327, RMSE: 0.0478, R2: 0.9648), PNorm: 189.6448, GNorm: 0.3596
[157/299] timecost: 56.92, lr: 0.000072, Train: (LOSS: 0.0097, MAE: 0.0097, RMSE: 0.0138, R2: 0.9970), Valid: (LOSS: 0.0316, MAE: 0.0316, RMSE: 0.0449, R2: 0.9687), PNorm: 189.6527, GNorm: 0.3913
[158/299] timecost: 55.97, lr: 0.000072, Train: (LOSS: 0.0101, MAE: 0.0101, RMSE: 0.0145, R2: 0.9966), Valid: (LOSS: 0.0328, MAE: 0.0328, RMSE: 0.0473, R2: 0.9644), PNorm: 189.6605, GNorm: 0.5000
[159/299] timecost: 56.34, lr: 0.000072, Train: (LOSS: 0.0095, MAE: 0.0095, RMSE: 0.0136, R2: 0.9971), Valid: (LOSS: 0.0329, MAE: 0.0329, RMSE: 0.0495, R2: 0.9624), PNorm: 189.6682, GNorm: 0.3622
[160/299] timecost: 56.09, lr: 0.000072, Train: (LOSS: 0.0100, MAE: 0.0100, RMSE: 0.0147, R2: 0.9967), Valid: (LOSS: 0.0352, MAE: 0.0352, RMSE: 0.0505, R2: 0.9604), PNorm: 189.6820, GNorm: 0.4161
[161/299] timecost: 56.04, lr: 0.000072, Train: (LOSS: 0.0097, MAE: 0.0097, RMSE: 0.0138, R2: 0.9971), Valid: (LOSS: 0.0344, MAE: 0.0344, RMSE: 0.0484, R2: 0.9643), PNorm: 189.6913, GNorm: 0.3814
[162/299] timecost: 55.89, lr: 0.000072, Train: (LOSS: 0.0103, MAE: 0.0103, RMSE: 0.0145, R2: 0.9967), Valid: (LOSS: 0.0322, MAE: 0.0322, RMSE: 0.0459, R2: 0.9679), PNorm: 189.6996, GNorm: 0.3616
Epoch 00164: reducing learning rate of group 0 to 6.1413e-05.
[163/299] timecost: 56.17, lr: 0.000061, Train: (LOSS: 0.0096, MAE: 0.0096, RMSE: 0.0139, R2: 0.9968), Valid: (LOSS: 0.0333, MAE: 0.0333, RMSE: 0.0477, R2: 0.9656), PNorm: 189.7107, GNorm: 0.3604
[164/299] timecost: 56.09, lr: 0.000061, Train: (LOSS: 0.0090, MAE: 0.0090, RMSE: 0.0131, R2: 0.9972), Valid: (LOSS: 0.0337, MAE: 0.0337, RMSE: 0.0481, R2: 0.9649), PNorm: 189.7189, GNorm: 0.5000
[165/299] timecost: 55.49, lr: 0.000061, Train: (LOSS: 0.0084, MAE: 0.0084, RMSE: 0.0122, R2: 0.9976), Valid: (LOSS: 0.0337, MAE: 0.0337, RMSE: 0.0485, R2: 0.9640), PNorm: 189.7238, GNorm: 0.3393
[166/299] timecost: 56.30, lr: 0.000061, Train: (LOSS: 0.0082, MAE: 0.0082, RMSE: 0.0119, R2: 0.9977), Valid: (LOSS: 0.0323, MAE: 0.0323, RMSE: 0.0461, R2: 0.9676), PNorm: 189.7295, GNorm: 0.4456
[167/299] timecost: 56.83, lr: 0.000061, Train: (LOSS: 0.0084, MAE: 0.0084, RMSE: 0.0122, R2: 0.9976), Valid: (LOSS: 0.0323, MAE: 0.0323, RMSE: 0.0460, R2: 0.9677), PNorm: 189.7366, GNorm: 0.5000
[168/299] timecost: 56.58, lr: 0.000061, Train: (LOSS: 0.0083, MAE: 0.0083, RMSE: 0.0119, R2: 0.9977), Valid: (LOSS: 0.0342, MAE: 0.0342, RMSE: 0.0503, R2: 0.9612), PNorm: 189.7420, GNorm: 0.4257
[169/299] timecost: 55.92, lr: 0.000061, Train: (LOSS: 0.0087, MAE: 0.0087, RMSE: 0.0124, R2: 0.9975), Valid: (LOSS: 0.0331, MAE: 0.0331, RMSE: 0.0473, R2: 0.9658), PNorm: 189.7465, GNorm: 0.4245
[170/299] timecost: 56.23, lr: 0.000061, Train: (LOSS: 0.0080, MAE: 0.0080, RMSE: 0.0114, R2: 0.9979), Valid: (LOSS: 0.0320, MAE: 0.0320, RMSE: 0.0456, R2: 0.9685), PNorm: 189.7534, GNorm: 0.3465
[171/299] timecost: 56.27, lr: 0.000061, Train: (LOSS: 0.0082, MAE: 0.0082, RMSE: 0.0118, R2: 0.9978), Valid: (LOSS: 0.0325, MAE: 0.0325, RMSE: 0.0466, R2: 0.9664), PNorm: 189.7601, GNorm: 0.4018
[172/299] timecost: 56.17, lr: 0.000061, Train: (LOSS: 0.0075, MAE: 0.0075, RMSE: 0.0109, R2: 0.9980), Valid: (LOSS: 0.0318, MAE: 0.0318, RMSE: 0.0461, R2: 0.9668), PNorm: 189.7644, GNorm: 0.3642
[173/299] timecost: 56.75, lr: 0.000061, Train: (LOSS: 0.0078, MAE: 0.0078, RMSE: 0.0113, R2: 0.9979), Valid: (LOSS: 0.0331, MAE: 0.0331, RMSE: 0.0474, R2: 0.9653), PNorm: 189.7713, GNorm: 0.4018
[174/299] timecost: 55.80, lr: 0.000061, Train: (LOSS: 0.0081, MAE: 0.0081, RMSE: 0.0116, R2: 0.9978), Valid: (LOSS: 0.0320, MAE: 0.0320, RMSE: 0.0453, R2: 0.9684), PNorm: 189.7780, GNorm: 0.3931
[175/299] timecost: 55.85, lr: 0.000061, Train: (LOSS: 0.0077, MAE: 0.0077, RMSE: 0.0109, R2: 0.9981), Valid: (LOSS: 0.0322, MAE: 0.0322, RMSE: 0.0463, R2: 0.9667), PNorm: 189.7847, GNorm: 0.5000
[176/299] timecost: 55.83, lr: 0.000061, Train: (LOSS: 0.0076, MAE: 0.0076, RMSE: 0.0111, R2: 0.9980), Valid: (LOSS: 0.0326, MAE: 0.0326, RMSE: 0.0465, R2: 0.9664), PNorm: 189.7910, GNorm: 0.3514
[177/299] timecost: 55.76, lr: 0.000061, Train: (LOSS: 0.0078, MAE: 0.0078, RMSE: 0.0114, R2: 0.9979), Valid: (LOSS: 0.0312, MAE: 0.0312, RMSE: 0.0456, R2: 0.9678), PNorm: 189.7971, GNorm: 0.4206
[178/299] timecost: 56.06, lr: 0.000061, Train: (LOSS: 0.0076, MAE: 0.0076, RMSE: 0.0110, R2: 0.9981), Valid: (LOSS: 0.0332, MAE: 0.0332, RMSE: 0.0478, R2: 0.9648), PNorm: 189.8041, GNorm: 0.4453
[179/299] timecost: 56.28, lr: 0.000061, Train: (LOSS: 0.0085, MAE: 0.0085, RMSE: 0.0121, R2: 0.9977), Valid: (LOSS: 0.0317, MAE: 0.0317, RMSE: 0.0457, R2: 0.9674), PNorm: 189.8130, GNorm: 0.3793
[180/299] timecost: 55.81, lr: 0.000061, Train: (LOSS: 0.0081, MAE: 0.0081, RMSE: 0.0116, R2: 0.9979), Valid: (LOSS: 0.0328, MAE: 0.0328, RMSE: 0.0474, R2: 0.9660), PNorm: 189.8191, GNorm: 0.5000
[181/299] timecost: 56.38, lr: 0.000061, Train: (LOSS: 0.0080, MAE: 0.0080, RMSE: 0.0115, R2: 0.9977), Valid: (LOSS: 0.0319, MAE: 0.0319, RMSE: 0.0451, R2: 0.9688), PNorm: 189.8262, GNorm: 0.5000
[182/299] timecost: 56.33, lr: 0.000061, Train: (LOSS: 0.0080, MAE: 0.0080, RMSE: 0.0112, R2: 0.9979), Valid: (LOSS: 0.0330, MAE: 0.0330, RMSE: 0.0470, R2: 0.9651), PNorm: 189.8323, GNorm: 0.3973
[183/299] timecost: 55.90, lr: 0.000061, Train: (LOSS: 0.0076, MAE: 0.0076, RMSE: 0.0110, R2: 0.9981), Valid: (LOSS: 0.0324, MAE: 0.0324, RMSE: 0.0461, R2: 0.9670), PNorm: 189.8402, GNorm: 0.4547
[184/299] timecost: 56.81, lr: 0.000061, Train: (LOSS: 0.0077, MAE: 0.0077, RMSE: 0.0111, R2: 0.9980), Valid: (LOSS: 0.0313, MAE: 0.0313, RMSE: 0.0439, R2: 0.9706), PNorm: 189.8462, GNorm: 0.4735
[185/299] timecost: 56.05, lr: 0.000061, Train: (LOSS: 0.0080, MAE: 0.0080, RMSE: 0.0114, R2: 0.9980), Valid: (LOSS: 0.0311, MAE: 0.0311, RMSE: 0.0442, R2: 0.9705), PNorm: 189.8527, GNorm: 0.4024
[186/299] timecost: 56.05, lr: 0.000061, Train: (LOSS: 0.0077, MAE: 0.0077, RMSE: 0.0114, R2: 0.9979), Valid: (LOSS: 0.0326, MAE: 0.0326, RMSE: 0.0472, R2: 0.9665), PNorm: 189.8588, GNorm: 0.3524
[187/299] timecost: 56.27, lr: 0.000061, Train: (LOSS: 0.0078, MAE: 0.0078, RMSE: 0.0115, R2: 0.9977), Valid: (LOSS: 0.0324, MAE: 0.0324, RMSE: 0.0470, R2: 0.9659), PNorm: 189.8669, GNorm: 0.3700
[188/299] timecost: 56.00, lr: 0.000061, Train: (LOSS: 0.0075, MAE: 0.0075, RMSE: 0.0111, R2: 0.9980), Valid: (LOSS: 0.0322, MAE: 0.0322, RMSE: 0.0476, R2: 0.9650), PNorm: 189.8721, GNorm: 0.5000
[189/299] timecost: 56.27, lr: 0.000061, Train: (LOSS: 0.0079, MAE: 0.0079, RMSE: 0.0113, R2: 0.9980), Valid: (LOSS: 0.0325, MAE: 0.0325, RMSE: 0.0470, R2: 0.9659), PNorm: 189.8803, GNorm: 0.3886
[190/299] timecost: 56.37, lr: 0.000061, Train: (LOSS: 0.0080, MAE: 0.0080, RMSE: 0.0114, R2: 0.9979), Valid: (LOSS: 0.0323, MAE: 0.0323, RMSE: 0.0465, R2: 0.9655), PNorm: 189.8863, GNorm: 0.4794
[191/299] timecost: 56.46, lr: 0.000061, Train: (LOSS: 0.0078, MAE: 0.0078, RMSE: 0.0111, R2: 0.9980), Valid: (LOSS: 0.0331, MAE: 0.0331, RMSE: 0.0488, R2: 0.9626), PNorm: 189.8950, GNorm: 0.3661
[192/299] timecost: 56.34, lr: 0.000061, Train: (LOSS: 0.0075, MAE: 0.0075, RMSE: 0.0109, R2: 0.9981), Valid: (LOSS: 0.0323, MAE: 0.0323, RMSE: 0.0462, R2: 0.9669), PNorm: 189.9017, GNorm: 0.4139
Epoch 00194: reducing learning rate of group 0 to 5.2201e-05.
[193/299] timecost: 56.87, lr: 0.000052, Train: (LOSS: 0.0076, MAE: 0.0076, RMSE: 0.0109, R2: 0.9981), Valid: (LOSS: 0.0331, MAE: 0.0331, RMSE: 0.0484, R2: 0.9623), PNorm: 189.9063, GNorm: 0.5000
[194/299] timecost: 56.90, lr: 0.000052, Train: (LOSS: 0.0072, MAE: 0.0072, RMSE: 0.0106, R2: 0.9981), Valid: (LOSS: 0.0335, MAE: 0.0335, RMSE: 0.0480, R2: 0.9645), PNorm: 189.9141, GNorm: 0.3772
[195/299] timecost: 56.84, lr: 0.000052, Train: (LOSS: 0.0067, MAE: 0.0067, RMSE: 0.0098, R2: 0.9985), Valid: (LOSS: 0.0317, MAE: 0.0317, RMSE: 0.0463, R2: 0.9653), PNorm: 189.9186, GNorm: 0.4125
[196/299] timecost: 56.98, lr: 0.000052, Train: (LOSS: 0.0064, MAE: 0.0064, RMSE: 0.0093, R2: 0.9985), Valid: (LOSS: 0.0325, MAE: 0.0325, RMSE: 0.0464, R2: 0.9663), PNorm: 189.9243, GNorm: 0.4310
[197/299] timecost: 56.39, lr: 0.000052, Train: (LOSS: 0.0065, MAE: 0.0065, RMSE: 0.0094, R2: 0.9985), Valid: (LOSS: 0.0320, MAE: 0.0320, RMSE: 0.0464, R2: 0.9663), PNorm: 189.9296, GNorm: 0.3387
[198/299] timecost: 56.07, lr: 0.000052, Train: (LOSS: 0.0068, MAE: 0.0068, RMSE: 0.0098, R2: 0.9984), Valid: (LOSS: 0.0336, MAE: 0.0336, RMSE: 0.0485, R2: 0.9624), PNorm: 189.9312, GNorm: 0.3201
[199/299] timecost: 56.33, lr: 0.000052, Train: (LOSS: 0.0067, MAE: 0.0067, RMSE: 0.0097, R2: 0.9984), Valid: (LOSS: 0.0310, MAE: 0.0310, RMSE: 0.0445, R2: 0.9691), PNorm: 189.9386, GNorm: 0.5000
[200/299] timecost: 56.18, lr: 0.000052, Train: (LOSS: 0.0064, MAE: 0.0064, RMSE: 0.0093, R2: 0.9986), Valid: (LOSS: 0.0313, MAE: 0.0313, RMSE: 0.0439, R2: 0.9705), PNorm: 189.9444, GNorm: 0.4116
[201/299] timecost: 56.21, lr: 0.000052, Train: (LOSS: 0.0066, MAE: 0.0066, RMSE: 0.0095, R2: 0.9985), Valid: (LOSS: 0.0325, MAE: 0.0325, RMSE: 0.0455, R2: 0.9683), PNorm: 189.9486, GNorm: 0.4707
[202/299] timecost: 56.18, lr: 0.000052, Train: (LOSS: 0.0064, MAE: 0.0064, RMSE: 0.0093, R2: 0.9986), Valid: (LOSS: 0.0317, MAE: 0.0317, RMSE: 0.0459, R2: 0.9672), PNorm: 189.9532, GNorm: 0.3385
[203/299] timecost: 55.90, lr: 0.000052, Train: (LOSS: 0.0067, MAE: 0.0067, RMSE: 0.0098, R2: 0.9985), Valid: (LOSS: 0.0317, MAE: 0.0317, RMSE: 0.0458, R2: 0.9673), PNorm: 189.9582, GNorm: 0.5000
[204/299] timecost: 55.76, lr: 0.000052, Train: (LOSS: 0.0064, MAE: 0.0064, RMSE: 0.0093, R2: 0.9985), Valid: (LOSS: 0.0324, MAE: 0.0324, RMSE: 0.0463, R2: 0.9668), PNorm: 189.9647, GNorm: 0.3267
[205/299] timecost: 56.04, lr: 0.000052, Train: (LOSS: 0.0062, MAE: 0.0062, RMSE: 0.0091, R2: 0.9985), Valid: (LOSS: 0.0327, MAE: 0.0327, RMSE: 0.0475, R2: 0.9640), PNorm: 189.9710, GNorm: 0.3786
[206/299] timecost: 55.79, lr: 0.000052, Train: (LOSS: 0.0061, MAE: 0.0061, RMSE: 0.0089, R2: 0.9987), Valid: (LOSS: 0.0321, MAE: 0.0321, RMSE: 0.0453, R2: 0.9676), PNorm: 189.9750, GNorm: 0.3764
[207/299] timecost: 55.86, lr: 0.000052, Train: (LOSS: 0.0063, MAE: 0.0063, RMSE: 0.0091, R2: 0.9986), Valid: (LOSS: 0.0322, MAE: 0.0322, RMSE: 0.0464, R2: 0.9661), PNorm: 189.9809, GNorm: 0.4612
[208/299] timecost: 55.67, lr: 0.000052, Train: (LOSS: 0.0064, MAE: 0.0064, RMSE: 0.0093, R2: 0.9986), Valid: (LOSS: 0.0332, MAE: 0.0332, RMSE: 0.0473, R2: 0.9654), PNorm: 189.9869, GNorm: 0.3459
Epoch 00210: reducing learning rate of group 0 to 4.4371e-05.
[209/299] timecost: 55.99, lr: 0.000044, Train: (LOSS: 0.0065, MAE: 0.0065, RMSE: 0.0094, R2: 0.9985), Valid: (LOSS: 0.0327, MAE: 0.0327, RMSE: 0.0466, R2: 0.9664), PNorm: 189.9905, GNorm: 0.3450
[210/299] timecost: 56.13, lr: 0.000044, Train: (LOSS: 0.0060, MAE: 0.0060, RMSE: 0.0087, R2: 0.9987), Valid: (LOSS: 0.0314, MAE: 0.0314, RMSE: 0.0449, R2: 0.9690), PNorm: 189.9945, GNorm: 0.3730
[211/299] timecost: 56.56, lr: 0.000044, Train: (LOSS: 0.0056, MAE: 0.0056, RMSE: 0.0081, R2: 0.9989), Valid: (LOSS: 0.0334, MAE: 0.0334, RMSE: 0.0473, R2: 0.9648), PNorm: 189.9981, GNorm: 0.5000
[212/299] timecost: 56.01, lr: 0.000044, Train: (LOSS: 0.0054, MAE: 0.0054, RMSE: 0.0080, R2: 0.9988), Valid: (LOSS: 0.0326, MAE: 0.0326, RMSE: 0.0467, R2: 0.9664), PNorm: 190.0006, GNorm: 0.3915
[213/299] timecost: 55.79, lr: 0.000044, Train: (LOSS: 0.0054, MAE: 0.0054, RMSE: 0.0078, R2: 0.9989), Valid: (LOSS: 0.0317, MAE: 0.0317, RMSE: 0.0448, R2: 0.9685), PNorm: 190.0042, GNorm: 0.3474
[214/299] timecost: 55.87, lr: 0.000044, Train: (LOSS: 0.0052, MAE: 0.0052, RMSE: 0.0078, R2: 0.9989), Valid: (LOSS: 0.0319, MAE: 0.0319, RMSE: 0.0448, R2: 0.9692), PNorm: 190.0075, GNorm: 0.2950
[215/299] timecost: 56.11, lr: 0.000044, Train: (LOSS: 0.0055, MAE: 0.0055, RMSE: 0.0081, R2: 0.9988), Valid: (LOSS: 0.0324, MAE: 0.0324, RMSE: 0.0467, R2: 0.9659), PNorm: 190.0114, GNorm: 0.3524
[216/299] timecost: 56.72, lr: 0.000044, Train: (LOSS: 0.0051, MAE: 0.0051, RMSE: 0.0076, R2: 0.9990), Valid: (LOSS: 0.0317, MAE: 0.0317, RMSE: 0.0455, R2: 0.9677), PNorm: 190.0155, GNorm: 0.3109
[217/299] timecost: 56.75, lr: 0.000044, Train: (LOSS: 0.0052, MAE: 0.0052, RMSE: 0.0076, R2: 0.9990), Valid: (LOSS: 0.0322, MAE: 0.0322, RMSE: 0.0461, R2: 0.9669), PNorm: 190.0195, GNorm: 0.4142
[218/299] timecost: 56.86, lr: 0.000044, Train: (LOSS: 0.0053, MAE: 0.0053, RMSE: 0.0077, R2: 0.9990), Valid: (LOSS: 0.0318, MAE: 0.0318, RMSE: 0.0456, R2: 0.9679), PNorm: 190.0239, GNorm: 0.4074
[219/299] timecost: 57.03, lr: 0.000044, Train: (LOSS: 0.0051, MAE: 0.0051, RMSE: 0.0077, R2: 0.9990), Valid: (LOSS: 0.0323, MAE: 0.0323, RMSE: 0.0459, R2: 0.9676), PNorm: 190.0263, GNorm: 0.3985
[220/299] timecost: 57.03, lr: 0.000044, Train: (LOSS: 0.0055, MAE: 0.0055, RMSE: 0.0079, R2: 0.9989), Valid: (LOSS: 0.0322, MAE: 0.0322, RMSE: 0.0460, R2: 0.9675), PNorm: 190.0291, GNorm: 0.5000
[221/299] timecost: 56.63, lr: 0.000044, Train: (LOSS: 0.0054, MAE: 0.0054, RMSE: 0.0080, R2: 0.9989), Valid: (LOSS: 0.0326, MAE: 0.0326, RMSE: 0.0468, R2: 0.9654), PNorm: 190.0342, GNorm: 0.4956
[222/299] timecost: 56.04, lr: 0.000044, Train: (LOSS: 0.0052, MAE: 0.0052, RMSE: 0.0075, R2: 0.9990), Valid: (LOSS: 0.0331, MAE: 0.0331, RMSE: 0.0475, R2: 0.9643), PNorm: 190.0381, GNorm: 0.4903
[223/299] timecost: 56.00, lr: 0.000044, Train: (LOSS: 0.0054, MAE: 0.0054, RMSE: 0.0079, R2: 0.9990), Valid: (LOSS: 0.0328, MAE: 0.0328, RMSE: 0.0467, R2: 0.9662), PNorm: 190.0417, GNorm: 0.4739
[224/299] timecost: 56.17, lr: 0.000044, Train: (LOSS: 0.0053, MAE: 0.0053, RMSE: 0.0078, R2: 0.9990), Valid: (LOSS: 0.0324, MAE: 0.0324, RMSE: 0.0461, R2: 0.9663), PNorm: 190.0456, GNorm: 0.4123
Epoch 00226: reducing learning rate of group 0 to 3.7715e-05.
[225/299] timecost: 56.55, lr: 0.000038, Train: (LOSS: 0.0052, MAE: 0.0052, RMSE: 0.0077, R2: 0.9989), Valid: (LOSS: 0.0328, MAE: 0.0328, RMSE: 0.0471, R2: 0.9654), PNorm: 190.0506, GNorm: 0.3743
[226/299] timecost: 57.00, lr: 0.000038, Train: (LOSS: 0.0050, MAE: 0.0050, RMSE: 0.0074, R2: 0.9990), Valid: (LOSS: 0.0313, MAE: 0.0313, RMSE: 0.0456, R2: 0.9677), PNorm: 190.0534, GNorm: 0.4707
[227/299] timecost: 56.45, lr: 0.000038, Train: (LOSS: 0.0044, MAE: 0.0044, RMSE: 0.0066, R2: 0.9992), Valid: (LOSS: 0.0318, MAE: 0.0318, RMSE: 0.0459, R2: 0.9669), PNorm: 190.0565, GNorm: 0.3471
[228/299] timecost: 56.20, lr: 0.000038, Train: (LOSS: 0.0045, MAE: 0.0045, RMSE: 0.0067, R2: 0.9992), Valid: (LOSS: 0.0315, MAE: 0.0315, RMSE: 0.0460, R2: 0.9666), PNorm: 190.0581, GNorm: 0.4634
[229/299] timecost: 56.11, lr: 0.000038, Train: (LOSS: 0.0048, MAE: 0.0048, RMSE: 0.0072, R2: 0.9991), Valid: (LOSS: 0.0318, MAE: 0.0318, RMSE: 0.0456, R2: 0.9675), PNorm: 190.0628, GNorm: 0.3846
[230/299] timecost: 57.13, lr: 0.000038, Train: (LOSS: 0.0044, MAE: 0.0044, RMSE: 0.0066, R2: 0.9991), Valid: (LOSS: 0.0319, MAE: 0.0319, RMSE: 0.0455, R2: 0.9675), PNorm: 190.0649, GNorm: 0.4102
[231/299] timecost: 57.02, lr: 0.000038, Train: (LOSS: 0.0046, MAE: 0.0046, RMSE: 0.0068, R2: 0.9992), Valid: (LOSS: 0.0318, MAE: 0.0318, RMSE: 0.0453, R2: 0.9681), PNorm: 190.0668, GNorm: 0.5000
[232/299] timecost: 57.17, lr: 0.000038, Train: (LOSS: 0.0045, MAE: 0.0045, RMSE: 0.0068, R2: 0.9992), Valid: (LOSS: 0.0322, MAE: 0.0322, RMSE: 0.0459, R2: 0.9673), PNorm: 190.0688, GNorm: 0.3287
[233/299] timecost: 56.48, lr: 0.000038, Train: (LOSS: 0.0045, MAE: 0.0045, RMSE: 0.0068, R2: 0.9991), Valid: (LOSS: 0.0320, MAE: 0.0320, RMSE: 0.0455, R2: 0.9681), PNorm: 190.0725, GNorm: 0.4751
[234/299] timecost: 57.12, lr: 0.000038, Train: (LOSS: 0.0046, MAE: 0.0046, RMSE: 0.0068, R2: 0.9992), Valid: (LOSS: 0.0321, MAE: 0.0321, RMSE: 0.0465, R2: 0.9662), PNorm: 190.0749, GNorm: 0.5000
[235/299] timecost: 56.56, lr: 0.000038, Train: (LOSS: 0.0046, MAE: 0.0046, RMSE: 0.0070, R2: 0.9992), Valid: (LOSS: 0.0316, MAE: 0.0316, RMSE: 0.0458, R2: 0.9669), PNorm: 190.0783, GNorm: 0.4905
[236/299] timecost: 57.12, lr: 0.000038, Train: (LOSS: 0.0046, MAE: 0.0046, RMSE: 0.0069, R2: 0.9992), Valid: (LOSS: 0.0323, MAE: 0.0323, RMSE: 0.0460, R2: 0.9666), PNorm: 190.0807, GNorm: 0.5000
[237/299] timecost: 57.04, lr: 0.000038, Train: (LOSS: 0.0047, MAE: 0.0047, RMSE: 0.0069, R2: 0.9992), Valid: (LOSS: 0.0318, MAE: 0.0318, RMSE: 0.0463, R2: 0.9663), PNorm: 190.0834, GNorm: 0.3832
[238/299] timecost: 56.63, lr: 0.000038, Train: (LOSS: 0.0046, MAE: 0.0046, RMSE: 0.0068, R2: 0.9992), Valid: (LOSS: 0.0319, MAE: 0.0319, RMSE: 0.0458, R2: 0.9675), PNorm: 190.0870, GNorm: 0.5000
[239/299] timecost: 56.56, lr: 0.000038, Train: (LOSS: 0.0045, MAE: 0.0045, RMSE: 0.0067, R2: 0.9992), Valid: (LOSS: 0.0321, MAE: 0.0321, RMSE: 0.0460, R2: 0.9671), PNorm: 190.0887, GNorm: 0.4113
[240/299] timecost: 56.96, lr: 0.000038, Train: (LOSS: 0.0044, MAE: 0.0044, RMSE: 0.0067, R2: 0.9992), Valid: (LOSS: 0.0331, MAE: 0.0331, RMSE: 0.0473, R2: 0.9652), PNorm: 190.0924, GNorm: 0.3278
Epoch 00242: reducing learning rate of group 0 to 3.2058e-05.
[241/299] timecost: 56.54, lr: 0.000032, Train: (LOSS: 0.0043, MAE: 0.0043, RMSE: 0.0064, R2: 0.9993), Valid: (LOSS: 0.0317, MAE: 0.0317, RMSE: 0.0451, R2: 0.9684), PNorm: 190.0951, GNorm: 0.3619
[242/299] timecost: 56.12, lr: 0.000032, Train: (LOSS: 0.0041, MAE: 0.0041, RMSE: 0.0062, R2: 0.9993), Valid: (LOSS: 0.0314, MAE: 0.0314, RMSE: 0.0448, R2: 0.9691), PNorm: 190.0977, GNorm: 0.4466
[243/299] timecost: 56.34, lr: 0.000032, Train: (LOSS: 0.0042, MAE: 0.0042, RMSE: 0.0065, R2: 0.9993), Valid: (LOSS: 0.0319, MAE: 0.0319, RMSE: 0.0452, R2: 0.9686), PNorm: 190.0983, GNorm: 0.3644
[244/299] timecost: 56.31, lr: 0.000032, Train: (LOSS: 0.0040, MAE: 0.0040, RMSE: 0.0061, R2: 0.9993), Valid: (LOSS: 0.0315, MAE: 0.0315, RMSE: 0.0454, R2: 0.9680), PNorm: 190.1005, GNorm: 0.3591
[245/299] timecost: 56.33, lr: 0.000032, Train: (LOSS: 0.0038, MAE: 0.0038, RMSE: 0.0058, R2: 0.9993), Valid: (LOSS: 0.0310, MAE: 0.0310, RMSE: 0.0444, R2: 0.9695), PNorm: 190.1023, GNorm: 0.3593
[246/299] timecost: 56.36, lr: 0.000032, Train: (LOSS: 0.0038, MAE: 0.0038, RMSE: 0.0056, R2: 0.9994), Valid: (LOSS: 0.0324, MAE: 0.0324, RMSE: 0.0460, R2: 0.9674), PNorm: 190.1053, GNorm: 0.3390
[247/299] timecost: 56.60, lr: 0.000032, Train: (LOSS: 0.0040, MAE: 0.0040, RMSE: 0.0060, R2: 0.9993), Valid: (LOSS: 0.0323, MAE: 0.0323, RMSE: 0.0466, R2: 0.9657), PNorm: 190.1064, GNorm: 0.4621
[248/299] timecost: 57.64, lr: 0.000032, Train: (LOSS: 0.0040, MAE: 0.0040, RMSE: 0.0060, R2: 0.9993), Valid: (LOSS: 0.0310, MAE: 0.0310, RMSE: 0.0442, R2: 0.9697), PNorm: 190.1097, GNorm: 0.4415
[249/299] timecost: 56.25, lr: 0.000032, Train: (LOSS: 0.0039, MAE: 0.0039, RMSE: 0.0061, R2: 0.9993), Valid: (LOSS: 0.0318, MAE: 0.0318, RMSE: 0.0452, R2: 0.9681), PNorm: 190.1111, GNorm: 0.3687
[250/299] timecost: 56.14, lr: 0.000032, Train: (LOSS: 0.0039, MAE: 0.0039, RMSE: 0.0060, R2: 0.9993), Valid: (LOSS: 0.0322, MAE: 0.0322, RMSE: 0.0463, R2: 0.9665), PNorm: 190.1135, GNorm: 0.4253
[251/299] timecost: 57.64, lr: 0.000032, Train: (LOSS: 0.0040, MAE: 0.0040, RMSE: 0.0061, R2: 0.9993), Valid: (LOSS: 0.0332, MAE: 0.0332, RMSE: 0.0473, R2: 0.9653), PNorm: 190.1155, GNorm: 0.5000
[252/299] timecost: 57.79, lr: 0.000032, Train: (LOSS: 0.0042, MAE: 0.0042, RMSE: 0.0063, R2: 0.9993), Valid: (LOSS: 0.0322, MAE: 0.0322, RMSE: 0.0459, R2: 0.9674), PNorm: 190.1170, GNorm: 0.5000
[253/299] timecost: 57.04, lr: 0.000032, Train: (LOSS: 0.0039, MAE: 0.0039, RMSE: 0.0059, R2: 0.9993), Valid: (LOSS: 0.0333, MAE: 0.0333, RMSE: 0.0481, R2: 0.9637), PNorm: 190.1208, GNorm: 0.5000
[254/299] timecost: 57.34, lr: 0.000032, Train: (LOSS: 0.0039, MAE: 0.0039, RMSE: 0.0059, R2: 0.9994), Valid: (LOSS: 0.0319, MAE: 0.0319, RMSE: 0.0453, R2: 0.9679), PNorm: 190.1228, GNorm: 0.4773
[255/299] timecost: 57.65, lr: 0.000032, Train: (LOSS: 0.0040, MAE: 0.0040, RMSE: 0.0060, R2: 0.9993), Valid: (LOSS: 0.0327, MAE: 0.0327, RMSE: 0.0465, R2: 0.9661), PNorm: 190.1241, GNorm: 0.3951
[256/299] timecost: 57.12, lr: 0.000032, Train: (LOSS: 0.0040, MAE: 0.0040, RMSE: 0.0061, R2: 0.9993), Valid: (LOSS: 0.0312, MAE: 0.0312, RMSE: 0.0448, R2: 0.9683), PNorm: 190.1268, GNorm: 0.3726
Epoch 00258: reducing learning rate of group 0 to 2.7249e-05.
[257/299] timecost: 55.72, lr: 0.000027, Train: (LOSS: 0.0038, MAE: 0.0038, RMSE: 0.0057, R2: 0.9994), Valid: (LOSS: 0.0319, MAE: 0.0319, RMSE: 0.0460, R2: 0.9664), PNorm: 190.1294, GNorm: 0.4547
[258/299] timecost: 56.34, lr: 0.000027, Train: (LOSS: 0.0035, MAE: 0.0035, RMSE: 0.0054, R2: 0.9994), Valid: (LOSS: 0.0324, MAE: 0.0324, RMSE: 0.0464, R2: 0.9663), PNorm: 190.1298, GNorm: 0.4159
[259/299] timecost: 55.68, lr: 0.000027, Train: (LOSS: 0.0034, MAE: 0.0034, RMSE: 0.0052, R2: 0.9995), Valid: (LOSS: 0.0320, MAE: 0.0320, RMSE: 0.0452, R2: 0.9682), PNorm: 190.1309, GNorm: 0.3174
[260/299] timecost: 55.83, lr: 0.000027, Train: (LOSS: 0.0034, MAE: 0.0034, RMSE: 0.0052, R2: 0.9994), Valid: (LOSS: 0.0329, MAE: 0.0329, RMSE: 0.0468, R2: 0.9659), PNorm: 190.1331, GNorm: 0.3446
[261/299] timecost: 56.00, lr: 0.000027, Train: (LOSS: 0.0034, MAE: 0.0034, RMSE: 0.0052, R2: 0.9994), Valid: (LOSS: 0.0325, MAE: 0.0325, RMSE: 0.0464, R2: 0.9666), PNorm: 190.1342, GNorm: 0.5000
[262/299] timecost: 55.71, lr: 0.000027, Train: (LOSS: 0.0036, MAE: 0.0036, RMSE: 0.0055, R2: 0.9994), Valid: (LOSS: 0.0319, MAE: 0.0319, RMSE: 0.0457, R2: 0.9671), PNorm: 190.1362, GNorm: 0.3562
[263/299] timecost: 56.21, lr: 0.000027, Train: (LOSS: 0.0035, MAE: 0.0035, RMSE: 0.0053, R2: 0.9995), Valid: (LOSS: 0.0324, MAE: 0.0324, RMSE: 0.0460, R2: 0.9670), PNorm: 190.1385, GNorm: 0.3244
[264/299] timecost: 56.00, lr: 0.000027, Train: (LOSS: 0.0034, MAE: 0.0034, RMSE: 0.0053, R2: 0.9995), Valid: (LOSS: 0.0319, MAE: 0.0319, RMSE: 0.0454, R2: 0.9678), PNorm: 190.1395, GNorm: 0.4052
[265/299] timecost: 55.56, lr: 0.000027, Train: (LOSS: 0.0038, MAE: 0.0038, RMSE: 0.0056, R2: 0.9994), Valid: (LOSS: 0.0323, MAE: 0.0323, RMSE: 0.0457, R2: 0.9679), PNorm: 190.1417, GNorm: 0.5000
[266/299] timecost: 55.64, lr: 0.000027, Train: (LOSS: 0.0035, MAE: 0.0035, RMSE: 0.0053, R2: 0.9994), Valid: (LOSS: 0.0319, MAE: 0.0319, RMSE: 0.0454, R2: 0.9675), PNorm: 190.1429, GNorm: 0.4539
[267/299] timecost: 55.56, lr: 0.000027, Train: (LOSS: 0.0033, MAE: 0.0033, RMSE: 0.0051, R2: 0.9995), Valid: (LOSS: 0.0322, MAE: 0.0322, RMSE: 0.0457, R2: 0.9674), PNorm: 190.1445, GNorm: 0.4504
[268/299] timecost: 56.91, lr: 0.000027, Train: (LOSS: 0.0032, MAE: 0.0032, RMSE: 0.0049, R2: 0.9995), Valid: (LOSS: 0.0323, MAE: 0.0323, RMSE: 0.0456, R2: 0.9676), PNorm: 190.1457, GNorm: 0.4330
[269/299] timecost: 57.46, lr: 0.000027, Train: (LOSS: 0.0033, MAE: 0.0033, RMSE: 0.0051, R2: 0.9995), Valid: (LOSS: 0.0314, MAE: 0.0314, RMSE: 0.0446, R2: 0.9687), PNorm: 190.1465, GNorm: 0.3919
[270/299] timecost: 56.75, lr: 0.000027, Train: (LOSS: 0.0033, MAE: 0.0033, RMSE: 0.0051, R2: 0.9995), Valid: (LOSS: 0.0322, MAE: 0.0322, RMSE: 0.0461, R2: 0.9666), PNorm: 190.1490, GNorm: 0.4046
[271/299] timecost: 55.79, lr: 0.000027, Train: (LOSS: 0.0034, MAE: 0.0034, RMSE: 0.0053, R2: 0.9994), Valid: (LOSS: 0.0318, MAE: 0.0318, RMSE: 0.0453, R2: 0.9677), PNorm: 190.1507, GNorm: 0.3158
[272/299] timecost: 56.61, lr: 0.000027, Train: (LOSS: 0.0033, MAE: 0.0033, RMSE: 0.0052, R2: 0.9995), Valid: (LOSS: 0.0322, MAE: 0.0322, RMSE: 0.0462, R2: 0.9667), PNorm: 190.1519, GNorm: 0.4993
Epoch 00274: reducing learning rate of group 0 to 2.3162e-05.
[273/299] timecost: 57.56, lr: 0.000023, Train: (LOSS: 0.0033, MAE: 0.0033, RMSE: 0.0051, R2: 0.9995), Valid: (LOSS: 0.0320, MAE: 0.0320, RMSE: 0.0461, R2: 0.9668), PNorm: 190.1544, GNorm: 0.4095
[274/299] timecost: 58.12, lr: 0.000023, Train: (LOSS: 0.0031, MAE: 0.0031, RMSE: 0.0048, R2: 0.9995), Valid: (LOSS: 0.0323, MAE: 0.0323, RMSE: 0.0462, R2: 0.9666), PNorm: 190.1546, GNorm: 0.4777
[275/299] timecost: 57.21, lr: 0.000023, Train: (LOSS: 0.0030, MAE: 0.0030, RMSE: 0.0047, R2: 0.9995), Valid: (LOSS: 0.0322, MAE: 0.0322, RMSE: 0.0460, R2: 0.9668), PNorm: 190.1562, GNorm: 0.3157
[276/299] timecost: 56.16, lr: 0.000023, Train: (LOSS: 0.0028, MAE: 0.0028, RMSE: 0.0045, R2: 0.9996), Valid: (LOSS: 0.0326, MAE: 0.0326, RMSE: 0.0466, R2: 0.9660), PNorm: 190.1573, GNorm: 0.3934
[277/299] timecost: 56.95, lr: 0.000023, Train: (LOSS: 0.0029, MAE: 0.0029, RMSE: 0.0046, R2: 0.9996), Valid: (LOSS: 0.0317, MAE: 0.0317, RMSE: 0.0454, R2: 0.9677), PNorm: 190.1586, GNorm: 0.5000
[278/299] timecost: 56.48, lr: 0.000023, Train: (LOSS: 0.0029, MAE: 0.0029, RMSE: 0.0046, R2: 0.9996), Valid: (LOSS: 0.0319, MAE: 0.0319, RMSE: 0.0455, R2: 0.9678), PNorm: 190.1597, GNorm: 0.3276
[279/299] timecost: 56.05, lr: 0.000023, Train: (LOSS: 0.0029, MAE: 0.0029, RMSE: 0.0045, R2: 0.9996), Valid: (LOSS: 0.0322, MAE: 0.0322, RMSE: 0.0459, R2: 0.9670), PNorm: 190.1613, GNorm: 0.5000
[280/299] timecost: 56.51, lr: 0.000023, Train: (LOSS: 0.0030, MAE: 0.0030, RMSE: 0.0047, R2: 0.9996), Valid: (LOSS: 0.0319, MAE: 0.0319, RMSE: 0.0457, R2: 0.9672), PNorm: 190.1622, GNorm: 0.5000
[281/299] timecost: 56.26, lr: 0.000023, Train: (LOSS: 0.0030, MAE: 0.0030, RMSE: 0.0047, R2: 0.9995), Valid: (LOSS: 0.0322, MAE: 0.0322, RMSE: 0.0461, R2: 0.9669), PNorm: 190.1634, GNorm: 0.3562
[282/299] timecost: 55.99, lr: 0.000023, Train: (LOSS: 0.0029, MAE: 0.0029, RMSE: 0.0046, R2: 0.9995), Valid: (LOSS: 0.0321, MAE: 0.0321, RMSE: 0.0458, R2: 0.9671), PNorm: 190.1644, GNorm: 0.3859
[283/299] timecost: 55.98, lr: 0.000023, Train: (LOSS: 0.0029, MAE: 0.0029, RMSE: 0.0046, R2: 0.9995), Valid: (LOSS: 0.0319, MAE: 0.0319, RMSE: 0.0455, R2: 0.9676), PNorm: 190.1658, GNorm: 0.4744
[284/299] timecost: 56.15, lr: 0.000023, Train: (LOSS: 0.0028, MAE: 0.0028, RMSE: 0.0045, R2: 0.9996), Valid: (LOSS: 0.0323, MAE: 0.0323, RMSE: 0.0464, R2: 0.9664), PNorm: 190.1667, GNorm: 0.3830
[285/299] timecost: 56.44, lr: 0.000023, Train: (LOSS: 0.0029, MAE: 0.0029, RMSE: 0.0045, R2: 0.9996), Valid: (LOSS: 0.0325, MAE: 0.0325, RMSE: 0.0462, R2: 0.9666), PNorm: 190.1683, GNorm: 0.5000
[286/299] timecost: 55.52, lr: 0.000023, Train: (LOSS: 0.0029, MAE: 0.0029, RMSE: 0.0045, R2: 0.9996), Valid: (LOSS: 0.0320, MAE: 0.0320, RMSE: 0.0456, R2: 0.9675), PNorm: 190.1694, GNorm: 0.3907
[287/299] timecost: 55.84, lr: 0.000023, Train: (LOSS: 0.0028, MAE: 0.0028, RMSE: 0.0045, R2: 0.9996), Valid: (LOSS: 0.0327, MAE: 0.0327, RMSE: 0.0466, R2: 0.9662), PNorm: 190.1699, GNorm: 0.5000
[288/299] timecost: 55.66, lr: 0.000023, Train: (LOSS: 0.0028, MAE: 0.0028, RMSE: 0.0044, R2: 0.9996), Valid: (LOSS: 0.0319, MAE: 0.0319, RMSE: 0.0453, R2: 0.9680), PNorm: 190.1714, GNorm: 0.4008
Epoch 00290: reducing learning rate of group 0 to 1.9687e-05.
[289/299] timecost: 55.91, lr: 0.000020, Train: (LOSS: 0.0028, MAE: 0.0028, RMSE: 0.0045, R2: 0.9996), Valid: (LOSS: 0.0323, MAE: 0.0323, RMSE: 0.0457, R2: 0.9674), PNorm: 190.1722, GNorm: 0.4780
[290/299] timecost: 56.13, lr: 0.000020, Train: (LOSS: 0.0026, MAE: 0.0026, RMSE: 0.0042, R2: 0.9996), Valid: (LOSS: 0.0326, MAE: 0.0326, RMSE: 0.0465, R2: 0.9662), PNorm: 190.1728, GNorm: 0.3586
[291/299] timecost: 56.09, lr: 0.000020, Train: (LOSS: 0.0027, MAE: 0.0027, RMSE: 0.0043, R2: 0.9996), Valid: (LOSS: 0.0326, MAE: 0.0326, RMSE: 0.0463, R2: 0.9667), PNorm: 190.1744, GNorm: 0.4789
[292/299] timecost: 57.45, lr: 0.000020, Train: (LOSS: 0.0026, MAE: 0.0026, RMSE: 0.0042, R2: 0.9996), Valid: (LOSS: 0.0323, MAE: 0.0323, RMSE: 0.0460, R2: 0.9670), PNorm: 190.1749, GNorm: 0.3572
[293/299] timecost: 57.60, lr: 0.000020, Train: (LOSS: 0.0026, MAE: 0.0026, RMSE: 0.0041, R2: 0.9996), Valid: (LOSS: 0.0324, MAE: 0.0324, RMSE: 0.0462, R2: 0.9667), PNorm: 190.1754, GNorm: 0.3709
[294/299] timecost: 57.30, lr: 0.000020, Train: (LOSS: 0.0025, MAE: 0.0025, RMSE: 0.0041, R2: 0.9997), Valid: (LOSS: 0.0324, MAE: 0.0324, RMSE: 0.0464, R2: 0.9665), PNorm: 190.1767, GNorm: 0.5000
[295/299] timecost: 57.42, lr: 0.000020, Train: (LOSS: 0.0026, MAE: 0.0026, RMSE: 0.0042, R2: 0.9996), Valid: (LOSS: 0.0321, MAE: 0.0321, RMSE: 0.0459, R2: 0.9672), PNorm: 190.1774, GNorm: 0.2733
[296/299] timecost: 57.36, lr: 0.000020, Train: (LOSS: 0.0024, MAE: 0.0024, RMSE: 0.0038, R2: 0.9997), Valid: (LOSS: 0.0322, MAE: 0.0322, RMSE: 0.0458, R2: 0.9672), PNorm: 190.1780, GNorm: 0.4022
[297/299] timecost: 57.89, lr: 0.000020, Train: (LOSS: 0.0026, MAE: 0.0026, RMSE: 0.0042, R2: 0.9996), Valid: (LOSS: 0.0323, MAE: 0.0323, RMSE: 0.0461, R2: 0.9669), PNorm: 190.1796, GNorm: 0.5000
[298/299] timecost: 56.13, lr: 0.000020, Train: (LOSS: 0.0025, MAE: 0.0025, RMSE: 0.0041, R2: 0.9996), Valid: (LOSS: 0.0324, MAE: 0.0324, RMSE: 0.0460, R2: 0.9670), PNorm: 190.1804, GNorm: 0.3426
[299/299] timecost: 56.08, lr: 0.000020, Train: (LOSS: 0.0024, MAE: 0.0024, RMSE: 0.0040, R2: 0.9997), Valid: (LOSS: 0.0318, MAE: 0.0318, RMSE: 0.0452, R2: 0.9682), PNorm: 190.1808, GNorm: 0.3823
==========Training End==========
==========Test Best Model==========
================Final Results=======================
mse: 0.0338 +- 0.0000:
rmse: 0.0530 +- 0.0000:
mae: 0.0338 +- 0.0000:
r2: 0.9577 +- 0.0000:
tensor([[0.0000, 0.0000],
        [0.1101, 0.1475],
        [0.1310, 0.1828],
        ...,
        [0.0000, 0.0000],
        [0.0000, 0.0000],
        [0.0000, 0.0000]], device='cuda:0')
