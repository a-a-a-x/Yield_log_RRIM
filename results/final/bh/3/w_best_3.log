cuda available with GPU: Tesla V100-PCIE-16GB
==========Load Seed==========
set_random_seed
0
==========Training Start==========
Training Graphs:  2491
Valid Graphs:  277
Test Graphs:  1187
============Loading pretrained weights to generate initialization============
============Creating new layers============
============Creating Model============
Training Graphs Batches:  78
Valid Graphs Batches:  9
Test Graphs Batches:  37
[0/299] timecost: 67.24, lr: 0.000030, Train: (LOSS: 0.2319, MAE: 0.2319, RMSE: 0.2710, R2: -0.0279), Valid: (LOSS: 0.2107, MAE: 0.2107, RMSE: 0.2620, R2: -0.0687), PNorm: 174.3683, GNorm: 1.6920
[1/299] timecost: 65.53, lr: 0.000030, Train: (LOSS: 0.2130, MAE: 0.2130, RMSE: 0.2554, R2: 0.0771), Valid: (LOSS: 0.1914, MAE: 0.1914, RMSE: 0.2328, R2: 0.1553), PNorm: 173.7829, GNorm: 0.6540
[2/299] timecost: 65.80, lr: 0.000030, Train: (LOSS: 0.2011, MAE: 0.2011, RMSE: 0.2431, R2: 0.1785), Valid: (LOSS: 0.2028, MAE: 0.2028, RMSE: 0.2406, R2: 0.0966), PNorm: 173.3399, GNorm: 1.3646
[3/299] timecost: 66.30, lr: 0.000030, Train: (LOSS: 0.1900, MAE: 0.1900, RMSE: 0.2337, R2: 0.2241), Valid: (LOSS: 0.1749, MAE: 0.1749, RMSE: 0.2158, R2: 0.2737), PNorm: 172.9539, GNorm: 2.8231
[4/299] timecost: 65.44, lr: 0.000030, Train: (LOSS: 0.1638, MAE: 0.1638, RMSE: 0.2088, R2: 0.3772), Valid: (LOSS: 0.1675, MAE: 0.1675, RMSE: 0.2198, R2: 0.2461), PNorm: 172.6224, GNorm: 2.3754
[5/299] timecost: 65.81, lr: 0.000030, Train: (LOSS: 0.1446, MAE: 0.1446, RMSE: 0.1899, R2: 0.4670), Valid: (LOSS: 0.1264, MAE: 0.1264, RMSE: 0.1738, R2: 0.5249), PNorm: 172.3205, GNorm: 2.0321
[6/299] timecost: 66.11, lr: 0.000030, Train: (LOSS: 0.1459, MAE: 0.1459, RMSE: 0.1928, R2: 0.4666), Valid: (LOSS: 0.1206, MAE: 0.1206, RMSE: 0.1686, R2: 0.5517), PNorm: 172.0426, GNorm: 2.3988
[7/299] timecost: 65.15, lr: 0.000030, Train: (LOSS: 0.1297, MAE: 0.1297, RMSE: 0.1768, R2: 0.5528), Valid: (LOSS: 0.1138, MAE: 0.1138, RMSE: 0.1619, R2: 0.5882), PNorm: 171.7805, GNorm: 1.1773
[8/299] timecost: 65.52, lr: 0.000030, Train: (LOSS: 0.1238, MAE: 0.1238, RMSE: 0.1698, R2: 0.5819), Valid: (LOSS: 0.1131, MAE: 0.1131, RMSE: 0.1645, R2: 0.5719), PNorm: 171.5266, GNorm: 2.2433
[9/299] timecost: 65.31, lr: 0.000030, Train: (LOSS: 0.1323, MAE: 0.1323, RMSE: 0.1806, R2: 0.5236), Valid: (LOSS: 0.1157, MAE: 0.1157, RMSE: 0.1655, R2: 0.5699), PNorm: 171.2860, GNorm: 3.7632
[10/299] timecost: 65.62, lr: 0.000030, Train: (LOSS: 0.1182, MAE: 0.1182, RMSE: 0.1649, R2: 0.5992), Valid: (LOSS: 0.1141, MAE: 0.1141, RMSE: 0.1688, R2: 0.5512), PNorm: 171.0514, GNorm: 2.0485
[11/299] timecost: 65.26, lr: 0.000030, Train: (LOSS: 0.1167, MAE: 0.1167, RMSE: 0.1628, R2: 0.6115), Valid: (LOSS: 0.1063, MAE: 0.1063, RMSE: 0.1519, R2: 0.6375), PNorm: 170.8221, GNorm: 2.9283
[12/299] timecost: 64.83, lr: 0.000030, Train: (LOSS: 0.1144, MAE: 0.1144, RMSE: 0.1627, R2: 0.6138), Valid: (LOSS: 0.1079, MAE: 0.1079, RMSE: 0.1512, R2: 0.6411), PNorm: 170.5975, GNorm: 2.9343
[13/299] timecost: 64.55, lr: 0.000030, Train: (LOSS: 0.1124, MAE: 0.1124, RMSE: 0.1592, R2: 0.6325), Valid: (LOSS: 0.1101, MAE: 0.1101, RMSE: 0.1543, R2: 0.6265), PNorm: 170.3779, GNorm: 3.5420
[14/299] timecost: 65.17, lr: 0.000030, Train: (LOSS: 0.1060, MAE: 0.1060, RMSE: 0.1520, R2: 0.6627), Valid: (LOSS: 0.1020, MAE: 0.1020, RMSE: 0.1527, R2: 0.6352), PNorm: 170.1606, GNorm: 2.1860
[15/299] timecost: 65.09, lr: 0.000030, Train: (LOSS: 0.1053, MAE: 0.1053, RMSE: 0.1509, R2: 0.6689), Valid: (LOSS: 0.1073, MAE: 0.1073, RMSE: 0.1533, R2: 0.6318), PNorm: 169.9456, GNorm: 4.8480
[16/299] timecost: 65.04, lr: 0.000030, Train: (LOSS: 0.1042, MAE: 0.1042, RMSE: 0.1497, R2: 0.6658), Valid: (LOSS: 0.1150, MAE: 0.1150, RMSE: 0.1717, R2: 0.5280), PNorm: 169.7362, GNorm: 2.3242
[17/299] timecost: 64.59, lr: 0.000030, Train: (LOSS: 0.1083, MAE: 0.1083, RMSE: 0.1530, R2: 0.6550), Valid: (LOSS: 0.1094, MAE: 0.1094, RMSE: 0.1587, R2: 0.6012), PNorm: 169.5307, GNorm: 1.0403
[18/299] timecost: 64.28, lr: 0.000030, Train: (LOSS: 0.0973, MAE: 0.0973, RMSE: 0.1416, R2: 0.7071), Valid: (LOSS: 0.0862, MAE: 0.0862, RMSE: 0.1267, R2: 0.7514), PNorm: 169.3280, GNorm: 1.2465
[19/299] timecost: 64.17, lr: 0.000030, Train: (LOSS: 0.1008, MAE: 0.1008, RMSE: 0.1461, R2: 0.6860), Valid: (LOSS: 0.1010, MAE: 0.1010, RMSE: 0.1485, R2: 0.6466), PNorm: 169.1294, GNorm: 2.0531
[20/299] timecost: 64.48, lr: 0.000030, Train: (LOSS: 0.0939, MAE: 0.0939, RMSE: 0.1356, R2: 0.7342), Valid: (LOSS: 0.0997, MAE: 0.0997, RMSE: 0.1499, R2: 0.6355), PNorm: 168.9340, GNorm: 1.9333
[21/299] timecost: 64.10, lr: 0.000030, Train: (LOSS: 0.0922, MAE: 0.0922, RMSE: 0.1349, R2: 0.7299), Valid: (LOSS: 0.0934, MAE: 0.0934, RMSE: 0.1325, R2: 0.7241), PNorm: 168.7404, GNorm: 1.0023
[22/299] timecost: 64.15, lr: 0.000030, Train: (LOSS: 0.0884, MAE: 0.0884, RMSE: 0.1303, R2: 0.7483), Valid: (LOSS: 0.0869, MAE: 0.0869, RMSE: 0.1256, R2: 0.7524), PNorm: 168.5468, GNorm: 1.5895
[23/299] timecost: 68.43, lr: 0.000030, Train: (LOSS: 0.0873, MAE: 0.0873, RMSE: 0.1289, R2: 0.7538), Valid: (LOSS: 0.0924, MAE: 0.0924, RMSE: 0.1361, R2: 0.7121), PNorm: 168.3547, GNorm: 1.2028
[24/299] timecost: 71.41, lr: 0.000030, Train: (LOSS: 0.0864, MAE: 0.0864, RMSE: 0.1305, R2: 0.7404), Valid: (LOSS: 0.0875, MAE: 0.0875, RMSE: 0.1318, R2: 0.7233), PNorm: 168.1639, GNorm: 0.8322
[25/299] timecost: 71.93, lr: 0.000030, Train: (LOSS: 0.0831, MAE: 0.0831, RMSE: 0.1249, R2: 0.7664), Valid: (LOSS: 0.0817, MAE: 0.0817, RMSE: 0.1180, R2: 0.7801), PNorm: 167.9744, GNorm: 1.4852
[26/299] timecost: 71.76, lr: 0.000030, Train: (LOSS: 0.0809, MAE: 0.0809, RMSE: 0.1227, R2: 0.7754), Valid: (LOSS: 0.0787, MAE: 0.0787, RMSE: 0.1179, R2: 0.7830), PNorm: 167.7874, GNorm: 1.5235
[27/299] timecost: 72.24, lr: 0.000030, Train: (LOSS: 0.0799, MAE: 0.0799, RMSE: 0.1207, R2: 0.7817), Valid: (LOSS: 0.0779, MAE: 0.0779, RMSE: 0.1153, R2: 0.7921), PNorm: 167.6029, GNorm: 1.5564
[28/299] timecost: 72.51, lr: 0.000030, Train: (LOSS: 0.0793, MAE: 0.0793, RMSE: 0.1206, R2: 0.7883), Valid: (LOSS: 0.0829, MAE: 0.0829, RMSE: 0.1233, R2: 0.7543), PNorm: 167.4204, GNorm: 1.8570
[29/299] timecost: 71.99, lr: 0.000030, Train: (LOSS: 0.0755, MAE: 0.0755, RMSE: 0.1162, R2: 0.8011), Valid: (LOSS: 0.0895, MAE: 0.0895, RMSE: 0.1343, R2: 0.7118), PNorm: 167.2369, GNorm: 1.5883
[30/299] timecost: 70.37, lr: 0.000030, Train: (LOSS: 0.0723, MAE: 0.0723, RMSE: 0.1117, R2: 0.8153), Valid: (LOSS: 0.0776, MAE: 0.0776, RMSE: 0.1164, R2: 0.7819), PNorm: 167.0556, GNorm: 1.3185
[31/299] timecost: 71.12, lr: 0.000030, Train: (LOSS: 0.0720, MAE: 0.0720, RMSE: 0.1119, R2: 0.8158), Valid: (LOSS: 0.0764, MAE: 0.0764, RMSE: 0.1163, R2: 0.7849), PNorm: 166.8760, GNorm: 1.1313
[32/299] timecost: 72.00, lr: 0.000030, Train: (LOSS: 0.0686, MAE: 0.0686, RMSE: 0.1083, R2: 0.8222), Valid: (LOSS: 0.0780, MAE: 0.0780, RMSE: 0.1177, R2: 0.7812), PNorm: 166.6962, GNorm: 1.8529
[33/299] timecost: 71.62, lr: 0.000030, Train: (LOSS: 0.0674, MAE: 0.0674, RMSE: 0.1063, R2: 0.8304), Valid: (LOSS: 0.0758, MAE: 0.0758, RMSE: 0.1156, R2: 0.7844), PNorm: 166.5171, GNorm: 1.5286
[34/299] timecost: 71.54, lr: 0.000030, Train: (LOSS: 0.0709, MAE: 0.0709, RMSE: 0.1098, R2: 0.8235), Valid: (LOSS: 0.0728, MAE: 0.0728, RMSE: 0.1097, R2: 0.8078), PNorm: 166.3416, GNorm: 1.3907
[35/299] timecost: 71.36, lr: 0.000030, Train: (LOSS: 0.0728, MAE: 0.0728, RMSE: 0.1129, R2: 0.8046), Valid: (LOSS: 0.0832, MAE: 0.0832, RMSE: 0.1228, R2: 0.7554), PNorm: 166.1685, GNorm: 3.5602
[36/299] timecost: 71.70, lr: 0.000030, Train: (LOSS: 0.0673, MAE: 0.0673, RMSE: 0.1069, R2: 0.8299), Valid: (LOSS: 0.0717, MAE: 0.0717, RMSE: 0.1103, R2: 0.8068), PNorm: 165.9958, GNorm: 1.3392
[37/299] timecost: 71.30, lr: 0.000030, Train: (LOSS: 0.0637, MAE: 0.0637, RMSE: 0.1015, R2: 0.8442), Valid: (LOSS: 0.0724, MAE: 0.0724, RMSE: 0.1099, R2: 0.8068), PNorm: 165.8232, GNorm: 1.1480
[38/299] timecost: 71.07, lr: 0.000030, Train: (LOSS: 0.0636, MAE: 0.0636, RMSE: 0.0998, R2: 0.8458), Valid: (LOSS: 0.0661, MAE: 0.0661, RMSE: 0.0995, R2: 0.8409), PNorm: 165.6531, GNorm: 1.8060
[39/299] timecost: 71.52, lr: 0.000030, Train: (LOSS: 0.0608, MAE: 0.0608, RMSE: 0.0970, R2: 0.8587), Valid: (LOSS: 0.0706, MAE: 0.0706, RMSE: 0.1037, R2: 0.8256), PNorm: 165.4845, GNorm: 2.0906
[40/299] timecost: 71.49, lr: 0.000030, Train: (LOSS: 0.0612, MAE: 0.0612, RMSE: 0.0967, R2: 0.8565), Valid: (LOSS: 0.0714, MAE: 0.0714, RMSE: 0.1105, R2: 0.8015), PNorm: 165.3168, GNorm: 3.2079
[41/299] timecost: 71.43, lr: 0.000030, Train: (LOSS: 0.0589, MAE: 0.0589, RMSE: 0.0940, R2: 0.8626), Valid: (LOSS: 0.0743, MAE: 0.0743, RMSE: 0.1108, R2: 0.8058), PNorm: 165.1505, GNorm: 3.2911
[42/299] timecost: 71.34, lr: 0.000030, Train: (LOSS: 0.0576, MAE: 0.0576, RMSE: 0.0941, R2: 0.8620), Valid: (LOSS: 0.0659, MAE: 0.0659, RMSE: 0.1009, R2: 0.8344), PNorm: 164.9855, GNorm: 2.0068
[43/299] timecost: 70.80, lr: 0.000030, Train: (LOSS: 0.0547, MAE: 0.0547, RMSE: 0.0896, R2: 0.8740), Valid: (LOSS: 0.0845, MAE: 0.0845, RMSE: 0.1167, R2: 0.7791), PNorm: 164.8220, GNorm: 3.4078
[44/299] timecost: 71.31, lr: 0.000030, Train: (LOSS: 0.0608, MAE: 0.0608, RMSE: 0.0952, R2: 0.8563), Valid: (LOSS: 0.0592, MAE: 0.0592, RMSE: 0.0912, R2: 0.8645), PNorm: 164.6623, GNorm: 1.3482
[45/299] timecost: 71.00, lr: 0.000030, Train: (LOSS: 0.0543, MAE: 0.0543, RMSE: 0.0888, R2: 0.8758), Valid: (LOSS: 0.0588, MAE: 0.0588, RMSE: 0.0879, R2: 0.8730), PNorm: 164.5021, GNorm: 1.1484
[46/299] timecost: 71.28, lr: 0.000030, Train: (LOSS: 0.0514, MAE: 0.0514, RMSE: 0.0840, R2: 0.8876), Valid: (LOSS: 0.0579, MAE: 0.0579, RMSE: 0.0914, R2: 0.8641), PNorm: 164.3420, GNorm: 1.0173
[47/299] timecost: 71.38, lr: 0.000030, Train: (LOSS: 0.0496, MAE: 0.0496, RMSE: 0.0827, R2: 0.8923), Valid: (LOSS: 0.0562, MAE: 0.0562, RMSE: 0.0851, R2: 0.8832), PNorm: 164.1826, GNorm: 1.1254
[48/299] timecost: 71.61, lr: 0.000030, Train: (LOSS: 0.0488, MAE: 0.0488, RMSE: 0.0802, R2: 0.8970), Valid: (LOSS: 0.0638, MAE: 0.0638, RMSE: 0.0954, R2: 0.8512), PNorm: 164.0240, GNorm: 1.3391
[49/299] timecost: 71.40, lr: 0.000030, Train: (LOSS: 0.0475, MAE: 0.0475, RMSE: 0.0788, R2: 0.9029), Valid: (LOSS: 0.0573, MAE: 0.0573, RMSE: 0.0886, R2: 0.8730), PNorm: 163.8659, GNorm: 1.9973
[50/299] timecost: 71.59, lr: 0.000030, Train: (LOSS: 0.0472, MAE: 0.0472, RMSE: 0.0778, R2: 0.8997), Valid: (LOSS: 0.0530, MAE: 0.0530, RMSE: 0.0814, R2: 0.8954), PNorm: 163.7102, GNorm: 1.0901
[51/299] timecost: 71.39, lr: 0.000030, Train: (LOSS: 0.0482, MAE: 0.0482, RMSE: 0.0781, R2: 0.9023), Valid: (LOSS: 0.0579, MAE: 0.0579, RMSE: 0.0884, R2: 0.8739), PNorm: 163.5546, GNorm: 1.0169
[52/299] timecost: 71.31, lr: 0.000030, Train: (LOSS: 0.0462, MAE: 0.0462, RMSE: 0.0751, R2: 0.9099), Valid: (LOSS: 0.0518, MAE: 0.0518, RMSE: 0.0792, R2: 0.8994), PNorm: 163.3989, GNorm: 1.5729
[53/299] timecost: 70.91, lr: 0.000030, Train: (LOSS: 0.0453, MAE: 0.0453, RMSE: 0.0740, R2: 0.9116), Valid: (LOSS: 0.0591, MAE: 0.0591, RMSE: 0.0843, R2: 0.8834), PNorm: 163.2460, GNorm: 2.3401
[54/299] timecost: 71.36, lr: 0.000030, Train: (LOSS: 0.0445, MAE: 0.0445, RMSE: 0.0726, R2: 0.9157), Valid: (LOSS: 0.0537, MAE: 0.0537, RMSE: 0.0822, R2: 0.8919), PNorm: 163.0926, GNorm: 0.8245
[55/299] timecost: 71.40, lr: 0.000030, Train: (LOSS: 0.0439, MAE: 0.0439, RMSE: 0.0711, R2: 0.9216), Valid: (LOSS: 0.0528, MAE: 0.0528, RMSE: 0.0817, R2: 0.8919), PNorm: 162.9397, GNorm: 1.6646
[56/299] timecost: 71.00, lr: 0.000030, Train: (LOSS: 0.0425, MAE: 0.0425, RMSE: 0.0699, R2: 0.9222), Valid: (LOSS: 0.0541, MAE: 0.0541, RMSE: 0.0825, R2: 0.8897), PNorm: 162.7882, GNorm: 1.3427
[57/299] timecost: 71.44, lr: 0.000030, Train: (LOSS: 0.0421, MAE: 0.0421, RMSE: 0.0704, R2: 0.9187), Valid: (LOSS: 0.0505, MAE: 0.0505, RMSE: 0.0771, R2: 0.9006), PNorm: 162.6375, GNorm: 1.0013
[58/299] timecost: 71.47, lr: 0.000030, Train: (LOSS: 0.0425, MAE: 0.0425, RMSE: 0.0693, R2: 0.9258), Valid: (LOSS: 0.0494, MAE: 0.0494, RMSE: 0.0741, R2: 0.9117), PNorm: 162.4873, GNorm: 1.3734
[59/299] timecost: 70.59, lr: 0.000030, Train: (LOSS: 0.0402, MAE: 0.0402, RMSE: 0.0678, R2: 0.9274), Valid: (LOSS: 0.0531, MAE: 0.0531, RMSE: 0.0814, R2: 0.8928), PNorm: 162.3361, GNorm: 2.0078
[60/299] timecost: 71.40, lr: 0.000030, Train: (LOSS: 0.0394, MAE: 0.0394, RMSE: 0.0656, R2: 0.9326), Valid: (LOSS: 0.0515, MAE: 0.0515, RMSE: 0.0759, R2: 0.9064), PNorm: 162.1872, GNorm: 2.5151
[61/299] timecost: 71.73, lr: 0.000030, Train: (LOSS: 0.0390, MAE: 0.0390, RMSE: 0.0637, R2: 0.9337), Valid: (LOSS: 0.0528, MAE: 0.0528, RMSE: 0.0784, R2: 0.8997), PNorm: 162.0380, GNorm: 1.5147
[62/299] timecost: 70.95, lr: 0.000030, Train: (LOSS: 0.0400, MAE: 0.0400, RMSE: 0.0658, R2: 0.9271), Valid: (LOSS: 0.0498, MAE: 0.0498, RMSE: 0.0761, R2: 0.9087), PNorm: 161.8915, GNorm: 1.1979
[63/299] timecost: 71.11, lr: 0.000030, Train: (LOSS: 0.0389, MAE: 0.0389, RMSE: 0.0641, R2: 0.9285), Valid: (LOSS: 0.0501, MAE: 0.0501, RMSE: 0.0759, R2: 0.9061), PNorm: 161.7433, GNorm: 1.6209
[64/299] timecost: 70.48, lr: 0.000030, Train: (LOSS: 0.0375, MAE: 0.0375, RMSE: 0.0617, R2: 0.9385), Valid: (LOSS: 0.0496, MAE: 0.0496, RMSE: 0.0757, R2: 0.9058), PNorm: 161.5961, GNorm: 1.6060
[65/299] timecost: 71.06, lr: 0.000030, Train: (LOSS: 0.0390, MAE: 0.0390, RMSE: 0.0640, R2: 0.9317), Valid: (LOSS: 0.0488, MAE: 0.0488, RMSE: 0.0734, R2: 0.9118), PNorm: 161.4497, GNorm: 0.9528
[66/299] timecost: 70.84, lr: 0.000030, Train: (LOSS: 0.0369, MAE: 0.0369, RMSE: 0.0607, R2: 0.9410), Valid: (LOSS: 0.0616, MAE: 0.0616, RMSE: 0.0895, R2: 0.8699), PNorm: 161.3034, GNorm: 2.1648
[67/299] timecost: 71.02, lr: 0.000030, Train: (LOSS: 0.0374, MAE: 0.0374, RMSE: 0.0595, R2: 0.9416), Valid: (LOSS: 0.0487, MAE: 0.0487, RMSE: 0.0762, R2: 0.9061), PNorm: 161.1583, GNorm: 1.0637
[68/299] timecost: 71.30, lr: 0.000030, Train: (LOSS: 0.0367, MAE: 0.0367, RMSE: 0.0592, R2: 0.9432), Valid: (LOSS: 0.0494, MAE: 0.0494, RMSE: 0.0738, R2: 0.9115), PNorm: 161.0133, GNorm: 1.4416
[69/299] timecost: 71.38, lr: 0.000030, Train: (LOSS: 0.0360, MAE: 0.0360, RMSE: 0.0588, R2: 0.9432), Valid: (LOSS: 0.0425, MAE: 0.0425, RMSE: 0.0642, R2: 0.9328), PNorm: 160.8681, GNorm: 1.2887
[70/299] timecost: 71.13, lr: 0.000030, Train: (LOSS: 0.0354, MAE: 0.0354, RMSE: 0.0571, R2: 0.9457), Valid: (LOSS: 0.0491, MAE: 0.0491, RMSE: 0.0714, R2: 0.9171), PNorm: 160.7235, GNorm: 1.6951
[71/299] timecost: 71.36, lr: 0.000030, Train: (LOSS: 0.0357, MAE: 0.0357, RMSE: 0.0580, R2: 0.9444), Valid: (LOSS: 0.0464, MAE: 0.0464, RMSE: 0.0705, R2: 0.9182), PNorm: 160.5796, GNorm: 1.1705
[72/299] timecost: 71.02, lr: 0.000030, Train: (LOSS: 0.0354, MAE: 0.0354, RMSE: 0.0562, R2: 0.9474), Valid: (LOSS: 0.0425, MAE: 0.0425, RMSE: 0.0628, R2: 0.9363), PNorm: 160.4356, GNorm: 1.4337
[73/299] timecost: 71.14, lr: 0.000030, Train: (LOSS: 0.0337, MAE: 0.0337, RMSE: 0.0551, R2: 0.9502), Valid: (LOSS: 0.0437, MAE: 0.0437, RMSE: 0.0661, R2: 0.9299), PNorm: 160.2920, GNorm: 1.7305
[74/299] timecost: 71.40, lr: 0.000030, Train: (LOSS: 0.0333, MAE: 0.0333, RMSE: 0.0537, R2: 0.9512), Valid: (LOSS: 0.0467, MAE: 0.0467, RMSE: 0.0707, R2: 0.9171), PNorm: 160.1486, GNorm: 1.9411
[75/299] timecost: 70.84, lr: 0.000030, Train: (LOSS: 0.0325, MAE: 0.0325, RMSE: 0.0523, R2: 0.9552), Valid: (LOSS: 0.0522, MAE: 0.0522, RMSE: 0.0813, R2: 0.8908), PNorm: 160.0057, GNorm: 1.3775
[76/299] timecost: 70.73, lr: 0.000030, Train: (LOSS: 0.0334, MAE: 0.0334, RMSE: 0.0533, R2: 0.9531), Valid: (LOSS: 0.0428, MAE: 0.0428, RMSE: 0.0662, R2: 0.9269), PNorm: 159.8619, GNorm: 0.9166
[77/299] timecost: 70.89, lr: 0.000030, Train: (LOSS: 0.0328, MAE: 0.0328, RMSE: 0.0521, R2: 0.9555), Valid: (LOSS: 0.0459, MAE: 0.0459, RMSE: 0.0666, R2: 0.9287), PNorm: 159.7189, GNorm: 0.9081
[78/299] timecost: 71.20, lr: 0.000030, Train: (LOSS: 0.0336, MAE: 0.0336, RMSE: 0.0551, R2: 0.9521), Valid: (LOSS: 0.0475, MAE: 0.0475, RMSE: 0.0738, R2: 0.9131), PNorm: 159.5779, GNorm: 1.0920
[79/299] timecost: 71.41, lr: 0.000030, Train: (LOSS: 0.0315, MAE: 0.0315, RMSE: 0.0502, R2: 0.9572), Valid: (LOSS: 0.0524, MAE: 0.0524, RMSE: 0.0792, R2: 0.8999), PNorm: 159.4356, GNorm: 1.9034
[80/299] timecost: 70.42, lr: 0.000030, Train: (LOSS: 0.0318, MAE: 0.0318, RMSE: 0.0506, R2: 0.9577), Valid: (LOSS: 0.0441, MAE: 0.0441, RMSE: 0.0653, R2: 0.9313), PNorm: 159.2931, GNorm: 1.3639
[81/299] timecost: 70.82, lr: 0.000030, Train: (LOSS: 0.0321, MAE: 0.0321, RMSE: 0.0520, R2: 0.9564), Valid: (LOSS: 0.0476, MAE: 0.0476, RMSE: 0.0767, R2: 0.9051), PNorm: 159.1515, GNorm: 1.3644
[82/299] timecost: 71.35, lr: 0.000030, Train: (LOSS: 0.0311, MAE: 0.0311, RMSE: 0.0497, R2: 0.9584), Valid: (LOSS: 0.0447, MAE: 0.0447, RMSE: 0.0688, R2: 0.9225), PNorm: 159.0096, GNorm: 0.8749
[83/299] timecost: 70.53, lr: 0.000030, Train: (LOSS: 0.0320, MAE: 0.0320, RMSE: 0.0512, R2: 0.9579), Valid: (LOSS: 0.0477, MAE: 0.0477, RMSE: 0.0724, R2: 0.9155), PNorm: 158.8681, GNorm: 1.1582
[84/299] timecost: 70.72, lr: 0.000030, Train: (LOSS: 0.0309, MAE: 0.0309, RMSE: 0.0496, R2: 0.9593), Valid: (LOSS: 0.0463, MAE: 0.0463, RMSE: 0.0736, R2: 0.9131), PNorm: 158.7263, GNorm: 1.1368
[85/299] timecost: 70.76, lr: 0.000030, Train: (LOSS: 0.0294, MAE: 0.0294, RMSE: 0.0473, R2: 0.9607), Valid: (LOSS: 0.0434, MAE: 0.0434, RMSE: 0.0642, R2: 0.9328), PNorm: 158.5830, GNorm: 1.1515
[86/299] timecost: 70.62, lr: 0.000030, Train: (LOSS: 0.0291, MAE: 0.0291, RMSE: 0.0471, R2: 0.9621), Valid: (LOSS: 0.0461, MAE: 0.0461, RMSE: 0.0737, R2: 0.9124), PNorm: 158.4404, GNorm: 1.1153
[87/299] timecost: 71.24, lr: 0.000030, Train: (LOSS: 0.0290, MAE: 0.0290, RMSE: 0.0466, R2: 0.9628), Valid: (LOSS: 0.0481, MAE: 0.0481, RMSE: 0.0761, R2: 0.9060), PNorm: 158.2972, GNorm: 1.3442
[88/299] timecost: 70.95, lr: 0.000030, Train: (LOSS: 0.0295, MAE: 0.0295, RMSE: 0.0475, R2: 0.9600), Valid: (LOSS: 0.0436, MAE: 0.0436, RMSE: 0.0695, R2: 0.9216), PNorm: 158.1543, GNorm: 1.0602
[89/299] timecost: 71.01, lr: 0.000030, Train: (LOSS: 0.0290, MAE: 0.0290, RMSE: 0.0466, R2: 0.9631), Valid: (LOSS: 0.0442, MAE: 0.0442, RMSE: 0.0713, R2: 0.9184), PNorm: 158.0121, GNorm: 1.4621
Epoch 00091: reducing learning rate of group 0 to 2.7000e-05.
[90/299] timecost: 70.95, lr: 0.000027, Train: (LOSS: 0.0279, MAE: 0.0279, RMSE: 0.0454, R2: 0.9649), Valid: (LOSS: 0.0459, MAE: 0.0459, RMSE: 0.0724, R2: 0.9153), PNorm: 157.8694, GNorm: 0.9358
[91/299] timecost: 70.73, lr: 0.000027, Train: (LOSS: 0.0278, MAE: 0.0278, RMSE: 0.0441, R2: 0.9657), Valid: (LOSS: 0.0442, MAE: 0.0442, RMSE: 0.0686, R2: 0.9243), PNorm: 157.7404, GNorm: 0.9399
[92/299] timecost: 70.58, lr: 0.000027, Train: (LOSS: 0.0278, MAE: 0.0278, RMSE: 0.0442, R2: 0.9674), Valid: (LOSS: 0.0445, MAE: 0.0445, RMSE: 0.0680, R2: 0.9263), PNorm: 157.6107, GNorm: 1.2295
[93/299] timecost: 71.72, lr: 0.000027, Train: (LOSS: 0.0271, MAE: 0.0271, RMSE: 0.0428, R2: 0.9695), Valid: (LOSS: 0.0448, MAE: 0.0448, RMSE: 0.0690, R2: 0.9217), PNorm: 157.4808, GNorm: 1.6037
[94/299] timecost: 71.83, lr: 0.000027, Train: (LOSS: 0.0263, MAE: 0.0263, RMSE: 0.0423, R2: 0.9701), Valid: (LOSS: 0.0406, MAE: 0.0406, RMSE: 0.0616, R2: 0.9390), PNorm: 157.3503, GNorm: 1.1145
[95/299] timecost: 71.81, lr: 0.000027, Train: (LOSS: 0.0264, MAE: 0.0264, RMSE: 0.0422, R2: 0.9698), Valid: (LOSS: 0.0446, MAE: 0.0446, RMSE: 0.0719, R2: 0.9156), PNorm: 157.2191, GNorm: 1.3640
[96/299] timecost: 71.68, lr: 0.000027, Train: (LOSS: 0.0258, MAE: 0.0258, RMSE: 0.0414, R2: 0.9688), Valid: (LOSS: 0.0447, MAE: 0.0447, RMSE: 0.0683, R2: 0.9235), PNorm: 157.0875, GNorm: 1.7582
[97/299] timecost: 71.36, lr: 0.000027, Train: (LOSS: 0.0256, MAE: 0.0256, RMSE: 0.0415, R2: 0.9696), Valid: (LOSS: 0.0434, MAE: 0.0434, RMSE: 0.0665, R2: 0.9267), PNorm: 156.9552, GNorm: 1.5083
[98/299] timecost: 71.20, lr: 0.000027, Train: (LOSS: 0.0258, MAE: 0.0258, RMSE: 0.0405, R2: 0.9713), Valid: (LOSS: 0.0427, MAE: 0.0427, RMSE: 0.0642, R2: 0.9335), PNorm: 156.8233, GNorm: 0.9794
[99/299] timecost: 70.88, lr: 0.000027, Train: (LOSS: 0.0247, MAE: 0.0247, RMSE: 0.0394, R2: 0.9725), Valid: (LOSS: 0.0509, MAE: 0.0509, RMSE: 0.0740, R2: 0.9132), PNorm: 156.6906, GNorm: 0.7539
[100/299] timecost: 70.44, lr: 0.000027, Train: (LOSS: 0.0256, MAE: 0.0256, RMSE: 0.0408, R2: 0.9722), Valid: (LOSS: 0.0433, MAE: 0.0433, RMSE: 0.0675, R2: 0.9263), PNorm: 156.5576, GNorm: 1.9383
[101/299] timecost: 71.48, lr: 0.000027, Train: (LOSS: 0.0262, MAE: 0.0262, RMSE: 0.0416, R2: 0.9700), Valid: (LOSS: 0.0426, MAE: 0.0426, RMSE: 0.0652, R2: 0.9322), PNorm: 156.4250, GNorm: 0.9092
[102/299] timecost: 70.88, lr: 0.000027, Train: (LOSS: 0.0239, MAE: 0.0239, RMSE: 0.0376, R2: 0.9764), Valid: (LOSS: 0.0455, MAE: 0.0455, RMSE: 0.0710, R2: 0.9188), PNorm: 156.2910, GNorm: 1.4735
[103/299] timecost: 70.83, lr: 0.000027, Train: (LOSS: 0.0262, MAE: 0.0262, RMSE: 0.0409, R2: 0.9707), Valid: (LOSS: 0.0406, MAE: 0.0406, RMSE: 0.0620, R2: 0.9377), PNorm: 156.1583, GNorm: 1.2668
[104/299] timecost: 70.81, lr: 0.000027, Train: (LOSS: 0.0245, MAE: 0.0245, RMSE: 0.0387, R2: 0.9736), Valid: (LOSS: 0.0441, MAE: 0.0441, RMSE: 0.0703, R2: 0.9190), PNorm: 156.0246, GNorm: 0.9190
[105/299] timecost: 71.18, lr: 0.000027, Train: (LOSS: 0.0241, MAE: 0.0241, RMSE: 0.0378, R2: 0.9771), Valid: (LOSS: 0.0419, MAE: 0.0419, RMSE: 0.0676, R2: 0.9242), PNorm: 155.8904, GNorm: 0.8966
[106/299] timecost: 70.83, lr: 0.000027, Train: (LOSS: 0.0256, MAE: 0.0256, RMSE: 0.0397, R2: 0.9739), Valid: (LOSS: 0.0425, MAE: 0.0425, RMSE: 0.0648, R2: 0.9316), PNorm: 155.7572, GNorm: 1.1721
[107/299] timecost: 70.23, lr: 0.000027, Train: (LOSS: 0.0242, MAE: 0.0242, RMSE: 0.0384, R2: 0.9758), Valid: (LOSS: 0.0456, MAE: 0.0456, RMSE: 0.0678, R2: 0.9247), PNorm: 155.6231, GNorm: 0.9942
[108/299] timecost: 70.14, lr: 0.000027, Train: (LOSS: 0.0238, MAE: 0.0238, RMSE: 0.0371, R2: 0.9776), Valid: (LOSS: 0.0423, MAE: 0.0423, RMSE: 0.0656, R2: 0.9301), PNorm: 155.4884, GNorm: 0.7598
[109/299] timecost: 70.49, lr: 0.000027, Train: (LOSS: 0.0233, MAE: 0.0233, RMSE: 0.0369, R2: 0.9766), Valid: (LOSS: 0.0414, MAE: 0.0414, RMSE: 0.0621, R2: 0.9381), PNorm: 155.3532, GNorm: 1.3357
[110/299] timecost: 70.71, lr: 0.000027, Train: (LOSS: 0.0239, MAE: 0.0239, RMSE: 0.0375, R2: 0.9768), Valid: (LOSS: 0.0406, MAE: 0.0406, RMSE: 0.0639, R2: 0.9341), PNorm: 155.2185, GNorm: 1.5152
[111/299] timecost: 70.63, lr: 0.000027, Train: (LOSS: 0.0233, MAE: 0.0233, RMSE: 0.0361, R2: 0.9781), Valid: (LOSS: 0.0432, MAE: 0.0432, RMSE: 0.0661, R2: 0.9287), PNorm: 155.0835, GNorm: 1.3247
[112/299] timecost: 71.27, lr: 0.000027, Train: (LOSS: 0.0225, MAE: 0.0225, RMSE: 0.0350, R2: 0.9803), Valid: (LOSS: 0.0431, MAE: 0.0431, RMSE: 0.0663, R2: 0.9288), PNorm: 154.9476, GNorm: 0.9511
[113/299] timecost: 70.96, lr: 0.000027, Train: (LOSS: 0.0225, MAE: 0.0225, RMSE: 0.0352, R2: 0.9779), Valid: (LOSS: 0.0440, MAE: 0.0440, RMSE: 0.0687, R2: 0.9232), PNorm: 154.8116, GNorm: 0.6811
[114/299] timecost: 71.53, lr: 0.000027, Train: (LOSS: 0.0216, MAE: 0.0216, RMSE: 0.0338, R2: 0.9806), Valid: (LOSS: 0.0426, MAE: 0.0426, RMSE: 0.0655, R2: 0.9299), PNorm: 154.6757, GNorm: 0.8265
Epoch 00116: reducing learning rate of group 0 to 2.4300e-05.
[115/299] timecost: 71.37, lr: 0.000024, Train: (LOSS: 0.0220, MAE: 0.0220, RMSE: 0.0339, R2: 0.9795), Valid: (LOSS: 0.0430, MAE: 0.0430, RMSE: 0.0670, R2: 0.9281), PNorm: 154.5388, GNorm: 0.8899
[116/299] timecost: 71.15, lr: 0.000024, Train: (LOSS: 0.0206, MAE: 0.0206, RMSE: 0.0322, R2: 0.9802), Valid: (LOSS: 0.0448, MAE: 0.0448, RMSE: 0.0707, R2: 0.9195), PNorm: 154.4155, GNorm: 1.2444
[117/299] timecost: 72.24, lr: 0.000024, Train: (LOSS: 0.0205, MAE: 0.0205, RMSE: 0.0317, R2: 0.9825), Valid: (LOSS: 0.0417, MAE: 0.0417, RMSE: 0.0635, R2: 0.9360), PNorm: 154.2919, GNorm: 1.3246
[118/299] timecost: 71.33, lr: 0.000024, Train: (LOSS: 0.0211, MAE: 0.0211, RMSE: 0.0328, R2: 0.9815), Valid: (LOSS: 0.0407, MAE: 0.0407, RMSE: 0.0605, R2: 0.9411), PNorm: 154.1678, GNorm: 1.1725
[119/299] timecost: 70.57, lr: 0.000024, Train: (LOSS: 0.0213, MAE: 0.0213, RMSE: 0.0334, R2: 0.9806), Valid: (LOSS: 0.0424, MAE: 0.0424, RMSE: 0.0653, R2: 0.9310), PNorm: 154.0439, GNorm: 1.0344
[120/299] timecost: 71.34, lr: 0.000024, Train: (LOSS: 0.0222, MAE: 0.0222, RMSE: 0.0344, R2: 0.9799), Valid: (LOSS: 0.0389, MAE: 0.0389, RMSE: 0.0593, R2: 0.9423), PNorm: 153.9202, GNorm: 0.8897
[121/299] timecost: 71.60, lr: 0.000024, Train: (LOSS: 0.0216, MAE: 0.0216, RMSE: 0.0332, R2: 0.9812), Valid: (LOSS: 0.0413, MAE: 0.0413, RMSE: 0.0639, R2: 0.9329), PNorm: 153.7963, GNorm: 0.8450
[122/299] timecost: 70.63, lr: 0.000024, Train: (LOSS: 0.0211, MAE: 0.0211, RMSE: 0.0328, R2: 0.9816), Valid: (LOSS: 0.0421, MAE: 0.0421, RMSE: 0.0668, R2: 0.9281), PNorm: 153.6721, GNorm: 1.1719
[123/299] timecost: 70.44, lr: 0.000024, Train: (LOSS: 0.0200, MAE: 0.0200, RMSE: 0.0314, R2: 0.9821), Valid: (LOSS: 0.0417, MAE: 0.0417, RMSE: 0.0631, R2: 0.9350), PNorm: 153.5482, GNorm: 0.9791
[124/299] timecost: 71.54, lr: 0.000024, Train: (LOSS: 0.0202, MAE: 0.0202, RMSE: 0.0316, R2: 0.9825), Valid: (LOSS: 0.0416, MAE: 0.0416, RMSE: 0.0654, R2: 0.9296), PNorm: 153.4235, GNorm: 0.8151
[125/299] timecost: 71.33, lr: 0.000024, Train: (LOSS: 0.0195, MAE: 0.0195, RMSE: 0.0305, R2: 0.9842), Valid: (LOSS: 0.0381, MAE: 0.0381, RMSE: 0.0595, R2: 0.9427), PNorm: 153.2980, GNorm: 0.7824
[126/299] timecost: 71.08, lr: 0.000024, Train: (LOSS: 0.0189, MAE: 0.0189, RMSE: 0.0298, R2: 0.9847), Valid: (LOSS: 0.0403, MAE: 0.0403, RMSE: 0.0637, R2: 0.9336), PNorm: 153.1723, GNorm: 0.6654
[127/299] timecost: 71.51, lr: 0.000024, Train: (LOSS: 0.0198, MAE: 0.0198, RMSE: 0.0309, R2: 0.9827), Valid: (LOSS: 0.0435, MAE: 0.0435, RMSE: 0.0688, R2: 0.9217), PNorm: 153.0469, GNorm: 0.9576
[128/299] timecost: 71.29, lr: 0.000024, Train: (LOSS: 0.0193, MAE: 0.0193, RMSE: 0.0303, R2: 0.9848), Valid: (LOSS: 0.0415, MAE: 0.0415, RMSE: 0.0640, R2: 0.9341), PNorm: 152.9217, GNorm: 0.9609
[129/299] timecost: 70.94, lr: 0.000024, Train: (LOSS: 0.0202, MAE: 0.0202, RMSE: 0.0317, R2: 0.9827), Valid: (LOSS: 0.0415, MAE: 0.0415, RMSE: 0.0667, R2: 0.9267), PNorm: 152.7963, GNorm: 1.3736
[130/299] timecost: 71.49, lr: 0.000024, Train: (LOSS: 0.0198, MAE: 0.0198, RMSE: 0.0305, R2: 0.9840), Valid: (LOSS: 0.0403, MAE: 0.0403, RMSE: 0.0634, R2: 0.9341), PNorm: 152.6704, GNorm: 0.9863
[131/299] timecost: 70.82, lr: 0.000024, Train: (LOSS: 0.0199, MAE: 0.0199, RMSE: 0.0309, R2: 0.9825), Valid: (LOSS: 0.0397, MAE: 0.0397, RMSE: 0.0622, R2: 0.9376), PNorm: 152.5453, GNorm: 0.9279
[132/299] timecost: 70.25, lr: 0.000024, Train: (LOSS: 0.0188, MAE: 0.0188, RMSE: 0.0290, R2: 0.9843), Valid: (LOSS: 0.0399, MAE: 0.0399, RMSE: 0.0628, R2: 0.9354), PNorm: 152.4197, GNorm: 1.3773
[133/299] timecost: 70.17, lr: 0.000024, Train: (LOSS: 0.0193, MAE: 0.0193, RMSE: 0.0300, R2: 0.9847), Valid: (LOSS: 0.0410, MAE: 0.0410, RMSE: 0.0632, R2: 0.9336), PNorm: 152.2943, GNorm: 0.7016
[134/299] timecost: 70.21, lr: 0.000024, Train: (LOSS: 0.0189, MAE: 0.0189, RMSE: 0.0292, R2: 0.9848), Valid: (LOSS: 0.0430, MAE: 0.0430, RMSE: 0.0664, R2: 0.9282), PNorm: 152.1683, GNorm: 0.8402
[135/299] timecost: 70.61, lr: 0.000024, Train: (LOSS: 0.0187, MAE: 0.0187, RMSE: 0.0293, R2: 0.9853), Valid: (LOSS: 0.0412, MAE: 0.0412, RMSE: 0.0644, R2: 0.9329), PNorm: 152.0422, GNorm: 1.0754
[136/299] timecost: 71.55, lr: 0.000024, Train: (LOSS: 0.0183, MAE: 0.0183, RMSE: 0.0284, R2: 0.9857), Valid: (LOSS: 0.0400, MAE: 0.0400, RMSE: 0.0625, R2: 0.9363), PNorm: 151.9155, GNorm: 1.3548
[137/299] timecost: 70.56, lr: 0.000024, Train: (LOSS: 0.0186, MAE: 0.0186, RMSE: 0.0289, R2: 0.9856), Valid: (LOSS: 0.0416, MAE: 0.0416, RMSE: 0.0645, R2: 0.9329), PNorm: 151.7887, GNorm: 0.9015
[138/299] timecost: 70.37, lr: 0.000024, Train: (LOSS: 0.0187, MAE: 0.0187, RMSE: 0.0286, R2: 0.9857), Valid: (LOSS: 0.0414, MAE: 0.0414, RMSE: 0.0650, R2: 0.9306), PNorm: 151.6623, GNorm: 0.8663
[139/299] timecost: 71.20, lr: 0.000024, Train: (LOSS: 0.0187, MAE: 0.0187, RMSE: 0.0287, R2: 0.9853), Valid: (LOSS: 0.0385, MAE: 0.0385, RMSE: 0.0609, R2: 0.9396), PNorm: 151.5357, GNorm: 0.9845
[140/299] timecost: 71.92, lr: 0.000024, Train: (LOSS: 0.0181, MAE: 0.0181, RMSE: 0.0284, R2: 0.9859), Valid: (LOSS: 0.0416, MAE: 0.0416, RMSE: 0.0640, R2: 0.9321), PNorm: 151.4095, GNorm: 0.7007
[141/299] timecost: 71.50, lr: 0.000024, Train: (LOSS: 0.0182, MAE: 0.0182, RMSE: 0.0286, R2: 0.9857), Valid: (LOSS: 0.0377, MAE: 0.0377, RMSE: 0.0591, R2: 0.9430), PNorm: 151.2826, GNorm: 0.8950
[142/299] timecost: 70.65, lr: 0.000024, Train: (LOSS: 0.0173, MAE: 0.0173, RMSE: 0.0267, R2: 0.9869), Valid: (LOSS: 0.0411, MAE: 0.0411, RMSE: 0.0664, R2: 0.9289), PNorm: 151.1548, GNorm: 0.8612
[143/299] timecost: 70.98, lr: 0.000024, Train: (LOSS: 0.0177, MAE: 0.0177, RMSE: 0.0274, R2: 0.9861), Valid: (LOSS: 0.0390, MAE: 0.0390, RMSE: 0.0624, R2: 0.9365), PNorm: 151.0274, GNorm: 0.9828
[144/299] timecost: 71.53, lr: 0.000024, Train: (LOSS: 0.0175, MAE: 0.0175, RMSE: 0.0274, R2: 0.9865), Valid: (LOSS: 0.0414, MAE: 0.0414, RMSE: 0.0656, R2: 0.9303), PNorm: 150.8995, GNorm: 0.6338
[145/299] timecost: 71.28, lr: 0.000024, Train: (LOSS: 0.0177, MAE: 0.0177, RMSE: 0.0276, R2: 0.9864), Valid: (LOSS: 0.0384, MAE: 0.0384, RMSE: 0.0607, R2: 0.9397), PNorm: 150.7710, GNorm: 1.1643
[146/299] timecost: 70.56, lr: 0.000024, Train: (LOSS: 0.0174, MAE: 0.0174, RMSE: 0.0270, R2: 0.9872), Valid: (LOSS: 0.0384, MAE: 0.0384, RMSE: 0.0601, R2: 0.9402), PNorm: 150.6423, GNorm: 1.0318
[147/299] timecost: 70.71, lr: 0.000024, Train: (LOSS: 0.0186, MAE: 0.0186, RMSE: 0.0287, R2: 0.9856), Valid: (LOSS: 0.0399, MAE: 0.0399, RMSE: 0.0618, R2: 0.9380), PNorm: 150.5146, GNorm: 0.8836
[148/299] timecost: 70.74, lr: 0.000024, Train: (LOSS: 0.0173, MAE: 0.0173, RMSE: 0.0273, R2: 0.9862), Valid: (LOSS: 0.0375, MAE: 0.0375, RMSE: 0.0601, R2: 0.9409), PNorm: 150.3869, GNorm: 0.9974
[149/299] timecost: 70.76, lr: 0.000024, Train: (LOSS: 0.0182, MAE: 0.0182, RMSE: 0.0281, R2: 0.9854), Valid: (LOSS: 0.0394, MAE: 0.0394, RMSE: 0.0641, R2: 0.9331), PNorm: 150.2595, GNorm: 1.1755
[150/299] timecost: 71.49, lr: 0.000024, Train: (LOSS: 0.0171, MAE: 0.0171, RMSE: 0.0267, R2: 0.9864), Valid: (LOSS: 0.0405, MAE: 0.0405, RMSE: 0.0644, R2: 0.9322), PNorm: 150.1319, GNorm: 1.2596
[151/299] timecost: 71.33, lr: 0.000024, Train: (LOSS: 0.0168, MAE: 0.0168, RMSE: 0.0262, R2: 0.9874), Valid: (LOSS: 0.0379, MAE: 0.0379, RMSE: 0.0611, R2: 0.9391), PNorm: 150.0042, GNorm: 0.8836
[152/299] timecost: 71.59, lr: 0.000024, Train: (LOSS: 0.0166, MAE: 0.0166, RMSE: 0.0255, R2: 0.9884), Valid: (LOSS: 0.0390, MAE: 0.0390, RMSE: 0.0621, R2: 0.9377), PNorm: 149.8762, GNorm: 0.8379
[153/299] timecost: 71.30, lr: 0.000024, Train: (LOSS: 0.0171, MAE: 0.0171, RMSE: 0.0267, R2: 0.9866), Valid: (LOSS: 0.0409, MAE: 0.0409, RMSE: 0.0638, R2: 0.9344), PNorm: 149.7482, GNorm: 1.0125
[154/299] timecost: 70.53, lr: 0.000024, Train: (LOSS: 0.0174, MAE: 0.0174, RMSE: 0.0269, R2: 0.9872), Valid: (LOSS: 0.0413, MAE: 0.0413, RMSE: 0.0636, R2: 0.9342), PNorm: 149.6199, GNorm: 1.4476
[155/299] timecost: 70.84, lr: 0.000024, Train: (LOSS: 0.0160, MAE: 0.0160, RMSE: 0.0253, R2: 0.9880), Valid: (LOSS: 0.0402, MAE: 0.0402, RMSE: 0.0638, R2: 0.9344), PNorm: 149.4917, GNorm: 1.0671
[156/299] timecost: 71.50, lr: 0.000024, Train: (LOSS: 0.0155, MAE: 0.0155, RMSE: 0.0245, R2: 0.9888), Valid: (LOSS: 0.0397, MAE: 0.0397, RMSE: 0.0622, R2: 0.9374), PNorm: 149.3630, GNorm: 1.0959
[157/299] timecost: 71.28, lr: 0.000024, Train: (LOSS: 0.0163, MAE: 0.0163, RMSE: 0.0250, R2: 0.9877), Valid: (LOSS: 0.0396, MAE: 0.0396, RMSE: 0.0626, R2: 0.9361), PNorm: 149.2338, GNorm: 0.8311
[158/299] timecost: 71.04, lr: 0.000024, Train: (LOSS: 0.0166, MAE: 0.0166, RMSE: 0.0257, R2: 0.9884), Valid: (LOSS: 0.0390, MAE: 0.0390, RMSE: 0.0620, R2: 0.9380), PNorm: 149.1062, GNorm: 0.7987
[159/299] timecost: 71.34, lr: 0.000024, Train: (LOSS: 0.0167, MAE: 0.0167, RMSE: 0.0255, R2: 0.9871), Valid: (LOSS: 0.0416, MAE: 0.0416, RMSE: 0.0641, R2: 0.9336), PNorm: 148.9779, GNorm: 0.9428
[160/299] timecost: 71.22, lr: 0.000024, Train: (LOSS: 0.0168, MAE: 0.0168, RMSE: 0.0259, R2: 0.9879), Valid: (LOSS: 0.0397, MAE: 0.0397, RMSE: 0.0617, R2: 0.9392), PNorm: 148.8493, GNorm: 0.9829
[161/299] timecost: 71.24, lr: 0.000024, Train: (LOSS: 0.0164, MAE: 0.0164, RMSE: 0.0254, R2: 0.9890), Valid: (LOSS: 0.0393, MAE: 0.0393, RMSE: 0.0612, R2: 0.9395), PNorm: 148.7204, GNorm: 0.7561
Epoch 00163: reducing learning rate of group 0 to 2.1870e-05.
[162/299] timecost: 70.48, lr: 0.000022, Train: (LOSS: 0.0165, MAE: 0.0165, RMSE: 0.0256, R2: 0.9887), Valid: (LOSS: 0.0387, MAE: 0.0387, RMSE: 0.0622, R2: 0.9366), PNorm: 148.5921, GNorm: 0.9753
[163/299] timecost: 71.07, lr: 0.000022, Train: (LOSS: 0.0161, MAE: 0.0161, RMSE: 0.0250, R2: 0.9871), Valid: (LOSS: 0.0366, MAE: 0.0366, RMSE: 0.0584, R2: 0.9436), PNorm: 148.4767, GNorm: 0.9856
[164/299] timecost: 71.54, lr: 0.000022, Train: (LOSS: 0.0152, MAE: 0.0152, RMSE: 0.0242, R2: 0.9885), Valid: (LOSS: 0.0378, MAE: 0.0378, RMSE: 0.0593, R2: 0.9433), PNorm: 148.3616, GNorm: 1.2924
[165/299] timecost: 71.02, lr: 0.000022, Train: (LOSS: 0.0159, MAE: 0.0159, RMSE: 0.0249, R2: 0.9889), Valid: (LOSS: 0.0367, MAE: 0.0367, RMSE: 0.0587, R2: 0.9440), PNorm: 148.2462, GNorm: 0.8723
[166/299] timecost: 71.67, lr: 0.000022, Train: (LOSS: 0.0156, MAE: 0.0156, RMSE: 0.0241, R2: 0.9895), Valid: (LOSS: 0.0377, MAE: 0.0377, RMSE: 0.0588, R2: 0.9437), PNorm: 148.1312, GNorm: 1.1718
[167/299] timecost: 71.10, lr: 0.000022, Train: (LOSS: 0.0142, MAE: 0.0142, RMSE: 0.0228, R2: 0.9895), Valid: (LOSS: 0.0368, MAE: 0.0368, RMSE: 0.0589, R2: 0.9422), PNorm: 148.0154, GNorm: 1.0637
[168/299] timecost: 70.52, lr: 0.000022, Train: (LOSS: 0.0146, MAE: 0.0146, RMSE: 0.0231, R2: 0.9896), Valid: (LOSS: 0.0365, MAE: 0.0365, RMSE: 0.0582, R2: 0.9446), PNorm: 147.8994, GNorm: 1.0094
[169/299] timecost: 71.37, lr: 0.000022, Train: (LOSS: 0.0147, MAE: 0.0147, RMSE: 0.0236, R2: 0.9889), Valid: (LOSS: 0.0382, MAE: 0.0382, RMSE: 0.0611, R2: 0.9383), PNorm: 147.7834, GNorm: 0.9897
[170/299] timecost: 71.38, lr: 0.000022, Train: (LOSS: 0.0147, MAE: 0.0147, RMSE: 0.0236, R2: 0.9898), Valid: (LOSS: 0.0391, MAE: 0.0391, RMSE: 0.0630, R2: 0.9359), PNorm: 147.6672, GNorm: 0.9131
[171/299] timecost: 71.69, lr: 0.000022, Train: (LOSS: 0.0141, MAE: 0.0141, RMSE: 0.0223, R2: 0.9905), Valid: (LOSS: 0.0375, MAE: 0.0375, RMSE: 0.0601, R2: 0.9407), PNorm: 147.5508, GNorm: 0.9902
[172/299] timecost: 70.73, lr: 0.000022, Train: (LOSS: 0.0149, MAE: 0.0149, RMSE: 0.0235, R2: 0.9896), Valid: (LOSS: 0.0377, MAE: 0.0377, RMSE: 0.0612, R2: 0.9384), PNorm: 147.4346, GNorm: 0.7435
[173/299] timecost: 70.83, lr: 0.000022, Train: (LOSS: 0.0142, MAE: 0.0142, RMSE: 0.0220, R2: 0.9905), Valid: (LOSS: 0.0405, MAE: 0.0405, RMSE: 0.0654, R2: 0.9301), PNorm: 147.3185, GNorm: 0.7857
[174/299] timecost: 71.05, lr: 0.000022, Train: (LOSS: 0.0145, MAE: 0.0145, RMSE: 0.0228, R2: 0.9897), Valid: (LOSS: 0.0388, MAE: 0.0388, RMSE: 0.0625, R2: 0.9373), PNorm: 147.2022, GNorm: 0.9530
[175/299] timecost: 70.42, lr: 0.000022, Train: (LOSS: 0.0150, MAE: 0.0150, RMSE: 0.0239, R2: 0.9895), Valid: (LOSS: 0.0387, MAE: 0.0387, RMSE: 0.0619, R2: 0.9377), PNorm: 147.0860, GNorm: 0.8336
[176/299] timecost: 70.37, lr: 0.000022, Train: (LOSS: 0.0140, MAE: 0.0140, RMSE: 0.0224, R2: 0.9897), Valid: (LOSS: 0.0383, MAE: 0.0383, RMSE: 0.0605, R2: 0.9409), PNorm: 146.9697, GNorm: 0.8366
[177/299] timecost: 70.63, lr: 0.000022, Train: (LOSS: 0.0143, MAE: 0.0143, RMSE: 0.0226, R2: 0.9905), Valid: (LOSS: 0.0380, MAE: 0.0380, RMSE: 0.0603, R2: 0.9413), PNorm: 146.8531, GNorm: 1.1391
[178/299] timecost: 70.35, lr: 0.000022, Train: (LOSS: 0.0142, MAE: 0.0142, RMSE: 0.0227, R2: 0.9897), Valid: (LOSS: 0.0385, MAE: 0.0385, RMSE: 0.0617, R2: 0.9382), PNorm: 146.7364, GNorm: 1.2515
[179/299] timecost: 70.20, lr: 0.000022, Train: (LOSS: 0.0144, MAE: 0.0144, RMSE: 0.0223, R2: 0.9904), Valid: (LOSS: 0.0382, MAE: 0.0382, RMSE: 0.0605, R2: 0.9409), PNorm: 146.6208, GNorm: 0.7661
[180/299] timecost: 70.48, lr: 0.000022, Train: (LOSS: 0.0136, MAE: 0.0136, RMSE: 0.0216, R2: 0.9913), Valid: (LOSS: 0.0397, MAE: 0.0397, RMSE: 0.0618, R2: 0.9371), PNorm: 146.5052, GNorm: 0.8180
[181/299] timecost: 70.83, lr: 0.000022, Train: (LOSS: 0.0146, MAE: 0.0146, RMSE: 0.0229, R2: 0.9901), Valid: (LOSS: 0.0399, MAE: 0.0399, RMSE: 0.0624, R2: 0.9372), PNorm: 146.3904, GNorm: 0.7797
[182/299] timecost: 71.49, lr: 0.000022, Train: (LOSS: 0.0136, MAE: 0.0136, RMSE: 0.0213, R2: 0.9910), Valid: (LOSS: 0.0384, MAE: 0.0384, RMSE: 0.0603, R2: 0.9408), PNorm: 146.2751, GNorm: 0.9120
[183/299] timecost: 71.48, lr: 0.000022, Train: (LOSS: 0.0133, MAE: 0.0133, RMSE: 0.0211, R2: 0.9910), Valid: (LOSS: 0.0396, MAE: 0.0396, RMSE: 0.0608, R2: 0.9397), PNorm: 146.1594, GNorm: 1.0719
Epoch 00185: reducing learning rate of group 0 to 1.9683e-05.
[184/299] timecost: 71.67, lr: 0.000020, Train: (LOSS: 0.0141, MAE: 0.0141, RMSE: 0.0223, R2: 0.9894), Valid: (LOSS: 0.0393, MAE: 0.0393, RMSE: 0.0614, R2: 0.9390), PNorm: 146.0443, GNorm: 1.0306
[185/299] timecost: 71.15, lr: 0.000020, Train: (LOSS: 0.0137, MAE: 0.0137, RMSE: 0.0217, R2: 0.9907), Valid: (LOSS: 0.0399, MAE: 0.0399, RMSE: 0.0624, R2: 0.9372), PNorm: 145.9406, GNorm: 0.9962
[186/299] timecost: 70.65, lr: 0.000020, Train: (LOSS: 0.0131, MAE: 0.0131, RMSE: 0.0211, R2: 0.9913), Valid: (LOSS: 0.0380, MAE: 0.0380, RMSE: 0.0590, R2: 0.9440), PNorm: 145.8369, GNorm: 0.9934
[187/299] timecost: 70.69, lr: 0.000020, Train: (LOSS: 0.0133, MAE: 0.0133, RMSE: 0.0211, R2: 0.9907), Valid: (LOSS: 0.0374, MAE: 0.0374, RMSE: 0.0597, R2: 0.9425), PNorm: 145.7330, GNorm: 0.8828
[188/299] timecost: 70.26, lr: 0.000020, Train: (LOSS: 0.0133, MAE: 0.0133, RMSE: 0.0210, R2: 0.9906), Valid: (LOSS: 0.0381, MAE: 0.0381, RMSE: 0.0612, R2: 0.9391), PNorm: 145.6290, GNorm: 0.9075
[189/299] timecost: 70.19, lr: 0.000020, Train: (LOSS: 0.0131, MAE: 0.0131, RMSE: 0.0207, R2: 0.9919), Valid: (LOSS: 0.0395, MAE: 0.0395, RMSE: 0.0618, R2: 0.9387), PNorm: 145.5251, GNorm: 1.0554
[190/299] timecost: 71.08, lr: 0.000020, Train: (LOSS: 0.0132, MAE: 0.0132, RMSE: 0.0208, R2: 0.9907), Valid: (LOSS: 0.0399, MAE: 0.0399, RMSE: 0.0626, R2: 0.9366), PNorm: 145.4209, GNorm: 0.8474
[191/299] timecost: 71.25, lr: 0.000020, Train: (LOSS: 0.0132, MAE: 0.0132, RMSE: 0.0207, R2: 0.9914), Valid: (LOSS: 0.0378, MAE: 0.0378, RMSE: 0.0596, R2: 0.9421), PNorm: 145.3171, GNorm: 0.9814
[192/299] timecost: 71.00, lr: 0.000020, Train: (LOSS: 0.0126, MAE: 0.0126, RMSE: 0.0202, R2: 0.9915), Valid: (LOSS: 0.0380, MAE: 0.0380, RMSE: 0.0594, R2: 0.9433), PNorm: 145.2138, GNorm: 0.9541
[193/299] timecost: 70.78, lr: 0.000020, Train: (LOSS: 0.0126, MAE: 0.0126, RMSE: 0.0200, R2: 0.9918), Valid: (LOSS: 0.0400, MAE: 0.0400, RMSE: 0.0627, R2: 0.9367), PNorm: 145.1101, GNorm: 0.9448
[194/299] timecost: 71.42, lr: 0.000020, Train: (LOSS: 0.0124, MAE: 0.0124, RMSE: 0.0197, R2: 0.9916), Valid: (LOSS: 0.0388, MAE: 0.0388, RMSE: 0.0610, R2: 0.9388), PNorm: 145.0070, GNorm: 0.7495
[195/299] timecost: 71.43, lr: 0.000020, Train: (LOSS: 0.0126, MAE: 0.0126, RMSE: 0.0200, R2: 0.9920), Valid: (LOSS: 0.0389, MAE: 0.0389, RMSE: 0.0599, R2: 0.9418), PNorm: 144.9039, GNorm: 1.1842
[196/299] timecost: 71.43, lr: 0.000020, Train: (LOSS: 0.0123, MAE: 0.0123, RMSE: 0.0199, R2: 0.9923), Valid: (LOSS: 0.0373, MAE: 0.0373, RMSE: 0.0579, R2: 0.9462), PNorm: 144.8010, GNorm: 0.6782
[197/299] timecost: 71.07, lr: 0.000020, Train: (LOSS: 0.0123, MAE: 0.0123, RMSE: 0.0199, R2: 0.9923), Valid: (LOSS: 0.0387, MAE: 0.0387, RMSE: 0.0598, R2: 0.9418), PNorm: 144.6980, GNorm: 0.7634
[198/299] timecost: 70.98, lr: 0.000020, Train: (LOSS: 0.0126, MAE: 0.0126, RMSE: 0.0200, R2: 0.9910), Valid: (LOSS: 0.0377, MAE: 0.0377, RMSE: 0.0604, R2: 0.9409), PNorm: 144.5957, GNorm: 0.8995
[199/299] timecost: 71.27, lr: 0.000020, Train: (LOSS: 0.0123, MAE: 0.0123, RMSE: 0.0193, R2: 0.9926), Valid: (LOSS: 0.0388, MAE: 0.0388, RMSE: 0.0616, R2: 0.9384), PNorm: 144.4933, GNorm: 0.9778
[200/299] timecost: 71.25, lr: 0.000020, Train: (LOSS: 0.0123, MAE: 0.0123, RMSE: 0.0197, R2: 0.9919), Valid: (LOSS: 0.0381, MAE: 0.0381, RMSE: 0.0614, R2: 0.9377), PNorm: 144.3911, GNorm: 0.8325
[201/299] timecost: 70.60, lr: 0.000020, Train: (LOSS: 0.0117, MAE: 0.0117, RMSE: 0.0189, R2: 0.9918), Valid: (LOSS: 0.0381, MAE: 0.0381, RMSE: 0.0614, R2: 0.9395), PNorm: 144.2886, GNorm: 0.9529
[202/299] timecost: 70.74, lr: 0.000020, Train: (LOSS: 0.0123, MAE: 0.0123, RMSE: 0.0199, R2: 0.9917), Valid: (LOSS: 0.0389, MAE: 0.0389, RMSE: 0.0619, R2: 0.9378), PNorm: 144.1873, GNorm: 0.8634
[203/299] timecost: 70.49, lr: 0.000020, Train: (LOSS: 0.0117, MAE: 0.0117, RMSE: 0.0187, R2: 0.9931), Valid: (LOSS: 0.0381, MAE: 0.0381, RMSE: 0.0604, R2: 0.9406), PNorm: 144.0856, GNorm: 0.7113
[204/299] timecost: 70.55, lr: 0.000020, Train: (LOSS: 0.0119, MAE: 0.0119, RMSE: 0.0191, R2: 0.9921), Valid: (LOSS: 0.0396, MAE: 0.0396, RMSE: 0.0622, R2: 0.9371), PNorm: 143.9839, GNorm: 0.9837
Epoch 00206: reducing learning rate of group 0 to 1.7715e-05.
[205/299] timecost: 71.04, lr: 0.000018, Train: (LOSS: 0.0117, MAE: 0.0117, RMSE: 0.0186, R2: 0.9934), Valid: (LOSS: 0.0382, MAE: 0.0382, RMSE: 0.0610, R2: 0.9392), PNorm: 143.8822, GNorm: 0.8692
[206/299] timecost: 70.91, lr: 0.000018, Train: (LOSS: 0.0114, MAE: 0.0114, RMSE: 0.0185, R2: 0.9932), Valid: (LOSS: 0.0379, MAE: 0.0379, RMSE: 0.0595, R2: 0.9427), PNorm: 143.7912, GNorm: 0.9733
[207/299] timecost: 70.62, lr: 0.000018, Train: (LOSS: 0.0114, MAE: 0.0114, RMSE: 0.0181, R2: 0.9936), Valid: (LOSS: 0.0378, MAE: 0.0378, RMSE: 0.0603, R2: 0.9409), PNorm: 143.7005, GNorm: 0.9940
[208/299] timecost: 70.64, lr: 0.000018, Train: (LOSS: 0.0108, MAE: 0.0108, RMSE: 0.0177, R2: 0.9930), Valid: (LOSS: 0.0394, MAE: 0.0394, RMSE: 0.0615, R2: 0.9381), PNorm: 143.6095, GNorm: 0.7088
[209/299] timecost: 70.37, lr: 0.000018, Train: (LOSS: 0.0108, MAE: 0.0108, RMSE: 0.0179, R2: 0.9933), Valid: (LOSS: 0.0385, MAE: 0.0385, RMSE: 0.0602, R2: 0.9410), PNorm: 143.5190, GNorm: 0.7486
[210/299] timecost: 70.74, lr: 0.000018, Train: (LOSS: 0.0115, MAE: 0.0115, RMSE: 0.0183, R2: 0.9932), Valid: (LOSS: 0.0398, MAE: 0.0398, RMSE: 0.0626, R2: 0.9373), PNorm: 143.4290, GNorm: 1.1509
[211/299] timecost: 70.97, lr: 0.000018, Train: (LOSS: 0.0110, MAE: 0.0110, RMSE: 0.0179, R2: 0.9935), Valid: (LOSS: 0.0394, MAE: 0.0394, RMSE: 0.0627, R2: 0.9362), PNorm: 143.3398, GNorm: 0.8172
[212/299] timecost: 70.27, lr: 0.000018, Train: (LOSS: 0.0106, MAE: 0.0106, RMSE: 0.0175, R2: 0.9935), Valid: (LOSS: 0.0406, MAE: 0.0406, RMSE: 0.0630, R2: 0.9360), PNorm: 143.2499, GNorm: 0.8974
[213/299] timecost: 70.84, lr: 0.000018, Train: (LOSS: 0.0110, MAE: 0.0110, RMSE: 0.0176, R2: 0.9935), Valid: (LOSS: 0.0393, MAE: 0.0393, RMSE: 0.0611, R2: 0.9398), PNorm: 143.1603, GNorm: 0.6797
[214/299] timecost: 70.45, lr: 0.000018, Train: (LOSS: 0.0110, MAE: 0.0110, RMSE: 0.0178, R2: 0.9935), Valid: (LOSS: 0.0393, MAE: 0.0393, RMSE: 0.0635, R2: 0.9338), PNorm: 143.0715, GNorm: 0.7621
[215/299] timecost: 71.58, lr: 0.000018, Train: (LOSS: 0.0110, MAE: 0.0110, RMSE: 0.0176, R2: 0.9935), Valid: (LOSS: 0.0389, MAE: 0.0389, RMSE: 0.0610, R2: 0.9402), PNorm: 142.9826, GNorm: 0.7365
[216/299] timecost: 71.03, lr: 0.000018, Train: (LOSS: 0.0112, MAE: 0.0112, RMSE: 0.0180, R2: 0.9932), Valid: (LOSS: 0.0387, MAE: 0.0387, RMSE: 0.0617, R2: 0.9378), PNorm: 142.8937, GNorm: 0.7022
[217/299] timecost: 71.84, lr: 0.000018, Train: (LOSS: 0.0113, MAE: 0.0113, RMSE: 0.0182, R2: 0.9926), Valid: (LOSS: 0.0388, MAE: 0.0388, RMSE: 0.0604, R2: 0.9413), PNorm: 142.8055, GNorm: 1.0647
[218/299] timecost: 71.40, lr: 0.000018, Train: (LOSS: 0.0107, MAE: 0.0107, RMSE: 0.0173, R2: 0.9933), Valid: (LOSS: 0.0391, MAE: 0.0391, RMSE: 0.0623, R2: 0.9367), PNorm: 142.7170, GNorm: 0.7174
[219/299] timecost: 71.12, lr: 0.000018, Train: (LOSS: 0.0111, MAE: 0.0111, RMSE: 0.0179, R2: 0.9931), Valid: (LOSS: 0.0392, MAE: 0.0392, RMSE: 0.0622, R2: 0.9367), PNorm: 142.6294, GNorm: 1.0714
[220/299] timecost: 71.31, lr: 0.000018, Train: (LOSS: 0.0109, MAE: 0.0109, RMSE: 0.0176, R2: 0.9936), Valid: (LOSS: 0.0380, MAE: 0.0380, RMSE: 0.0607, R2: 0.9400), PNorm: 142.5430, GNorm: 0.7648
[221/299] timecost: 70.98, lr: 0.000018, Train: (LOSS: 0.0102, MAE: 0.0102, RMSE: 0.0169, R2: 0.9942), Valid: (LOSS: 0.0381, MAE: 0.0381, RMSE: 0.0606, R2: 0.9403), PNorm: 142.4557, GNorm: 0.7768
[222/299] timecost: 71.11, lr: 0.000018, Train: (LOSS: 0.0110, MAE: 0.0110, RMSE: 0.0178, R2: 0.9929), Valid: (LOSS: 0.0400, MAE: 0.0400, RMSE: 0.0640, R2: 0.9335), PNorm: 142.3692, GNorm: 0.8956
[223/299] timecost: 70.93, lr: 0.000018, Train: (LOSS: 0.0105, MAE: 0.0105, RMSE: 0.0173, R2: 0.9939), Valid: (LOSS: 0.0395, MAE: 0.0395, RMSE: 0.0614, R2: 0.9394), PNorm: 142.2831, GNorm: 0.8007
[224/299] timecost: 71.02, lr: 0.000018, Train: (LOSS: 0.0108, MAE: 0.0108, RMSE: 0.0171, R2: 0.9939), Valid: (LOSS: 0.0399, MAE: 0.0399, RMSE: 0.0628, R2: 0.9367), PNorm: 142.1965, GNorm: 1.1034
[225/299] timecost: 71.47, lr: 0.000018, Train: (LOSS: 0.0102, MAE: 0.0102, RMSE: 0.0166, R2: 0.9942), Valid: (LOSS: 0.0389, MAE: 0.0389, RMSE: 0.0606, R2: 0.9406), PNorm: 142.1106, GNorm: 0.8718
Epoch 00227: reducing learning rate of group 0 to 1.5943e-05.
[226/299] timecost: 71.29, lr: 0.000016, Train: (LOSS: 0.0106, MAE: 0.0106, RMSE: 0.0175, R2: 0.9936), Valid: (LOSS: 0.0387, MAE: 0.0387, RMSE: 0.0621, R2: 0.9374), PNorm: 142.0253, GNorm: 0.9421
[227/299] timecost: 70.87, lr: 0.000016, Train: (LOSS: 0.0101, MAE: 0.0101, RMSE: 0.0165, R2: 0.9941), Valid: (LOSS: 0.0395, MAE: 0.0395, RMSE: 0.0631, R2: 0.9360), PNorm: 141.9481, GNorm: 0.9234
[228/299] timecost: 70.69, lr: 0.000016, Train: (LOSS: 0.0105, MAE: 0.0105, RMSE: 0.0170, R2: 0.9936), Valid: (LOSS: 0.0400, MAE: 0.0400, RMSE: 0.0624, R2: 0.9371), PNorm: 141.8711, GNorm: 0.6249
[229/299] timecost: 70.57, lr: 0.000016, Train: (LOSS: 0.0099, MAE: 0.0099, RMSE: 0.0161, R2: 0.9940), Valid: (LOSS: 0.0390, MAE: 0.0390, RMSE: 0.0613, R2: 0.9386), PNorm: 141.7941, GNorm: 1.2175
[230/299] timecost: 71.26, lr: 0.000016, Train: (LOSS: 0.0106, MAE: 0.0106, RMSE: 0.0170, R2: 0.9939), Valid: (LOSS: 0.0397, MAE: 0.0397, RMSE: 0.0621, R2: 0.9372), PNorm: 141.7178, GNorm: 1.0024
[231/299] timecost: 71.50, lr: 0.000016, Train: (LOSS: 0.0102, MAE: 0.0102, RMSE: 0.0165, R2: 0.9944), Valid: (LOSS: 0.0387, MAE: 0.0387, RMSE: 0.0612, R2: 0.9389), PNorm: 141.6412, GNorm: 1.3234
[232/299] timecost: 71.23, lr: 0.000016, Train: (LOSS: 0.0103, MAE: 0.0103, RMSE: 0.0165, R2: 0.9945), Valid: (LOSS: 0.0390, MAE: 0.0390, RMSE: 0.0618, R2: 0.9381), PNorm: 141.5652, GNorm: 0.9857
[233/299] timecost: 70.86, lr: 0.000016, Train: (LOSS: 0.0099, MAE: 0.0099, RMSE: 0.0161, R2: 0.9946), Valid: (LOSS: 0.0388, MAE: 0.0388, RMSE: 0.0617, R2: 0.9382), PNorm: 141.4886, GNorm: 0.8437
[234/299] timecost: 70.75, lr: 0.000016, Train: (LOSS: 0.0102, MAE: 0.0102, RMSE: 0.0163, R2: 0.9942), Valid: (LOSS: 0.0381, MAE: 0.0381, RMSE: 0.0610, R2: 0.9390), PNorm: 141.4126, GNorm: 0.9410
[235/299] timecost: 70.91, lr: 0.000016, Train: (LOSS: 0.0098, MAE: 0.0098, RMSE: 0.0158, R2: 0.9932), Valid: (LOSS: 0.0379, MAE: 0.0379, RMSE: 0.0605, R2: 0.9409), PNorm: 141.3365, GNorm: 0.9911
[236/299] timecost: 70.99, lr: 0.000016, Train: (LOSS: 0.0096, MAE: 0.0096, RMSE: 0.0160, R2: 0.9941), Valid: (LOSS: 0.0387, MAE: 0.0387, RMSE: 0.0603, R2: 0.9412), PNorm: 141.2603, GNorm: 1.0963
[237/299] timecost: 71.83, lr: 0.000016, Train: (LOSS: 0.0097, MAE: 0.0097, RMSE: 0.0159, R2: 0.9946), Valid: (LOSS: 0.0399, MAE: 0.0399, RMSE: 0.0637, R2: 0.9337), PNorm: 141.1843, GNorm: 0.7381
[238/299] timecost: 71.48, lr: 0.000016, Train: (LOSS: 0.0098, MAE: 0.0098, RMSE: 0.0160, R2: 0.9942), Valid: (LOSS: 0.0392, MAE: 0.0392, RMSE: 0.0621, R2: 0.9371), PNorm: 141.1090, GNorm: 0.9663
[239/299] timecost: 70.78, lr: 0.000016, Train: (LOSS: 0.0094, MAE: 0.0094, RMSE: 0.0156, R2: 0.9947), Valid: (LOSS: 0.0395, MAE: 0.0395, RMSE: 0.0621, R2: 0.9376), PNorm: 141.0334, GNorm: 1.0360
[240/299] timecost: 71.09, lr: 0.000016, Train: (LOSS: 0.0096, MAE: 0.0096, RMSE: 0.0157, R2: 0.9951), Valid: (LOSS: 0.0395, MAE: 0.0395, RMSE: 0.0634, R2: 0.9343), PNorm: 140.9580, GNorm: 0.8110
[241/299] timecost: 71.02, lr: 0.000016, Train: (LOSS: 0.0096, MAE: 0.0096, RMSE: 0.0155, R2: 0.9948), Valid: (LOSS: 0.0392, MAE: 0.0392, RMSE: 0.0627, R2: 0.9363), PNorm: 140.8828, GNorm: 0.8013
[242/299] timecost: 70.79, lr: 0.000016, Train: (LOSS: 0.0093, MAE: 0.0093, RMSE: 0.0153, R2: 0.9950), Valid: (LOSS: 0.0388, MAE: 0.0388, RMSE: 0.0619, R2: 0.9382), PNorm: 140.8081, GNorm: 0.8448
[243/299] timecost: 70.60, lr: 0.000016, Train: (LOSS: 0.0093, MAE: 0.0093, RMSE: 0.0153, R2: 0.9944), Valid: (LOSS: 0.0391, MAE: 0.0391, RMSE: 0.0620, R2: 0.9377), PNorm: 140.7331, GNorm: 0.9582
[244/299] timecost: 70.80, lr: 0.000016, Train: (LOSS: 0.0095, MAE: 0.0095, RMSE: 0.0155, R2: 0.9946), Valid: (LOSS: 0.0392, MAE: 0.0392, RMSE: 0.0636, R2: 0.9346), PNorm: 140.6585, GNorm: 0.7868
[245/299] timecost: 70.92, lr: 0.000016, Train: (LOSS: 0.0094, MAE: 0.0094, RMSE: 0.0153, R2: 0.9939), Valid: (LOSS: 0.0395, MAE: 0.0395, RMSE: 0.0616, R2: 0.9386), PNorm: 140.5839, GNorm: 0.6985
[246/299] timecost: 71.43, lr: 0.000016, Train: (LOSS: 0.0093, MAE: 0.0093, RMSE: 0.0152, R2: 0.9947), Valid: (LOSS: 0.0394, MAE: 0.0394, RMSE: 0.0621, R2: 0.9371), PNorm: 140.5088, GNorm: 0.8081
Epoch 00248: reducing learning rate of group 0 to 1.4349e-05.
[247/299] timecost: 71.48, lr: 0.000014, Train: (LOSS: 0.0090, MAE: 0.0090, RMSE: 0.0148, R2: 0.9947), Valid: (LOSS: 0.0385, MAE: 0.0385, RMSE: 0.0619, R2: 0.9383), PNorm: 140.4345, GNorm: 0.7822
[248/299] timecost: 71.43, lr: 0.000014, Train: (LOSS: 0.0090, MAE: 0.0090, RMSE: 0.0147, R2: 0.9952), Valid: (LOSS: 0.0393, MAE: 0.0393, RMSE: 0.0610, R2: 0.9401), PNorm: 140.3677, GNorm: 0.7396
[249/299] timecost: 71.45, lr: 0.000014, Train: (LOSS: 0.0084, MAE: 0.0084, RMSE: 0.0142, R2: 0.9955), Valid: (LOSS: 0.0398, MAE: 0.0398, RMSE: 0.0629, R2: 0.9350), PNorm: 140.3009, GNorm: 0.9395
[250/299] timecost: 70.93, lr: 0.000014, Train: (LOSS: 0.0089, MAE: 0.0089, RMSE: 0.0147, R2: 0.9940), Valid: (LOSS: 0.0396, MAE: 0.0396, RMSE: 0.0628, R2: 0.9362), PNorm: 140.2341, GNorm: 0.6764
[251/299] timecost: 71.20, lr: 0.000014, Train: (LOSS: 0.0091, MAE: 0.0091, RMSE: 0.0148, R2: 0.9955), Valid: (LOSS: 0.0396, MAE: 0.0396, RMSE: 0.0620, R2: 0.9373), PNorm: 140.1678, GNorm: 0.9596
[252/299] timecost: 70.54, lr: 0.000014, Train: (LOSS: 0.0088, MAE: 0.0088, RMSE: 0.0145, R2: 0.9951), Valid: (LOSS: 0.0392, MAE: 0.0392, RMSE: 0.0620, R2: 0.9374), PNorm: 140.1014, GNorm: 1.1942
[253/299] timecost: 71.31, lr: 0.000014, Train: (LOSS: 0.0084, MAE: 0.0084, RMSE: 0.0140, R2: 0.9954), Valid: (LOSS: 0.0385, MAE: 0.0385, RMSE: 0.0623, R2: 0.9362), PNorm: 140.0350, GNorm: 0.8156
[254/299] timecost: 70.56, lr: 0.000014, Train: (LOSS: 0.0083, MAE: 0.0083, RMSE: 0.0138, R2: 0.9950), Valid: (LOSS: 0.0403, MAE: 0.0403, RMSE: 0.0631, R2: 0.9358), PNorm: 139.9686, GNorm: 0.9412
[255/299] timecost: 70.47, lr: 0.000014, Train: (LOSS: 0.0082, MAE: 0.0082, RMSE: 0.0138, R2: 0.9956), Valid: (LOSS: 0.0409, MAE: 0.0409, RMSE: 0.0637, R2: 0.9345), PNorm: 139.9021, GNorm: 1.1172
[256/299] timecost: 71.49, lr: 0.000014, Train: (LOSS: 0.0088, MAE: 0.0088, RMSE: 0.0144, R2: 0.9954), Valid: (LOSS: 0.0387, MAE: 0.0387, RMSE: 0.0617, R2: 0.9388), PNorm: 139.8362, GNorm: 1.0058
[257/299] timecost: 70.51, lr: 0.000014, Train: (LOSS: 0.0085, MAE: 0.0085, RMSE: 0.0138, R2: 0.9954), Valid: (LOSS: 0.0400, MAE: 0.0400, RMSE: 0.0643, R2: 0.9323), PNorm: 139.7698, GNorm: 0.8000
[258/299] timecost: 70.74, lr: 0.000014, Train: (LOSS: 0.0087, MAE: 0.0087, RMSE: 0.0141, R2: 0.9956), Valid: (LOSS: 0.0405, MAE: 0.0405, RMSE: 0.0643, R2: 0.9329), PNorm: 139.7042, GNorm: 0.8923
[259/299] timecost: 71.56, lr: 0.000014, Train: (LOSS: 0.0083, MAE: 0.0083, RMSE: 0.0138, R2: 0.9950), Valid: (LOSS: 0.0400, MAE: 0.0400, RMSE: 0.0655, R2: 0.9303), PNorm: 139.6384, GNorm: 0.8263
[260/299] timecost: 70.61, lr: 0.000014, Train: (LOSS: 0.0085, MAE: 0.0085, RMSE: 0.0139, R2: 0.9954), Valid: (LOSS: 0.0394, MAE: 0.0394, RMSE: 0.0634, R2: 0.9347), PNorm: 139.5725, GNorm: 0.8843
[261/299] timecost: 70.80, lr: 0.000014, Train: (LOSS: 0.0082, MAE: 0.0082, RMSE: 0.0134, R2: 0.9959), Valid: (LOSS: 0.0400, MAE: 0.0400, RMSE: 0.0635, R2: 0.9345), PNorm: 139.5070, GNorm: 0.8998
[262/299] timecost: 71.27, lr: 0.000014, Train: (LOSS: 0.0082, MAE: 0.0082, RMSE: 0.0135, R2: 0.9956), Valid: (LOSS: 0.0390, MAE: 0.0390, RMSE: 0.0620, R2: 0.9381), PNorm: 139.4410, GNorm: 1.0889
[263/299] timecost: 71.69, lr: 0.000014, Train: (LOSS: 0.0082, MAE: 0.0082, RMSE: 0.0134, R2: 0.9960), Valid: (LOSS: 0.0397, MAE: 0.0397, RMSE: 0.0637, R2: 0.9342), PNorm: 139.3755, GNorm: 0.9007
[264/299] timecost: 71.50, lr: 0.000014, Train: (LOSS: 0.0082, MAE: 0.0082, RMSE: 0.0137, R2: 0.9957), Valid: (LOSS: 0.0396, MAE: 0.0396, RMSE: 0.0615, R2: 0.9387), PNorm: 139.3105, GNorm: 0.7719
[265/299] timecost: 71.23, lr: 0.000014, Train: (LOSS: 0.0085, MAE: 0.0085, RMSE: 0.0140, R2: 0.9959), Valid: (LOSS: 0.0388, MAE: 0.0388, RMSE: 0.0604, R2: 0.9409), PNorm: 139.2454, GNorm: 0.9203
[266/299] timecost: 70.86, lr: 0.000014, Train: (LOSS: 0.0083, MAE: 0.0083, RMSE: 0.0135, R2: 0.9956), Valid: (LOSS: 0.0393, MAE: 0.0393, RMSE: 0.0629, R2: 0.9361), PNorm: 139.1807, GNorm: 0.8894
[267/299] timecost: 71.34, lr: 0.000014, Train: (LOSS: 0.0081, MAE: 0.0081, RMSE: 0.0135, R2: 0.9960), Valid: (LOSS: 0.0392, MAE: 0.0392, RMSE: 0.0619, R2: 0.9379), PNorm: 139.1160, GNorm: 0.7648
Epoch 00269: reducing learning rate of group 0 to 1.2914e-05.
[268/299] timecost: 70.54, lr: 0.000013, Train: (LOSS: 0.0085, MAE: 0.0085, RMSE: 0.0142, R2: 0.9960), Valid: (LOSS: 0.0395, MAE: 0.0395, RMSE: 0.0626, R2: 0.9363), PNorm: 139.0518, GNorm: 0.9576
[269/299] timecost: 70.36, lr: 0.000013, Train: (LOSS: 0.0082, MAE: 0.0082, RMSE: 0.0135, R2: 0.9960), Valid: (LOSS: 0.0394, MAE: 0.0394, RMSE: 0.0623, R2: 0.9368), PNorm: 138.9941, GNorm: 0.7828
[270/299] timecost: 71.26, lr: 0.000013, Train: (LOSS: 0.0080, MAE: 0.0080, RMSE: 0.0132, R2: 0.9962), Valid: (LOSS: 0.0385, MAE: 0.0385, RMSE: 0.0609, R2: 0.9402), PNorm: 138.9366, GNorm: 0.8070
[271/299] timecost: 71.22, lr: 0.000013, Train: (LOSS: 0.0079, MAE: 0.0079, RMSE: 0.0131, R2: 0.9962), Valid: (LOSS: 0.0388, MAE: 0.0388, RMSE: 0.0622, R2: 0.9373), PNorm: 138.8790, GNorm: 0.8542
[272/299] timecost: 70.89, lr: 0.000013, Train: (LOSS: 0.0079, MAE: 0.0079, RMSE: 0.0131, R2: 0.9955), Valid: (LOSS: 0.0384, MAE: 0.0384, RMSE: 0.0604, R2: 0.9414), PNorm: 138.8219, GNorm: 1.2100
[273/299] timecost: 71.45, lr: 0.000013, Train: (LOSS: 0.0076, MAE: 0.0076, RMSE: 0.0130, R2: 0.9956), Valid: (LOSS: 0.0395, MAE: 0.0395, RMSE: 0.0631, R2: 0.9354), PNorm: 138.7644, GNorm: 0.8591
[274/299] timecost: 71.11, lr: 0.000013, Train: (LOSS: 0.0075, MAE: 0.0075, RMSE: 0.0125, R2: 0.9962), Valid: (LOSS: 0.0397, MAE: 0.0397, RMSE: 0.0632, R2: 0.9353), PNorm: 138.7072, GNorm: 0.8908
[275/299] timecost: 71.20, lr: 0.000013, Train: (LOSS: 0.0083, MAE: 0.0083, RMSE: 0.0136, R2: 0.9962), Valid: (LOSS: 0.0398, MAE: 0.0398, RMSE: 0.0627, R2: 0.9362), PNorm: 138.6506, GNorm: 0.8642
[276/299] timecost: 70.99, lr: 0.000013, Train: (LOSS: 0.0078, MAE: 0.0078, RMSE: 0.0128, R2: 0.9963), Valid: (LOSS: 0.0401, MAE: 0.0401, RMSE: 0.0639, R2: 0.9341), PNorm: 138.5936, GNorm: 0.9544
[277/299] timecost: 70.50, lr: 0.000013, Train: (LOSS: 0.0077, MAE: 0.0077, RMSE: 0.0128, R2: 0.9959), Valid: (LOSS: 0.0405, MAE: 0.0405, RMSE: 0.0643, R2: 0.9332), PNorm: 138.5374, GNorm: 0.9893
[278/299] timecost: 70.69, lr: 0.000013, Train: (LOSS: 0.0074, MAE: 0.0074, RMSE: 0.0124, R2: 0.9962), Valid: (LOSS: 0.0393, MAE: 0.0393, RMSE: 0.0625, R2: 0.9365), PNorm: 138.4810, GNorm: 0.7520
[279/299] timecost: 70.69, lr: 0.000013, Train: (LOSS: 0.0075, MAE: 0.0075, RMSE: 0.0125, R2: 0.9961), Valid: (LOSS: 0.0389, MAE: 0.0389, RMSE: 0.0622, R2: 0.9373), PNorm: 138.4248, GNorm: 0.7113
[280/299] timecost: 71.31, lr: 0.000013, Train: (LOSS: 0.0074, MAE: 0.0074, RMSE: 0.0124, R2: 0.9964), Valid: (LOSS: 0.0400, MAE: 0.0400, RMSE: 0.0639, R2: 0.9332), PNorm: 138.3685, GNorm: 0.6678
[281/299] timecost: 71.29, lr: 0.000013, Train: (LOSS: 0.0072, MAE: 0.0072, RMSE: 0.0123, R2: 0.9966), Valid: (LOSS: 0.0404, MAE: 0.0404, RMSE: 0.0643, R2: 0.9330), PNorm: 138.3127, GNorm: 0.8070
[282/299] timecost: 70.99, lr: 0.000013, Train: (LOSS: 0.0075, MAE: 0.0075, RMSE: 0.0125, R2: 0.9963), Valid: (LOSS: 0.0397, MAE: 0.0397, RMSE: 0.0629, R2: 0.9363), PNorm: 138.2566, GNorm: 0.8780
[283/299] timecost: 70.93, lr: 0.000013, Train: (LOSS: 0.0072, MAE: 0.0072, RMSE: 0.0123, R2: 0.9964), Valid: (LOSS: 0.0394, MAE: 0.0394, RMSE: 0.0622, R2: 0.9376), PNorm: 138.2006, GNorm: 0.9468
[284/299] timecost: 70.81, lr: 0.000013, Train: (LOSS: 0.0073, MAE: 0.0073, RMSE: 0.0122, R2: 0.9965), Valid: (LOSS: 0.0402, MAE: 0.0402, RMSE: 0.0636, R2: 0.9345), PNorm: 138.1443, GNorm: 0.8393
[285/299] timecost: 70.32, lr: 0.000013, Train: (LOSS: 0.0072, MAE: 0.0072, RMSE: 0.0120, R2: 0.9964), Valid: (LOSS: 0.0397, MAE: 0.0397, RMSE: 0.0625, R2: 0.9368), PNorm: 138.0886, GNorm: 0.8043
[286/299] timecost: 70.30, lr: 0.000013, Train: (LOSS: 0.0075, MAE: 0.0075, RMSE: 0.0123, R2: 0.9963), Valid: (LOSS: 0.0395, MAE: 0.0395, RMSE: 0.0627, R2: 0.9363), PNorm: 138.0330, GNorm: 1.1708
[287/299] timecost: 70.26, lr: 0.000013, Train: (LOSS: 0.0073, MAE: 0.0073, RMSE: 0.0121, R2: 0.9965), Valid: (LOSS: 0.0394, MAE: 0.0394, RMSE: 0.0619, R2: 0.9379), PNorm: 137.9776, GNorm: 0.7702
[288/299] timecost: 70.63, lr: 0.000013, Train: (LOSS: 0.0074, MAE: 0.0074, RMSE: 0.0122, R2: 0.9966), Valid: (LOSS: 0.0400, MAE: 0.0400, RMSE: 0.0635, R2: 0.9347), PNorm: 137.9215, GNorm: 0.7827
Epoch 00290: reducing learning rate of group 0 to 1.1623e-05.
[289/299] timecost: 71.32, lr: 0.000012, Train: (LOSS: 0.0076, MAE: 0.0076, RMSE: 0.0123, R2: 0.9966), Valid: (LOSS: 0.0393, MAE: 0.0393, RMSE: 0.0630, R2: 0.9356), PNorm: 137.8667, GNorm: 0.8163
[290/299] timecost: 71.42, lr: 0.000012, Train: (LOSS: 0.0072, MAE: 0.0072, RMSE: 0.0120, R2: 0.9967), Valid: (LOSS: 0.0401, MAE: 0.0401, RMSE: 0.0629, R2: 0.9359), PNorm: 137.8168, GNorm: 0.8025
[291/299] timecost: 71.22, lr: 0.000012, Train: (LOSS: 0.0070, MAE: 0.0070, RMSE: 0.0116, R2: 0.9964), Valid: (LOSS: 0.0391, MAE: 0.0391, RMSE: 0.0630, R2: 0.9353), PNorm: 137.7672, GNorm: 0.9200
[292/299] timecost: 70.73, lr: 0.000012, Train: (LOSS: 0.0072, MAE: 0.0072, RMSE: 0.0121, R2: 0.9963), Valid: (LOSS: 0.0394, MAE: 0.0394, RMSE: 0.0628, R2: 0.9363), PNorm: 137.7176, GNorm: 0.6602
[293/299] timecost: 70.22, lr: 0.000012, Train: (LOSS: 0.0070, MAE: 0.0070, RMSE: 0.0116, R2: 0.9965), Valid: (LOSS: 0.0403, MAE: 0.0403, RMSE: 0.0630, R2: 0.9357), PNorm: 137.6681, GNorm: 0.7969
[294/299] timecost: 70.89, lr: 0.000012, Train: (LOSS: 0.0068, MAE: 0.0068, RMSE: 0.0116, R2: 0.9964), Valid: (LOSS: 0.0389, MAE: 0.0389, RMSE: 0.0623, R2: 0.9373), PNorm: 137.6189, GNorm: 0.7393
[295/299] timecost: 70.83, lr: 0.000012, Train: (LOSS: 0.0068, MAE: 0.0068, RMSE: 0.0114, R2: 0.9969), Valid: (LOSS: 0.0384, MAE: 0.0384, RMSE: 0.0617, R2: 0.9376), PNorm: 137.5699, GNorm: 0.7768
[296/299] timecost: 70.64, lr: 0.000012, Train: (LOSS: 0.0066, MAE: 0.0066, RMSE: 0.0114, R2: 0.9969), Valid: (LOSS: 0.0393, MAE: 0.0393, RMSE: 0.0629, R2: 0.9358), PNorm: 137.5206, GNorm: 0.5685
[297/299] timecost: 71.04, lr: 0.000012, Train: (LOSS: 0.0070, MAE: 0.0070, RMSE: 0.0116, R2: 0.9965), Valid: (LOSS: 0.0393, MAE: 0.0393, RMSE: 0.0622, R2: 0.9371), PNorm: 137.4713, GNorm: 0.6086
[298/299] timecost: 71.73, lr: 0.000012, Train: (LOSS: 0.0069, MAE: 0.0069, RMSE: 0.0115, R2: 0.9970), Valid: (LOSS: 0.0398, MAE: 0.0398, RMSE: 0.0632, R2: 0.9350), PNorm: 137.4224, GNorm: 0.8146
[299/299] timecost: 71.53, lr: 0.000012, Train: (LOSS: 0.0067, MAE: 0.0067, RMSE: 0.0112, R2: 0.9971), Valid: (LOSS: 0.0399, MAE: 0.0399, RMSE: 0.0633, R2: 0.9352), PNorm: 137.3733, GNorm: 0.5280
==========Training End==========
==========Test Best Model==========
================Final Results=======================
mse: 0.0326 +- 0.0000:
rmse: 0.0498 +- 0.0000:
mae: 0.0326 +- 0.0000:
r2: 0.9618 +- 0.0000:
tensor([[0.1073, 0.1066],
        [0.1352, 0.1475],
        [0.0000, 0.0000],
        ...,
        [0.0000, 0.0000],
        [0.4466, 0.4354],
        [0.0000, 0.0000]], device='cuda:0')
