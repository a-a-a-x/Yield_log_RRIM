cuda available with GPU: Tesla V100-PCIE-16GB
==========Load Seed==========
set_random_seed
0
==========Training Start==========
Training Graphs:  2491
Valid Graphs:  277
Test Graphs:  1187
============Not pretrained weights used============
============Creating new layers============
============Creating Model============
Training Graphs Batches:  78
Valid Graphs Batches:  9
Test Graphs Batches:  37
[0/299] timecost: 62.68, lr: 0.000100, Train: (LOSS: 0.2700, MAE: 0.2700, RMSE: 0.3305, R2: -0.6182), Valid: (LOSS: 0.4030, MAE: 0.4030, RMSE: 0.4542, R2: -2.2405), PNorm: 187.0264, GNorm: 0.5000
[1/299] timecost: 59.20, lr: 0.000100, Train: (LOSS: 0.2190, MAE: 0.2190, RMSE: 0.2735, R2: -0.0855), Valid: (LOSS: 0.1562, MAE: 0.1562, RMSE: 0.1972, R2: 0.3969), PNorm: 187.0620, GNorm: 0.5000
[2/299] timecost: 59.16, lr: 0.000100, Train: (LOSS: 0.1778, MAE: 0.1778, RMSE: 0.2284, R2: 0.2750), Valid: (LOSS: 0.1391, MAE: 0.1391, RMSE: 0.1832, R2: 0.4761), PNorm: 187.0916, GNorm: 0.5000
[3/299] timecost: 58.66, lr: 0.000100, Train: (LOSS: 0.1589, MAE: 0.1589, RMSE: 0.2080, R2: 0.3832), Valid: (LOSS: 0.1279, MAE: 0.1279, RMSE: 0.1785, R2: 0.4990), PNorm: 187.1413, GNorm: 0.3917
[4/299] timecost: 58.52, lr: 0.000100, Train: (LOSS: 0.1439, MAE: 0.1439, RMSE: 0.1943, R2: 0.4616), Valid: (LOSS: 0.1348, MAE: 0.1348, RMSE: 0.1786, R2: 0.5026), PNorm: 187.1726, GNorm: 0.4642
[5/299] timecost: 58.48, lr: 0.000100, Train: (LOSS: 0.1349, MAE: 0.1349, RMSE: 0.1832, R2: 0.5092), Valid: (LOSS: 0.1324, MAE: 0.1324, RMSE: 0.1764, R2: 0.5129), PNorm: 187.2009, GNorm: 0.5000
[6/299] timecost: 58.57, lr: 0.000100, Train: (LOSS: 0.1321, MAE: 0.1321, RMSE: 0.1802, R2: 0.5406), Valid: (LOSS: 0.1258, MAE: 0.1258, RMSE: 0.1815, R2: 0.4746), PNorm: 187.2313, GNorm: 0.5000
[7/299] timecost: 58.90, lr: 0.000100, Train: (LOSS: 0.1324, MAE: 0.1324, RMSE: 0.1823, R2: 0.5024), Valid: (LOSS: 0.1354, MAE: 0.1354, RMSE: 0.1892, R2: 0.4340), PNorm: 187.2539, GNorm: 0.5000
[8/299] timecost: 58.48, lr: 0.000100, Train: (LOSS: 0.1306, MAE: 0.1306, RMSE: 0.1806, R2: 0.5311), Valid: (LOSS: 0.1238, MAE: 0.1238, RMSE: 0.1754, R2: 0.5142), PNorm: 187.2839, GNorm: 0.5000
[9/299] timecost: 58.76, lr: 0.000100, Train: (LOSS: 0.1235, MAE: 0.1235, RMSE: 0.1693, R2: 0.5852), Valid: (LOSS: 0.1180, MAE: 0.1180, RMSE: 0.1668, R2: 0.5612), PNorm: 187.3086, GNorm: 0.5000
[10/299] timecost: 58.91, lr: 0.000100, Train: (LOSS: 0.1198, MAE: 0.1198, RMSE: 0.1665, R2: 0.6029), Valid: (LOSS: 0.1064, MAE: 0.1064, RMSE: 0.1520, R2: 0.6402), PNorm: 187.3479, GNorm: 0.5000
[11/299] timecost: 58.38, lr: 0.000100, Train: (LOSS: 0.1140, MAE: 0.1140, RMSE: 0.1601, R2: 0.6167), Valid: (LOSS: 0.1093, MAE: 0.1093, RMSE: 0.1570, R2: 0.6138), PNorm: 187.3851, GNorm: 0.4319
[12/299] timecost: 58.96, lr: 0.000100, Train: (LOSS: 0.1059, MAE: 0.1059, RMSE: 0.1505, R2: 0.6652), Valid: (LOSS: 0.1022, MAE: 0.1022, RMSE: 0.1412, R2: 0.6886), PNorm: 187.4199, GNorm: 0.5000
[13/299] timecost: 59.00, lr: 0.000100, Train: (LOSS: 0.1033, MAE: 0.1033, RMSE: 0.1483, R2: 0.6732), Valid: (LOSS: 0.1003, MAE: 0.1003, RMSE: 0.1454, R2: 0.6671), PNorm: 187.4467, GNorm: 0.5000
[14/299] timecost: 59.76, lr: 0.000100, Train: (LOSS: 0.0977, MAE: 0.0977, RMSE: 0.1417, R2: 0.6994), Valid: (LOSS: 0.0906, MAE: 0.0906, RMSE: 0.1347, R2: 0.7185), PNorm: 187.4694, GNorm: 0.5000
[15/299] timecost: 59.58, lr: 0.000100, Train: (LOSS: 0.0952, MAE: 0.0952, RMSE: 0.1407, R2: 0.7068), Valid: (LOSS: 0.0870, MAE: 0.0870, RMSE: 0.1334, R2: 0.7168), PNorm: 187.4890, GNorm: 0.5000
[16/299] timecost: 59.55, lr: 0.000100, Train: (LOSS: 0.0925, MAE: 0.0925, RMSE: 0.1342, R2: 0.7313), Valid: (LOSS: 0.0922, MAE: 0.0922, RMSE: 0.1300, R2: 0.7313), PNorm: 187.5062, GNorm: 0.5000
[17/299] timecost: 59.67, lr: 0.000100, Train: (LOSS: 0.0904, MAE: 0.0904, RMSE: 0.1313, R2: 0.7424), Valid: (LOSS: 0.0839, MAE: 0.0839, RMSE: 0.1244, R2: 0.7573), PNorm: 187.5244, GNorm: 0.5000
[18/299] timecost: 59.48, lr: 0.000100, Train: (LOSS: 0.0846, MAE: 0.0846, RMSE: 0.1232, R2: 0.7640), Valid: (LOSS: 0.0788, MAE: 0.0788, RMSE: 0.1114, R2: 0.8017), PNorm: 187.5438, GNorm: 0.5000
[19/299] timecost: 59.50, lr: 0.000100, Train: (LOSS: 0.0822, MAE: 0.0822, RMSE: 0.1222, R2: 0.7814), Valid: (LOSS: 0.0780, MAE: 0.0780, RMSE: 0.1144, R2: 0.7930), PNorm: 187.5639, GNorm: 0.5000
[20/299] timecost: 59.72, lr: 0.000100, Train: (LOSS: 0.0839, MAE: 0.0839, RMSE: 0.1226, R2: 0.7719), Valid: (LOSS: 0.0819, MAE: 0.0819, RMSE: 0.1180, R2: 0.7815), PNorm: 187.5815, GNorm: 0.5000
[21/299] timecost: 59.62, lr: 0.000100, Train: (LOSS: 0.0796, MAE: 0.0796, RMSE: 0.1182, R2: 0.7848), Valid: (LOSS: 0.0716, MAE: 0.0716, RMSE: 0.1080, R2: 0.8164), PNorm: 187.6006, GNorm: 0.5000
[22/299] timecost: 59.53, lr: 0.000100, Train: (LOSS: 0.0766, MAE: 0.0766, RMSE: 0.1149, R2: 0.8037), Valid: (LOSS: 0.0793, MAE: 0.0793, RMSE: 0.1155, R2: 0.7907), PNorm: 187.6203, GNorm: 0.5000
[23/299] timecost: 59.50, lr: 0.000100, Train: (LOSS: 0.0728, MAE: 0.0728, RMSE: 0.1088, R2: 0.8225), Valid: (LOSS: 0.0817, MAE: 0.0817, RMSE: 0.1194, R2: 0.7750), PNorm: 187.6371, GNorm: 0.5000
[24/299] timecost: 59.34, lr: 0.000100, Train: (LOSS: 0.0705, MAE: 0.0705, RMSE: 0.1077, R2: 0.8188), Valid: (LOSS: 0.0692, MAE: 0.0692, RMSE: 0.1026, R2: 0.8312), PNorm: 187.6561, GNorm: 0.5000
[25/299] timecost: 59.46, lr: 0.000100, Train: (LOSS: 0.0673, MAE: 0.0673, RMSE: 0.1036, R2: 0.8362), Valid: (LOSS: 0.0682, MAE: 0.0682, RMSE: 0.0985, R2: 0.8461), PNorm: 187.6731, GNorm: 0.5000
[26/299] timecost: 59.65, lr: 0.000100, Train: (LOSS: 0.0656, MAE: 0.0656, RMSE: 0.1001, R2: 0.8437), Valid: (LOSS: 0.0612, MAE: 0.0612, RMSE: 0.0911, R2: 0.8690), PNorm: 187.6898, GNorm: 0.5000
[27/299] timecost: 59.59, lr: 0.000100, Train: (LOSS: 0.0609, MAE: 0.0609, RMSE: 0.0928, R2: 0.8640), Valid: (LOSS: 0.0620, MAE: 0.0620, RMSE: 0.0939, R2: 0.8600), PNorm: 187.7051, GNorm: 0.5000
[28/299] timecost: 59.49, lr: 0.000100, Train: (LOSS: 0.0582, MAE: 0.0582, RMSE: 0.0897, R2: 0.8752), Valid: (LOSS: 0.0617, MAE: 0.0617, RMSE: 0.0926, R2: 0.8653), PNorm: 187.7235, GNorm: 0.5000
[29/299] timecost: 59.37, lr: 0.000100, Train: (LOSS: 0.0616, MAE: 0.0616, RMSE: 0.0939, R2: 0.8663), Valid: (LOSS: 0.0669, MAE: 0.0669, RMSE: 0.0974, R2: 0.8470), PNorm: 187.7414, GNorm: 0.5000
[30/299] timecost: 59.93, lr: 0.000100, Train: (LOSS: 0.0578, MAE: 0.0578, RMSE: 0.0885, R2: 0.8813), Valid: (LOSS: 0.0590, MAE: 0.0590, RMSE: 0.0887, R2: 0.8771), PNorm: 187.7566, GNorm: 0.5000
[31/299] timecost: 59.82, lr: 0.000100, Train: (LOSS: 0.0544, MAE: 0.0544, RMSE: 0.0844, R2: 0.8861), Valid: (LOSS: 0.0644, MAE: 0.0644, RMSE: 0.0935, R2: 0.8594), PNorm: 187.7748, GNorm: 0.5000
[32/299] timecost: 58.62, lr: 0.000100, Train: (LOSS: 0.0535, MAE: 0.0535, RMSE: 0.0855, R2: 0.8887), Valid: (LOSS: 0.0567, MAE: 0.0567, RMSE: 0.0828, R2: 0.8906), PNorm: 187.7922, GNorm: 0.5000
[33/299] timecost: 58.39, lr: 0.000100, Train: (LOSS: 0.0523, MAE: 0.0523, RMSE: 0.0817, R2: 0.8963), Valid: (LOSS: 0.0561, MAE: 0.0561, RMSE: 0.0830, R2: 0.8918), PNorm: 187.8101, GNorm: 0.5000
[34/299] timecost: 58.27, lr: 0.000100, Train: (LOSS: 0.0514, MAE: 0.0514, RMSE: 0.0803, R2: 0.9017), Valid: (LOSS: 0.0553, MAE: 0.0553, RMSE: 0.0806, R2: 0.8964), PNorm: 187.8298, GNorm: 0.5000
[35/299] timecost: 59.33, lr: 0.000100, Train: (LOSS: 0.0505, MAE: 0.0505, RMSE: 0.0811, R2: 0.8974), Valid: (LOSS: 0.0511, MAE: 0.0511, RMSE: 0.0744, R2: 0.9126), PNorm: 187.8474, GNorm: 0.5000
[36/299] timecost: 59.81, lr: 0.000100, Train: (LOSS: 0.0466, MAE: 0.0466, RMSE: 0.0740, R2: 0.9164), Valid: (LOSS: 0.0637, MAE: 0.0637, RMSE: 0.0873, R2: 0.8812), PNorm: 187.8595, GNorm: 0.5000
[37/299] timecost: 60.11, lr: 0.000100, Train: (LOSS: 0.0472, MAE: 0.0472, RMSE: 0.0735, R2: 0.9155), Valid: (LOSS: 0.0522, MAE: 0.0522, RMSE: 0.0758, R2: 0.9106), PNorm: 187.8749, GNorm: 0.5000
[38/299] timecost: 60.35, lr: 0.000100, Train: (LOSS: 0.0442, MAE: 0.0442, RMSE: 0.0701, R2: 0.9243), Valid: (LOSS: 0.0500, MAE: 0.0500, RMSE: 0.0689, R2: 0.9245), PNorm: 187.8871, GNorm: 0.5000
[39/299] timecost: 60.60, lr: 0.000100, Train: (LOSS: 0.0459, MAE: 0.0459, RMSE: 0.0722, R2: 0.9183), Valid: (LOSS: 0.0492, MAE: 0.0492, RMSE: 0.0697, R2: 0.9221), PNorm: 187.9029, GNorm: 0.5000
[40/299] timecost: 59.13, lr: 0.000100, Train: (LOSS: 0.0444, MAE: 0.0444, RMSE: 0.0691, R2: 0.9241), Valid: (LOSS: 0.0515, MAE: 0.0515, RMSE: 0.0734, R2: 0.9148), PNorm: 187.9191, GNorm: 0.5000
[41/299] timecost: 58.02, lr: 0.000100, Train: (LOSS: 0.0441, MAE: 0.0441, RMSE: 0.0681, R2: 0.9232), Valid: (LOSS: 0.0496, MAE: 0.0496, RMSE: 0.0685, R2: 0.9255), PNorm: 187.9322, GNorm: 0.5000
[42/299] timecost: 58.09, lr: 0.000100, Train: (LOSS: 0.0436, MAE: 0.0436, RMSE: 0.0685, R2: 0.9232), Valid: (LOSS: 0.0501, MAE: 0.0501, RMSE: 0.0686, R2: 0.9240), PNorm: 187.9474, GNorm: 0.5000
[43/299] timecost: 59.69, lr: 0.000100, Train: (LOSS: 0.0416, MAE: 0.0416, RMSE: 0.0651, R2: 0.9334), Valid: (LOSS: 0.0459, MAE: 0.0459, RMSE: 0.0672, R2: 0.9276), PNorm: 187.9631, GNorm: 0.5000
[44/299] timecost: 59.21, lr: 0.000100, Train: (LOSS: 0.0415, MAE: 0.0415, RMSE: 0.0639, R2: 0.9339), Valid: (LOSS: 0.0480, MAE: 0.0480, RMSE: 0.0680, R2: 0.9268), PNorm: 187.9773, GNorm: 0.5000
[45/299] timecost: 59.10, lr: 0.000100, Train: (LOSS: 0.0405, MAE: 0.0405, RMSE: 0.0633, R2: 0.9355), Valid: (LOSS: 0.0447, MAE: 0.0447, RMSE: 0.0632, R2: 0.9363), PNorm: 187.9901, GNorm: 0.5000
[46/299] timecost: 59.10, lr: 0.000100, Train: (LOSS: 0.0382, MAE: 0.0382, RMSE: 0.0600, R2: 0.9419), Valid: (LOSS: 0.0457, MAE: 0.0457, RMSE: 0.0650, R2: 0.9332), PNorm: 188.0026, GNorm: 0.5000
[47/299] timecost: 59.31, lr: 0.000100, Train: (LOSS: 0.0395, MAE: 0.0395, RMSE: 0.0616, R2: 0.9424), Valid: (LOSS: 0.0473, MAE: 0.0473, RMSE: 0.0641, R2: 0.9351), PNorm: 188.0172, GNorm: 0.4706
[48/299] timecost: 59.03, lr: 0.000100, Train: (LOSS: 0.0369, MAE: 0.0369, RMSE: 0.0582, R2: 0.9438), Valid: (LOSS: 0.0440, MAE: 0.0440, RMSE: 0.0637, R2: 0.9360), PNorm: 188.0308, GNorm: 0.5000
[49/299] timecost: 58.81, lr: 0.000100, Train: (LOSS: 0.0376, MAE: 0.0376, RMSE: 0.0584, R2: 0.9439), Valid: (LOSS: 0.0472, MAE: 0.0472, RMSE: 0.0653, R2: 0.9328), PNorm: 188.0460, GNorm: 0.5000
[50/299] timecost: 59.07, lr: 0.000100, Train: (LOSS: 0.0373, MAE: 0.0373, RMSE: 0.0581, R2: 0.9462), Valid: (LOSS: 0.0478, MAE: 0.0478, RMSE: 0.0706, R2: 0.9193), PNorm: 188.0605, GNorm: 0.5000
[51/299] timecost: 59.51, lr: 0.000100, Train: (LOSS: 0.0366, MAE: 0.0366, RMSE: 0.0577, R2: 0.9474), Valid: (LOSS: 0.0485, MAE: 0.0485, RMSE: 0.0700, R2: 0.9213), PNorm: 188.0720, GNorm: 0.5000
[52/299] timecost: 59.17, lr: 0.000100, Train: (LOSS: 0.0370, MAE: 0.0370, RMSE: 0.0583, R2: 0.9460), Valid: (LOSS: 0.0446, MAE: 0.0446, RMSE: 0.0628, R2: 0.9381), PNorm: 188.0839, GNorm: 0.4097
[53/299] timecost: 59.16, lr: 0.000100, Train: (LOSS: 0.0356, MAE: 0.0356, RMSE: 0.0555, R2: 0.9513), Valid: (LOSS: 0.0518, MAE: 0.0518, RMSE: 0.0742, R2: 0.9110), PNorm: 188.0964, GNorm: 0.4239
[54/299] timecost: 59.21, lr: 0.000100, Train: (LOSS: 0.0341, MAE: 0.0341, RMSE: 0.0534, R2: 0.9554), Valid: (LOSS: 0.0457, MAE: 0.0457, RMSE: 0.0669, R2: 0.9269), PNorm: 188.1118, GNorm: 0.5000
[55/299] timecost: 59.67, lr: 0.000100, Train: (LOSS: 0.0344, MAE: 0.0344, RMSE: 0.0537, R2: 0.9546), Valid: (LOSS: 0.0462, MAE: 0.0462, RMSE: 0.0644, R2: 0.9342), PNorm: 188.1257, GNorm: 0.5000
[56/299] timecost: 58.77, lr: 0.000100, Train: (LOSS: 0.0332, MAE: 0.0332, RMSE: 0.0524, R2: 0.9556), Valid: (LOSS: 0.0449, MAE: 0.0449, RMSE: 0.0656, R2: 0.9300), PNorm: 188.1401, GNorm: 0.4792
[57/299] timecost: 58.36, lr: 0.000100, Train: (LOSS: 0.0330, MAE: 0.0330, RMSE: 0.0520, R2: 0.9561), Valid: (LOSS: 0.0460, MAE: 0.0460, RMSE: 0.0636, R2: 0.9364), PNorm: 188.1511, GNorm: 0.5000
[58/299] timecost: 58.69, lr: 0.000100, Train: (LOSS: 0.0322, MAE: 0.0322, RMSE: 0.0510, R2: 0.9577), Valid: (LOSS: 0.0426, MAE: 0.0426, RMSE: 0.0608, R2: 0.9421), PNorm: 188.1628, GNorm: 0.5000
[59/299] timecost: 58.58, lr: 0.000100, Train: (LOSS: 0.0308, MAE: 0.0308, RMSE: 0.0486, R2: 0.9616), Valid: (LOSS: 0.0430, MAE: 0.0430, RMSE: 0.0610, R2: 0.9413), PNorm: 188.1754, GNorm: 0.4344
[60/299] timecost: 58.86, lr: 0.000100, Train: (LOSS: 0.0311, MAE: 0.0311, RMSE: 0.0481, R2: 0.9609), Valid: (LOSS: 0.0427, MAE: 0.0427, RMSE: 0.0625, R2: 0.9360), PNorm: 188.1889, GNorm: 0.5000
[61/299] timecost: 59.11, lr: 0.000100, Train: (LOSS: 0.0326, MAE: 0.0326, RMSE: 0.0503, R2: 0.9595), Valid: (LOSS: 0.0414, MAE: 0.0414, RMSE: 0.0605, R2: 0.9418), PNorm: 188.2021, GNorm: 0.5000
[62/299] timecost: 58.61, lr: 0.000100, Train: (LOSS: 0.0302, MAE: 0.0302, RMSE: 0.0475, R2: 0.9625), Valid: (LOSS: 0.0399, MAE: 0.0399, RMSE: 0.0572, R2: 0.9488), PNorm: 188.2161, GNorm: 0.5000
[63/299] timecost: 59.12, lr: 0.000100, Train: (LOSS: 0.0305, MAE: 0.0305, RMSE: 0.0473, R2: 0.9632), Valid: (LOSS: 0.0411, MAE: 0.0411, RMSE: 0.0592, R2: 0.9452), PNorm: 188.2258, GNorm: 0.5000
[64/299] timecost: 58.68, lr: 0.000100, Train: (LOSS: 0.0286, MAE: 0.0286, RMSE: 0.0451, R2: 0.9669), Valid: (LOSS: 0.0418, MAE: 0.0418, RMSE: 0.0606, R2: 0.9414), PNorm: 188.2395, GNorm: 0.5000
[65/299] timecost: 58.87, lr: 0.000100, Train: (LOSS: 0.0299, MAE: 0.0299, RMSE: 0.0464, R2: 0.9658), Valid: (LOSS: 0.0416, MAE: 0.0416, RMSE: 0.0590, R2: 0.9447), PNorm: 188.2545, GNorm: 0.3941
[66/299] timecost: 58.90, lr: 0.000100, Train: (LOSS: 0.0287, MAE: 0.0287, RMSE: 0.0438, R2: 0.9693), Valid: (LOSS: 0.0409, MAE: 0.0409, RMSE: 0.0599, R2: 0.9423), PNorm: 188.2663, GNorm: 0.4909
[67/299] timecost: 60.30, lr: 0.000100, Train: (LOSS: 0.0289, MAE: 0.0289, RMSE: 0.0446, R2: 0.9675), Valid: (LOSS: 0.0395, MAE: 0.0395, RMSE: 0.0576, R2: 0.9467), PNorm: 188.2795, GNorm: 0.5000
[68/299] timecost: 60.37, lr: 0.000100, Train: (LOSS: 0.0277, MAE: 0.0277, RMSE: 0.0430, R2: 0.9708), Valid: (LOSS: 0.0411, MAE: 0.0411, RMSE: 0.0601, R2: 0.9428), PNorm: 188.2919, GNorm: 0.5000
[69/299] timecost: 59.29, lr: 0.000100, Train: (LOSS: 0.0271, MAE: 0.0271, RMSE: 0.0413, R2: 0.9723), Valid: (LOSS: 0.0418, MAE: 0.0418, RMSE: 0.0600, R2: 0.9426), PNorm: 188.3034, GNorm: 0.5000
[70/299] timecost: 59.55, lr: 0.000100, Train: (LOSS: 0.0274, MAE: 0.0274, RMSE: 0.0424, R2: 0.9716), Valid: (LOSS: 0.0436, MAE: 0.0436, RMSE: 0.0650, R2: 0.9328), PNorm: 188.3161, GNorm: 0.4707
[71/299] timecost: 59.49, lr: 0.000100, Train: (LOSS: 0.0271, MAE: 0.0271, RMSE: 0.0418, R2: 0.9720), Valid: (LOSS: 0.0419, MAE: 0.0419, RMSE: 0.0611, R2: 0.9387), PNorm: 188.3295, GNorm: 0.5000
[72/299] timecost: 59.36, lr: 0.000100, Train: (LOSS: 0.0265, MAE: 0.0265, RMSE: 0.0408, R2: 0.9738), Valid: (LOSS: 0.0423, MAE: 0.0423, RMSE: 0.0607, R2: 0.9416), PNorm: 188.3436, GNorm: 0.4131
[73/299] timecost: 59.27, lr: 0.000100, Train: (LOSS: 0.0263, MAE: 0.0263, RMSE: 0.0407, R2: 0.9729), Valid: (LOSS: 0.0421, MAE: 0.0421, RMSE: 0.0627, R2: 0.9359), PNorm: 188.3549, GNorm: 0.4617
[74/299] timecost: 59.15, lr: 0.000100, Train: (LOSS: 0.0255, MAE: 0.0255, RMSE: 0.0397, R2: 0.9730), Valid: (LOSS: 0.0417, MAE: 0.0417, RMSE: 0.0618, R2: 0.9390), PNorm: 188.3700, GNorm: 0.5000
[75/299] timecost: 58.04, lr: 0.000100, Train: (LOSS: 0.0251, MAE: 0.0251, RMSE: 0.0392, R2: 0.9742), Valid: (LOSS: 0.0412, MAE: 0.0412, RMSE: 0.0610, R2: 0.9401), PNorm: 188.3811, GNorm: 0.4857
[76/299] timecost: 58.80, lr: 0.000100, Train: (LOSS: 0.0251, MAE: 0.0251, RMSE: 0.0385, R2: 0.9754), Valid: (LOSS: 0.0417, MAE: 0.0417, RMSE: 0.0607, R2: 0.9413), PNorm: 188.3919, GNorm: 0.5000
[77/299] timecost: 59.58, lr: 0.000100, Train: (LOSS: 0.0243, MAE: 0.0243, RMSE: 0.0379, R2: 0.9767), Valid: (LOSS: 0.0421, MAE: 0.0421, RMSE: 0.0619, R2: 0.9401), PNorm: 188.4038, GNorm: 0.5000
[78/299] timecost: 61.04, lr: 0.000100, Train: (LOSS: 0.0251, MAE: 0.0251, RMSE: 0.0386, R2: 0.9753), Valid: (LOSS: 0.0430, MAE: 0.0430, RMSE: 0.0614, R2: 0.9402), PNorm: 188.4172, GNorm: 0.5000
[79/299] timecost: 62.30, lr: 0.000100, Train: (LOSS: 0.0243, MAE: 0.0243, RMSE: 0.0376, R2: 0.9757), Valid: (LOSS: 0.0404, MAE: 0.0404, RMSE: 0.0589, R2: 0.9444), PNorm: 188.4263, GNorm: 0.4773
[80/299] timecost: 62.32, lr: 0.000100, Train: (LOSS: 0.0236, MAE: 0.0236, RMSE: 0.0367, R2: 0.9767), Valid: (LOSS: 0.0390, MAE: 0.0390, RMSE: 0.0577, R2: 0.9460), PNorm: 188.4385, GNorm: 0.4489
[81/299] timecost: 62.34, lr: 0.000100, Train: (LOSS: 0.0248, MAE: 0.0248, RMSE: 0.0381, R2: 0.9762), Valid: (LOSS: 0.0400, MAE: 0.0400, RMSE: 0.0595, R2: 0.9442), PNorm: 188.4524, GNorm: 0.4952
[82/299] timecost: 62.24, lr: 0.000100, Train: (LOSS: 0.0244, MAE: 0.0244, RMSE: 0.0377, R2: 0.9760), Valid: (LOSS: 0.0397, MAE: 0.0397, RMSE: 0.0579, R2: 0.9469), PNorm: 188.4645, GNorm: 0.4816
[83/299] timecost: 62.31, lr: 0.000100, Train: (LOSS: 0.0240, MAE: 0.0240, RMSE: 0.0376, R2: 0.9771), Valid: (LOSS: 0.0401, MAE: 0.0401, RMSE: 0.0587, R2: 0.9448), PNorm: 188.4765, GNorm: 0.5000
[84/299] timecost: 60.69, lr: 0.000100, Train: (LOSS: 0.0224, MAE: 0.0224, RMSE: 0.0352, R2: 0.9786), Valid: (LOSS: 0.0415, MAE: 0.0415, RMSE: 0.0601, R2: 0.9432), PNorm: 188.4872, GNorm: 0.4810
[85/299] timecost: 59.29, lr: 0.000100, Train: (LOSS: 0.0239, MAE: 0.0239, RMSE: 0.0371, R2: 0.9778), Valid: (LOSS: 0.0420, MAE: 0.0420, RMSE: 0.0622, R2: 0.9383), PNorm: 188.4973, GNorm: 0.4912
[86/299] timecost: 59.16, lr: 0.000100, Train: (LOSS: 0.0223, MAE: 0.0223, RMSE: 0.0347, R2: 0.9807), Valid: (LOSS: 0.0403, MAE: 0.0403, RMSE: 0.0598, R2: 0.9439), PNorm: 188.5084, GNorm: 0.4365
[87/299] timecost: 59.61, lr: 0.000100, Train: (LOSS: 0.0226, MAE: 0.0226, RMSE: 0.0358, R2: 0.9787), Valid: (LOSS: 0.0406, MAE: 0.0406, RMSE: 0.0591, R2: 0.9452), PNorm: 188.5209, GNorm: 0.5000
[88/299] timecost: 59.19, lr: 0.000100, Train: (LOSS: 0.0221, MAE: 0.0221, RMSE: 0.0349, R2: 0.9790), Valid: (LOSS: 0.0405, MAE: 0.0405, RMSE: 0.0590, R2: 0.9437), PNorm: 188.5331, GNorm: 0.5000
[89/299] timecost: 59.42, lr: 0.000100, Train: (LOSS: 0.0226, MAE: 0.0226, RMSE: 0.0353, R2: 0.9797), Valid: (LOSS: 0.0408, MAE: 0.0408, RMSE: 0.0604, R2: 0.9421), PNorm: 188.5469, GNorm: 0.4927
[90/299] timecost: 58.88, lr: 0.000100, Train: (LOSS: 0.0226, MAE: 0.0226, RMSE: 0.0352, R2: 0.9797), Valid: (LOSS: 0.0392, MAE: 0.0392, RMSE: 0.0581, R2: 0.9469), PNorm: 188.5579, GNorm: 0.3506
[91/299] timecost: 59.61, lr: 0.000100, Train: (LOSS: 0.0217, MAE: 0.0217, RMSE: 0.0340, R2: 0.9804), Valid: (LOSS: 0.0388, MAE: 0.0388, RMSE: 0.0572, R2: 0.9483), PNorm: 188.5700, GNorm: 0.4742
[92/299] timecost: 59.14, lr: 0.000100, Train: (LOSS: 0.0212, MAE: 0.0212, RMSE: 0.0329, R2: 0.9815), Valid: (LOSS: 0.0401, MAE: 0.0401, RMSE: 0.0588, R2: 0.9449), PNorm: 188.5812, GNorm: 0.5000
[93/299] timecost: 59.60, lr: 0.000100, Train: (LOSS: 0.0222, MAE: 0.0222, RMSE: 0.0346, R2: 0.9790), Valid: (LOSS: 0.0395, MAE: 0.0395, RMSE: 0.0585, R2: 0.9454), PNorm: 188.5962, GNorm: 0.3854
[94/299] timecost: 59.03, lr: 0.000100, Train: (LOSS: 0.0214, MAE: 0.0214, RMSE: 0.0332, R2: 0.9806), Valid: (LOSS: 0.0394, MAE: 0.0394, RMSE: 0.0572, R2: 0.9479), PNorm: 188.6073, GNorm: 0.5000
[95/299] timecost: 60.95, lr: 0.000100, Train: (LOSS: 0.0219, MAE: 0.0219, RMSE: 0.0345, R2: 0.9807), Valid: (LOSS: 0.0396, MAE: 0.0396, RMSE: 0.0588, R2: 0.9453), PNorm: 188.6199, GNorm: 0.5000
Epoch 00097: reducing learning rate of group 0 to 8.5000e-05.
[96/299] timecost: 59.15, lr: 0.000085, Train: (LOSS: 0.0211, MAE: 0.0211, RMSE: 0.0327, R2: 0.9814), Valid: (LOSS: 0.0402, MAE: 0.0402, RMSE: 0.0607, R2: 0.9411), PNorm: 188.6335, GNorm: 0.5000
[97/299] timecost: 58.07, lr: 0.000085, Train: (LOSS: 0.0202, MAE: 0.0202, RMSE: 0.0321, R2: 0.9830), Valid: (LOSS: 0.0381, MAE: 0.0381, RMSE: 0.0577, R2: 0.9475), PNorm: 188.6403, GNorm: 0.4673
[98/299] timecost: 58.35, lr: 0.000085, Train: (LOSS: 0.0190, MAE: 0.0190, RMSE: 0.0308, R2: 0.9846), Valid: (LOSS: 0.0409, MAE: 0.0409, RMSE: 0.0615, R2: 0.9388), PNorm: 188.6515, GNorm: 0.5000
[99/299] timecost: 58.04, lr: 0.000085, Train: (LOSS: 0.0192, MAE: 0.0192, RMSE: 0.0302, R2: 0.9844), Valid: (LOSS: 0.0422, MAE: 0.0422, RMSE: 0.0627, R2: 0.9366), PNorm: 188.6607, GNorm: 0.5000
[100/299] timecost: 59.52, lr: 0.000085, Train: (LOSS: 0.0184, MAE: 0.0184, RMSE: 0.0296, R2: 0.9849), Valid: (LOSS: 0.0395, MAE: 0.0395, RMSE: 0.0587, R2: 0.9457), PNorm: 188.6678, GNorm: 0.5000
[101/299] timecost: 59.87, lr: 0.000085, Train: (LOSS: 0.0183, MAE: 0.0183, RMSE: 0.0292, R2: 0.9858), Valid: (LOSS: 0.0396, MAE: 0.0396, RMSE: 0.0592, R2: 0.9443), PNorm: 188.6751, GNorm: 0.5000
[102/299] timecost: 59.36, lr: 0.000085, Train: (LOSS: 0.0179, MAE: 0.0179, RMSE: 0.0289, R2: 0.9846), Valid: (LOSS: 0.0386, MAE: 0.0386, RMSE: 0.0568, R2: 0.9489), PNorm: 188.6840, GNorm: 0.5000
[103/299] timecost: 59.54, lr: 0.000085, Train: (LOSS: 0.0182, MAE: 0.0182, RMSE: 0.0293, R2: 0.9850), Valid: (LOSS: 0.0387, MAE: 0.0387, RMSE: 0.0569, R2: 0.9488), PNorm: 188.6900, GNorm: 0.5000
[104/299] timecost: 59.39, lr: 0.000085, Train: (LOSS: 0.0179, MAE: 0.0179, RMSE: 0.0284, R2: 0.9855), Valid: (LOSS: 0.0403, MAE: 0.0403, RMSE: 0.0604, R2: 0.9416), PNorm: 188.7010, GNorm: 0.4274
[105/299] timecost: 59.10, lr: 0.000085, Train: (LOSS: 0.0185, MAE: 0.0185, RMSE: 0.0291, R2: 0.9850), Valid: (LOSS: 0.0390, MAE: 0.0390, RMSE: 0.0584, R2: 0.9461), PNorm: 188.7107, GNorm: 0.5000
[106/299] timecost: 59.00, lr: 0.000085, Train: (LOSS: 0.0184, MAE: 0.0184, RMSE: 0.0292, R2: 0.9850), Valid: (LOSS: 0.0386, MAE: 0.0386, RMSE: 0.0574, R2: 0.9471), PNorm: 188.7201, GNorm: 0.5000
[107/299] timecost: 59.42, lr: 0.000085, Train: (LOSS: 0.0181, MAE: 0.0181, RMSE: 0.0286, R2: 0.9862), Valid: (LOSS: 0.0400, MAE: 0.0400, RMSE: 0.0604, R2: 0.9420), PNorm: 188.7299, GNorm: 0.4398
[108/299] timecost: 59.19, lr: 0.000085, Train: (LOSS: 0.0176, MAE: 0.0176, RMSE: 0.0283, R2: 0.9863), Valid: (LOSS: 0.0415, MAE: 0.0415, RMSE: 0.0603, R2: 0.9424), PNorm: 188.7383, GNorm: 0.5000
[109/299] timecost: 59.22, lr: 0.000085, Train: (LOSS: 0.0182, MAE: 0.0182, RMSE: 0.0292, R2: 0.9860), Valid: (LOSS: 0.0393, MAE: 0.0393, RMSE: 0.0594, R2: 0.9435), PNorm: 188.7466, GNorm: 0.4770
[110/299] timecost: 58.97, lr: 0.000085, Train: (LOSS: 0.0192, MAE: 0.0192, RMSE: 0.0304, R2: 0.9847), Valid: (LOSS: 0.0389, MAE: 0.0389, RMSE: 0.0587, R2: 0.9454), PNorm: 188.7572, GNorm: 0.5000
[111/299] timecost: 59.05, lr: 0.000085, Train: (LOSS: 0.0176, MAE: 0.0176, RMSE: 0.0278, R2: 0.9857), Valid: (LOSS: 0.0398, MAE: 0.0398, RMSE: 0.0587, R2: 0.9443), PNorm: 188.7685, GNorm: 0.4038
[112/299] timecost: 59.01, lr: 0.000085, Train: (LOSS: 0.0172, MAE: 0.0172, RMSE: 0.0280, R2: 0.9863), Valid: (LOSS: 0.0396, MAE: 0.0396, RMSE: 0.0586, R2: 0.9443), PNorm: 188.7799, GNorm: 0.5000
[113/299] timecost: 59.31, lr: 0.000085, Train: (LOSS: 0.0170, MAE: 0.0170, RMSE: 0.0274, R2: 0.9870), Valid: (LOSS: 0.0368, MAE: 0.0368, RMSE: 0.0549, R2: 0.9523), PNorm: 188.7864, GNorm: 0.5000
[114/299] timecost: 59.20, lr: 0.000085, Train: (LOSS: 0.0175, MAE: 0.0175, RMSE: 0.0273, R2: 0.9870), Valid: (LOSS: 0.0383, MAE: 0.0383, RMSE: 0.0568, R2: 0.9480), PNorm: 188.7968, GNorm: 0.3944
[115/299] timecost: 59.17, lr: 0.000085, Train: (LOSS: 0.0179, MAE: 0.0179, RMSE: 0.0289, R2: 0.9855), Valid: (LOSS: 0.0413, MAE: 0.0413, RMSE: 0.0630, R2: 0.9374), PNorm: 188.8063, GNorm: 0.5000
[116/299] timecost: 59.14, lr: 0.000085, Train: (LOSS: 0.0190, MAE: 0.0190, RMSE: 0.0336, R2: 0.9757), Valid: (LOSS: 0.0447, MAE: 0.0447, RMSE: 0.0704, R2: 0.9160), PNorm: 188.8178, GNorm: 0.5000
[117/299] timecost: 58.97, lr: 0.000085, Train: (LOSS: 0.0182, MAE: 0.0182, RMSE: 0.0286, R2: 0.9863), Valid: (LOSS: 0.0387, MAE: 0.0387, RMSE: 0.0587, R2: 0.9454), PNorm: 188.8313, GNorm: 0.4474
[118/299] timecost: 59.63, lr: 0.000085, Train: (LOSS: 0.0170, MAE: 0.0170, RMSE: 0.0269, R2: 0.9874), Valid: (LOSS: 0.0385, MAE: 0.0385, RMSE: 0.0572, R2: 0.9478), PNorm: 188.8371, GNorm: 0.4667
[119/299] timecost: 58.88, lr: 0.000085, Train: (LOSS: 0.0176, MAE: 0.0176, RMSE: 0.0275, R2: 0.9870), Valid: (LOSS: 0.0404, MAE: 0.0404, RMSE: 0.0601, R2: 0.9422), PNorm: 188.8486, GNorm: 0.4593
[120/299] timecost: 59.07, lr: 0.000085, Train: (LOSS: 0.0170, MAE: 0.0170, RMSE: 0.0278, R2: 0.9862), Valid: (LOSS: 0.0395, MAE: 0.0395, RMSE: 0.0585, R2: 0.9453), PNorm: 188.8569, GNorm: 0.4967
[121/299] timecost: 59.03, lr: 0.000085, Train: (LOSS: 0.0166, MAE: 0.0166, RMSE: 0.0269, R2: 0.9878), Valid: (LOSS: 0.0398, MAE: 0.0398, RMSE: 0.0591, R2: 0.9438), PNorm: 188.8657, GNorm: 0.5000
[122/299] timecost: 62.06, lr: 0.000085, Train: (LOSS: 0.0166, MAE: 0.0166, RMSE: 0.0265, R2: 0.9873), Valid: (LOSS: 0.0392, MAE: 0.0392, RMSE: 0.0589, R2: 0.9450), PNorm: 188.8752, GNorm: 0.4147
[123/299] timecost: 62.38, lr: 0.000085, Train: (LOSS: 0.0161, MAE: 0.0161, RMSE: 0.0261, R2: 0.9880), Valid: (LOSS: 0.0385, MAE: 0.0385, RMSE: 0.0568, R2: 0.9496), PNorm: 188.8838, GNorm: 0.4464
[124/299] timecost: 61.14, lr: 0.000085, Train: (LOSS: 0.0160, MAE: 0.0160, RMSE: 0.0260, R2: 0.9875), Valid: (LOSS: 0.0385, MAE: 0.0385, RMSE: 0.0579, R2: 0.9463), PNorm: 188.8923, GNorm: 0.3847
[125/299] timecost: 60.27, lr: 0.000085, Train: (LOSS: 0.0156, MAE: 0.0156, RMSE: 0.0254, R2: 0.9885), Valid: (LOSS: 0.0394, MAE: 0.0394, RMSE: 0.0580, R2: 0.9463), PNorm: 188.9017, GNorm: 0.4810
[126/299] timecost: 60.17, lr: 0.000085, Train: (LOSS: 0.0163, MAE: 0.0163, RMSE: 0.0262, R2: 0.9876), Valid: (LOSS: 0.0380, MAE: 0.0380, RMSE: 0.0573, R2: 0.9475), PNorm: 188.9107, GNorm: 0.4214
[127/299] timecost: 59.64, lr: 0.000085, Train: (LOSS: 0.0163, MAE: 0.0163, RMSE: 0.0261, R2: 0.9880), Valid: (LOSS: 0.0406, MAE: 0.0406, RMSE: 0.0622, R2: 0.9385), PNorm: 188.9200, GNorm: 0.5000
[128/299] timecost: 60.03, lr: 0.000085, Train: (LOSS: 0.0157, MAE: 0.0157, RMSE: 0.0249, R2: 0.9894), Valid: (LOSS: 0.0373, MAE: 0.0373, RMSE: 0.0560, R2: 0.9504), PNorm: 188.9292, GNorm: 0.5000
Epoch 00130: reducing learning rate of group 0 to 7.2250e-05.
[129/299] timecost: 60.66, lr: 0.000072, Train: (LOSS: 0.0164, MAE: 0.0164, RMSE: 0.0257, R2: 0.9886), Valid: (LOSS: 0.0388, MAE: 0.0388, RMSE: 0.0584, R2: 0.9460), PNorm: 188.9402, GNorm: 0.5000
[130/299] timecost: 60.40, lr: 0.000072, Train: (LOSS: 0.0148, MAE: 0.0148, RMSE: 0.0241, R2: 0.9895), Valid: (LOSS: 0.0384, MAE: 0.0384, RMSE: 0.0591, R2: 0.9448), PNorm: 188.9479, GNorm: 0.5000
[131/299] timecost: 60.27, lr: 0.000072, Train: (LOSS: 0.0145, MAE: 0.0145, RMSE: 0.0241, R2: 0.9892), Valid: (LOSS: 0.0394, MAE: 0.0394, RMSE: 0.0586, R2: 0.9450), PNorm: 188.9545, GNorm: 0.4477
[132/299] timecost: 60.62, lr: 0.000072, Train: (LOSS: 0.0143, MAE: 0.0143, RMSE: 0.0233, R2: 0.9903), Valid: (LOSS: 0.0413, MAE: 0.0413, RMSE: 0.0618, R2: 0.9384), PNorm: 188.9609, GNorm: 0.4468
[133/299] timecost: 59.83, lr: 0.000072, Train: (LOSS: 0.0143, MAE: 0.0143, RMSE: 0.0231, R2: 0.9893), Valid: (LOSS: 0.0378, MAE: 0.0378, RMSE: 0.0581, R2: 0.9465), PNorm: 188.9677, GNorm: 0.5000
[134/299] timecost: 59.39, lr: 0.000072, Train: (LOSS: 0.0143, MAE: 0.0143, RMSE: 0.0231, R2: 0.9901), Valid: (LOSS: 0.0385, MAE: 0.0385, RMSE: 0.0574, R2: 0.9481), PNorm: 188.9752, GNorm: 0.5000
[135/299] timecost: 59.24, lr: 0.000072, Train: (LOSS: 0.0142, MAE: 0.0142, RMSE: 0.0234, R2: 0.9894), Valid: (LOSS: 0.0382, MAE: 0.0382, RMSE: 0.0580, R2: 0.9469), PNorm: 188.9835, GNorm: 0.4719
[136/299] timecost: 58.41, lr: 0.000072, Train: (LOSS: 0.0141, MAE: 0.0141, RMSE: 0.0233, R2: 0.9907), Valid: (LOSS: 0.0381, MAE: 0.0381, RMSE: 0.0575, R2: 0.9477), PNorm: 188.9898, GNorm: 0.5000
[137/299] timecost: 57.99, lr: 0.000072, Train: (LOSS: 0.0136, MAE: 0.0136, RMSE: 0.0229, R2: 0.9907), Valid: (LOSS: 0.0382, MAE: 0.0382, RMSE: 0.0577, R2: 0.9465), PNorm: 188.9970, GNorm: 0.4864
[138/299] timecost: 59.09, lr: 0.000072, Train: (LOSS: 0.0137, MAE: 0.0137, RMSE: 0.0227, R2: 0.9912), Valid: (LOSS: 0.0396, MAE: 0.0396, RMSE: 0.0593, R2: 0.9443), PNorm: 189.0060, GNorm: 0.5000
[139/299] timecost: 59.12, lr: 0.000072, Train: (LOSS: 0.0137, MAE: 0.0137, RMSE: 0.0224, R2: 0.9911), Valid: (LOSS: 0.0378, MAE: 0.0378, RMSE: 0.0571, R2: 0.9481), PNorm: 189.0122, GNorm: 0.5000
[140/299] timecost: 59.66, lr: 0.000072, Train: (LOSS: 0.0141, MAE: 0.0141, RMSE: 0.0231, R2: 0.9903), Valid: (LOSS: 0.0402, MAE: 0.0402, RMSE: 0.0601, R2: 0.9427), PNorm: 189.0210, GNorm: 0.4156
[141/299] timecost: 59.61, lr: 0.000072, Train: (LOSS: 0.0144, MAE: 0.0144, RMSE: 0.0230, R2: 0.9908), Valid: (LOSS: 0.0389, MAE: 0.0389, RMSE: 0.0588, R2: 0.9443), PNorm: 189.0304, GNorm: 0.4964
[142/299] timecost: 60.05, lr: 0.000072, Train: (LOSS: 0.0135, MAE: 0.0135, RMSE: 0.0216, R2: 0.9921), Valid: (LOSS: 0.0390, MAE: 0.0390, RMSE: 0.0580, R2: 0.9463), PNorm: 189.0382, GNorm: 0.4677
[143/299] timecost: 60.08, lr: 0.000072, Train: (LOSS: 0.0134, MAE: 0.0134, RMSE: 0.0213, R2: 0.9925), Valid: (LOSS: 0.0374, MAE: 0.0374, RMSE: 0.0562, R2: 0.9496), PNorm: 189.0473, GNorm: 0.4633
[144/299] timecost: 59.16, lr: 0.000072, Train: (LOSS: 0.0133, MAE: 0.0133, RMSE: 0.0211, R2: 0.9923), Valid: (LOSS: 0.0389, MAE: 0.0389, RMSE: 0.0577, R2: 0.9471), PNorm: 189.0541, GNorm: 0.5000
Epoch 00146: reducing learning rate of group 0 to 6.1413e-05.
[145/299] timecost: 58.84, lr: 0.000061, Train: (LOSS: 0.0135, MAE: 0.0135, RMSE: 0.0214, R2: 0.9920), Valid: (LOSS: 0.0401, MAE: 0.0401, RMSE: 0.0605, R2: 0.9424), PNorm: 189.0620, GNorm: 0.4240
[146/299] timecost: 62.21, lr: 0.000061, Train: (LOSS: 0.0125, MAE: 0.0125, RMSE: 0.0198, R2: 0.9933), Valid: (LOSS: 0.0380, MAE: 0.0380, RMSE: 0.0569, R2: 0.9487), PNorm: 189.0694, GNorm: 0.5000
[147/299] timecost: 61.16, lr: 0.000061, Train: (LOSS: 0.0122, MAE: 0.0122, RMSE: 0.0188, R2: 0.9945), Valid: (LOSS: 0.0390, MAE: 0.0390, RMSE: 0.0601, R2: 0.9425), PNorm: 189.0756, GNorm: 0.3427
[148/299] timecost: 58.94, lr: 0.000061, Train: (LOSS: 0.0120, MAE: 0.0120, RMSE: 0.0186, R2: 0.9941), Valid: (LOSS: 0.0376, MAE: 0.0376, RMSE: 0.0564, R2: 0.9497), PNorm: 189.0819, GNorm: 0.3588
[149/299] timecost: 60.04, lr: 0.000061, Train: (LOSS: 0.0118, MAE: 0.0118, RMSE: 0.0182, R2: 0.9947), Valid: (LOSS: 0.0376, MAE: 0.0376, RMSE: 0.0554, R2: 0.9515), PNorm: 189.0883, GNorm: 0.5000
[150/299] timecost: 59.23, lr: 0.000061, Train: (LOSS: 0.0119, MAE: 0.0119, RMSE: 0.0187, R2: 0.9944), Valid: (LOSS: 0.0373, MAE: 0.0373, RMSE: 0.0559, R2: 0.9507), PNorm: 189.0953, GNorm: 0.5000
[151/299] timecost: 58.69, lr: 0.000061, Train: (LOSS: 0.0119, MAE: 0.0119, RMSE: 0.0180, R2: 0.9948), Valid: (LOSS: 0.0365, MAE: 0.0365, RMSE: 0.0552, R2: 0.9521), PNorm: 189.1039, GNorm: 0.5000
[152/299] timecost: 58.11, lr: 0.000061, Train: (LOSS: 0.0113, MAE: 0.0113, RMSE: 0.0173, R2: 0.9953), Valid: (LOSS: 0.0371, MAE: 0.0371, RMSE: 0.0560, R2: 0.9504), PNorm: 189.1095, GNorm: 0.5000
[153/299] timecost: 58.75, lr: 0.000061, Train: (LOSS: 0.0110, MAE: 0.0110, RMSE: 0.0167, R2: 0.9955), Valid: (LOSS: 0.0368, MAE: 0.0368, RMSE: 0.0560, R2: 0.9503), PNorm: 189.1145, GNorm: 0.4525
[154/299] timecost: 62.17, lr: 0.000061, Train: (LOSS: 0.0115, MAE: 0.0115, RMSE: 0.0174, R2: 0.9954), Valid: (LOSS: 0.0375, MAE: 0.0375, RMSE: 0.0565, R2: 0.9495), PNorm: 189.1205, GNorm: 0.4198
[155/299] timecost: 62.32, lr: 0.000061, Train: (LOSS: 0.0108, MAE: 0.0108, RMSE: 0.0161, R2: 0.9959), Valid: (LOSS: 0.0369, MAE: 0.0369, RMSE: 0.0560, R2: 0.9502), PNorm: 189.1270, GNorm: 0.3915
[156/299] timecost: 62.27, lr: 0.000061, Train: (LOSS: 0.0105, MAE: 0.0105, RMSE: 0.0158, R2: 0.9959), Valid: (LOSS: 0.0362, MAE: 0.0362, RMSE: 0.0544, R2: 0.9534), PNorm: 189.1352, GNorm: 0.5000
[157/299] timecost: 59.42, lr: 0.000061, Train: (LOSS: 0.0104, MAE: 0.0104, RMSE: 0.0157, R2: 0.9962), Valid: (LOSS: 0.0361, MAE: 0.0361, RMSE: 0.0537, R2: 0.9545), PNorm: 189.1409, GNorm: 0.4126
[158/299] timecost: 59.11, lr: 0.000061, Train: (LOSS: 0.0107, MAE: 0.0107, RMSE: 0.0156, R2: 0.9961), Valid: (LOSS: 0.0369, MAE: 0.0369, RMSE: 0.0548, R2: 0.9524), PNorm: 189.1459, GNorm: 0.5000
[159/299] timecost: 60.13, lr: 0.000061, Train: (LOSS: 0.0103, MAE: 0.0103, RMSE: 0.0152, R2: 0.9963), Valid: (LOSS: 0.0371, MAE: 0.0371, RMSE: 0.0551, R2: 0.9520), PNorm: 189.1524, GNorm: 0.5000
[160/299] timecost: 62.26, lr: 0.000061, Train: (LOSS: 0.0101, MAE: 0.0101, RMSE: 0.0152, R2: 0.9964), Valid: (LOSS: 0.0365, MAE: 0.0365, RMSE: 0.0547, R2: 0.9528), PNorm: 189.1578, GNorm: 0.5000
[161/299] timecost: 62.30, lr: 0.000061, Train: (LOSS: 0.0103, MAE: 0.0103, RMSE: 0.0156, R2: 0.9961), Valid: (LOSS: 0.0366, MAE: 0.0366, RMSE: 0.0555, R2: 0.9508), PNorm: 189.1628, GNorm: 0.5000
[162/299] timecost: 62.29, lr: 0.000061, Train: (LOSS: 0.0103, MAE: 0.0103, RMSE: 0.0153, R2: 0.9963), Valid: (LOSS: 0.0358, MAE: 0.0358, RMSE: 0.0530, R2: 0.9560), PNorm: 189.1693, GNorm: 0.4917
[163/299] timecost: 62.29, lr: 0.000061, Train: (LOSS: 0.0105, MAE: 0.0105, RMSE: 0.0152, R2: 0.9964), Valid: (LOSS: 0.0354, MAE: 0.0354, RMSE: 0.0537, R2: 0.9544), PNorm: 189.1746, GNorm: 0.4159
[164/299] timecost: 62.17, lr: 0.000061, Train: (LOSS: 0.0099, MAE: 0.0099, RMSE: 0.0146, R2: 0.9967), Valid: (LOSS: 0.0357, MAE: 0.0357, RMSE: 0.0547, R2: 0.9525), PNorm: 189.1808, GNorm: 0.5000
[165/299] timecost: 62.11, lr: 0.000061, Train: (LOSS: 0.0101, MAE: 0.0101, RMSE: 0.0150, R2: 0.9966), Valid: (LOSS: 0.0353, MAE: 0.0353, RMSE: 0.0538, R2: 0.9545), PNorm: 189.1862, GNorm: 0.4210
[166/299] timecost: 59.72, lr: 0.000061, Train: (LOSS: 0.0100, MAE: 0.0100, RMSE: 0.0149, R2: 0.9963), Valid: (LOSS: 0.0347, MAE: 0.0347, RMSE: 0.0521, R2: 0.9570), PNorm: 189.1928, GNorm: 0.5000
[167/299] timecost: 58.17, lr: 0.000061, Train: (LOSS: 0.0100, MAE: 0.0100, RMSE: 0.0148, R2: 0.9966), Valid: (LOSS: 0.0375, MAE: 0.0375, RMSE: 0.0555, R2: 0.9513), PNorm: 189.1990, GNorm: 0.4772
[168/299] timecost: 60.45, lr: 0.000061, Train: (LOSS: 0.0105, MAE: 0.0105, RMSE: 0.0152, R2: 0.9963), Valid: (LOSS: 0.0367, MAE: 0.0367, RMSE: 0.0551, R2: 0.9526), PNorm: 189.2027, GNorm: 0.4437
[169/299] timecost: 60.97, lr: 0.000061, Train: (LOSS: 0.0098, MAE: 0.0098, RMSE: 0.0145, R2: 0.9967), Valid: (LOSS: 0.0353, MAE: 0.0353, RMSE: 0.0532, R2: 0.9554), PNorm: 189.2086, GNorm: 0.4333
[170/299] timecost: 60.21, lr: 0.000061, Train: (LOSS: 0.0099, MAE: 0.0099, RMSE: 0.0145, R2: 0.9967), Valid: (LOSS: 0.0364, MAE: 0.0364, RMSE: 0.0548, R2: 0.9527), PNorm: 189.2136, GNorm: 0.5000
[171/299] timecost: 59.44, lr: 0.000061, Train: (LOSS: 0.0099, MAE: 0.0099, RMSE: 0.0142, R2: 0.9969), Valid: (LOSS: 0.0352, MAE: 0.0352, RMSE: 0.0535, R2: 0.9549), PNorm: 189.2203, GNorm: 0.4472
[172/299] timecost: 59.32, lr: 0.000061, Train: (LOSS: 0.0096, MAE: 0.0096, RMSE: 0.0140, R2: 0.9970), Valid: (LOSS: 0.0367, MAE: 0.0367, RMSE: 0.0555, R2: 0.9516), PNorm: 189.2255, GNorm: 0.4853
[173/299] timecost: 59.43, lr: 0.000061, Train: (LOSS: 0.0097, MAE: 0.0097, RMSE: 0.0142, R2: 0.9969), Valid: (LOSS: 0.0359, MAE: 0.0359, RMSE: 0.0536, R2: 0.9550), PNorm: 189.2315, GNorm: 0.3499
[174/299] timecost: 59.02, lr: 0.000061, Train: (LOSS: 0.0100, MAE: 0.0100, RMSE: 0.0146, R2: 0.9967), Valid: (LOSS: 0.0357, MAE: 0.0357, RMSE: 0.0541, R2: 0.9537), PNorm: 189.2375, GNorm: 0.4847
[175/299] timecost: 59.27, lr: 0.000061, Train: (LOSS: 0.0104, MAE: 0.0104, RMSE: 0.0155, R2: 0.9962), Valid: (LOSS: 0.0368, MAE: 0.0368, RMSE: 0.0551, R2: 0.9517), PNorm: 189.2439, GNorm: 0.5000
[176/299] timecost: 59.24, lr: 0.000061, Train: (LOSS: 0.0096, MAE: 0.0096, RMSE: 0.0140, R2: 0.9970), Valid: (LOSS: 0.0352, MAE: 0.0352, RMSE: 0.0537, R2: 0.9545), PNorm: 189.2495, GNorm: 0.4955
[177/299] timecost: 59.33, lr: 0.000061, Train: (LOSS: 0.0094, MAE: 0.0094, RMSE: 0.0137, R2: 0.9971), Valid: (LOSS: 0.0360, MAE: 0.0360, RMSE: 0.0550, R2: 0.9524), PNorm: 189.2547, GNorm: 0.4455
[178/299] timecost: 59.62, lr: 0.000061, Train: (LOSS: 0.0094, MAE: 0.0094, RMSE: 0.0138, R2: 0.9971), Valid: (LOSS: 0.0360, MAE: 0.0360, RMSE: 0.0538, R2: 0.9544), PNorm: 189.2610, GNorm: 0.5000
[179/299] timecost: 59.33, lr: 0.000061, Train: (LOSS: 0.0095, MAE: 0.0095, RMSE: 0.0137, R2: 0.9971), Valid: (LOSS: 0.0367, MAE: 0.0367, RMSE: 0.0553, R2: 0.9519), PNorm: 189.2663, GNorm: 0.4947
[180/299] timecost: 59.39, lr: 0.000061, Train: (LOSS: 0.0093, MAE: 0.0093, RMSE: 0.0132, R2: 0.9972), Valid: (LOSS: 0.0370, MAE: 0.0370, RMSE: 0.0544, R2: 0.9535), PNorm: 189.2722, GNorm: 0.3890
[181/299] timecost: 60.08, lr: 0.000061, Train: (LOSS: 0.0093, MAE: 0.0093, RMSE: 0.0133, R2: 0.9974), Valid: (LOSS: 0.0358, MAE: 0.0358, RMSE: 0.0539, R2: 0.9538), PNorm: 189.2777, GNorm: 0.5000
Epoch 00183: reducing learning rate of group 0 to 5.2201e-05.
[182/299] timecost: 60.42, lr: 0.000052, Train: (LOSS: 0.0092, MAE: 0.0092, RMSE: 0.0132, R2: 0.9973), Valid: (LOSS: 0.0366, MAE: 0.0366, RMSE: 0.0533, R2: 0.9555), PNorm: 189.2820, GNorm: 0.4936
[183/299] timecost: 59.47, lr: 0.000052, Train: (LOSS: 0.0085, MAE: 0.0085, RMSE: 0.0123, R2: 0.9974), Valid: (LOSS: 0.0356, MAE: 0.0356, RMSE: 0.0540, R2: 0.9539), PNorm: 189.2861, GNorm: 0.4348
[184/299] timecost: 58.93, lr: 0.000052, Train: (LOSS: 0.0081, MAE: 0.0081, RMSE: 0.0118, R2: 0.9978), Valid: (LOSS: 0.0348, MAE: 0.0348, RMSE: 0.0524, R2: 0.9569), PNorm: 189.2894, GNorm: 0.4116
[185/299] timecost: 59.38, lr: 0.000052, Train: (LOSS: 0.0080, MAE: 0.0080, RMSE: 0.0116, R2: 0.9979), Valid: (LOSS: 0.0352, MAE: 0.0352, RMSE: 0.0528, R2: 0.9564), PNorm: 189.2926, GNorm: 0.4499
[186/299] timecost: 60.62, lr: 0.000052, Train: (LOSS: 0.0078, MAE: 0.0078, RMSE: 0.0115, R2: 0.9980), Valid: (LOSS: 0.0350, MAE: 0.0350, RMSE: 0.0534, R2: 0.9551), PNorm: 189.2969, GNorm: 0.4791
[187/299] timecost: 60.51, lr: 0.000052, Train: (LOSS: 0.0087, MAE: 0.0087, RMSE: 0.0123, R2: 0.9977), Valid: (LOSS: 0.0345, MAE: 0.0345, RMSE: 0.0517, R2: 0.9581), PNorm: 189.3007, GNorm: 0.5000
[188/299] timecost: 60.31, lr: 0.000052, Train: (LOSS: 0.0084, MAE: 0.0084, RMSE: 0.0124, R2: 0.9977), Valid: (LOSS: 0.0346, MAE: 0.0346, RMSE: 0.0528, R2: 0.9565), PNorm: 189.3051, GNorm: 0.5000
[189/299] timecost: 59.01, lr: 0.000052, Train: (LOSS: 0.0081, MAE: 0.0081, RMSE: 0.0118, R2: 0.9979), Valid: (LOSS: 0.0352, MAE: 0.0352, RMSE: 0.0531, R2: 0.9558), PNorm: 189.3093, GNorm: 0.4414
[190/299] timecost: 57.90, lr: 0.000052, Train: (LOSS: 0.0080, MAE: 0.0080, RMSE: 0.0116, R2: 0.9979), Valid: (LOSS: 0.0353, MAE: 0.0353, RMSE: 0.0536, R2: 0.9550), PNorm: 189.3127, GNorm: 0.4565
[191/299] timecost: 59.12, lr: 0.000052, Train: (LOSS: 0.0081, MAE: 0.0081, RMSE: 0.0119, R2: 0.9978), Valid: (LOSS: 0.0362, MAE: 0.0362, RMSE: 0.0548, R2: 0.9528), PNorm: 189.3189, GNorm: 0.5000
[192/299] timecost: 59.09, lr: 0.000052, Train: (LOSS: 0.0084, MAE: 0.0084, RMSE: 0.0127, R2: 0.9972), Valid: (LOSS: 0.0350, MAE: 0.0350, RMSE: 0.0528, R2: 0.9564), PNorm: 189.3228, GNorm: 0.5000
[193/299] timecost: 59.40, lr: 0.000052, Train: (LOSS: 0.0076, MAE: 0.0076, RMSE: 0.0112, R2: 0.9980), Valid: (LOSS: 0.0350, MAE: 0.0350, RMSE: 0.0534, R2: 0.9548), PNorm: 189.3257, GNorm: 0.4916
[194/299] timecost: 58.13, lr: 0.000052, Train: (LOSS: 0.0079, MAE: 0.0079, RMSE: 0.0113, R2: 0.9980), Valid: (LOSS: 0.0351, MAE: 0.0351, RMSE: 0.0535, R2: 0.9552), PNorm: 189.3310, GNorm: 0.5000
[195/299] timecost: 58.10, lr: 0.000052, Train: (LOSS: 0.0079, MAE: 0.0079, RMSE: 0.0114, R2: 0.9978), Valid: (LOSS: 0.0344, MAE: 0.0344, RMSE: 0.0526, R2: 0.9563), PNorm: 189.3354, GNorm: 0.4091
[196/299] timecost: 58.31, lr: 0.000052, Train: (LOSS: 0.0074, MAE: 0.0074, RMSE: 0.0109, R2: 0.9981), Valid: (LOSS: 0.0352, MAE: 0.0352, RMSE: 0.0532, R2: 0.9555), PNorm: 189.3378, GNorm: 0.3549
[197/299] timecost: 58.44, lr: 0.000052, Train: (LOSS: 0.0079, MAE: 0.0079, RMSE: 0.0113, R2: 0.9980), Valid: (LOSS: 0.0354, MAE: 0.0354, RMSE: 0.0536, R2: 0.9546), PNorm: 189.3422, GNorm: 0.5000
Epoch 00199: reducing learning rate of group 0 to 4.4371e-05.
[198/299] timecost: 58.51, lr: 0.000044, Train: (LOSS: 0.0077, MAE: 0.0077, RMSE: 0.0113, R2: 0.9981), Valid: (LOSS: 0.0351, MAE: 0.0351, RMSE: 0.0523, R2: 0.9570), PNorm: 189.3463, GNorm: 0.5000
[199/299] timecost: 59.54, lr: 0.000044, Train: (LOSS: 0.0072, MAE: 0.0072, RMSE: 0.0105, R2: 0.9982), Valid: (LOSS: 0.0347, MAE: 0.0347, RMSE: 0.0526, R2: 0.9567), PNorm: 189.3503, GNorm: 0.5000
[200/299] timecost: 58.90, lr: 0.000044, Train: (LOSS: 0.0070, MAE: 0.0070, RMSE: 0.0102, R2: 0.9984), Valid: (LOSS: 0.0345, MAE: 0.0345, RMSE: 0.0526, R2: 0.9564), PNorm: 189.3530, GNorm: 0.4059
[201/299] timecost: 59.59, lr: 0.000044, Train: (LOSS: 0.0067, MAE: 0.0067, RMSE: 0.0099, R2: 0.9984), Valid: (LOSS: 0.0351, MAE: 0.0351, RMSE: 0.0532, R2: 0.9553), PNorm: 189.3551, GNorm: 0.5000
[202/299] timecost: 59.25, lr: 0.000044, Train: (LOSS: 0.0069, MAE: 0.0069, RMSE: 0.0101, R2: 0.9984), Valid: (LOSS: 0.0348, MAE: 0.0348, RMSE: 0.0524, R2: 0.9570), PNorm: 189.3586, GNorm: 0.5000
[203/299] timecost: 58.87, lr: 0.000044, Train: (LOSS: 0.0072, MAE: 0.0072, RMSE: 0.0105, R2: 0.9982), Valid: (LOSS: 0.0354, MAE: 0.0354, RMSE: 0.0532, R2: 0.9558), PNorm: 189.3612, GNorm: 0.4546
[204/299] timecost: 59.28, lr: 0.000044, Train: (LOSS: 0.0069, MAE: 0.0069, RMSE: 0.0102, R2: 0.9983), Valid: (LOSS: 0.0357, MAE: 0.0357, RMSE: 0.0533, R2: 0.9554), PNorm: 189.3642, GNorm: 0.4377
[205/299] timecost: 58.97, lr: 0.000044, Train: (LOSS: 0.0068, MAE: 0.0068, RMSE: 0.0100, R2: 0.9984), Valid: (LOSS: 0.0357, MAE: 0.0357, RMSE: 0.0537, R2: 0.9547), PNorm: 189.3670, GNorm: 0.3819
[206/299] timecost: 58.88, lr: 0.000044, Train: (LOSS: 0.0070, MAE: 0.0070, RMSE: 0.0102, R2: 0.9984), Valid: (LOSS: 0.0351, MAE: 0.0351, RMSE: 0.0530, R2: 0.9557), PNorm: 189.3704, GNorm: 0.4033
[207/299] timecost: 59.17, lr: 0.000044, Train: (LOSS: 0.0067, MAE: 0.0067, RMSE: 0.0099, R2: 0.9983), Valid: (LOSS: 0.0358, MAE: 0.0358, RMSE: 0.0540, R2: 0.9541), PNorm: 189.3743, GNorm: 0.5000
[208/299] timecost: 59.67, lr: 0.000044, Train: (LOSS: 0.0067, MAE: 0.0067, RMSE: 0.0098, R2: 0.9985), Valid: (LOSS: 0.0352, MAE: 0.0352, RMSE: 0.0532, R2: 0.9558), PNorm: 189.3759, GNorm: 0.3999
[209/299] timecost: 58.86, lr: 0.000044, Train: (LOSS: 0.0069, MAE: 0.0069, RMSE: 0.0102, R2: 0.9983), Valid: (LOSS: 0.0361, MAE: 0.0361, RMSE: 0.0547, R2: 0.9529), PNorm: 189.3801, GNorm: 0.3544
[210/299] timecost: 58.91, lr: 0.000044, Train: (LOSS: 0.0069, MAE: 0.0069, RMSE: 0.0100, R2: 0.9984), Valid: (LOSS: 0.0349, MAE: 0.0349, RMSE: 0.0533, R2: 0.9551), PNorm: 189.3831, GNorm: 0.4386
[211/299] timecost: 59.16, lr: 0.000044, Train: (LOSS: 0.0070, MAE: 0.0070, RMSE: 0.0102, R2: 0.9984), Valid: (LOSS: 0.0350, MAE: 0.0350, RMSE: 0.0531, R2: 0.9557), PNorm: 189.3861, GNorm: 0.4286
[212/299] timecost: 58.87, lr: 0.000044, Train: (LOSS: 0.0064, MAE: 0.0064, RMSE: 0.0094, R2: 0.9986), Valid: (LOSS: 0.0348, MAE: 0.0348, RMSE: 0.0528, R2: 0.9564), PNorm: 189.3890, GNorm: 0.5000
[213/299] timecost: 59.80, lr: 0.000044, Train: (LOSS: 0.0063, MAE: 0.0063, RMSE: 0.0093, R2: 0.9986), Valid: (LOSS: 0.0348, MAE: 0.0348, RMSE: 0.0530, R2: 0.9557), PNorm: 189.3924, GNorm: 0.5000
Epoch 00215: reducing learning rate of group 0 to 3.7715e-05.
[214/299] timecost: 59.35, lr: 0.000038, Train: (LOSS: 0.0064, MAE: 0.0064, RMSE: 0.0095, R2: 0.9985), Valid: (LOSS: 0.0346, MAE: 0.0346, RMSE: 0.0523, R2: 0.9572), PNorm: 189.3954, GNorm: 0.5000
[215/299] timecost: 59.31, lr: 0.000038, Train: (LOSS: 0.0060, MAE: 0.0060, RMSE: 0.0088, R2: 0.9987), Valid: (LOSS: 0.0348, MAE: 0.0348, RMSE: 0.0526, R2: 0.9566), PNorm: 189.3965, GNorm: 0.4254
[216/299] timecost: 59.16, lr: 0.000038, Train: (LOSS: 0.0057, MAE: 0.0057, RMSE: 0.0086, R2: 0.9987), Valid: (LOSS: 0.0353, MAE: 0.0353, RMSE: 0.0534, R2: 0.9555), PNorm: 189.3985, GNorm: 0.4908
[217/299] timecost: 60.73, lr: 0.000038, Train: (LOSS: 0.0058, MAE: 0.0058, RMSE: 0.0086, R2: 0.9988), Valid: (LOSS: 0.0345, MAE: 0.0345, RMSE: 0.0526, R2: 0.9566), PNorm: 189.4012, GNorm: 0.4389
[218/299] timecost: 60.21, lr: 0.000038, Train: (LOSS: 0.0056, MAE: 0.0056, RMSE: 0.0083, R2: 0.9989), Valid: (LOSS: 0.0339, MAE: 0.0339, RMSE: 0.0520, R2: 0.9577), PNorm: 189.4042, GNorm: 0.5000
[219/299] timecost: 59.02, lr: 0.000038, Train: (LOSS: 0.0058, MAE: 0.0058, RMSE: 0.0087, R2: 0.9988), Valid: (LOSS: 0.0348, MAE: 0.0348, RMSE: 0.0523, R2: 0.9573), PNorm: 189.4067, GNorm: 0.4675
[220/299] timecost: 58.39, lr: 0.000038, Train: (LOSS: 0.0056, MAE: 0.0056, RMSE: 0.0083, R2: 0.9989), Valid: (LOSS: 0.0349, MAE: 0.0349, RMSE: 0.0520, R2: 0.9578), PNorm: 189.4086, GNorm: 0.5000
[221/299] timecost: 60.07, lr: 0.000038, Train: (LOSS: 0.0057, MAE: 0.0057, RMSE: 0.0086, R2: 0.9988), Valid: (LOSS: 0.0349, MAE: 0.0349, RMSE: 0.0529, R2: 0.9558), PNorm: 189.4102, GNorm: 0.4245
[222/299] timecost: 60.41, lr: 0.000038, Train: (LOSS: 0.0059, MAE: 0.0059, RMSE: 0.0087, R2: 0.9988), Valid: (LOSS: 0.0349, MAE: 0.0349, RMSE: 0.0523, R2: 0.9568), PNorm: 189.4131, GNorm: 0.4189
[223/299] timecost: 59.43, lr: 0.000038, Train: (LOSS: 0.0056, MAE: 0.0056, RMSE: 0.0082, R2: 0.9989), Valid: (LOSS: 0.0338, MAE: 0.0338, RMSE: 0.0514, R2: 0.9583), PNorm: 189.4153, GNorm: 0.4018
[224/299] timecost: 59.01, lr: 0.000038, Train: (LOSS: 0.0055, MAE: 0.0055, RMSE: 0.0083, R2: 0.9988), Valid: (LOSS: 0.0341, MAE: 0.0341, RMSE: 0.0519, R2: 0.9578), PNorm: 189.4173, GNorm: 0.5000
[225/299] timecost: 58.70, lr: 0.000038, Train: (LOSS: 0.0057, MAE: 0.0057, RMSE: 0.0085, R2: 0.9989), Valid: (LOSS: 0.0349, MAE: 0.0349, RMSE: 0.0530, R2: 0.9560), PNorm: 189.4191, GNorm: 0.4334
[226/299] timecost: 59.06, lr: 0.000038, Train: (LOSS: 0.0054, MAE: 0.0054, RMSE: 0.0080, R2: 0.9989), Valid: (LOSS: 0.0343, MAE: 0.0343, RMSE: 0.0522, R2: 0.9571), PNorm: 189.4213, GNorm: 0.4525
[227/299] timecost: 58.91, lr: 0.000038, Train: (LOSS: 0.0055, MAE: 0.0055, RMSE: 0.0082, R2: 0.9988), Valid: (LOSS: 0.0345, MAE: 0.0345, RMSE: 0.0525, R2: 0.9567), PNorm: 189.4235, GNorm: 0.5000
[228/299] timecost: 59.26, lr: 0.000038, Train: (LOSS: 0.0055, MAE: 0.0055, RMSE: 0.0083, R2: 0.9989), Valid: (LOSS: 0.0352, MAE: 0.0352, RMSE: 0.0526, R2: 0.9564), PNorm: 189.4257, GNorm: 0.4650
[229/299] timecost: 58.51, lr: 0.000038, Train: (LOSS: 0.0055, MAE: 0.0055, RMSE: 0.0081, R2: 0.9989), Valid: (LOSS: 0.0349, MAE: 0.0349, RMSE: 0.0531, R2: 0.9555), PNorm: 189.4283, GNorm: 0.4097
[230/299] timecost: 62.18, lr: 0.000038, Train: (LOSS: 0.0054, MAE: 0.0054, RMSE: 0.0081, R2: 0.9989), Valid: (LOSS: 0.0349, MAE: 0.0349, RMSE: 0.0528, R2: 0.9563), PNorm: 189.4313, GNorm: 0.5000
[231/299] timecost: 62.29, lr: 0.000038, Train: (LOSS: 0.0055, MAE: 0.0055, RMSE: 0.0080, R2: 0.9990), Valid: (LOSS: 0.0351, MAE: 0.0351, RMSE: 0.0529, R2: 0.9560), PNorm: 189.4336, GNorm: 0.4792
[232/299] timecost: 62.20, lr: 0.000038, Train: (LOSS: 0.0055, MAE: 0.0055, RMSE: 0.0080, R2: 0.9989), Valid: (LOSS: 0.0341, MAE: 0.0341, RMSE: 0.0514, R2: 0.9583), PNorm: 189.4350, GNorm: 0.5000
[233/299] timecost: 62.16, lr: 0.000038, Train: (LOSS: 0.0055, MAE: 0.0055, RMSE: 0.0081, R2: 0.9989), Valid: (LOSS: 0.0343, MAE: 0.0343, RMSE: 0.0528, R2: 0.9559), PNorm: 189.4379, GNorm: 0.3689
Epoch 00235: reducing learning rate of group 0 to 3.2058e-05.
[234/299] timecost: 62.38, lr: 0.000032, Train: (LOSS: 0.0055, MAE: 0.0055, RMSE: 0.0082, R2: 0.9989), Valid: (LOSS: 0.0345, MAE: 0.0345, RMSE: 0.0530, R2: 0.9556), PNorm: 189.4397, GNorm: 0.5000
[235/299] timecost: 62.16, lr: 0.000032, Train: (LOSS: 0.0053, MAE: 0.0053, RMSE: 0.0079, R2: 0.9990), Valid: (LOSS: 0.0346, MAE: 0.0346, RMSE: 0.0523, R2: 0.9570), PNorm: 189.4420, GNorm: 0.4627
[236/299] timecost: 62.18, lr: 0.000032, Train: (LOSS: 0.0049, MAE: 0.0049, RMSE: 0.0072, R2: 0.9990), Valid: (LOSS: 0.0345, MAE: 0.0345, RMSE: 0.0519, R2: 0.9575), PNorm: 189.4431, GNorm: 0.4872
[237/299] timecost: 61.91, lr: 0.000032, Train: (LOSS: 0.0050, MAE: 0.0050, RMSE: 0.0074, R2: 0.9991), Valid: (LOSS: 0.0347, MAE: 0.0347, RMSE: 0.0526, R2: 0.9564), PNorm: 189.4446, GNorm: 0.5000
[238/299] timecost: 60.38, lr: 0.000032, Train: (LOSS: 0.0051, MAE: 0.0051, RMSE: 0.0077, R2: 0.9990), Valid: (LOSS: 0.0347, MAE: 0.0347, RMSE: 0.0531, R2: 0.9554), PNorm: 189.4467, GNorm: 0.4377
[239/299] timecost: 58.70, lr: 0.000032, Train: (LOSS: 0.0046, MAE: 0.0046, RMSE: 0.0070, R2: 0.9992), Valid: (LOSS: 0.0348, MAE: 0.0348, RMSE: 0.0526, R2: 0.9564), PNorm: 189.4486, GNorm: 0.5000
[240/299] timecost: 58.31, lr: 0.000032, Train: (LOSS: 0.0048, MAE: 0.0048, RMSE: 0.0070, R2: 0.9992), Valid: (LOSS: 0.0350, MAE: 0.0350, RMSE: 0.0532, R2: 0.9555), PNorm: 189.4497, GNorm: 0.4327
[241/299] timecost: 58.57, lr: 0.000032, Train: (LOSS: 0.0046, MAE: 0.0046, RMSE: 0.0069, R2: 0.9992), Valid: (LOSS: 0.0352, MAE: 0.0352, RMSE: 0.0533, R2: 0.9552), PNorm: 189.4515, GNorm: 0.5000
[242/299] timecost: 58.41, lr: 0.000032, Train: (LOSS: 0.0050, MAE: 0.0050, RMSE: 0.0073, R2: 0.9991), Valid: (LOSS: 0.0352, MAE: 0.0352, RMSE: 0.0531, R2: 0.9554), PNorm: 189.4540, GNorm: 0.4924
[243/299] timecost: 58.67, lr: 0.000032, Train: (LOSS: 0.0049, MAE: 0.0049, RMSE: 0.0074, R2: 0.9991), Valid: (LOSS: 0.0348, MAE: 0.0348, RMSE: 0.0531, R2: 0.9553), PNorm: 189.4551, GNorm: 0.4028
[244/299] timecost: 58.71, lr: 0.000032, Train: (LOSS: 0.0047, MAE: 0.0047, RMSE: 0.0070, R2: 0.9992), Valid: (LOSS: 0.0346, MAE: 0.0346, RMSE: 0.0524, R2: 0.9568), PNorm: 189.4568, GNorm: 0.5000
[245/299] timecost: 58.41, lr: 0.000032, Train: (LOSS: 0.0047, MAE: 0.0047, RMSE: 0.0070, R2: 0.9992), Valid: (LOSS: 0.0344, MAE: 0.0344, RMSE: 0.0520, R2: 0.9574), PNorm: 189.4586, GNorm: 0.3890
[246/299] timecost: 58.19, lr: 0.000032, Train: (LOSS: 0.0048, MAE: 0.0048, RMSE: 0.0072, R2: 0.9992), Valid: (LOSS: 0.0339, MAE: 0.0339, RMSE: 0.0519, R2: 0.9575), PNorm: 189.4607, GNorm: 0.4300
[247/299] timecost: 58.16, lr: 0.000032, Train: (LOSS: 0.0048, MAE: 0.0048, RMSE: 0.0072, R2: 0.9991), Valid: (LOSS: 0.0346, MAE: 0.0346, RMSE: 0.0524, R2: 0.9567), PNorm: 189.4623, GNorm: 0.5000
[248/299] timecost: 58.37, lr: 0.000032, Train: (LOSS: 0.0048, MAE: 0.0048, RMSE: 0.0072, R2: 0.9991), Valid: (LOSS: 0.0346, MAE: 0.0346, RMSE: 0.0528, R2: 0.9560), PNorm: 189.4638, GNorm: 0.5000
[249/299] timecost: 58.47, lr: 0.000032, Train: (LOSS: 0.0047, MAE: 0.0047, RMSE: 0.0070, R2: 0.9992), Valid: (LOSS: 0.0350, MAE: 0.0350, RMSE: 0.0532, R2: 0.9556), PNorm: 189.4653, GNorm: 0.4872
Epoch 00251: reducing learning rate of group 0 to 2.7249e-05.
[250/299] timecost: 58.56, lr: 0.000027, Train: (LOSS: 0.0048, MAE: 0.0048, RMSE: 0.0071, R2: 0.9992), Valid: (LOSS: 0.0345, MAE: 0.0345, RMSE: 0.0524, R2: 0.9566), PNorm: 189.4668, GNorm: 0.5000
[251/299] timecost: 58.41, lr: 0.000027, Train: (LOSS: 0.0044, MAE: 0.0044, RMSE: 0.0068, R2: 0.9992), Valid: (LOSS: 0.0343, MAE: 0.0343, RMSE: 0.0524, R2: 0.9569), PNorm: 189.4681, GNorm: 0.4114
[252/299] timecost: 59.64, lr: 0.000027, Train: (LOSS: 0.0041, MAE: 0.0041, RMSE: 0.0064, R2: 0.9993), Valid: (LOSS: 0.0348, MAE: 0.0348, RMSE: 0.0529, R2: 0.9561), PNorm: 189.4693, GNorm: 0.5000
[253/299] timecost: 59.98, lr: 0.000027, Train: (LOSS: 0.0041, MAE: 0.0041, RMSE: 0.0063, R2: 0.9993), Valid: (LOSS: 0.0346, MAE: 0.0346, RMSE: 0.0524, R2: 0.9568), PNorm: 189.4698, GNorm: 0.5000
[254/299] timecost: 58.58, lr: 0.000027, Train: (LOSS: 0.0041, MAE: 0.0041, RMSE: 0.0063, R2: 0.9993), Valid: (LOSS: 0.0344, MAE: 0.0344, RMSE: 0.0519, R2: 0.9576), PNorm: 189.4716, GNorm: 0.4044
[255/299] timecost: 58.17, lr: 0.000027, Train: (LOSS: 0.0041, MAE: 0.0041, RMSE: 0.0063, R2: 0.9993), Valid: (LOSS: 0.0345, MAE: 0.0345, RMSE: 0.0525, R2: 0.9567), PNorm: 189.4727, GNorm: 0.4983
[256/299] timecost: 58.41, lr: 0.000027, Train: (LOSS: 0.0041, MAE: 0.0041, RMSE: 0.0062, R2: 0.9993), Valid: (LOSS: 0.0345, MAE: 0.0345, RMSE: 0.0523, R2: 0.9569), PNorm: 189.4743, GNorm: 0.4641
[257/299] timecost: 58.31, lr: 0.000027, Train: (LOSS: 0.0040, MAE: 0.0040, RMSE: 0.0062, R2: 0.9993), Valid: (LOSS: 0.0349, MAE: 0.0349, RMSE: 0.0530, R2: 0.9558), PNorm: 189.4757, GNorm: 0.4440
[258/299] timecost: 58.36, lr: 0.000027, Train: (LOSS: 0.0040, MAE: 0.0040, RMSE: 0.0062, R2: 0.9993), Valid: (LOSS: 0.0343, MAE: 0.0343, RMSE: 0.0522, R2: 0.9570), PNorm: 189.4771, GNorm: 0.4043
[259/299] timecost: 58.52, lr: 0.000027, Train: (LOSS: 0.0041, MAE: 0.0041, RMSE: 0.0063, R2: 0.9993), Valid: (LOSS: 0.0343, MAE: 0.0343, RMSE: 0.0519, R2: 0.9576), PNorm: 189.4785, GNorm: 0.5000
[260/299] timecost: 58.06, lr: 0.000027, Train: (LOSS: 0.0041, MAE: 0.0041, RMSE: 0.0062, R2: 0.9993), Valid: (LOSS: 0.0346, MAE: 0.0346, RMSE: 0.0521, R2: 0.9574), PNorm: 189.4794, GNorm: 0.3842
[261/299] timecost: 58.15, lr: 0.000027, Train: (LOSS: 0.0041, MAE: 0.0041, RMSE: 0.0063, R2: 0.9993), Valid: (LOSS: 0.0342, MAE: 0.0342, RMSE: 0.0520, R2: 0.9574), PNorm: 189.4806, GNorm: 0.5000
[262/299] timecost: 58.39, lr: 0.000027, Train: (LOSS: 0.0038, MAE: 0.0038, RMSE: 0.0059, R2: 0.9994), Valid: (LOSS: 0.0344, MAE: 0.0344, RMSE: 0.0521, R2: 0.9573), PNorm: 189.4817, GNorm: 0.4250
[263/299] timecost: 58.20, lr: 0.000027, Train: (LOSS: 0.0040, MAE: 0.0040, RMSE: 0.0062, R2: 0.9994), Valid: (LOSS: 0.0344, MAE: 0.0344, RMSE: 0.0529, R2: 0.9560), PNorm: 189.4832, GNorm: 0.3665
[264/299] timecost: 58.32, lr: 0.000027, Train: (LOSS: 0.0039, MAE: 0.0039, RMSE: 0.0061, R2: 0.9994), Valid: (LOSS: 0.0348, MAE: 0.0348, RMSE: 0.0525, R2: 0.9567), PNorm: 189.4843, GNorm: 0.5000
[265/299] timecost: 58.24, lr: 0.000027, Train: (LOSS: 0.0039, MAE: 0.0039, RMSE: 0.0058, R2: 0.9994), Valid: (LOSS: 0.0347, MAE: 0.0347, RMSE: 0.0530, R2: 0.9558), PNorm: 189.4865, GNorm: 0.4532
Epoch 00267: reducing learning rate of group 0 to 2.3162e-05.
[266/299] timecost: 58.22, lr: 0.000023, Train: (LOSS: 0.0040, MAE: 0.0040, RMSE: 0.0061, R2: 0.9993), Valid: (LOSS: 0.0344, MAE: 0.0344, RMSE: 0.0529, R2: 0.9559), PNorm: 189.4871, GNorm: 0.3955
[267/299] timecost: 58.73, lr: 0.000023, Train: (LOSS: 0.0038, MAE: 0.0038, RMSE: 0.0058, R2: 0.9994), Valid: (LOSS: 0.0345, MAE: 0.0345, RMSE: 0.0524, R2: 0.9566), PNorm: 189.4877, GNorm: 0.5000
[268/299] timecost: 59.39, lr: 0.000023, Train: (LOSS: 0.0036, MAE: 0.0036, RMSE: 0.0056, R2: 0.9994), Valid: (LOSS: 0.0346, MAE: 0.0346, RMSE: 0.0525, R2: 0.9565), PNorm: 189.4885, GNorm: 0.4659
[269/299] timecost: 59.44, lr: 0.000023, Train: (LOSS: 0.0035, MAE: 0.0035, RMSE: 0.0055, R2: 0.9995), Valid: (LOSS: 0.0345, MAE: 0.0345, RMSE: 0.0523, R2: 0.9569), PNorm: 189.4897, GNorm: 0.4601
[270/299] timecost: 59.36, lr: 0.000023, Train: (LOSS: 0.0036, MAE: 0.0036, RMSE: 0.0055, R2: 0.9994), Valid: (LOSS: 0.0344, MAE: 0.0344, RMSE: 0.0524, R2: 0.9567), PNorm: 189.4907, GNorm: 0.5000
[271/299] timecost: 59.27, lr: 0.000023, Train: (LOSS: 0.0038, MAE: 0.0038, RMSE: 0.0058, R2: 0.9994), Valid: (LOSS: 0.0349, MAE: 0.0349, RMSE: 0.0529, R2: 0.9558), PNorm: 189.4919, GNorm: 0.4338
[272/299] timecost: 59.50, lr: 0.000023, Train: (LOSS: 0.0037, MAE: 0.0037, RMSE: 0.0056, R2: 0.9994), Valid: (LOSS: 0.0341, MAE: 0.0341, RMSE: 0.0522, R2: 0.9569), PNorm: 189.4926, GNorm: 0.4482
[273/299] timecost: 59.49, lr: 0.000023, Train: (LOSS: 0.0036, MAE: 0.0036, RMSE: 0.0056, R2: 0.9995), Valid: (LOSS: 0.0348, MAE: 0.0348, RMSE: 0.0528, R2: 0.9560), PNorm: 189.4934, GNorm: 0.5000
[274/299] timecost: 59.79, lr: 0.000023, Train: (LOSS: 0.0035, MAE: 0.0035, RMSE: 0.0056, R2: 0.9994), Valid: (LOSS: 0.0344, MAE: 0.0344, RMSE: 0.0522, R2: 0.9572), PNorm: 189.4946, GNorm: 0.4690
[275/299] timecost: 59.79, lr: 0.000023, Train: (LOSS: 0.0036, MAE: 0.0036, RMSE: 0.0056, R2: 0.9994), Valid: (LOSS: 0.0344, MAE: 0.0344, RMSE: 0.0525, R2: 0.9566), PNorm: 189.4955, GNorm: 0.4408
[276/299] timecost: 60.13, lr: 0.000023, Train: (LOSS: 0.0034, MAE: 0.0034, RMSE: 0.0054, R2: 0.9995), Valid: (LOSS: 0.0342, MAE: 0.0342, RMSE: 0.0521, R2: 0.9572), PNorm: 189.4965, GNorm: 0.4197
[277/299] timecost: 60.03, lr: 0.000023, Train: (LOSS: 0.0034, MAE: 0.0034, RMSE: 0.0054, R2: 0.9995), Valid: (LOSS: 0.0347, MAE: 0.0347, RMSE: 0.0530, R2: 0.9556), PNorm: 189.4974, GNorm: 0.3807
[278/299] timecost: 60.35, lr: 0.000023, Train: (LOSS: 0.0035, MAE: 0.0035, RMSE: 0.0055, R2: 0.9995), Valid: (LOSS: 0.0344, MAE: 0.0344, RMSE: 0.0523, R2: 0.9567), PNorm: 189.4982, GNorm: 0.4836
[279/299] timecost: 60.05, lr: 0.000023, Train: (LOSS: 0.0034, MAE: 0.0034, RMSE: 0.0055, R2: 0.9995), Valid: (LOSS: 0.0347, MAE: 0.0347, RMSE: 0.0523, R2: 0.9568), PNorm: 189.4990, GNorm: 0.4783
[280/299] timecost: 60.46, lr: 0.000023, Train: (LOSS: 0.0035, MAE: 0.0035, RMSE: 0.0054, R2: 0.9994), Valid: (LOSS: 0.0343, MAE: 0.0343, RMSE: 0.0525, R2: 0.9566), PNorm: 189.5003, GNorm: 0.3486
[281/299] timecost: 62.30, lr: 0.000023, Train: (LOSS: 0.0036, MAE: 0.0036, RMSE: 0.0055, R2: 0.9994), Valid: (LOSS: 0.0348, MAE: 0.0348, RMSE: 0.0529, R2: 0.9559), PNorm: 189.5013, GNorm: 0.5000
Epoch 00283: reducing learning rate of group 0 to 1.9687e-05.
[282/299] timecost: 62.22, lr: 0.000020, Train: (LOSS: 0.0035, MAE: 0.0035, RMSE: 0.0054, R2: 0.9995), Valid: (LOSS: 0.0344, MAE: 0.0344, RMSE: 0.0522, R2: 0.9570), PNorm: 189.5024, GNorm: 0.4803
[283/299] timecost: 61.79, lr: 0.000020, Train: (LOSS: 0.0032, MAE: 0.0032, RMSE: 0.0051, R2: 0.9995), Valid: (LOSS: 0.0344, MAE: 0.0344, RMSE: 0.0523, R2: 0.9569), PNorm: 189.5030, GNorm: 0.3877
[284/299] timecost: 59.51, lr: 0.000020, Train: (LOSS: 0.0031, MAE: 0.0031, RMSE: 0.0050, R2: 0.9995), Valid: (LOSS: 0.0340, MAE: 0.0340, RMSE: 0.0519, R2: 0.9575), PNorm: 189.5033, GNorm: 0.4830
[285/299] timecost: 59.19, lr: 0.000020, Train: (LOSS: 0.0030, MAE: 0.0030, RMSE: 0.0048, R2: 0.9995), Valid: (LOSS: 0.0342, MAE: 0.0342, RMSE: 0.0521, R2: 0.9571), PNorm: 189.5038, GNorm: 0.4356
[286/299] timecost: 59.34, lr: 0.000020, Train: (LOSS: 0.0030, MAE: 0.0030, RMSE: 0.0049, R2: 0.9995), Valid: (LOSS: 0.0342, MAE: 0.0342, RMSE: 0.0522, R2: 0.9571), PNorm: 189.5042, GNorm: 0.4399
[287/299] timecost: 59.69, lr: 0.000020, Train: (LOSS: 0.0033, MAE: 0.0033, RMSE: 0.0052, R2: 0.9995), Valid: (LOSS: 0.0341, MAE: 0.0341, RMSE: 0.0521, R2: 0.9573), PNorm: 189.5055, GNorm: 0.5000
[288/299] timecost: 59.31, lr: 0.000020, Train: (LOSS: 0.0032, MAE: 0.0032, RMSE: 0.0051, R2: 0.9995), Valid: (LOSS: 0.0342, MAE: 0.0342, RMSE: 0.0523, R2: 0.9569), PNorm: 189.5060, GNorm: 0.5000
[289/299] timecost: 59.87, lr: 0.000020, Train: (LOSS: 0.0031, MAE: 0.0031, RMSE: 0.0050, R2: 0.9995), Valid: (LOSS: 0.0341, MAE: 0.0341, RMSE: 0.0518, R2: 0.9578), PNorm: 189.5068, GNorm: 0.4866
[290/299] timecost: 59.24, lr: 0.000020, Train: (LOSS: 0.0033, MAE: 0.0033, RMSE: 0.0051, R2: 0.9996), Valid: (LOSS: 0.0344, MAE: 0.0344, RMSE: 0.0521, R2: 0.9572), PNorm: 189.5072, GNorm: 0.3561
[291/299] timecost: 59.45, lr: 0.000020, Train: (LOSS: 0.0033, MAE: 0.0033, RMSE: 0.0052, R2: 0.9995), Valid: (LOSS: 0.0343, MAE: 0.0343, RMSE: 0.0522, R2: 0.9570), PNorm: 189.5086, GNorm: 0.4603
[292/299] timecost: 59.28, lr: 0.000020, Train: (LOSS: 0.0030, MAE: 0.0030, RMSE: 0.0049, R2: 0.9996), Valid: (LOSS: 0.0339, MAE: 0.0339, RMSE: 0.0518, R2: 0.9577), PNorm: 189.5090, GNorm: 0.5000
[293/299] timecost: 59.39, lr: 0.000020, Train: (LOSS: 0.0031, MAE: 0.0031, RMSE: 0.0049, R2: 0.9995), Valid: (LOSS: 0.0343, MAE: 0.0343, RMSE: 0.0522, R2: 0.9571), PNorm: 189.5101, GNorm: 0.4505
[294/299] timecost: 59.12, lr: 0.000020, Train: (LOSS: 0.0030, MAE: 0.0030, RMSE: 0.0049, R2: 0.9995), Valid: (LOSS: 0.0341, MAE: 0.0341, RMSE: 0.0521, R2: 0.9572), PNorm: 189.5101, GNorm: 0.4308
[295/299] timecost: 58.57, lr: 0.000020, Train: (LOSS: 0.0031, MAE: 0.0031, RMSE: 0.0049, R2: 0.9995), Valid: (LOSS: 0.0342, MAE: 0.0342, RMSE: 0.0522, R2: 0.9572), PNorm: 189.5115, GNorm: 0.5000
[296/299] timecost: 58.47, lr: 0.000020, Train: (LOSS: 0.0031, MAE: 0.0031, RMSE: 0.0049, R2: 0.9996), Valid: (LOSS: 0.0339, MAE: 0.0339, RMSE: 0.0520, R2: 0.9574), PNorm: 189.5120, GNorm: 0.4277
[297/299] timecost: 58.69, lr: 0.000020, Train: (LOSS: 0.0031, MAE: 0.0031, RMSE: 0.0049, R2: 0.9995), Valid: (LOSS: 0.0341, MAE: 0.0341, RMSE: 0.0519, R2: 0.9575), PNorm: 189.5129, GNorm: 0.5000
Epoch 00299: reducing learning rate of group 0 to 1.6734e-05.
[298/299] timecost: 58.71, lr: 0.000017, Train: (LOSS: 0.0031, MAE: 0.0031, RMSE: 0.0048, R2: 0.9995), Valid: (LOSS: 0.0341, MAE: 0.0341, RMSE: 0.0518, R2: 0.9575), PNorm: 189.5135, GNorm: 0.4807
[299/299] timecost: 58.65, lr: 0.000017, Train: (LOSS: 0.0029, MAE: 0.0029, RMSE: 0.0047, R2: 0.9996), Valid: (LOSS: 0.0346, MAE: 0.0346, RMSE: 0.0525, R2: 0.9566), PNorm: 189.5140, GNorm: 0.4869
==========Training End==========
==========Test Best Model==========
================Final Results=======================
mse: 0.0331 +- 0.0000:
rmse: 0.0485 +- 0.0000:
mae: 0.0331 +- 0.0000:
r2: 0.9651 +- 0.0000:
tensor([[0.1017, 0.1066],
        [0.1569, 0.1475],
        [0.0000, 0.0000],
        ...,
        [0.0000, 0.0000],
        [0.5600, 0.4354],
        [0.0000, 0.0000]], device='cuda:0')
